{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/_]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":""},{"location":"#_1","title":"Overview","text":"<p>Vulcan is a complete stack for building data products.</p> <p>Vulcan is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can efficiently run and deploy data transformations written in SQL or Python with visibility and control at any size.</p> <p>Architecture Diagram</p> <p>Get instant SQL impact analysis of your changes, whether in the CLI or in Vulcan Plan Mode</p> Virtual Data Environments <ul> <li>See a full diagram of how Virtual Data Environments work</li> <li>Watch this video to learn more</li> </ul> <ul> <li>Create isolated development environments without data warehouse costs</li> <li>Plan / Apply workflow like Terraform to understand potential impact of changes</li> <li>Easy to use CI/CD workflows for true blue-green deployments</li> </ul> Efficiency and Testing <p>Running this command will generate a unit test file in the <code>tests/</code> folder: <code>test_stg_payments.yaml</code></p> <p>Runs a live query to generate the expected output of the model</p> <pre><code>vulcan create_test tcloud_demo.stg_payments --query tcloud_demo.seed_raw_payments \"select * from tcloud_demo.seed_raw_payments limit 5\"\n\n# run the unit test\nvulcan test\n</code></pre> <pre><code>MODEL (\n  name tcloud_demo.stg_payments,\n  cron '@daily',\n  grain payment_id,\n  audits (UNIQUE_VALUES(columns = (\n      payment_id\n  )), NOT_NULL(columns = (\n      payment_id\n  )))\n);\n\nSELECT\n    id AS payment_id,\n    order_id,\n    payment_method,\n    amount / 100 AS amount, /* `amount` is currently stored in cents, so we convert it to dollars */\n    'new_column' AS new_column, /* non-breaking change example  */\nFROM tcloud_demo.seed_raw_payments\n</code></pre> <pre><code>test_stg_payments:\nmodel: tcloud_demo.stg_payments\ninputs:\n    tcloud_demo.seed_raw_payments:\n      - id: 66\n        order_id: 58\n        payment_method: coupon\n        amount: 1800\n      - id: 27\n        order_id: 24\n        payment_method: coupon\n        amount: 2600\n      - id: 30\n        order_id: 25\n        payment_method: coupon\n        amount: 1600\n      - id: 109\n        order_id: 95\n        payment_method: coupon\n        amount: 2400\n      - id: 3\n        order_id: 3\n        payment_method: coupon\n        amount: 100\noutputs:\n    query:\n      - payment_id: 66\n        order_id: 58\n        payment_method: coupon\n        amount: 18.0\n        new_column: new_column\n      - payment_id: 27\n        order_id: 24\n        payment_method: coupon\n        amount: 26.0\n        new_column: new_column\n      - payment_id: 30\n        order_id: 25\n        payment_method: coupon\n        amount: 16.0\n        new_column: new_column\n      - payment_id: 109\n        order_id: 95\n        payment_method: coupon\n        amount: 24.0\n        new_column: new_column\n      - payment_id: 3\n        order_id: 3\n        payment_method: coupon\n        amount: 1.0\n        new_column: new_column\n</code></pre> <ul> <li>Never build a table more than once</li> <li>Track what data's been modified and run only the necessary transformations for incremental models</li> <li>Run unit tests for free and configure automated audits</li> </ul> Level Up Your SQL <p>Write SQL in any dialect and Vulcan will transpile it to your target SQL dialect on the fly before sending it to the warehouse. </p> <ul> <li>Debug transformation errors before you run them in your warehouse in 10+ different SQL dialects</li> <li>Definitions using simply SQL (no need for redundant and confusing <code>Jinja</code> + <code>YAML</code>)</li> <li>See impact of changes before you run them in your warehouse with column-level lineage</li> </ul> <p>For more information, check out the documentation.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Install Vulcan through pypi by running:</p> <pre><code>mkdir vulcan-example\ncd vulcan-example\npython -m venv .venv\nsource .venv/bin/activate\npip install vulcan\nsource .venv/bin/activate # reactivate the venv to ensure you're using the right installation\nvulcan init duckdb # get started right away with a local duckdb instance\nvulcan plan # see the plan for the changes you're making\n</code></pre> <p>Note: You may need to run <code>python3</code> or <code>pip3</code> instead of <code>python</code> or <code>pip</code>, depending on your python installation.</p> <p>Follow the quickstart guide to learn how to use Vulcan. You already have a head start!</p> <p>Follow this example to learn how to use Vulcan in a full walkthrough.</p>"},{"location":"#join-our-community","title":"Join Our Community","text":"<p>Together, we want to build data transformation without the waste. Connect with us in the following ways:</p> <ul> <li>Join our community to ask questions, or just to say hi!</li> <li>File an issue on our GitHub</li> <li>Send us an email with your questions or feedback</li> <li>Read our blog</li> </ul>"},{"location":"#contribution","title":"Contribution","text":"<p>Contributions in the form of issues or pull requests are greatly appreciated. </p> <p>Read more on how to contribute to Vulcan open source.</p>"},{"location":"HOWTO/","title":"Vulcan Docs: Editing Guide","text":""},{"location":"HOWTO/#vulcan-docs-editing-guide","title":"Vulcan Docs: Editing Guide","text":"<p>You have been asked/told to work on Vulcan's docs - congratulations!</p> <p>This document will get you set up to modify or create new Vulcan documentation. It describes:</p> <ul> <li>The workflow for modifying or adding docs</li> <li>How we approach writing style for the docs</li> <li>The tools used to work with docs</li> <li>How to write docs in markdown</li> <li>Configuring the docs site</li> <li>Hosting the docs on readthedocs.io</li> </ul> <p>From a technical perspective, docs modifications are just like modifications to any other code file. Therefore, they are made and approved via pull requests in the Vulcan Github repo.</p>"},{"location":"HOWTO/#workflow","title":"Workflow","text":"<p>When modifying or adding the docs, you will generally follow these steps:</p> <ol> <li>Clone the latest version of the Vulcan Github repo</li> <li>Locate the file to edit in the repo's <code>/docs</code> directory (or create a new file)</li> <li>Ensure the docs tools are set up and working</li> <li>Start a local version of the docs site</li> <li>Make your changes, examining them in the local docs site</li> <li>Create a new git branch</li> <li>Commit your changes to the new branch</li> <li>Push the branch to Github</li> <li>Open a pull request</li> </ol> <p>Depending on the scale/complexity of the changes, reviews may happen in one of two ways:</p> <p>For larger changes or new pages, Trey will do a full review and editing pass. He will make edits directly in the doc file, create a new git branch, and make a PR against your PR branch (NOT against Vulcan main).</p> <p>You will review the changes and provide feedback, Trey will update the doc, and you will approve the PR when you are satisfied.</p> <p>Trey edits first</p> <p>Because Trey will make large changes, his review and editing pass should occur BEFORE other team members spend time reviewing.</p> <p>After Trey's PR has been merged into your branch, you will receive comments/feedback from other team members in the Github PR interface. You will then make the requested changes and push them to the branch, the PR will be approved by Trey or another team member, and it can be merged.</p> <p>If your changes are smaller, Trey will not do a full edit and will provide comments/feedback in the Github PR interface like everyone else.</p>"},{"location":"HOWTO/#new-docs","title":"New docs","text":"<p>Brand new docs pages usually require a significant amount of editing. Therefore, when drafting a new page your main focus is ensuring all the content is present, accurate, and ordered/structured sensibly.</p> <p>If you built the feature being documented, you have the most knowledge about how it works and which parts are important. Your opinion and context are critical.</p> <p>Do not spend too much time wordsmithing and styling. Because so much editing will happen, language you work hard on may be removed or altered. That's demoralizing, even if it's replaced by something you agree is better (and especially if it's replaced with something worse).</p> <p>Your wordsmithing and style are important, but they should be the last step of the writing process. Doing them on the first draft does not provide a good ROI.</p>"},{"location":"HOWTO/#writing-style","title":"Writing style","text":"<p>We do not have a written style guide, but we try to follow a few stylistic conventions.</p> <p>At a high level, think \"simpler is better.\"</p> <p>Data engineering is complex, so Vulcan is complex. We must focus on minimizing cognitive load for readers, while ensuring all the content is present and accurate. This is a difficult balance.</p> <p>The most important specific stylistic conventions are:</p> <ol> <li>Use second person voice when providing instructions<ul> <li>DO: \"Add an audit to the model.\"</li> <li>DO: \"You can partition the table if necessary.\"</li> </ul> </li> <li>Use first person plural when describing actions but not providing instructions (e.g., extended example)<ul> <li>DO: \"First, we create a new Python environment.\"</li> <li>DO: \"After running the plan command, we see the following output.\"</li> </ul> </li> <li>Use active voice<ul> <li>DO: \"Vulcan automatically infers the table schema.\"</li> <li>DO NOT: \"The table schema is inferred automatically by Vulcan.\"</li> </ul> </li> <li>Prefer short sentences to long<ul> <li>Not dogmatic, use your judgment</li> </ul> </li> <li>Liberally use code examples and graphics<ul> <li>Abstract discussion is boring and difficult for people to follow</li> </ul> </li> <li>Liberally use headers to structure content within a page<ul> <li>But don't go overboard</li> </ul> </li> </ol>"},{"location":"HOWTO/#tools","title":"Tools","text":"<p>Vulcan docs are built with the <code>MkDocs</code> library.</p> <p><code>MkDocs</code> is a static site generator that converts the files in our <code>/docs</code> directory into website files. When Vulcan's Github repo has a PR merged to main, a build is triggered that converts and uploads the files to <code>readthedocs.io</code>, which then serves them to end users.</p> <p><code>MkDocs</code> is configured in the <code>mkdocs.yml</code> configuration file, which specifies the site page hierarchy, color theme, and MkDocs plugins used (e.g., Material for MkDocs).</p>"},{"location":"HOWTO/#setup","title":"Setup","text":"<p>You will work on the docs in a local copy of the vulcan git repository.</p> <p>If you don't have a copy of the repo on your machine, open a terminal and clone it into a <code>vulcan</code> directory by executing:</p> <pre><code>git clone https://github.com/TobikoData/vulcan.git\n</code></pre> <p>And navigate to the directory:</p> <pre><code>cd vulcan\n</code></pre> <p><code>MkDocs</code> is a Python library, so we first create and activate a new virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>We will now run three separate installation commands.</p> <p>First, we install the \"pre-commit\" tools that automatically validate files when the <code>git commit</code> command is run:</p> <pre><code>make install-pre-commit\n</code></pre> <p>Next, we install the core Vulcan dev dependencies:</p> <pre><code>make install-dev\n</code></pre> <p>And, finally, we install <code>MkDocs</code> and other docs dependencies:</p> <pre><code>make install-doc\n</code></pre> <p>The docs requirements file pins library versions, which can sometimes cause unresolvable conflicts. If you receive a \"cannot find compatible versions\" error for the final command, run this instead:</p> <pre><code>pip install mkdocs mkdocs-include-markdown-plugin mkdocs-material mkdocs-material-extensions mkdocs-glightbox pdoc\n</code></pre>"},{"location":"HOWTO/#usage","title":"Usage","text":"<p>It is helpful run a local version of the docs site while editing or adding docs. That way you can preview how your changes will look on the Vulcan hosted docs.</p> <p>Navigate to the <code>vulcan</code> directory we created before and run <code>mkdocs serve</code>:</p> <pre><code>&gt; mkdocs serve\n\nINFO     -  Building documentation...\nINFO     -  Cleaning site directory\nINFO     -  Documentation built in 3.63 seconds\nINFO     -  [16:02:59] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO     -  [16:02:59] Serving on http://127.0.0.1:8000/\n</code></pre> <p>View the docs site by navigating to <code>http://127.0.0.1:8000/</code> in a web browser. To view the HOTWO doc, navigate to <code>http://127.0.0.1:8000/HOWTO/</code>.</p> <p>The command will block the terminal in which it is run, so you must open a new terminal to do anything on the command line.</p> <p>The docs site will update in real time as you edit and save changes to the underlying files.</p>"},{"location":"HOWTO/#docs-markdown","title":"Docs markdown","text":"<p>We use <code>MkDocs</code> so we can control almost all of the site's appearance and behavior with markdown. That makes it simple to maintain the docs as text files in the Vulcan Github repo.</p> <p>This section discusses the different ways we use markdown to control the appearance and behavior of the docs site.</p>"},{"location":"HOWTO/#document-structure","title":"Document structure","text":"<p>A docs page's structure (headers and within-page navigation) is defined by the use of markdown headers.</p> <p>A markdown header is a line that begins with between one and four hash marks <code>#</code>. The number of hash marks determines the \"level\" of the header, with one hash mark being the highest level.</p> <p>Every docs page must begin with a top-level header (one hash mark). This header is used as the page's title in the navigation bar.</p> <p>Important</p> <p>The page may only have one top-level header that begins with a single hash mark!</p> <p>Subsequent headers are used to divide the page into sections, with each level down nested within its parent (e.g., three-level <code>###</code> headers are nested within two-level <code>##</code> headers).</p> <p>A within-page table of contents bar is automatically generated from the headers of the page and displayed on the right side.</p> <p>For example, the Configuration guide's navigation bar uses multiple header levels to group content:</p> <p></p>"},{"location":"HOWTO/#lists","title":"Lists","text":"<p>We do not want pages that are a \"wall of text,\" which is difficult to read and understand. Instead, use lists to break up a page and more effectively communicate its content.</p> <p>For example, if we are describing a process with multiple steps, it is clearer to use a numbered list of those steps than a separate sentence for each step.</p> <p>Similarly, any time a sentence contains a long list of items, you should consider using a bulleted list instead.</p> <p>Lists are useful for breaking up a page, but that visual distinction draws people's attention. Be careful not to use so many lists that you tip the balance from \"too much text\" to \"too little text.\"</p> <p>To specify a list, put each element on its own line. Start the line with:</p> <ul> <li>A dash <code>-</code> or asterisk <code>*</code> for a bullet list</li> <li>A number and period <code>1.</code> for a numbered list</li> <li>A letter and period <code>a.</code> for a lettered list</li> </ul> <p>Empty line before list!</p> <p>You must put an empty line before the first list element, or it will not render.</p> <p>We can specify a simple bullet list like this:</p> <pre><code>Here's a bullet list!\n\n- First item\n- Second item\n</code></pre> <p>And it renders to this:</p> <p>Here's a bullet list!</p> <ul> <li>First item</li> <li>Second item</li> </ul> <p> Or a numbered list:</p> <pre><code>1. First item\n2. Second item\n</code></pre> <ol> <li>First item</li> <li>Second item</li> </ol> <p></p> <p>You can nest list items by adding 4 spaces of indentation:</p> <pre><code>- First item\n    - First subitem\n    - Second subitem\n- Second item\n</code></pre> <ul> <li>First item<ul> <li>First subitem</li> <li>Second subitem</li> </ul> </li> <li>Second item</li> </ul>"},{"location":"HOWTO/#inline-code","title":"Inline code","text":"<p>Sometimes we need to display a simple code snippet inline with regular text.</p> <p>For example, we might be describing the <code>vulcan plan</code> command and want to differentiate the words \"vulcan plan\" from the other words.</p> <p>Do this by wrapping the code in single backticks:</p> <pre><code>I want to make sure `vulcan plan` looks different than the other words!\n</code></pre>"},{"location":"HOWTO/#code-blocks","title":"Code blocks","text":"<p>The Vulcan docs include many examples of code or command output. These examples are displayed in special \"code blocks\" that display and highlight the code.</p> <p>Code blocks begin and end with three backticks ```. The code to display goes between the first and second set of backticks.</p> <p>Specify the code language next to the first set of backticks to ensure proper syntax highlighting. For example, we could specify Python highlighting like this:</p> <pre><code> ``` python\n\n my_result = 1 + 1\n\n ```\n</code></pre> <p>For terminal commands and output, specify the language as <code>bash</code>.</p> <p>Code blocks have a number of options for display, the most important of which are line numbers and highlighted lines.</p> <p>Line numbers are important for larger code blocks, making it easier for the text to reference specific parts of the code.</p> <p>Highlighted lines provide an even more direct way to draw attention to specific parts of the code.</p> <p>This figure shows examples of the different code block options:</p> <p></p>"},{"location":"HOWTO/#callouts","title":"Callouts","text":"<p>Callouts are used to draw attention to important points or to highlight important information.</p> <p>Use them to ensure that readers notice key points. They are particularly useful if the important point is embedded in a large section of text.</p> <p>We use the \"admonitions\" library for callouts, and they have 12 built-in types with different icons and styles:</p> <ul> <li><code>note</code></li> <li><code>abstract</code></li> <li><code>info</code></li> <li><code>tip</code></li> <li><code>example</code></li> <li><code>quote</code></li> <li><code>success</code></li> <li><code>question</code></li> <li><code>warning</code></li> <li><code>failure</code></li> <li><code>danger</code></li> <li><code>bug</code></li> </ul> <p>Create a callout by starting a line with three exclamation marks <code>!!!</code> and the name of the callout type you want to use. For example:</p> <pre><code>!!! note\n    This creates a note callout!\n</code></pre> <p>And this is what that callout looks like:</p> <p>Note</p> <p>This creates a note callout!</p> <p>By default, the callout title is its type. You can change the title by adding it in quote after the callout type:</p> <pre><code>!!! important \"Custom title\"\n    This creates an important callout with a custom title!\n</code></pre> <p>Custom title</p> <p>This creates an important callout with a custom title!</p> <p>You can make a callout collapsible by using three question marks <code>???</code> instead of exclamation marks:</p> <pre><code>??? tip\n    This creates a collapsible tip callout!\n</code></pre> Tip <p>This creates a collapsible tip callout!</p> <p>You can make the collapsible open by default by adding a plus sign <code>+</code> to the three question marks:</p> <pre><code>???+ warning\n    This creates a collapsible warning callout that is open by default!\n</code></pre> Warning <p>This creates a collapsible warning callout that is open by default!</p>"},{"location":"HOWTO/#images","title":"Images","text":"<p>The Vulcan docs use screenshots of output, graphics, and other images to supplement the text.</p> <p>To add an image, first create it and save it in PNG format. Save it in a folder in the directory where its doc's markdown file is located.</p> <p>Add the image to a page with this markdown:</p> <pre><code>![Image's alt text](./relative/path/to/image.png)\n</code></pre> <p>Note that: - The line starts with an exclamation point <code>!</code> - Brackets containing the image's alt text come next - The relative path to the image follows the brackets</p> <p>There may not be spaces between the exclamation point, brackets, and path.</p> <p>Specify alt text for all images.</p>"},{"location":"HOWTO/#custom-css-and-inline-html","title":"Custom CSS and inline HTML","text":"<p>Sometimes markdown just doesn't cut it.</p> <p><code>MkDocs</code> supports custom CSS and inline HTML, both of which we use as necessary (but sparingly).</p> <p>For example, by default you can only link to navigation elements within a page (like section titles). We sometimes want to link to individual pieces of content, so we use inline HTML to create a custom anchor link.</p> <p>For example, in the FAQ we make a link to the \"schema question\" with the inline HTML <code>&lt;a id=\"schema-question\"&gt;&lt;/a&gt;</code>.</p>"},{"location":"HOWTO/#configuring-docs","title":"Configuring docs","text":"<p>Docs are configured in the <code>mkdocs.yml</code> file.</p> <p>The first section of the file defines high-level information about Vulcan, such as the docs site's name and our Github repo URL/name.</p> <p></p> <p>We describe subsequent sections below.</p>"},{"location":"HOWTO/#site-layout-and-navigation","title":"Site layout and navigation","text":"<p>The bulk of the file defines the structure/layout of the docs site's pages under the <code>nav</code> key.</p> <p>It defines a hierarchy of pages and subpages that is reflected in the site's navigation elements (e.g., top menu, left sidebar).</p> <p>As with all YAML files, indentation plays a key role. Each level of indentation generates a new level down in the hierarchy.</p> <p>One indentation below <code>nav</code> corresponds to top-level navigation elements like the menu bar links.</p> <p>Here we see links generated from the first three top-level <code>nav</code> entries <code>Overview</code>, <code>Get started</code>, and <code>Guides</code>:</p> <p></p> <p>As we continue downward in the hierarchy, we may either add specific pages or new section(s) that contain subpages. Vulcan docs use both these approaches in different places.</p> <p>For example, the <code>Get started</code> section contains 6 subpages, while the <code>Guides</code> section contains 4 sections that each specify their own subpages.</p> <p>Here we see the 6 subpages specified directly under the <code>Get started</code> entry in the lefthand navbar:</p> <p></p> <p>And here we see the first three subsections <code>Project structure</code>, <code>Project setup</code>, and <code>Project content</code> (and their subpages) specified under the <code>Guides</code> entry:</p> <p></p> <p>You may continue to add subsections as needed. At the time of writing, only the <code>Tobiko Cloud</code> section uses a 3<sup>rd</sup> level of nested sections.</p>"},{"location":"HOWTO/#theme-and-colors","title":"Theme and colors","text":"<p>The <code>theme</code> section defines the appearance of the docs site. It is rarely modified.</p> <p>It specifies the theme name, logo, and color palette, and configures features like the navigation bar and sidebar.</p> <p></p>"},{"location":"HOWTO/#pluginsextensions","title":"Plugins/extensions","text":"<p>The <code>plugins</code> and <code>markdown_extensions</code> sections specify different plugins and extensions we use to add functionality to the docs site. It is rarely modified.</p> <p>Some examples: - Plugin <code>glightbox</code> allows users to expand and zoom on images - Markdown extension <code>pymdownx.tabbed</code> specifies how tabbed content is displayed - Markdown extension <code>admonition</code> allows us to add callout boxes</p> <p></p>"},{"location":"HOWTO/#extra","title":"Extra","text":"<p>The final sections of the <code>mkdocs.yml</code> file define assorted metadata about the site.</p> <p>The <code>extra_css</code> key specifies the location of the file containing custom CSS. We use this to add custom colors to some elements. It is rarely modified.</p> <p>The <code>extra</code> section specifies links embedded in the site footer and our Google Analytics ID. It is rarely modified.</p> <p></p>"},{"location":"HOWTO/#docs-hosting","title":"Docs hosting","text":"<p>Our docs are built with <code>MkDocs</code>, but they are hosted on <code>readthedocs.io</code>.</p> <p>When a PR is merged to main, it triggers a docs build and deployment. That process is configured in the <code>.readthedocs.yaml</code> file.</p> <p>Readthedocs supports multiple versions of the docs, with two important versions: <code>stable</code> and <code>latest</code>.</p> <p>The <code>stable</code> docs are built from the latest Github release tag, while the <code>latest</code> docs are built from the latest commit on main.</p> <p>We have hidden the interface for accessing <code>latest</code>, so users will generally not be able to access it. However, you may access it by replacing the word \"stable\" with \"latest\" in a URL:</p> <p>For example, the Getting started page is at:</p> <ul> <li>Stable: <code>https://vulcan.readthedocs.io/en/stable/quick_start/</code></li> <li>Latest: <code>https://vulcan.readthedocs.io/en/latest/quick_start/</code></li> </ul>"},{"location":"comparisons/","title":"Comparisons","text":""},{"location":"comparisons/#comparisons","title":"Comparisons","text":"<p>This documentation is a work in progress.</p> <p>There are many tools and frameworks in the data ecosystem. This page tries to make sense of it all.</p> <p>If you are not familiar with Vulcan, it will be helpful to first read about Vulcan to better understand the comparisons. </p>"},{"location":"comparisons/#dbt","title":"dbt","text":"<p>dbt is a tool for data transformations. It is a pioneer in this space and has shown how valuable transformation frameworks can be. Although dbt is a fantastic tool, it has trouble scaling with data and organizational size.</p> <p>dbt built their product focused on simple data transformations. By default, it fully refreshes data warehouses by executing templated SQL in the correct order.</p> <p>Over time dbt has seen that data transformations are not enough to operate a scalable and robust data product. As a result, advanced features are patched in, such as state management (defer) and incremental loads, to try to address these needs while pushing the burden of correctness onto users with increased complexity. These \"advanced\" features make up some of the fundamental building blocks of a DataOps framework.</p> <p>In other words, the challenge of implementing these features in dbt falls primarily on you: more jinja macro blocks, more manual configuration, more custom tooling, and more opportunities for error. We needed an easier, more reliable way, so we designed Vulcan from the ground up to be a robust DataOps framework.</p> <p>Vulcan aims to be dbt format-compatible. Importing existing dbt projects with minor changes is in development.</p>"},{"location":"comparisons/#feature-comparisons","title":"Feature comparisons","text":"Feature dbt Vulcan Modeling <code>SQL models</code> \u2705 \u2705 <code>Python models</code> \u2705 \u2705\u2705 <code>Jinja support</code> \u2705 \u2705 <code>Jinja macros</code> \u2705 \u2705 <code>Python macros</code> \u274c \u2705 Validation <code>SQL semantic validation</code> \u274c \u2705 <code>Unit tests</code> \u274c \u2705 <code>Table diff</code> \u274c \u2705 <code>Data audits</code> \u2705 \u2705 <code>Schema contracts</code> \u2705 \u2705 <code>Data contracts</code> \u274c \u2705 Deployment <code>Virtual Data Environments</code> \u274c \u2705 <code>Open-source CI/CD bot</code> \u274c \u2705 <code>Data consistency enforcement</code> \u274c \u2705 Interfaces <code>CLI</code> \u2705 \u2705 <code>Paid UI</code> \u2705 \u274c <code>Open-source UI</code> \u274c \u2705 <code>Native Notebook Support</code> \u274c \u2705  Visualization <code>Documentation generation</code> \u2705 \u2705 <code>Column-level lineage</code> \u274c \u2705 Miscellaneous <code>Package manager</code> \u2705 \u274c <code>Multi-repository support</code> \u274c \u2705  <code>SQL transpilation</code> \u274c \u2705"},{"location":"comparisons/#environments","title":"Environments","text":"<p>Development and staging environments in dbt are costly to make and not fully representative of what will go into production.</p> <p>The standard approach to creating a new environment in dbt is to rerun your entire warehouse in a new environment. This may work at small scales, but even then it wastes time and money. Here's why:</p> <p>The first part of running a data transformation system is repeatedly iterating through three steps: create or modify model code, execute the models, evaluate the outputs. Practitioners may repeat these steps many times in a day's work.</p> <p>These steps incur costs to organizations: compute costs to run the models and staff time spent waiting on them to run. Inefficiencies compound rapidly because the steps are repeated so frequently. dbt's default full refresh approach leads to the most costly version of this loop: recomputing every model every time.</p> <p>Vulcan takes another approach. It examines the code modifications and the dependency structure among the models to determine which models are affected -- and executes only those models. This results in the least costly version of the loop: computing only what is required every time through.</p> <p>This enables Vulcan to provide efficient isolated Virtual Environments. Environments in dbt cost compute and storage, but creating a development environment in Vulcan is free -- you can quickly access a full replica of any other environment with a single command.</p> <p>Additionally, Vulcan ensures that promotion of staging environments to production is predictable and consistent. There is no concept of promotion in dbt, so queries are all rerun when it's time to deploy something. In Vulcan, promotions are simple pointer swaps so there is no wasted compute.</p>"},{"location":"comparisons/#incremental-models","title":"Incremental models","text":"<p>Implementing incremental models is difficult and error-prone in dbt because it does not keep track of state.</p>"},{"location":"comparisons/#complexity","title":"Complexity","text":"<p>Since there is no state of which incremental intervals have already run in dbt, users must write and maintain subqueries to find missing date boundaries themselves:</p> <pre><code>-- dbt incremental\nSELECT *\nFROM {{ ref(raw.events) }} e\nJOIN {{ ref(raw.event_dims) }} d\n  ON e.id = d.id\n-- must specify the is_incremental flag because this predicate will fail if the model has never run before\n{% if is_incremental() %}\n    -- this filter dynamically scans the current model to find the date boundary\n    AND d.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n{% if is_incremental() %}\n  WHERE e.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Manually specifying macros to find date boundaries is repetitive and error-prone.</p> <p>The example above shows how incremental models behave differently in dbt depending on whether they have been run before. As models become more complex, the cognitive burden of having two run times, \"first time full refresh\" vs. \"subsequent incremental\", increases.</p> <p>Vulcan keeps track of which date ranges exist, producing a simplified and efficient query as follows:</p> <pre><code>-- Vulcan incremental\nSELECT *\nFROM raw.events e\nJOIN raw.event_dims d\n  -- date ranges are handled automatically by Vulcan\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n</code></pre>"},{"location":"comparisons/#data-leakage","title":"Data leakage","text":"<p>dbt does not check whether the data inserted into an incremental table should be there or not. This can lead to problems and consistency issues, such as late-arriving data overriding past partitions. These problems are called \"data leakage.\"</p> <p>Vulcan wraps all queries in a subquery with a time filter under the hood to enforce that the data inserted for a particular batch is as expected and reproducible every time.</p> <p>In addition, dbt only supports the 'insert/overwrite' incremental load pattern for systems that natively support it. Vulcan enables 'insert/overwrite' on any system, because it is the most robust approach to incremental loading, while 'Append' pipelines risk data inaccuracy in the variety of scenarios where your pipelines may run more than once for a given date.</p> <p>This example shows the time filtering subquery Vulcan applies to all queries as a guard against data leakage: </p><pre><code>-- original query\nSELECT *\nFROM raw.events\nJOIN raw.event_dims d\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n\n-- with automated data leakage guard\nSELECT *\nFROM (\n  SELECT *\n  FROM raw.events\n  JOIN raw.event_dims d\n    ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\n  WHERE d.ds BETWEEN @start_ds AND @end_ds\n)\nWHERE ds BETWEEN @start_ds AND @end_ds\n</code></pre><p></p>"},{"location":"comparisons/#data-gaps","title":"Data gaps","text":"<p>The main pattern used to implement incremental models in dbt is checking for the most recent data with MAX(date). This pattern does not catch missing data from the past, or \"data gaps.\"</p> <p>Vulcan stores each date interval a model has been run with, so it knows exactly what dates are missing: </p><pre><code>Expected dates: 2022-01-01, 2022-01-02, 2022-01-03\nMissing past data: ?, 2022-01-02, 2022-01-03\nData gap: 2022-01-01, ?, 2022-01-03\n</code></pre><p></p> <p>Vulcan will automatically fill these data gaps on the next run.</p>"},{"location":"comparisons/#performance","title":"Performance","text":"<p>Subqueries that look for MAX(date) could have a performance impact on the primary query. Vulcan is able to avoid these extra subqueries.</p> <p>Additionally, dbt expects an incremental model to be able to fully refresh the first time it runs. For some large data sets, this is cost-prohibitive or infeasible.</p> <p>Vulcan is able to batch up backfills into more manageable chunks.</p>"},{"location":"comparisons/#sql-understanding","title":"SQL understanding","text":"<p>dbt heavily relies on Jinja. It has no understanding of SQL and treats all queries as raw strings without context. This means that simple syntax errors like trailing commas are difficult to debug and require a full run to detect.</p> <p>Vulcan supports Jinja, but it does not rely on it - instead, it parses/understands SQL through SQLGlot. Simple errors can be detected at compile time, so you no longer have to wait minutes or even longer to see that you've referenced a column incorrectly or missed a comma.</p> <p>Additionally, having a first-class understanding of SQL supercharges Vulcan with features such as transpilation, column-level lineage, and automatic change categorization.</p>"},{"location":"comparisons/#testing","title":"Testing","text":"<p>Data quality checks such as detecting NULL values and duplicated rows are extremely valuable for detecting upstream data issues and large scale problems. However, they are not meant for testing edge cases or business logic, and they are not sufficient for creating robust data pipelines.</p> <p>Unit and integration tests are the tools to use to validate business logic. Vulcan encourages users to add unit tests to all of their models to ensure changes don't unexpectedly break assumptions. Unit tests are designed to be fast and self contained so that they can run in continuous integration (CI) frameworks.</p>"},{"location":"comparisons/#python-models","title":"Python models","text":"<p>dbt's Python models only run remotely on adapters of data platforms that have a full Python runtime, limiting the number of users that can take advantage of them and making the models difficult to debug.</p> <p>Vulcan's Python models run locally and can be used with any data warehouse. Breakpoints can be added to debug the model.</p>"},{"location":"comparisons/#data-contracts","title":"Data contracts","text":"<p>dbt offers manually configured schema contracts that will check the model's schema against the yaml schema at runtime. Models can be versioned to allow downstream teams time to migrate to the latest version, at the risk of a fragmented source of truth during the migration period.</p> <p>Vulcan provides automatic schema contracts and data contracts via <code>vulcan plan</code>, which checks the model's schema and query logic for changes that affect downstream users. <code>vulcan plan</code> will show which models have breaking changes and which downstream models are affected.</p> <p>While breaking changes can be rolled out as separate models to allow for a migration period, Vulcan's Virtual Preview empowers teams to collaborate on migrations before the changes are deployed to prod, maintaining a single source of truth across the business.</p>"},{"location":"development/","title":"Contribute to development","text":""},{"location":"development/#contribute-to-development","title":"Contribute to development","text":"<p>Vulcan is licensed under Apache 2.0. We encourage community contribution and would love for you to get involved. The following document outlines the process to contribute to Vulcan.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed on your machine. Exactly how to install these is dependent on your operating system.</p> <ul> <li>Docker</li> <li>Docker Compose V2</li> <li>OpenJDK &gt;= 11</li> <li>Python &gt;= 3.9 &lt; 3.13</li> </ul>"},{"location":"development/#virtual-environment-setup","title":"Virtual environment setup","text":"<p>We do recommend using a virtual environment to develop Vulcan.</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>Once you have activated your virtual environment, you can install the dependencies by running the following command.</p> <pre><code>make install-dev\n</code></pre> <p>Optionally, you can use pre-commit to automatically run linters/formatters:</p> <pre><code>make install-pre-commit\n</code></pre>"},{"location":"development/#python-development","title":"Python development","text":"<p>Run linters and formatters:</p> <pre><code>make style\n</code></pre> <p>Run faster tests for quicker local feedback:</p> <pre><code>make fast-test\n</code></pre> <p>Run more comprehensive tests that run on each commit:</p> <pre><code>make slow-test\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>In order to run the documentation server, you will need to install the dependencies by running the following command.</p> <pre><code>make install-doc\n</code></pre> <p>Once you have installed the dependencies, you can run the documentation server by running the following command.</p> <pre><code>make docs-serve\n</code></pre> <p>Run docs tests:</p> <pre><code>make doc-test\n</code></pre>"},{"location":"development/#ui-development","title":"UI development","text":"<p>In addition to the Python development, you can also develop the UI.</p> <p>The UI is built using React and Typescript. To run the UI, you will need to install the dependencies by running the following command.</p> <pre><code>pnpm install\n</code></pre> <p>Run ide:</p> <pre><code>make ui-up\n</code></pre>"},{"location":"development/#developing-the-vscode-extension","title":"Developing the VSCode extension","text":"<p>Similar to UI development, you can also develop the VSCode extension. To do so, make sure you have the dependencies installed by running the following command inside the <code>vscode/extension</code> directory.</p> <pre><code>pnpm install\n</code></pre> <p>Once that is done, developing the VSCode extension is most easily done by launching the <code>Run Extensions</code> debug task from a Visual Studio Code workspace opened at the root of the Vulcan repository. By default, the VSCode extension will run the Vulcan server locally and open a new Visual Studio Code window that allows you to try out the Vulcan IDE. It opens the <code>examples/sushi</code> project by default. To set up Visual Studio Code to run the <code>Run Extensions</code> debug task, you can run the following command which will copy the <code>launch.json</code> and <code>tasks.json</code> files to the <code>.vscode</code> directory.</p> <pre><code>make vscode_settings\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This page provides instructions for installing Vulcan on your computer.</p>"},{"location":"installation/#python-virtual-environment","title":"Python virtual environment","text":"<p>It is recommended, but not required, that you use a python virtual environment with Vulcan.</p> <p>First, create the virtual environment: </p><pre><code>python -m venv .venv\n</code></pre><p></p> <p>Then activate it: </p><pre><code>source .venv/bin/activate\n</code></pre><p></p>"},{"location":"installation/#install-vulcan-core","title":"Install Vulcan core","text":"<p>Install the core Vulcan library with <code>pip</code>: </p><pre><code>pip install vulcan\n</code></pre><p></p>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#prerequisites","title":"Prerequisites","text":"<p>This page describes the system prerequisites needed to run Vulcan and provides instructions for meeting them.</p>"},{"location":"prerequisites/#vulcan-prerequisites","title":"Vulcan prerequisites","text":"<p>You'll need Python 3.8 or higher to use Vulcan. You can check your python version by running the following command: </p><pre><code>python3 --version\n</code></pre><p></p> <p>or:</p> <pre><code>python --version\n</code></pre> <p>Note: If <code>python --version</code> returns 2.x, replace all <code>python</code> commands with <code>python3</code>, and <code>pip</code> with <code>pip3</code>.</p>"},{"location":"prerequisites/#next-steps","title":"Next steps","text":"<p>Now that your machine meets the prerequisites, install Vulcan.</p>"},{"location":"quick_start/","title":"Quickstart","text":""},{"location":"quick_start/#quickstart","title":"Quickstart","text":"<p>Welcome to the Vulcan quickstart, which will get you up and running with an example project.</p> <p>The example project runs locally on your machine with a DuckDB SQL engine, and Vulcan will generate all the necessary project files - no configuration necessary!</p> <p>All you need to do is install Vulcan on your machine - get started by ensuring your system meets the basic prerequisites for using Vulcan.</p> <p>Head over to the CLI Quickstart</p>"},{"location":"concepts/audits/","title":"Auditing","text":""},{"location":"concepts/audits/#auditing","title":"Auditing","text":"<p>Audits are one of the tools Vulcan provides to validate your models. Along with tests, they are a great way to ensure the quality of your data and to build trust in it across your organization.</p> <p>Unlike tests, audits are used to validate the output of a model after every run. When you apply a plan, Vulcan will automatically run each model's audits.</p> <p>All audits in Vulcan are blocking - when an audit fails, Vulcan halts plan application or run execution to prevent potentially invalid data from propagating further downstream. This ensures data quality by stopping the pipeline whenever data validation fails.</p> <p>A comprehensive suite of audits can identify data issues upstream, whether they are from your vendors or other teams. Audits also empower your data engineers and analysts to work with confidence by catching problems early as they work on new features or make updates to your models.</p> <p>NOTE: For incremental by time range models, audits are only applied to intervals being processed - not for the entire underlying table.</p>"},{"location":"concepts/audits/#terminology-audits-and-assertions","title":"Terminology: Audits and Assertions","text":"<p>Vulcan uses two related but distinct concepts:</p> <ul> <li>AUDIT - The validation rule (the SQL query that checks for problems)</li> <li>ASSERTION - Attaching an audit to a model (claiming it should pass)</li> </ul> <p>In MODEL definitions:</p> <pre><code>-- Define the AUDIT (the rule)\nAUDIT (name check_positive_price);\nSELECT * FROM @this_model WHERE price &lt;= 0;\n\n-- Make ASSERTIONS about your model (attach the audit)\nMODEL (\n  name products,\n  assertions (check_positive_price)  -- Declaring this audit should pass\n);\n</code></pre> <p>Note: You may encounter older code that attaches audits using <code>audits</code> instead of <code>assertions</code> in MODEL definitions. While both work identically, please use <code>assertions</code> for clearer semantics. This documentation uses <code>assertions</code> throughout.</p>"},{"location":"concepts/audits/#how-audits-work","title":"How Audits Work","text":"<p>A failed audit halts the execution of a <code>plan</code> or <code>run</code> to prevent invalid data from propagating to downstream models. The impact of a failure depends on whether you are running a <code>plan</code> or a <code>run</code>.</p> <p>Vulcan's audit process is:</p> <ol> <li>Evaluate the model (e.g., insert new data or rebuild the table)</li> <li>Run the audit query against the newly updated model table. For incremental models, the audit only runs on the processed time intervals.</li> <li>If the query returns any rows, the audit fails, halting the <code>plan</code> or <code>run</code>.</li> </ol>"},{"location":"concepts/audits/#plan-vs-run","title":"Plan vs. Run","text":"<p>The key difference is when the model's data is promoted to the production environment:</p> <ul> <li> <p><code>plan</code>: Vulcan evaluates and audits all modified models before promoting them to production. If an audit fails, the <code>plan</code> stops, and the production table is untouched. Invalid data is contained in an isolated table and never reaches the production environment.</p> </li> <li> <p><code>run</code>: Vulcan evaluates and audits models directly against the production environment. If an audit fails, the <code>run</code> stops, but the invalid data is already present in the production table. The blocking action prevents this bad data from being used to build other downstream models.</p> </li> </ul>"},{"location":"concepts/audits/#fixing-a-failed-audit","title":"Fixing a Failed Audit","text":"<p>If an audit fails during a <code>run</code>, you must fix the invalid data in the production table. To do so:</p> <ol> <li>Find the root cause: examine upstream models and data sources</li> <li>Fix the source<ul> <li>If the cause is an external data source, fix it there. Then, run a restatement plan on the first Vulcan model that ingests the source data. This will restate all downstream models, including the one with the failed audit.</li> <li>If the cause is a Vulcan model, update the model's logic. Then apply the change with a <code>plan</code>, which will automatically re-evaluate all downstream models.</li> </ul> </li> </ol>"},{"location":"concepts/audits/#user-defined-audits","title":"User-Defined Audits","text":"<p>In Vulcan, user-defined audits are defined in <code>.sql</code> files in an <code>audits</code> directory in your Vulcan project. Multiple audits can be defined in a single file, so you can organize them to your liking. Alternatively, audits can be defined inline within the model definition itself.</p> <p>Audits are SQL queries that should not return any rows; in other words, they query for bad data, so returned rows indicates that something is wrong.</p> <p>In its simplest form, an audit is defined with the <code>AUDIT</code> statement along with a query, as in the following example:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  dialect spark\n);\nSELECT * from sushi.items\nWHERE\n  ds BETWEEN @start_ds AND @end_ds\n  AND price IS NULL;\n</code></pre> <p>In the example, we defined an audit named <code>assert_item_price_is_not_null</code>, ensuring that every sushi item has a price.</p> <p>Note: If the query is in a different dialect than the rest of your project, you can specify it in the <code>AUDIT</code> statement. In the example above we set it to <code>spark</code>, so SQLGlot will automatically understand how to execute the query behind the scenes.</p> <p>To run the audit, attach it to a model using <code>assertions</code> in the <code>MODEL</code> statement:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (assert_item_price_is_not_null)\n);\n</code></pre> <p>Now <code>assert_item_price_is_not_null</code> will run every time the <code>sushi.items</code> model is run.</p>"},{"location":"concepts/audits/#generic-audits","title":"Generic audits","text":"<p>Audits can also be parameterized and implemented in a model-agnostic way so the same audit can be used for multiple models.</p> <p>Consider the following audit definition that checks whether the target column exceeds a configured threshold:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>This example utilizes macros to parameterize the audit. <code>@this_model</code> is a special macro which refers to the model that is being audited. For incremental models, this macro also ensures that only relevant data intervals are affected.</p> <p><code>@column</code> and <code>@threshold</code> are parameters whose values are specified in a model definition's <code>MODEL</code> statement.</p> <p>Apply the generic audit to a model by referencing it in the <code>MODEL</code> statement using <code>assertions</code>:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    does_not_exceed_threshold(column := id, threshold := 1000),\n    does_not_exceed_threshold(column := price, threshold := 100)\n  )\n);\n</code></pre> <p>Notice how <code>column</code> and <code>threshold</code> parameters have been set. These values will be propagated into the audit query and substituted into the <code>@column</code> and <code>@threshold</code> macro variables.</p> <p>Note that the same audit can be applied more than once to the a model using different sets of parameters.</p> <p>Generic audits can define default values as follows: </p><pre><code>AUDIT (\n  name does_not_exceed_threshold,\n  defaults (\n    threshold = 10,\n    column = id\n  )\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre><p></p> <p>Alternatively, you can apply specific audits globally by including them in the model defaults configuration:</p> <pre><code>model_defaults:\n  assertions:\n    - assert_positive_order_ids\n    - does_not_exceed_threshold(column := id, threshold := 1000)\n</code></pre> <p>Note: In <code>model_defaults</code>, you can use either <code>audits</code> or <code>assertions</code> - both are supported for backward compatibility.</p>"},{"location":"concepts/audits/#naming","title":"Naming","text":"<p>We recommended avoiding SQL keywords when naming audit parameters. Quote any audit argument that is also a SQL keyword.</p> <p>For example, if an audit <code>my_audit</code> uses a <code>values</code> parameter, invoking it will require quotes because <code>values</code> is a SQL keyword:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    my_audit(column := a, \"values\" := (1,2,3))\n  )\n)\n</code></pre>"},{"location":"concepts/audits/#inline-audits","title":"Inline audits","text":"<p>You can also define audits directly within a model definition using the same syntax. Multiple audits can be specified within the same SQL model file:</p> <pre><code>MODEL (\n    name sushi.items,\n    assertions(does_not_exceed_threshold(column := id, threshold := 1000), price_is_not_null)\n);\nSELECT id, price\nFROM sushi.seed;\n\nAUDIT (name does_not_exceed_threshold);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n\nAUDIT (name price_is_not_null);\nSELECT * FROM @this_model\nWHERE price IS NULL;\n</code></pre>"},{"location":"concepts/audits/#built-in-audits","title":"Built-in audits","text":"<p>Vulcan comes with a suite of built-in generic audits that cover a broad set of common use cases. All built-in audits are blocking - when they fail, execution halts immediately.</p> <p>This section describes the audits, grouped by general purpose.</p>"},{"location":"concepts/audits/#generic-assertion-audit","title":"Generic assertion audit","text":"<p>The <code>forall</code> audit is the most generic built-in audit, allowing arbitrary boolean SQL expressions.</p>"},{"location":"concepts/audits/#forall","title":"forall","text":"<p>Ensures that a set of arbitrary boolean expressions evaluate to <code>TRUE</code> for all rows in the model. The boolean expressions should be written in SQL.</p> <p>This example asserts that all rows have a <code>price</code> greater than 0 and a <code>name</code> value containing at least one character:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    forall(criteria := (\n      price &gt; 0,\n      LENGTH(name) &gt; 0\n    ))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#row-counts-and-null-value-audits","title":"Row counts and NULL value audits","text":"<p>These audits concern row counts and presence of <code>NULL</code> values.</p>"},{"location":"concepts/audits/#number_of_rows","title":"number_of_rows","text":"<p>Ensures that the number of rows in the model's table exceeds the threshold.</p> <p>This example asserts that the model has more than 10 rows:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    number_of_rows(threshold := 10)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#not_null","title":"not_null","text":"<p>Ensures that specified columns do not contain <code>NULL</code> values.</p> <p>This example asserts that none of the <code>id</code>, <code>customer_id</code>, or <code>waiter_id</code> columns contain <code>NULL</code> values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    not_null(columns := (id, customer_id, waiter_id))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#at_least_one","title":"at_least_one","text":"<p>Ensures that specified columns contain at least one non-NULL value.</p> <p>This example asserts that the <code>zip</code> column contains at least one non-NULL value:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    at_least_one(column := zip)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#not_null_proportion","title":"not_null_proportion","text":"<p>Ensures that the specified column's proportion of <code>NULL</code> values is no greater than a threshold.</p> <p>This example asserts that the <code>zip</code> column has no more than 80% <code>NULL</code> values:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    not_null_proportion(column := zip, threshold := 0.8)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#specific-data-values-audits","title":"Specific data values audits","text":"<p>These audits concern the specific set of data values present in a column.</p>"},{"location":"concepts/audits/#not_constant","title":"not_constant","text":"<p>Ensures that the specified columns are not constant (i.e., have at least two non-NULL values).</p> <p>This example asserts that the column <code>customer_id</code> has at least two non-NULL values:</p> <pre><code>MODEL (\n  name sushi.customer_revenue_by_day,\n  assertions (\n    not_constant(column := customer_id)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#unique_values","title":"unique_values","text":"<p>Ensures that specified columns contain unique values (i.e., have no duplicated values).</p> <p>This example asserts that the <code>id</code> and <code>item_id</code> columns have unique values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_values(columns := (id, item_id))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#unique_combination_of_columns","title":"unique_combination_of_columns","text":"<p>Ensures that each row has a unique combination of values over the specified columns.</p> <p>This example asserts that the combination of <code>id</code> and <code>ds</code> columns has no duplicated values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_combination_of_columns(columns := (id, ds))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#accepted_values","title":"accepted_values","text":"<p>Ensures that all rows of the specified column contain one of the accepted values.</p> <p>Note</p> <p>Rows with <code>NULL</code> values for the column will pass this audit in most databases/engines. Use the <code>not_null</code> audit to ensure there are no <code>NULL</code> values present in a column.</p> <p>This example asserts that column <code>name</code> has a value of 'Hamachi', 'Unagi', or 'Sake':</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_values(column := name, is_in := ('Hamachi', 'Unagi', 'Sake'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#not_accepted_values","title":"not_accepted_values","text":"<p>Ensures that no rows of the specified column contain one of the not accepted values.</p> <p>Note</p> <p>This audit does not support rejecting <code>NULL</code> values. Use the <code>not_null</code> audit to ensure there are no <code>NULL</code> values present in a column.</p> <p>This example asserts that column <code>name</code> is not one of 'Hamburger' or 'French fries':</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_accepted_values(column := name, is_in := ('Hamburger', 'French fries'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#numeric-data-audits","title":"Numeric data audits","text":"<p>These audits concern the distribution of values in numeric columns.</p>"},{"location":"concepts/audits/#sequential_values","title":"sequential_values","text":"<p>Ensures that each of an ordered numeric column's values contains the previous row's value plus <code>interval</code>.</p> <p>For example, with a column having minimum value 1 and maximum value 4 and <code>interval := 1</code>, it ensures that the rows contain values <code>[1, 2, 3, 4]</code>.</p> <p>This example asserts that column <code>item_id</code> contains sequential values that differ by <code>1</code>:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    sequential_values(column := item_id, interval := 1)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#accepted_range","title":"accepted_range","text":"<p>Ensures that a column's values are in a numeric range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that all rows have a <code>price</code> greater than or equal 1 and less than or equal to 100:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 1, max_v := 100)\n  )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that all rows have a <code>price</code> greater than 0 and less than 100:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 0, max_v := 100, inclusive := false)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#mutually_exclusive_ranges","title":"mutually_exclusive_ranges","text":"<p>Ensures that each row's numeric range does not overlap with any other row's range.</p> <p>This example asserts that each row's range [min_price, max_price] does not overlap with any other row's range:</p> <pre><code>MODEL (\n  name pricing.tier_ranges,\n  assertions (\n    mutually_exclusive_ranges(lower_bound_column := min_price, upper_bound_column := max_price)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#character-data-audits","title":"Character data audits","text":"<p>These audits concern the characteristics of values in character/string columns.</p> <p>Warning</p> <p>Databases/engines may exhibit different behavior for different character sets or languages.</p>"},{"location":"concepts/audits/#not_empty_string","title":"not_empty_string","text":"<p>Ensures that no rows of a column contain an empty string value <code>''</code>.</p> <p>This example asserts that no <code>name</code> is an empty string:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_empty_string(column := name)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#string_length_equal","title":"string_length_equal","text":"<p>Ensures that all rows of a column contain a string with the specified number of characters.</p> <p>This example asserts that all <code>zip</code> values are 5 characters long:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_equal(column := zip, v := 5)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#string_length_between","title":"string_length_between","text":"<p>Ensures that all rows of a column contain a string with number of characters in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that all <code>name</code> values have 5 or more and 50 or fewer characters:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := name, min_v := 5, max_v := 50)\n    )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that all rows have a <code>name</code> with 5 or more and 59 or fewer characters:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := zip, min_v := 4, max_v := 60, inclusive := false)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#valid_uuid","title":"valid_uuid","text":"<p>Ensures that all non-NULL rows of a column contain a string with the UUID structure.</p> <p>UUID structure determined by matching regular expression <code>'^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$'</code>.</p> <p>This example asserts that all <code>uuid</code> values have the UUID structure:</p> <pre><code>MODEL (\n  name events.user_sessions,\n  assertions (\n    valid_uuid(column := uuid)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#valid_email","title":"valid_email","text":"<p>Ensures that all non-NULL rows of a column contain a string with the email address structure.</p> <p>Email address structure determined by matching regular expression <code>'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'</code>.</p> <p>This example asserts that all <code>email</code> values have the email address structure:</p> <pre><code>MODEL (\n  name dim.users,\n  assertions (\n    valid_email(column := email)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#valid_url","title":"valid_url","text":"<p>Ensures that all non-NULL rows of a column contain a string with the URL structure.</p> <p>URL structure determined by matching regular expression <code>'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$'</code>.</p> <p>This example asserts that all <code>url</code> values have the URL structure:</p> <pre><code>MODEL (\n  name dim.products,\n  assertions (\n    valid_url(column := url)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#valid_http_method","title":"valid_http_method","text":"<p>Ensures that all non-NULL rows of a column contain a valid HTTP method.</p> <p>Valid HTTP methods determined by matching values <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>PATCH</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.</p> <p>This example asserts that all <code>http_method</code> values are valid HTTP methods:</p> <pre><code>MODEL (\n  name logs.api_requests,\n  assertions (\n    valid_http_method(column := http_method)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#match_regex_pattern_list","title":"match_regex_pattern_list","text":"<p>Ensures that all non-NULL rows of a column match at least one of the specified regular expressions.</p> <p>This example asserts that all <code>todo</code> values match one of <code>'^\\d.*'</code> (string starts with a digit) or <code>'.*!$'</code> (ends with an exclamation mark):</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    match_regex_pattern_list(column := todo, patterns := ('^\\d.*', '.*!$'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#not_match_regex_pattern_list","title":"not_match_regex_pattern_list","text":"<p>Ensures that no non-NULL rows of a column match any of the specified regular expressions.</p> <p>This example asserts that no <code>todo</code> values match one of <code>'^!.*'</code> (string starts with an exclamation mark) or <code>'.*\\d$'</code> (ends with a digit):</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    not_match_regex_pattern_list(column := todo, patterns := ('^!.*', '.*\\d$'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#match_like_pattern_list","title":"match_like_pattern_list","text":"<p>Ensures that all non-NULL rows of a column are <code>LIKE</code> at least one of the specified patterns.</p> <p>This example asserts that all <code>name</code> values are <code>LIKE</code> one of <code>'jim%'</code> or <code>'pam%'</code>:</p> <pre><code>MODEL (\n  name sales.customers,\n  assertions (\n    match_like_pattern_list(column := name, patterns := ('jim%', 'pam%'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#not_match_like_pattern_list","title":"not_match_like_pattern_list","text":"<p>Ensures that no non-NULL rows of a column are <code>LIKE</code> any of the specified patterns.</p> <p>This example asserts that no <code>name</code> values are <code>LIKE</code> <code>'%doe'</code> or <code>'%smith'</code>:</p> <pre><code>MODEL (\n  name products.catalog,\n  assertions (\n    not_match_like_pattern_list(column := name, patterns := ('%doe', '%smith'))\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#statistical-audits","title":"Statistical audits","text":"<p>These audits concern the statistical distributions of numeric columns.</p> <p>Note</p> <p>Audit thresholds will likely require fine-tuning via trial and error for each column being audited.</p>"},{"location":"concepts/audits/#mean_in_range","title":"mean_in_range","text":"<p>Ensures that a numeric column's mean is in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that the <code>age</code> column has a mean of at least 21 and at most 50:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 21, max_v := 50)\n    )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that <code>age</code> has a mean greater than 18 and less than 65:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 18, max_v := 65, inclusive := false)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#stddev_in_range","title":"stddev_in_range","text":"<p>Ensures that a numeric column's standard deviation is in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that the <code>age</code> column has a standard deviation of at least 2 and at most 5:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 2, max_v := 5)\n  )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that <code>age</code> has a standard deviation greater than 3 and less than 6:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 3, max_v := 6, inclusive := false)\n  )\n);\n</code></pre>"},{"location":"concepts/audits/#z_score","title":"z_score","text":"<p>Ensures that no rows of a numeric column contain a value whose absolute z-score exceeds the threshold.</p> <p>z-score is calculated as <code>ABS(([row value] - [column mean]) / NULLIF([column standard deviation], 0))</code>.</p> <p>This example asserts that the <code>age</code> column contains no rows with z-scores greater than 3:</p> <pre><code>MODEL (\n  name sales.transactions,\n  assertions (\n    z_score(column := age, threshold := 3)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#kl_divergence","title":"kl_divergence","text":"<p>Ensures that the symmetrised Kullback-Leibler divergence (aka \"Jeffreys divergence\" or \"Population Stability Index\") between two columns does not exceed a threshold.</p> <p>This example asserts that the symmetrised KL Divergence between columns <code>age</code> and <code>reference_age</code> is less than or equal to 0.1:</p> <pre><code>MODEL (\n  name analytics.cohort_comparison,\n  assertions (\n    kl_divergence(column := age, target_column := reference_age, threshold := 0.1)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#chi_square","title":"chi_square","text":"<p>Ensures that the chi-square statistic for two categorical columns does not exceed a critical value.</p> <p>You can look up the critical value corresponding to a p-value with a table (such as this one) or by using the Python scipy library:</p> <p></p><pre><code>from scipy.stats import chi2\n\n# critical value for p-value := 0.95 and degrees of freedom := 1\nchi2.ppf(0.95, 1)\n</code></pre> This example asserts that the chi-square statistic0 for columns <code>user_state</code> and <code>user_type</code> does not exceed 6.635:<p></p> <pre><code>MODEL (\n  name analytics.user_segments,\n  assertions (\n    chi_square(column := user_state, target_column := user_type, critical_value := 6.635)\n    )\n);\n</code></pre>"},{"location":"concepts/audits/#running-audits","title":"Running audits","text":""},{"location":"concepts/audits/#the-cli-audit-command","title":"The CLI audit command","text":"<p>You can execute audits with the <code>vulcan audit</code> command as follows:</p> <pre><code>$ vulcan -p project audit --start 2022-01-01 --end 2022-01-02\nFound 1 audit(s).\nassert_item_price_is_not_null FAIL.\n\nFinished with 1 audit error(s).\n\nFailure in audit assert_item_price_is_not_null for model sushi.items (audits/items.sql).\nGot 3 results, expected 0.\nSELECT * FROM vulcan.sushi__items__1836721418_83893210 WHERE ds BETWEEN '2022-01-01' AND '2022-01-02' AND price IS NULL\nDone.\n</code></pre>"},{"location":"concepts/audits/#automated-auditing","title":"Automated auditing","text":"<p>When you apply a plan, Vulcan will automatically run each model's audits.</p> <p>Vulcan will halt the pipeline when an audit fails to prevent potentially invalid data from propagating further downstream.</p>"},{"location":"concepts/audits/#advanced-usage","title":"Advanced usage","text":""},{"location":"concepts/audits/#skipping-audits","title":"Skipping audits","text":"<p>Audits can be skipped by setting the <code>skip</code> argument to <code>true</code> as in the following example:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  skip true\n);\nSELECT * from sushi.items\nWHERE ds BETWEEN @start_ds AND @end_ds AND\n   price IS NULL;\n</code></pre>"},{"location":"concepts/checks/","title":"Checks","text":""},{"location":"concepts/checks/#checks","title":"Checks","text":"<p>Quality checks are comprehensive validation rules configured in YAML files that monitor data quality over time. Unlike audits (which block pipeline execution), checks:</p> <ul> <li>Run separately from model execution (or alongside it)</li> <li>Don't block pipelines (non-blocking validation)</li> <li>Track trends and historical patterns</li> <li>Support complex statistical analysis</li> <li>Integrate with Activity API for monitoring</li> </ul> <p>Key characteristics: - Configured in <code>checks/</code> directory - Use declarative YAML syntax - Organized by data quality dimensions - Results stored for historical analysis - Integrated with Activity API</p>"},{"location":"concepts/checks/#checks-vs-audits-vs-profiles","title":"Checks vs Audits vs Profiles","text":"<p>Understanding the three data quality mechanisms:</p> Feature Audits Checks Profiles Purpose Critical validation Monitoring &amp; analysis Observation &amp; tracking When runs With model (inline) Separately or with models With model Blocks pipeline? Yes (always) No No Configuration In MODEL DDL or .sql files YAML files (<code>checks/</code>) In MODEL DDL Output Pass/fail Pass/fail + samples Statistical metrics Best for Business rules, data integrity Trend monitoring, anomalies Understanding data Historical tracking No Yes (Activity API) Yes (<code>_check_profiles</code>) <p>The Three-Layer Strategy:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUDITS (Critical - Blocks Pipeline)   \u2502\n\u2502  \u2022 Primary keys must be unique          \u2502\n\u2502  \u2022 Revenue must be non-negative         \u2502\n\u2502  \u2022 Foreign key relationships valid      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CHECKS (Monitoring - Non-Blocking)     \u2502\n\u2502  \u2022 Row count within expected range      \u2502\n\u2502  \u2022 Anomaly detection on metrics         \u2502\n\u2502  \u2022 Cross-table consistency              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PROFILES (Observation - Metrics)       \u2502\n\u2502  \u2022 Track null percentages               \u2502\n\u2502  \u2022 Monitor column distributions         \u2502\n\u2502  \u2022 Detect data drift                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/checks/#when-to-use-checks","title":"When to Use Checks","text":"<p>\u2705 Use Quality Checks for: - Monitoring data quality trends over time - Statistical anomaly detection - Cross-model validation (joins across models) - Non-critical validation (warnings, not blockers) - Complex validation requiring historical context - Building data quality dashboards</p> <p>\u274c Use Audits Instead for: - Critical business rules that must pass - Model-specific validation (runs inline) - Simple SQL assertions - Blocking invalid data from flowing downstream</p> <p>\u274c Use Profiles Instead for: - Understanding data characteristics - Discovering patterns (not validation) - Detecting data drift - Informing which checks/audits to add</p> <p>Example: Revenue validation strategy</p> <pre><code>-- AUDIT (Critical - blocks if fails)\nMODEL (\n  name analytics.revenue,\n  assertions (\n    not_null(columns := (customer_id, revenue)),\n    accepted_range(column := revenue, min_v := 0, max_v := 100000000)\n  )\n);\n</code></pre> <pre><code># CHECK (Monitoring - warns if unusual)\nchecks:\n  analytics.revenue:\n    accuracy:\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n      - change for row_count &gt;= -30%:\n          name: row_count_drop_alert\n</code></pre> <pre><code>-- PROFILE (Observation - tracks over time)\nMODEL (\n  name analytics.revenue,\n  profiles (revenue, order_count, customer_tier)\n);\n</code></pre>"},{"location":"concepts/checks/#quick-start","title":"Quick Start","text":""},{"location":"concepts/checks/#your-first-check","title":"Your First Check","text":"<p>Create <code>checks/customers.yml</code>:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_emails\n          attributes:\n            description: \"All customers must have an email address\"\n</code></pre> <p>Run the check:</p> <pre><code>vulcan check\n</code></pre> <p>Output:</p> <pre><code>Running checks...\n\n\u2713 analytics.customers.no_missing_emails\n  Pass: missing_count(email) = 0 (actual: 0)\n\n----------------------------------------------------------------------\nRan 1 check in 0.234s\n\nOK\n</code></pre>"},{"location":"concepts/checks/#common-check-patterns","title":"Common Check Patterns","text":""},{"location":"concepts/checks/#pattern-1-completeness-checks","title":"Pattern 1: Completeness Checks","text":"<p>Ensure required data is present:</p> <pre><code>checks:\n  analytics.orders:\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: customer_id_required\n\n      - missing_percent(email) &lt; 5:\n          name: email_mostly_complete\n\n      - row_count &gt; 1000:\n          name: sufficient_orders\n</code></pre>"},{"location":"concepts/checks/#pattern-2-validity-checks","title":"Pattern 2: Validity Checks","text":"<p>Validate data format and values:</p> <pre><code>checks:\n  analytics.users:\n    validity:\n      - failed rows:\n          name: invalid_emails\n          fail query: |\n            SELECT user_id, email\n            FROM analytics.users\n            WHERE email NOT LIKE '%@%'\n          samples limit: 10\n\n      - failed rows:\n          name: invalid_ages\n          fail query: |\n            SELECT user_id, age\n            FROM analytics.users\n            WHERE age &lt; 0 OR age &gt; 120\n</code></pre>"},{"location":"concepts/checks/#pattern-3-uniqueness-checks","title":"Pattern 3: Uniqueness Checks","text":"<p>Ensure no duplicates:</p> <pre><code>checks:\n  analytics.customers:\n    uniqueness:\n      - duplicate_count(email) = 0:\n          name: unique_emails\n\n      - duplicate_count(customer_id, order_date) = 0:\n          name: unique_customer_date_combination\n</code></pre>"},{"location":"concepts/checks/#pattern-4-anomaly-detection","title":"Pattern 4: Anomaly Detection","text":"<p>Detect unusual patterns:</p> <pre><code>checks:\n  analytics.daily_revenue:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly\n</code></pre>"},{"location":"concepts/checks/#pattern-5-change-monitoring","title":"Pattern 5: Change Monitoring","text":"<p>Track changes over time:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - change for row_count &gt;= -50%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if row count drops more than 50%\"\n</code></pre>"},{"location":"concepts/checks/#check-configuration","title":"Check Configuration","text":""},{"location":"concepts/checks/#file-structure","title":"File Structure","text":"<p>Checks are YAML files in the <code>checks/</code> directory:</p> <pre><code>project/\n\u251c\u2500\u2500 models/\n\u251c\u2500\u2500 checks/\n\u2502   \u251c\u2500\u2500 users.yml           # Checks for user tables\n\u2502   \u251c\u2500\u2500 orders.yml          # Checks for order tables\n\u2502   \u251c\u2500\u2500 revenue.yml         # Checks for revenue tables\n\u2502   \u2514\u2500\u2500 cross_model.yml     # Checks spanning multiple tables\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming: - Must end with <code>.yml</code> or <code>.yaml</code> - Name doesn't matter (Vulcan reads all files) - Organize by domain or table for clarity</p>"},{"location":"concepts/checks/#basic-check-syntax","title":"Basic Check Syntax","text":"<pre><code>checks:\n  &lt;fully_qualified_table_name&gt;:\n    &lt;dimension&gt;:\n      - &lt;check_expression&gt;:\n          name: &lt;check_name&gt;\n          attributes:\n            description: &lt;human_readable_description&gt;\n            severity: &lt;warning|error&gt;\n            tags: [&lt;tag1&gt;, &lt;tag2&gt;]\n</code></pre> <p>Example:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_customers\n          attributes:\n            description: \"At least 100 customers expected in production\"\n            severity: warning\n            tags: [critical, daily]\n</code></pre>"},{"location":"concepts/checks/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Organize checks by 8 standard dimensions (ODPS v3.1):</p>"},{"location":"concepts/checks/#1-completeness","title":"1. Completeness","text":"<p>No missing required data</p> <pre><code>completeness:\n  - missing_count(customer_id) = 0\n  - missing_percent(email) &lt; 5\n  - row_count &gt; 1000\n</code></pre>"},{"location":"concepts/checks/#2-validity","title":"2. Validity","text":"<p>Data conforms to format/syntax</p> <pre><code>validity:\n  - failed rows:\n      fail query: |\n        SELECT * FROM table\n        WHERE email NOT LIKE '%@%'\n</code></pre>"},{"location":"concepts/checks/#3-accuracy","title":"3. Accuracy","text":"<p>Data matches reality</p> <pre><code>accuracy:\n  - anomaly detection for avg(revenue)\n  - avg(age) between 18 and 65\n</code></pre>"},{"location":"concepts/checks/#4-consistency","title":"4. Consistency","text":"<p>Data agrees across sources</p> <pre><code>consistency:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders o\n        LEFT JOIN customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n</code></pre>"},{"location":"concepts/checks/#5-uniqueness","title":"5. Uniqueness","text":"<p>No duplicates</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0\n  - duplicate_count(order_id) = 0\n</code></pre>"},{"location":"concepts/checks/#6-timeliness","title":"6. Timeliness","text":"<p>Data is current</p> <pre><code>timeliness:\n  - change for row_count &gt;= -30%\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders\n        WHERE updated_at &lt; CURRENT_DATE - INTERVAL '7 days'\n</code></pre>"},{"location":"concepts/checks/#7-conformity","title":"7. Conformity","text":"<p>Follows standards</p> <pre><code>conformity:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM addresses\n        WHERE LENGTH(zip_code) != 5\n</code></pre>"},{"location":"concepts/checks/#8-coverage","title":"8. Coverage","text":"<p>All records are present</p> <pre><code>coverage:\n  - row_count &gt;= 95% of historical_avg(row_count)\n</code></pre>"},{"location":"concepts/checks/#filtering-checks","title":"Filtering Checks","text":"<p>Apply checks to a subset of data:</p> <pre><code>checks:\n  analytics.orders:\n    filter: \"status = 'completed' AND order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: completed_orders_have_customers\n</code></pre> <p>Multiple filters:</p> <pre><code>checks:\n  analytics.customers:\n    filter: \"country = 'US'\"\n    completeness:\n      - row_count &gt; 1000\n\n  analytics.customers:\n    filter: \"country = 'EU'\"\n    completeness:\n      - row_count &gt; 500\n</code></pre>"},{"location":"concepts/checks/#check-attributes","title":"Check Attributes","text":"<p>Add metadata to checks:</p> <pre><code>checks:\n  analytics.revenue:\n    completeness:\n      - row_count &gt; 1000:\n          name: sufficient_revenue_data\n          attributes:\n            description: \"Revenue table must have at least 1000 rows for analysis\"\n            severity: error\n            tags: [critical, daily, revenue]\n            owner: data-team\n            jira: DATA-1234\n            sla: \"&lt; 1 hour\"\n</code></pre> <p>Standard attributes: - <code>description</code> - Human-readable explanation - <code>severity</code> - <code>error</code> (default) or <code>warning</code> - <code>tags</code> - List of tags for filtering/organization - <code>owner</code> - Team or person responsible - Custom attributes - Any key-value pairs</p>"},{"location":"concepts/checks/#built-in-check-types","title":"Built-in Check Types","text":""},{"location":"concepts/checks/#missing-data-checks","title":"Missing Data Checks","text":""},{"location":"concepts/checks/#missing_countcolumn","title":"<code>missing_count(column)</code>","text":"<p>Count of NULL values:</p> <pre><code>completeness:\n  - missing_count(email) = 0:\n      name: no_missing_emails\n\n  - missing_count(phone) &lt;= 100:\n      name: phone_mostly_complete\n</code></pre>"},{"location":"concepts/checks/#missing_percentcolumn","title":"<code>missing_percent(column)</code>","text":"<p>Percentage of NULL values:</p> <pre><code>completeness:\n  - missing_percent(email) &lt; 5:\n      name: email_95_percent_complete\n\n  - missing_percent(optional_field) &lt; 50:\n      name: optional_field_half_complete\n</code></pre>"},{"location":"concepts/checks/#row-count-checks","title":"Row Count Checks","text":""},{"location":"concepts/checks/#row_count","title":"<code>row_count</code>","text":"<p>Total rows in table:</p> <pre><code>completeness:\n  - row_count &gt; 1000:\n      name: sufficient_data\n\n  - row_count between 1000 and 100000:\n      name: expected_row_range\n</code></pre>"},{"location":"concepts/checks/#row_count-with-filter","title":"<code>row_count</code> with filter","text":"<pre><code>completeness:\n  - row_count &gt; 500:\n      name: sufficient_active_users\n      filter: \"status = 'active'\"\n</code></pre>"},{"location":"concepts/checks/#duplicate-count-checks","title":"Duplicate Count Checks","text":""},{"location":"concepts/checks/#duplicate_countcolumn","title":"<code>duplicate_count(column)</code>","text":"<p>Count of duplicate values:</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0:\n      name: unique_emails\n\n  - duplicate_count(customer_id) = 0:\n      name: unique_customer_ids\n</code></pre>"},{"location":"concepts/checks/#duplicate_countcolumn1-column2","title":"<code>duplicate_count(column1, column2)</code>","text":"<p>Composite key duplicates:</p> <pre><code>uniqueness:\n  - duplicate_count(customer_id, order_date) = 0:\n      name: unique_customer_date\n      attributes:\n        description: \"Each customer can have at most one order per day\"\n</code></pre>"},{"location":"concepts/checks/#failed-rows-checks","title":"Failed Rows Checks","text":""},{"location":"concepts/checks/#sql-based-validation-with-samples","title":"SQL-based validation with samples","text":"<p>Most flexible check type - any SQL query:</p> <pre><code>validity:\n  - failed rows:\n      name: invalid_revenue\n      fail query: |\n        SELECT customer_id, revenue, order_date\n        FROM analytics.orders\n        WHERE revenue &lt; 0 OR revenue &gt; 10000000\n      samples limit: 20\n      attributes:\n        description: \"Revenue must be between 0 and 10M\"\n</code></pre> <p>Key features: - <code>fail query</code> - SELECT statement that returns invalid rows - <code>samples limit</code> - How many example rows to capture (default: 5) - Returns empty = check passes - Returns rows = check fails (captures samples)</p> <p>Complex validation:</p> <pre><code>validity:\n  - failed rows:\n      name: orphaned_orders\n      fail query: |\n        SELECT o.order_id, o.customer_id\n        FROM analytics.orders o\n        LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n      samples limit: 10\n</code></pre>"},{"location":"concepts/checks/#threshold-checks","title":"Threshold Checks","text":""},{"location":"concepts/checks/#numeric-aggregations","title":"Numeric aggregations","text":"<pre><code>accuracy:\n  - avg(revenue) between 100 and 10000:\n      name: revenue_in_expected_range\n\n  - sum(amount) &gt; 1000000:\n      name: sufficient_total_revenue\n\n  - max(age) &lt;= 120:\n      name: age_within_human_range\n\n  - min(price) &gt;= 0:\n      name: non_negative_prices\n</code></pre>"},{"location":"concepts/checks/#statistical-checks","title":"Statistical checks","text":"<pre><code>accuracy:\n  - stddev(revenue) &lt; 5000:\n      name: revenue_low_variance\n\n  - percentile(revenue, 95) &lt; 50000:\n      name: revenue_95th_percentile_check\n</code></pre>"},{"location":"concepts/checks/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"concepts/checks/#ml-based-anomaly-detection","title":"ML-based anomaly detection","text":"<p>Uses historical check results to detect anomalies:</p> <pre><code>accuracy:\n  - anomaly detection for row_count:\n      name: row_count_anomaly\n      attributes:\n        description: \"Detect unusual changes in row count\"\n\n  - anomaly detection for avg(revenue):\n      name: revenue_anomaly\n\n  - anomaly detection for distinct_count(customer_id):\n      name: customer_count_anomaly\n</code></pre> <p>How it works: 1. Collects historical metric values over time 2. Builds statistical model (mean, std dev, trends) 3. Compares current value to expected range 4. Flags significant deviations (typically &gt; 3 std devs)</p> <p>Requirements: - Needs historical data (runs multiple times) - Works best with regular schedules (daily, hourly) - More accurate after 30+ data points</p>"},{"location":"concepts/checks/#change-over-time-checks","title":"Change Over Time Checks","text":""},{"location":"concepts/checks/#monitor-changes-compared-to-previous-run","title":"Monitor changes compared to previous run","text":"<pre><code>timeliness:\n  - change for row_count &gt;= -50%:\n      name: row_count_drop_alert\n      attributes:\n        description: \"Alert if row count drops more than 50% from last week\"\n\n  - change for avg(revenue) &gt;= -20%:\n      name: revenue_drop_alert\n\n  - change for distinct_count(customer_id) &gt;= 10%:\n      name: customer_growth_check\n</code></pre> <p>Change calculation: </p><pre><code>change = (current_value - previous_value) / previous_value * 100\n</code></pre><p></p> <p>Examples: - <code>change &gt;= -30%</code> - Alert if metric drops more than 30% - <code>change &gt;= 10%</code> - Alert if metric grows more than 10% - <code>change between -10% and 10%</code> - Alert if metric changes more than 10% either way</p>"},{"location":"concepts/checks/#data-profiling","title":"Data Profiling","text":""},{"location":"concepts/checks/#what-is-profiling","title":"What is Profiling?","text":"<p>Profiles automatically collect statistical metrics about your data over time.</p> <p>Unlike checks (which validate), profiles observe and track data characteristics:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind FULL,\n  grains (customer_id),\n  profiles (revenue, signup_date, customer_tier, order_count)\n);\n</code></pre> <p>What gets profiled:</p> <p>Table-level metrics: - Row count</p> <p>Column-level metrics (all columns): - Null count &amp; percentage - Distinct count - Duplicate count - Uniqueness percentage</p> <p>Numeric columns: - Min, max, avg, sum - Standard deviation, variance - Histogram buckets</p> <p>Text columns: - Min, max, avg length - Most frequent values</p>"},{"location":"concepts/checks/#profile-configuration","title":"Profile Configuration","text":"<p>Enable profiling in MODEL:</p> <pre><code>MODEL (\n  name analytics.revenue_metrics,\n  kind INCREMENTAL_BY_TIME_RANGE (time_column metric_date),\n\n  -- Profile these columns\n  profiles (\n    revenue,\n    order_count,\n    customer_tier,\n    region\n  )\n);\n</code></pre>"},{"location":"concepts/checks/#profile-storage","title":"Profile Storage","text":"<p>Profiles are stored in the <code>_check_profiles</code> table:</p> <pre><code>SELECT\n  data_source,        -- Table name (e.g., 'analytics.customers')\n  column_name,        -- Column profiled\n  null_count,         -- Number of NULLs\n  null_percentage,    -- Percentage NULLs\n  distinct_count,     -- Number of unique values\n  duplicate_count,    -- Number of duplicates\n  min_value,          -- Minimum value (numeric/date)\n  max_value,          -- Maximum value (numeric/date)\n  avg_value,          -- Average (numeric)\n  stddev_value,       -- Standard deviation (numeric)\n  profiled_at,        -- When profile was collected\n  histogram           -- Distribution (JSON)\nFROM _check_profiles\nWHERE data_source = 'analytics.customers'\n  AND column_name = 'revenue'\nORDER BY profiled_at DESC;\n</code></pre>"},{"location":"concepts/checks/#querying-profiles","title":"Querying Profiles","text":""},{"location":"concepts/checks/#track-null-percentage-over-time","title":"Track null percentage over time","text":"<pre><code>SELECT\n  profiled_at::DATE as date,\n  null_percentage\nFROM _check_profiles\nWHERE data_source = 'analytics.customers'\n  AND column_name = 'email'\nORDER BY profiled_at DESC\nLIMIT 30;  -- Last 30 days\n</code></pre>"},{"location":"concepts/checks/#monitor-data-drift","title":"Monitor data drift","text":"<pre><code>WITH current AS (\n  SELECT distinct_count, avg_value\n  FROM _check_profiles\n  WHERE data_source = 'analytics.customers'\n    AND column_name = 'revenue'\n  ORDER BY profiled_at DESC\n  LIMIT 1\n),\nhistorical AS (\n  SELECT AVG(distinct_count) as avg_distinct, AVG(avg_value) as avg_revenue\n  FROM _check_profiles\n  WHERE data_source = 'analytics.customers'\n    AND column_name = 'revenue'\n    AND profiled_at &gt;= CURRENT_DATE - INTERVAL '30 days'\n)\nSELECT\n  c.distinct_count,\n  h.avg_distinct,\n  (c.distinct_count - h.avg_distinct) / h.avg_distinct * 100 as distinct_change_pct,\n  c.avg_value,\n  h.avg_revenue,\n  (c.avg_value - h.avg_revenue) / h.avg_revenue * 100 as revenue_change_pct\nFROM current c, historical h;\n</code></pre>"},{"location":"concepts/checks/#using-profiles-to-inform-checks","title":"Using Profiles to Inform Checks","text":"<p>Workflow:</p> <ol> <li>Enable profiling on new models</li> <li>Observe patterns for 30+ days</li> <li>Identify anomalies in profile data</li> <li>Create checks based on observed patterns</li> </ol> <p>Example:</p> <pre><code>-- Step 1: Enable profiling\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <pre><code>-- Step 2: Query profiles after 30 days\nSELECT\n  MIN(avg_value) as min_revenue,\n  MAX(avg_value) as max_revenue,\n  AVG(avg_value) as typical_revenue,\n  STDDEV(avg_value) as revenue_stddev\nFROM _check_profiles\nWHERE data_source = 'analytics.orders'\n  AND column_name = 'revenue'\n  AND profiled_at &gt;= CURRENT_DATE - INTERVAL '30 days';\n\n-- Results:\n-- min_revenue: 45000\n-- max_revenue: 75000\n-- typical_revenue: 58000\n-- revenue_stddev: 6000\n</code></pre> <pre><code># Step 3: Create checks based on observed patterns\nchecks:\n  analytics.orders:\n    accuracy:\n      - avg(revenue) between 40000 and 80000:\n          name: revenue_within_observed_range\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n</code></pre>"},{"location":"concepts/checks/#profile-best-practices","title":"Profile Best Practices","text":"<p>\u2705 DO: - Profile high-value production tables - Profile columns used in downstream analysis - Use profiles to understand new data sources - Query profiles to detect data drift - Use profiles to inform check thresholds</p> <p>\u274c DON'T: - Profile sensitive/PII columns (privacy risk) - Profile every column (performance overhead) - Profile temporary/experimental models - Use profiles as a replacement for checks - Profile very high-frequency models (storage cost)</p> <p>When to use profiles: - Building new models (understand the data) - Monitoring production tables - Detecting data drift - Informing audit/check strategy - Debugging data quality issues</p> <p>When to skip profiles: - Temporary models - Models with sensitive data - Very high-frequency models (&gt; 100 runs/day) - Models where you only need pass/fail validation</p>"},{"location":"concepts/checks/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"concepts/checks/#cross-model-validation","title":"Cross-Model Validation","text":"<p>Validate relationships between models:</p> <pre><code># checks/cross_model.yml\nchecks:\n  analytics.orders:\n    consistency:\n      - failed rows:\n          name: orphaned_orders\n          fail query: |\n            SELECT o.order_id, o.customer_id\n            FROM analytics.orders o\n            LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n            WHERE c.customer_id IS NULL\n          samples limit: 10\n          attributes:\n            description: \"All orders must have a valid customer\"\n\n      - failed rows:\n          name: revenue_mismatch\n          fail query: |\n            SELECT\n              o.order_id,\n              o.revenue as order_revenue,\n              r.revenue as revenue_table_revenue\n            FROM analytics.orders o\n            JOIN analytics.revenue r ON o.order_id = r.order_id\n            WHERE ABS(o.revenue - r.revenue) &gt; 0.01\n</code></pre>"},{"location":"concepts/checks/#time-based-validation","title":"Time-Based Validation","text":"<p>Ensure data timeliness:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - failed rows:\n          name: stale_data\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE updated_at &lt; CURRENT_TIMESTAMP - INTERVAL '24 hours'\n              AND status != 'completed'\n          attributes:\n            description: \"Pending orders should update within 24 hours\"\n\n      - failed rows:\n          name: future_dates\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE order_date &gt; CURRENT_DATE\n</code></pre>"},{"location":"concepts/checks/#statistical-outlier-detection","title":"Statistical Outlier Detection","text":"<p>Custom outlier detection:</p> <pre><code>checks:\n  analytics.revenue:\n    accuracy:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            WITH stats AS (\n              SELECT\n                AVG(revenue) as mean,\n                STDDEV(revenue) as stddev\n              FROM analytics.revenue\n            )\n            SELECT r.*,\n              (r.revenue - s.mean) / s.stddev as z_score\n            FROM analytics.revenue r, stats s\n            WHERE ABS((r.revenue - s.mean) / s.stddev) &gt; 3\n          samples limit: 20\n</code></pre>"},{"location":"concepts/checks/#best-practices","title":"Best Practices","text":""},{"location":"concepts/checks/#check-organization","title":"Check Organization","text":"<p>By domain:</p> <pre><code>checks/\n\u251c\u2500\u2500 customers/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u251c\u2500\u2500 validity.yml\n\u2502   \u2514\u2500\u2500 consistency.yml\n\u251c\u2500\u2500 orders/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u2514\u2500\u2500 timeliness.yml\n\u2514\u2500\u2500 revenue/\n    \u2514\u2500\u2500 accuracy.yml\n</code></pre> <p>By priority:</p> <pre><code>checks/\n\u251c\u2500\u2500 critical.yml      # Must never fail\n\u251c\u2500\u2500 important.yml     # Should rarely fail\n\u251c\u2500\u2500 monitoring.yml    # Track trends\n\u2514\u2500\u2500 experimental.yml  # Testing new checks\n</code></pre>"},{"location":"concepts/checks/#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive names:</p> <pre><code># \u274c Bad\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: check1\n\n# \u2705 Good\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_customer_emails\n          attributes:\n            description: \"All customers must have an email for marketing\"\n</code></pre> <p>Naming pattern: - <code>&lt;dimension&gt;_&lt;what&gt;_&lt;constraint&gt;</code> - Examples:   - <code>completeness_email_required</code>   - <code>validity_email_format</code>   - <code>uniqueness_email_no_duplicates</code>   - <code>timeliness_order_within_24hrs</code></p>"},{"location":"concepts/checks/#threshold-selection","title":"Threshold Selection","text":"<p>Start conservative, adjust based on data:</p> <pre><code># Step 1: Start with wide range\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_orders_v1\n\n# Step 2: Monitor for 30 days, see actual range: 5000-10000\n\n# Step 3: Tighten based on observed patterns\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 4000 and 12000:\n          name: sufficient_orders_v2\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n</code></pre> <p>Use profiles to inform thresholds:</p> <pre><code>-- Query profiles\nSELECT\n  MIN(metric_value) as min_observed,\n  MAX(metric_value) as max_observed,\n  AVG(metric_value) as typical,\n  STDDEV(metric_value) as stddev\nFROM check_results\nWHERE check_name = 'row_count'\n  AND executed_at &gt;= CURRENT_DATE - INTERVAL '90 days';\n\n-- Set threshold as: typical \u00b1 3*stddev\n</code></pre>"},{"location":"concepts/checks/#integration-strategy","title":"Integration Strategy","text":"<p>Layer validation:</p> <pre><code>-- LAYER 1: Audits (critical - blocks)\nMODEL (\n  name analytics.orders,\n  assertions (\n    not_null(columns := (order_id, customer_id)),\n    unique_values(columns := (order_id))\n  )\n);\n</code></pre> <pre><code># LAYER 2: Checks (monitoring - warns)\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 5000 and 15000:\n          name: order_count_in_range\n\n    timeliness:\n      - change for row_count &gt;= -30%:\n          name: order_count_stable\n</code></pre> <pre><code>-- LAYER 3: Profiles (observe - tracks)\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre>"},{"location":"concepts/checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/checks/#check-failures","title":"Check Failures","text":""},{"location":"concepts/checks/#investigate-failed-check","title":"Investigate failed check","text":"<pre><code># Run specific check with verbose output\nvulcan check --select analytics.customers.invalid_emails --verbose\n</code></pre>"},{"location":"concepts/checks/#query-failed-samples","title":"Query failed samples","text":"<pre><code>-- Get samples from last failed run\nSELECT *\nFROM check_samples\nWHERE check_name = 'invalid_emails'\n  AND status = 'failed'\nORDER BY executed_at DESC\nLIMIT 10;\n</code></pre>"},{"location":"concepts/checks/#performance-issues","title":"Performance Issues","text":""},{"location":"concepts/checks/#slow-check-queries","title":"Slow check queries","text":"<p>Problem: Check takes too long to run</p> <p>Solution 1: Add filters</p> <pre><code># \u274c Slow - scans entire table\nchecks:\n  analytics.orders:\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n\n# \u2705 Fast - filters to recent data\nchecks:\n  analytics.orders:\n    filter: \"order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n</code></pre> <p>Solution 2: Add indexes</p> <pre><code>-- Add index on frequently checked columns\nCREATE INDEX idx_orders_email ON analytics.orders(email);\nCREATE INDEX idx_orders_order_date ON analytics.orders(order_date);\n</code></pre>"},{"location":"concepts/checks/#false-positives","title":"False Positives","text":""},{"location":"concepts/checks/#threshold-too-strict","title":"Threshold too strict","text":"<p>Problem: Check fails during normal variance</p> <pre><code># \u274c Too strict\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count = 10000  # Exact match\n\n# \u2705 Allow variance\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 9000 and 11000  # \u00b110% variance\n</code></pre>"},{"location":"concepts/checks/#use-anomaly-detection-instead","title":"Use anomaly detection instead","text":"<pre><code># Replace strict threshold with ML-based detection\nchecks:\n  analytics.orders:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n</code></pre>"},{"location":"concepts/checks/#summary","title":"Summary","text":"<p>Quality checks provide a comprehensive way to monitor data quality over time:</p>"},{"location":"concepts/checks/#core-concepts","title":"Core Concepts","text":"<p>1. Quality Checks</p> <ul> <li>YAML-configured validation rules</li> <li>Non-blocking (don't stop pipelines)</li> <li>Track trends over time</li> <li>Integrate with Activity API</li> </ul> <p>2. Check Types</p> <ul> <li>Missing data checks (<code>missing_count</code>, <code>missing_percent</code>)</li> <li>Row count checks (<code>row_count</code>)</li> <li>Duplicate checks (<code>duplicate_count</code>)</li> <li>Failed rows (SQL-based)</li> <li>Anomaly detection (ML-based)</li> <li>Change monitoring (compare to previous)</li> </ul> <p>3. Data Profiling</p> <ul> <li>Automatic statistical metric collection</li> <li>Stored in <code>_check_profiles</code> table</li> <li>Observe patterns without validation</li> <li>Inform check threshold selection</li> </ul> <p>4. Data Quality Strategy</p> <ul> <li>Audits - Critical, blocking</li> <li>Checks - Monitoring, non-blocking</li> <li>Profiles - Observation, tracking</li> </ul>"},{"location":"concepts/environments/","title":"Environments","text":""},{"location":"concepts/environments/#environments","title":"Environments","text":"<p>Environments are isolated namespaces that allow you to test and preview your changes.</p> <p>Vulcan differentiates between production and development environments. Currently, only the environment with the name <code>prod</code> is treated by Vulcan as the production one. Environments with other names are considered to be development ones.</p> <p>Models in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name <code>db.model_a</code> in the target environment <code>my_dev</code>, the <code>db__my_dev.model_a</code> table name should be used in a query. Models in the production environment are referred to by their original names.</p>"},{"location":"concepts/environments/#why-use-environments","title":"Why use environments","text":"<p>Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.</p> <p>Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment do not impact other environments. At the same time, any computation that was done in this environment can be safely reused in other environments.</p>"},{"location":"concepts/environments/#how-to-use-environments","title":"How to use environments","text":"<p>When running the plan command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is <code>prod</code>, which refers to the production environment. Environment with names other than <code>prod</code> are considered to be development environments.</p> <p>By default, the <code>vulcan plan</code> command targets the production (<code>prod</code>) environment.</p>"},{"location":"concepts/environments/#example","title":"Example","text":"<p>A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name <code>my_dev</code>, run:</p> <p></p><pre><code>vulcan plan my_dev\n</code></pre> A new environment is created automatically the first time a plan is applied to it.<p></p>"},{"location":"concepts/environments/#how-environments-work","title":"How environments work","text":"<p>Whenever a model definition changes, a new model snapshot is created with a unique fingerprint. This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model snapshots.</p> <p>Refer to plans for additional details.</p>"},{"location":"concepts/environments/#date-range","title":"Date range","text":"<p>A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the plan creation.</p>"},{"location":"concepts/glossary/","title":"Glossary","text":""},{"location":"concepts/glossary/#glossary","title":"Glossary","text":""},{"location":"concepts/glossary/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.</p>"},{"location":"concepts/glossary/#backfill","title":"Backfill","text":"<p>Load or refresh model data, triggered by a vulcan plan command.</p>"},{"location":"concepts/glossary/#catalog","title":"Catalog","text":"<p>A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.</p>"},{"location":"concepts/glossary/#cicd","title":"CI/CD","text":"<p>An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with tests and audits.</p>"},{"location":"concepts/glossary/#cte","title":"CTE","text":"<p>A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to tests.</p>"},{"location":"concepts/glossary/#dag","title":"DAG","text":"<p>Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.</p>"},{"location":"concepts/glossary/#data-modeling","title":"Data modeling","text":"<p>Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.</p>"},{"location":"concepts/glossary/#data-pipeline","title":"Data pipeline","text":"<p>The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.</p>"},{"location":"concepts/glossary/#data-transformation","title":"Data transformation","text":"<p>Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.</p>"},{"location":"concepts/glossary/#data-warehouse","title":"Data warehouse","text":"<p>The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.</p>"},{"location":"concepts/glossary/#direct-modification","title":"Direct Modification","text":"<p>A change to a model's definition from the user instead of being inherited from an upstream dependency like Indirect Modification.</p>"},{"location":"concepts/glossary/#elt","title":"ELT","text":"<p>Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.</p>"},{"location":"concepts/glossary/#etl","title":"ETL","text":"<p>Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.</p>"},{"location":"concepts/glossary/#full-refresh","title":"Full refresh","text":"<p>In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.</p>"},{"location":"concepts/glossary/#idempotency","title":"Idempotency","text":"<p>The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.</p>"},{"location":"concepts/glossary/#incremental-loads","title":"Incremental Loads","text":"<p>Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See Model Kinds for more information.</p>"},{"location":"concepts/glossary/#indirect-modification","title":"Indirect Modification","text":"<p>A change to model's upstream dependency and not to the model itself like a Direct Modification.</p>"},{"location":"concepts/glossary/#integration","title":"Integration","text":"<p>Combining data from various sources (such as from a data warehouse) into one unified view.</p>"},{"location":"concepts/glossary/#lineage","title":"Lineage","text":"<p>The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.</p>"},{"location":"concepts/glossary/#physical-layer","title":"Physical Layer","text":"<p>The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the Vulcan virtual layer's views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.</p>"},{"location":"concepts/glossary/#plan-summaries","title":"Plan Summaries","text":"<p>An upcoming feature that allows users to see a summary of changes applied to a given environment.</p>"},{"location":"concepts/glossary/#semantic-understanding","title":"Semantic Understanding","text":"<p>Vulcan, by leveraging SQLGlot, understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.</p>"},{"location":"concepts/glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.</p>"},{"location":"concepts/glossary/#table","title":"Table","text":"<p>A table is the visual representation of data stored in rows and columns.</p>"},{"location":"concepts/glossary/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.</p>"},{"location":"concepts/glossary/#view","title":"View","text":"<p>A view is the result of a SQL query on a database.</p>"},{"location":"concepts/glossary/#virtual-environments","title":"Virtual Environments","text":"<p>Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See plan application for more information.</p>"},{"location":"concepts/glossary/#virtual-layer","title":"Virtual Layer","text":"<p>The virtual layer is Vulcan's abstraction layer over the physical layer and physical data storage. While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.</p>"},{"location":"concepts/glossary/#virtual-update","title":"Virtual Update","text":"<p>Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See Virtual Update for more information.</p>"},{"location":"concepts/glossary/#virtual-preview","title":"Virtual Preview","text":"<p>Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.</p>"},{"location":"concepts/overview/","title":"Overview","text":""},{"location":"concepts/overview/#overview","title":"Overview","text":"<p>This page provides a conceptual overview of what Vulcan does and how its components fit together.</p>"},{"location":"concepts/overview/#what-vulcan-is","title":"What Vulcan is","text":"<p>Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of engines and orchestrators.</p> <p>It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.</p> <p>You can use Vulcan with the CLI, Notebook, or Python APIs.</p>"},{"location":"concepts/overview/#how-vulcan-works","title":"How Vulcan works","text":""},{"location":"concepts/overview/#create-models","title":"Create models","text":"<p>You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.</p>"},{"location":"concepts/overview/#make-a-plan","title":"Make a plan","text":"<p>Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.</p> <p>Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change before you expend the time and resources to actually perform the computations.</p> <p>Vulcan automatically identifies all affected models and the computations a change entails by creating a \"Vulcan plan.\" When you execute the <code>plan</code> command, Vulcan generates the plan for the environment specified in the command (e.g., dev, test, prod).</p> <p>The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.</p> <p>Learn more about plans.</p>"},{"location":"concepts/overview/#apply-the-plan","title":"Apply the plan","text":"<p>After using <code>plan</code> to understand the impacts of a change in an environment, Vulcan offers to execute the computations by <code>apply</code>ing the plan. However, you must provide additional information that determines the scope of what computations are executed.</p> <p>The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.</p> <p>\"Backfilling\" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.</p> <p>Most business data is temporal \u2014 each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.</p> <p>The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.</p>"},{"location":"concepts/overview/#build-a-virtual-environment","title":"Build a Virtual Environment","text":"<p>Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.</p> <p>One challenge with using multiple data environments is that backfill and other computations must happen twice \u2014 once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.</p> <p>Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.</p> <p>Vulcan uses its knowledge of equivalent outputs to create a Virtual Environment. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but without computation or data movement.</p> <p>Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.</p>"},{"location":"concepts/overview/#test-your-code-and-data","title":"Test your code and data","text":"<p>Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.</p>"},{"location":"concepts/overview/#tests","title":"Tests","text":"<p>Vulcan \"tests\" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model code \u2014 you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.</p> <p>Vulcan automatically runs tests when you apply a <code>plan</code>, or you can run them on demand with the <code>test</code> command.</p>"},{"location":"concepts/overview/#audits","title":"Audits","text":"<p>In contrast to tests, Vulcan \"audits\" validate the results of model code applied to your actual data.</p> <p>You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure <code>your_field</code> has no <code>NULL</code> values would include <code>WHERE your_field IS NULL</code>. If any NULLs are detected, the query will return at least one row and the audit will fail.</p> <p>Audits are flexible \u2014 they can be tied to a specific model's contents, or you can use macros to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.</p> <p>You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.</p> <p>Vulcan automatically runs audits when you apply a <code>plan</code> to an environment, or you can run them on demand with the <code>audit</code> command.</p>"},{"location":"concepts/overview/#infrastructure-and-orchestration","title":"Infrastructure and orchestration","text":"<p>Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use \u2014 its only requirement is access to the target SQL/analytics engine.</p> <p>Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a <code>vulcan</code> schema in your data warehouse for its internal metadata.</p>"},{"location":"concepts/plans/","title":"Plans","text":""},{"location":"concepts/plans/#plans","title":"Plans","text":"<p>A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target environment. In order for any model changes to take effect in a target environment, a plan needs to be created and applied.</p> <p>During plan creation:</p> <ul> <li>The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.</li> <li>Users may be prompted to categorize changes to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through configuration.</li> <li>Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.</li> </ul> <p>The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:</p> <ul> <li>A list of added models</li> <li>A list of removed models</li> <li>A list of directly modified models and a text diff of changes that have been made</li> <li>A list of indirectly modified models</li> <li>Missing data intervals for affected models</li> <li>A date range that will be affected by the plan application</li> </ul> <p>To create a new plan, run the following command: </p><pre><code>vulcan plan [environment name]\n</code></pre><p></p> <p>If no environment name is specified, the plan is generated for the <code>prod</code> environment.</p>"},{"location":"concepts/plans/#change-categories","title":"Change categories","text":"<p>Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.</p> <p>If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.</p>"},{"location":"concepts/plans/#breaking-change","title":"Breaking change","text":"<p>If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.</p> <p>In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to backfilling).</p> <p>Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's <code>WHERE</code> clause is a breaking change because downstream models contain rows that would now be filtered out.</p>"},{"location":"concepts/plans/#non-breaking-change","title":"Non-breaking change","text":"<p>A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.</p> <p>This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.</p> <p>If any downstream models contain a <code>select *</code> from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.</p>"},{"location":"concepts/plans/#summary","title":"Summary","text":"Change Category Change Type Behaviour Breaking Direct or Indirect Backfill Non-breaking Direct Backfill Non-breaking Indirect No Backfill"},{"location":"concepts/plans/#forward-only-change","title":"Forward-only change","text":"<p>In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.</p> <p>A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the <code>prod</code> environment). This means that no backfill will take place.</p> <p>While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.</p> <p>In either case the data produced this way in the development environment can only be used for preview and will not be reused once the change is deployed to production. See Forward-only Plans for more details.</p> <p>This category is assigned by Vulcan automatically either when a user opts into using a forward-only plan or when a model is explicitly configured to be forward-only.</p>"},{"location":"concepts/plans/#plan-application","title":"Plan application","text":"<p>Once a plan has been created and reviewed, it is then applied to the target environment in order for its changes to take effect.</p> <p>Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a snapshot with a unique fingerprint is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for forward-only plans.</p> <p>When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.</p> <p></p> <p>Each model variant gets its own physical table while environments only contain references to these tables.</p> <p>This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.</p> <p>Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to Virtual Update) as long as its physical table hasn't been garbage collected by the janitor process.</p> <p>Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.</p>"},{"location":"concepts/plans/#backfilling","title":"Backfilling","text":"<p>Despite all the benefits, the approach described above is not without trade-offs.</p> <p>When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.</p> <p>We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:</p> <ul> <li>When a VIEW model is created</li> <li>When a FULL model is built</li> <li>When an INCREMENTAL model is built for the first time</li> <li>When an INCREMENTAL model has recent data appended to it</li> <li>When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)</li> </ul> <p>Note for incremental models: despite the fact that backfilling can happen incrementally (see <code>batch_size</code> parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a forward-only plan instead.</p>"},{"location":"concepts/plans/#virtual-update","title":"Virtual Update","text":"<p>A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.</p> <p>With this approach, the process of promoting a change to production is reduced to reference swapping.</p> <p>If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.</p>"},{"location":"concepts/plans/#start-and-end-dates","title":"Start and end dates","text":"<p>The <code>plan</code> command provides two temporal options: <code>--start</code> and <code>--end</code>. These options are only applicable to plans for non-prod environments.</p> <p>For context, every model has a start date. The start can be specified in the model definition, in the project configuration's <code>model_defaults</code>, or by Vulcan's default value of yesterday.</p> <p>Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the <code>plan</code> command's <code>--start</code> and <code>--end</code> options are not supported for regular plans against prod. The options are supported for restatement plans against prod to allow re-processing a subset of existing data.</p> <p>Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the <code>--start</code> and <code>--end</code> options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.</p>"},{"location":"concepts/plans/#model-kind-limitations","title":"Model kind limitations","text":"<p>Some model kinds do not support backfilling a limited date range.</p> <p>For context, Vulcan strives to make models idempotent, meaning that if we ran them multiple times we would get the same correct result every time.</p> <p>However, some model kinds are inherently non-idempotent:</p> <ul> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>SCD_TYPE_2_BY_TIME</li> <li>SCD_TYPE_2_BY_COLUMN</li> <li>Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)</li> </ul> <p>Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:</p> <ul> <li>If the <code>--start</code> option date is the same as or before the model's start date, the model is fully refreshed for all of time</li> <li>If the <code>--start</code> option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production</li> </ul>"},{"location":"concepts/plans/#example","title":"Example","text":"<p>Consider a Vulcan project with a default start date of 2024-09-20.</p> <p>It contains the following <code>INCREMENTAL_BY_UNIQUE_KEY</code> model that specifies an explicit start date of 2024-09-23:</p> <pre><code>MODEL (\n  name vulcan_example.start_end_model,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key item_id\n  ),\n  start '2024-09-23'\n);\n\nSELECT\n  item_id,\n  num_orders\nFROM\n  vulcan_example.full_model\n</code></pre> <p>When we run the project's first plan, we see that Vulcan correctly detected a different start date for our <code>start_end_model</code> than the other models (which have the project default start of 2024-09-20):</p> <pre><code>\u276f vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u251c\u2500\u2500 vulcan_example.seed_model\n    \u2514\u2500\u2500 vulcan_example.start_end_model\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example.full_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.incremental_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.seed_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example.start_end_model: 2024-09-23 - 2024-09-26\nApply - Backfill Tables [y/n]:\n</code></pre> <p>After executing that plan, we add columns to both the <code>incremental_model</code> and <code>start_end_model</code> queries.</p> <p>We then execute <code>vulcan plan dev</code> to create the new <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre> <p>Note two things about the output:</p> <ol> <li>As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for <code>incremental_model</code> and 2024-09-23 for <code>start_end_model</code></li> <li>Vulcan prompted us for a backfill start date because we didn't pass the <code>--start</code> option to the <code>vulcan plan dev</code> command</li> </ol> <p>Let's cancel that plan and start a new one, passing a start date of 2024-09-24.</p> <p>The <code>start_end_model</code> is of kind <code>INCREMENTAL_BY_UNIQUE_KEY</code>, which is non-idempotent and cannot be backfilled for a limited time range.</p> <p>Because the command's <code>--start</code> of 2024-09-24 is after <code>start_end_model</code>'s start date 2024-09-23, <code>start_end_model</code> is marked as preview:</p> <pre><code>\u276f vulcan plan dev --start 2024-09-24\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':\n</code></pre>"},{"location":"concepts/plans/#minimum-intervals","title":"Minimum intervals","text":"<p>When you run a plan with a fixed <code>--start</code> or <code>--end</code> date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.</p> <p>For example, if you have a model like so:</p> <pre><code>MODEL(\n    name vulcan_example.monthly_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column month\n    ),\n    cron '@monthly'\n);\n\nSELECT SUM(a) AS sum_a, MONTH(day) AS month\nFROM vulcan_example.upstream_model\nWHERE day BETWEEN @start_ds AND @end_ds\n</code></pre> <p>make a change to it and run the following:</p> <pre><code>$ vulcan plan dev --start '1 day ago' \n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\nSKIP: No model batches to execute\n</code></pre> <p>No data will be backfilled because <code>1 day ago</code> does not contain a complete month. However, you can use the <code>--min-intervals</code> option to override this behaviour like so:</p> <pre><code>$ vulcan plan dev --start '1 day ago' --min-intervals 1\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\n[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   \nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00                                                             \n\n\u2714 Model batches executed\n</code></pre> <p>This will ensure that regardless of the plan <code>--start</code> date, all added or modified models will have at least <code>--min-intervals</code> intervals considered for backfill.</p> <p>Info</p> <p>If you are running plans manually you can just adjust the <code>--start</code> date to be wide enough to cover the models in question.</p> <p>The <code>--min-intervals</code> option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) \"2 weeks worth of data\" in the target environment.</p>"},{"location":"concepts/plans/#data-preview-for-forward-only-changes","title":"Data preview for forward-only changes","text":"<p>As mentioned earlier, the data output produced by forward-only changes in a development environment can only be used for preview and will not be reused in production.</p> <p>The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.</p> <p>Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with <code>(preview)</code> in the plan summary: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28\n\u251c\u2500\u2500 sushi__dev.top_waiters: 2023-12-22 - 2023-12-28\n\u2514\u2500\u2500 sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)\n</code></pre><p></p>"},{"location":"concepts/plans/#forward-only-plans","title":"Forward-only plans","text":"<p>Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.</p> <p>The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and restatements.</p> <p>Note that once a forward-only change is applied to <code>prod</code>, all development environments that referred to the previous versions of the updated models will be impacted.</p> <p>A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, <code>vulcan plan [environment name]</code> evaluates code in non-<code>prod</code> environments while targeting shallow (a.k.a. \"zero-copy\") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.</p> <p>This means that only a limited preview of changes is available in the development environment before the change is promoted to <code>prod</code>. The date range of the preview is provided as part of plan creation command.</p> <p>Engines for which table cloning is supported include:</p> <ul> <li><code>BigQuery</code></li> <li><code>Databricks</code></li> <li><code>Snowflake</code></li> </ul> <p>Note that all changes made as part of a forward-only plan automatically get a forward-only category assigned to them. These types of changes can't be mixed together with breaking and non-breaking changes within the same plan.</p> <p>To create a forward-only plan, add the <code>--forward-only</code> option to the <code>plan</code> command: </p><pre><code>vulcan plan [environment name] --forward-only\n</code></pre><p></p> <p>Note</p> <p>The <code>--forward-only</code> flag is not required when applying changes to models that have been explicitly configured as forward-only.</p> <p>Use it only if you need to provide a time range for the preview window or the effective date.</p>"},{"location":"concepts/plans/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to forward-only models - learn more here.</p> <p>Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy: </p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models.  Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. </p> <p>For example, to allow destructive changes to all models in the <code>analytics</code> schema: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"analytics.*\"\n</code></pre><p></p> <p>Or to allow destructive changes to multiple specific models: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"sales.revenue_model\" --allow-destructive-model \"marketing.campaign_model\"\n</code></pre><p></p> <p>Learn more about model selectors here.</p>"},{"location":"concepts/plans/#effective-date","title":"Effective date","text":"<p>Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:</p> <pre><code>vulcan plan --forward-only --effective-from 2023-01-01\n</code></pre> <p>This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.</p>"},{"location":"concepts/plans/#restatement-plans","title":"Restatement plans","text":"<p>Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.</p> <p>For example, these scenarios all require re-evaluating model data that already exists:</p> <ul> <li>Correcting an upstream data issue by reprocessing some of a model's existing data</li> <li>Retroactively applying a forward-only plan change to some historical data</li> <li>Fully refreshing a model</li> </ul> <p>In Vulcan, reprocessing existing data is called a \"restatement.\"</p> <p>Restate one or more models' data with the <code>plan</code> command's <code>--restate-model</code> selector. The selector lets you specify which models to restate by name, wildcard, or tag (syntax below).</p> <p>No changes allowed</p> <p>Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.</p> <p>You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running <code>vulcan plan</code> without the <code>--restate-model</code> option.</p> <p>Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.</p> <p>You may restate external models. An external model is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.</p> <p>The plan's <code>--start</code> and <code>--end</code> date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more below.</p> <p>Just catching up</p> <p>Restatement plans \"catch models up\" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.</p> <p>If you pass an <code>--end</code> date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.</p> <p>To prevent models from ever being restated, set the disable_restatement attribute to <code>true</code>.</p> <p> These examples demonstrate how to select which models to restate based on model names or model tags.</p> Names OnlyUpstreamWildcardsUpstream + WildcardsSpecific Date Range <pre><code>vulcan plan --restate-model \"db.model_a\" --restate-model \"tag:expensive\"\n</code></pre> <pre><code># All selected models (including upstream models) will also include their downstream models\nvulcan plan --restate-model \"+db.model_a\" --restate-model \"+tag:expensive\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db*\" --restate-model \"tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"+db*\" --restate-model \"+tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre>"},{"location":"concepts/plans/#restating-production-vs-development","title":"Restating production vs development","text":"<p>Restatement plans behave differently depending on if you're targeting the <code>prod</code> environment or a development environment.</p> <p>If you target a development environment by including an environment name like <code>dev</code>:</p> <pre><code>vulcan plan dev --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the requested intervals for the specified model in the <code>dev</code> environment. In other environments, the model will be unaffected.</p> <p>However, if you target the <code>prod</code> environment by omitting an environment name:</p> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the intervals in the <code>prod</code> table and clear the model's time intervals from state in every other environment.</p> <p>The next time you do a run in <code>dev</code>, the intervals already reprocessed in <code>prod</code> are reprocessed in <code>dev</code> as well. This is to prevent old data from getting promoted to <code>prod</code> in the future.</p> <p>This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:</p> <ul> <li>Table <code>A</code> exists in <code>prod</code></li> <li>A virtual environment <code>dev</code> is created with new tables <code>B</code> and <code>C</code> downstream of <code>A</code><ul> <li>the DAG in <code>prod</code> looks like <code>A</code></li> <li>the DAG in <code>dev</code> looks like <code>A &lt;- B &lt;- C</code></li> </ul> </li> <li>A restatement plan is executed against table <code>A</code> in <code>prod</code></li> <li>Vulcan will clear the affected intervals for <code>B</code> and <code>C</code> in <code>dev</code> even though those tables do not exist in <code>prod</code></li> </ul> <p>Bringing development environments up to date</p> <p>A restatement plan against <code>prod</code> clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.</p> <p>Execute <code>vulcan run &lt;environment name&gt;</code> to trigger reprocessing in the development environment.</p> <p>This is necessary because a <code>prod</code> restatement plan only does work in the <code>prod</code> environment for speed and efficiency.</p>"},{"location":"concepts/state/","title":"State","text":""},{"location":"concepts/state/#state","title":"State","text":"<p>Vulcan stores information about your project in a state database that is usually separate from your main warehouse.</p> <p>The Vulcan state database contains:</p> <ul> <li>Information about every Model Version in your project (query, loaded intervals, dependencies)</li> <li>A list of every Virtual Data Environment in the project</li> <li>Which model versions are promoted into each Virtual Data Environment</li> <li>Information about any auto restatements present in your project</li> <li>Other metadata about your project such as current Vulcan / SQLGlot version</li> </ul> <p>The state database is how Vulcan \"remembers\" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for incremental models so you dont need to add branching logic into the model query to handle this.</p> <p>State database performance</p> <p>The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.</p> <p>For the best experience, we recommend databases designed for OLTP workloads such as PostgreSQL.</p> <p>Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and will lead to poor performance and consistency.</p> <p>For more information on engines suitable for the Vulcan state database, see the configuration guide.</p>"},{"location":"concepts/state/#exporting-importing-state","title":"Exporting / Importing State","text":"<p>Vulcan supports exporting the state database to a <code>.json</code> file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.</p>"},{"location":"concepts/state/#exporting-state","title":"Exporting state","text":"<p>Vulcan can export the state database to a file like so:</p> <pre><code>$ vulcan state export -o state.json\nExporting state to 'state.json' from the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\nContinue? [y/n]: y\n\n    Exporting versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Exporting snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nExporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState exported successfully to 'state.json'\n</code></pre> <p>This will produce a file <code>state.json</code> in the current directory containing the Vulcan state.</p> <p>The state file is a simple <code>json</code> file that looks like:</p> <pre><code>{\n    /* State export metadata */\n    \"metadata\": {\n        \"timestamp\": \"2025-03-16 23:09:00+00:00\", /* UTC timestamp of when the file was produced */\n        \"file_version\": 1, /* state export file format version */\n        \"importable\": true /* whether or not this file can be imported with `vulcan state import` */\n    },\n    /* Library versions used to produce this state export file */\n    \"versions\": {\n        \"schema_version\": 76 /* vulcan state database schema version */,\n        \"sqlglot_version\": \"26.10.1\" /* version of SQLGlot used to produce the state file */,\n        \"vulcan_version\": \"0.165.1\" /* version of Vulcan used to produce the state file */,\n    },\n    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */\n    \"snapshots\": [\n        { \"name\": \"...\" }\n    ],\n    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */\n    \"environments\": {\n        \"prod\": {\n            /* information about the environment itself */\n            \"environment\": {\n                \"...\"\n            },\n            /* information about any before_all / after_all statements for this environment */\n            \"statements\": [\n                \"...\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"concepts/state/#specific-environments","title":"Specific environments","text":"<p>You can export a specific environment like so:</p> <pre><code>$ vulcan state export --environment my_dev -o my_dev_state.json\n</code></pre> <p>Note that every snapshot that is part of the environment will be exported, not just the differences from <code>prod</code>. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.</p>"},{"location":"concepts/state/#local-state","title":"Local state","text":"<p>You can export local state like so:</p> <pre><code>$ vulcan state export --local -o local_state.json\n</code></pre> <p>This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.</p> <p>Therefore, a local state export will only have <code>snapshots</code> populated. <code>environments</code> will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as not importable so it cannot be used with a subsequent <code>vulcan state import</code> command.</p>"},{"location":"concepts/state/#importing-state","title":"Importing state","text":"<p>Back up your state database first!</p> <p>Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.</p> <p>Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means a import error has the potential to leave the state database in an inconsistent state.</p> <p>Vulcan can import a state file into the state database like so:</p> <pre><code>$ vulcan state import -i state.json --replace\nLoading state from 'state.json' into the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\n[WARNING] This destructive operation will delete all existing state against the 'dev' gateway\nand replace it with what\\'s in the 'state.json' file.\n\nAre you sure? [y/n]: y\n\nState File Information:\n\u251c\u2500\u2500 Creation Timestamp: 2025-03-31 02:15:00+00:00\n\u251c\u2500\u2500 File Version: 1\n\u251c\u2500\u2500 Vulcan version: 0.170.1.dev0\n\u251c\u2500\u2500 Vulcan migration version: 76\n\u2514\u2500\u2500 SQLGlot version: 26.12.0\n\n    Importing versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Importing snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nImporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState imported successfully from 'state.json'\n</code></pre> <p>Note that the state database structure needs to be present and up to date, so run <code>vulcan migrate</code> before running <code>vulcan state import</code> if you get a version mismatch error.</p> <p>If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the <code>--replace</code> parameter:</p> <pre><code>$ vulcan state import -i state.json\n...\n\n[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.\nMatching snapshots or environments will be replaced.\nNon-matching snapshots or environments will be ignored.\n\nAre you sure? [y/n]: y\n\n...\nState imported successfully from 'state.json'\n</code></pre>"},{"location":"concepts/state/#specific-gateways","title":"Specific gateways","text":"<p>If your project has multiple gateways with different state connections per gateway, you can target the state_connection of a specific gateway like so:</p> <pre><code># state export\n$ vulcan --gateway &lt;gateway&gt; state export -o state.json\n\n# state import\n$ vulcan --gateway &lt;gateway&gt; state import -i state.json\n</code></pre>"},{"location":"concepts/state/#version-compatibility","title":"Version Compatibility","text":"<p>When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.</p> <p>If you attempt to import state with an incompatible version, you will get the following error:</p> <pre><code>$ vulcan state import -i state.json\n...SNIP...\n\nState import failed!\nError: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.\nPlease upgrade/downgrade your Vulcan version to match the state file before performing the import.\n</code></pre>"},{"location":"concepts/state/#upgrading-a-state-file","title":"Upgrading a state file","text":"<p>You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:</p> <ul> <li>Loading it into a local database using the older Vulcan version</li> <li>Installing the newer Vulcan version</li> <li>Running <code>vulcan migrate</code> to upgrade the state within the local database</li> <li>Running <code>vulcan state export</code> to export it back out again. The new export is now compatible with the newer version of Vulcan.</li> </ul> <p>Below is an example of how to upgrade a state file created with Vulcan <code>0.164.1</code> to be compatible with Vulcan <code>0.165.1</code>.</p> <p>First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:</p> <pre><code>$ python -m venv migration-env\n\n$ . ./migration-env/bin/activate\n\n(migration-env)$\n</code></pre> <p>Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg <code>the state file was created with '0.164.1'</code> means you need to install Vulcan <code>0.164.1</code>:</p> <pre><code>(migration-env)$ pip install \"vulcan==0.164.1\"\n</code></pre> <p>Add a gateway to your <code>config.yaml</code> like so:</p> <pre><code>gateways:\n  migration:\n    connection:\n      type: duckdb\n      database: ./state-migration.duckdb\n</code></pre> <p>The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the <code>model_defaults</code> from your project in order to migrate state correctly which is why we have not used an isolated directory.</p> <p>Warning</p> <p>From here on, be sure to specify <code>--gateway migration</code> to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway</p> <p>You can now import your state export using the same version of Vulcan it was created with:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n\n(migration-env)$ vulcan --gateway migration state import -i state.json\n...\nState imported successfully from 'state.json'\n</code></pre> <p>Now we have the state imported, we can upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, eg <code>You are running '0.165.1'</code></p> <p>To upgrade Vulcan, simply install the new version:</p> <pre><code>(migration-env)$ pip install --upgrade \"vulcan==0.165.1\"\n</code></pre> <p>Migrate the state to the new version:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n</code></pre> <p>And finally, create a new state file which is now compatible with the new Vulcan version:</p> <pre><code> (migration-env)$ vulcan --gateway migration state export -o state-migrated.json\n</code></pre> <p>The <code>state-migrated.json</code> file is now compatible with the newer version of Vulcan. You can then transfer it to the place you originally needed it and import it in:</p> <pre><code>$ vulcan state import -i state-migrated.json\n...\nState imported successfully from 'state-migrated.json'\n</code></pre>"},{"location":"concepts/tests/","title":"Testing","text":""},{"location":"concepts/tests/#testing","title":"Testing","text":"<p>Testing allows you to protect your project from regression by continuously verifying that the output of each model matches your expectations. Unlike audits, tests are executed either on demand (for example, as part of a CI/CD job) or every time a new plan is created.</p> <p>Similar to unit testing in software development, Vulcan evaluates the model's logic against predefined inputs and then compares the output to expected outcomes provided as part of each test.</p> <p>A comprehensive suite of tests can empower data practitioners to work with confidence, as it allows them to ensure models behave as expected after changes have been applied to them.</p>"},{"location":"concepts/tests/#creating-tests","title":"Creating tests","text":"<p>A test suite is a YAML file contained in the <code>tests/</code> folder of a Vulcan project, whose name begins with <code>test</code> and ends with either <code>.yaml</code> or <code>.yml</code>. It can contain one or more uniquely named unit tests, each with a number of attributes that define its behavior.</p> <p>At minimum, a unit test must specify the model being tested, the input values for its upstream models, and the expected outputs for the target model's query and/or its Common Table Expressions. Other optional attributes include a description, the gateway to use, and a mapping that assigns values to macro variables referenced in the model.</p> <p>Learn more about the supported attributes in the unit test structure section.</p>"},{"location":"concepts/tests/#example","title":"Example","text":"<p>In this example, we'll use the <code>vulcan_example.full_model</code> model, which is provided as part of the <code>vulcan init</code> command and is defined as follows:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model aggregates the number of orders per <code>item_id</code> from the upstream <code>vulcan_example.incremental_model</code>. One way to test it is shown below:</p> <pre><code>test_example_full_model:\n  model: vulcan_example.full_model\n  inputs:\n    vulcan_example.incremental_model:\n      rows:\n      - id: 1\n        item_id: 1\n      - id: 2\n        item_id: 1\n      - id: 3\n        item_id: 2\n  outputs:\n    query:\n      rows:\n      - item_id: 1\n        num_orders: 2\n      - item_id: 2\n        num_orders: 1\n</code></pre> <p>This test verifies that <code>vulcan_example.full_model</code> correctly counts the number of orders per <code>item_id</code>. It provides three rows as input to <code>vulcan_example.incremental_model</code> and expects two rows as output from the target model's query.</p>"},{"location":"concepts/tests/#testing-ctes","title":"Testing CTEs","text":"<p>Individual CTEs within the model's query can also be tested. To demonstrate this, let's slightly modify the query of <code>vulcan_example.full_model</code> to include a CTE named <code>filtered_orders_cte</code>:</p> <pre><code>WITH filtered_orders_cte AS (\n  SELECT\n    id,\n    item_id\n  FROM\n    vulcan_example.incremental_model\n  WHERE\n    item_id = 1\n)\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\nFROM\n  filtered_orders_cte\nGROUP BY item_id\n</code></pre> <p>The following test verifies the output of this CTE before aggregation takes place:</p> <pre><code>test_example_full_model:\n  model: vulcan_example.full_model\n  inputs:\n    vulcan_example.incremental_model:\n        rows:\n        - id: 1\n          item_id: 1\n        - id: 2\n          item_id: 1\n        - id: 3\n          item_id: 2\n  outputs:\n    ctes:\n      filtered_orders_cte:\n        rows:\n          - id: 1\n            item_id: 1\n          - id: 2\n            item_id: 1\n    query:\n      rows:\n      - item_id: 1\n        num_orders: 2\n</code></pre>"},{"location":"concepts/tests/#supported-data-formats","title":"Supported data formats","text":"<p>Vulcan currently supports the following ways to define input and output data in unit tests:</p> <ol> <li>Listing YAML dictionaries where columns are mapped to their values for each row</li> <li>Listing the rows as comma-separated values (CSV)</li> <li>Executing a SQL query against the testing connection to generate the data</li> </ol> <p>The previous examples demonstrate the first method, which is the default way to define data in unit tests. The following examples will cover the remaining methods.</p>"},{"location":"concepts/tests/#defining-data-as-csv","title":"Defining data as CSV","text":"<p>This is how we could define the same test as in the first example, but with the input data formatted as CSV:</p> <pre><code>test_example_full_model:\n  model: vulcan_example.full_model\n  inputs:\n    vulcan_example.incremental_model:\n      format: csv\n      rows: |\n        id,item_id\n        1,1\n        2,1\n        3,2\n  outputs:\n    query:\n      rows:\n      - item_id: 1\n        num_orders: 2\n      - item_id: 2\n        num_orders: 1\n</code></pre>"},{"location":"concepts/tests/#generating-data-using-sql-queries","title":"Generating data using SQL queries","text":"<p>This is how we could define the same test as in the first example, but with the input data generated from a SQL query:</p> <pre><code>test_example_full_model:\n  model: vulcan_example.full_model\n  inputs:\n    vulcan_example.incremental_model:\n      query: |\n        SELECT 1 AS id, 1 AS item_id\n        UNION ALL\n        SELECT 2 AS id, 1 AS item_id\n        UNION ALL\n        SELECT 3 AS id, 2 AS item_id\n  outputs:\n    query:\n      rows:\n      - item_id: 1\n        num_orders: 2\n      - item_id: 2\n        num_orders: 1\n</code></pre>"},{"location":"concepts/tests/#using-files-to-populate-data","title":"Using files to populate data","text":"<p>Vulcan supports loading data from external files. To achieve this, you can use the <code>path</code> attribute, which specifies the pathname of the data to be loaded:</p> <pre><code>test_example_full_model:\n  model: vulcan_example.full_model\n  inputs:\n    vulcan_example.incremental_model:\n      format: csv\n      path: filepath/test_data.csv\n</code></pre> <p>When <code>format</code> is omitted, the file will be loaded as a YAML document.</p>"},{"location":"concepts/tests/#omitting-columns","title":"Omitting columns","text":"<p>Defining the complete inputs and expected outputs for wide tables, i.e. tables with many columns, can become cumbersome. Therefore, if certain columns can be safely ignored they may be omitted from any row and their value will be treated as <code>NULL</code> for that row.</p> <p>Additionally, it's possible to test only a subset of the output columns by setting <code>partial</code> to <code>true</code> for the outputs of interest:</p> <pre><code>  outputs:\n    query:\n      partial: true\n      rows:\n        - &lt;column_name&gt;: &lt;column_value&gt;\n          ...\n</code></pre> <p>This is useful when the missing columns can't be treated as <code>NULL</code>, but we still want to ignore them. In order to apply this setting to all expected outputs, set it under the <code>outputs</code> key:</p> <pre><code>  outputs:\n    partial: true\n    ...\n</code></pre>"},{"location":"concepts/tests/#freezing-time","title":"Freezing time","text":"<p>Some models may use SQL expressions that compute datetime values at a given point in time, such as <code>CURRENT_TIMESTAMP</code>. Since these expressions are non-deterministic, it's not enough to simply specify an expected output value in order to test them.</p> <p>Setting the <code>execution_time</code> macro variable addresses this problem by mocking out the current time in the context of the test, thus making its value deterministic.</p> <p>The following example demonstrates how <code>execution_time</code> can be used to test a column that is computed using <code>CURRENT_TIMESTAMP</code>. The model we're going to test is defined as:</p> <pre><code>MODEL (\n  name colors,\n  kind FULL\n);\n\nSELECT\n  'Yellow' AS color,\n  CURRENT_TIMESTAMP AS created_at\n</code></pre> <p>And the corresponding test is:</p> <pre><code>test_colors:\n  model: colors\n  outputs:\n    query:\n      - color: \"Yellow\"\n        created_at: \"2023-01-01 12:05:03\"\n  vars:\n    execution_time: \"2023-01-01 12:05:03\"\n</code></pre> <p>It's also possible to set a time zone for <code>execution_time</code>, by including it in the timestamp string.</p> <p>If a time zone is provided, it is currently required that the test's expected datetime values are timestamps without time zone, meaning that they need to be offset accordingly.</p> <p>Here's how we would write the above test if we wanted to freeze the time to UTC+2:</p> <pre><code>test_colors:\n  model: colors\n  outputs:\n    query:\n      - color: \"Yellow\"\n        created_at: \"2023-01-01 10:05:03\"\n  vars:\n    execution_time: \"2023-01-01 12:05:03+02:00\"\n</code></pre>"},{"location":"concepts/tests/#parameterized-model-names","title":"Parameterized model names","text":"<p>Testing models with parameterized names, such as <code>@{gold}.some_schema.some_table</code>, is possible using Jinja:</p> <pre><code>test_parameterized_model:\n  model: {{ var('gold') }}.some_schema.some_table\n  ...\n</code></pre> <p>For example, assuming <code>gold</code> is a config variable with value <code>gold_db</code>, the above test would be rendered as:</p> <pre><code>test_parameterized_model:\n  model: gold_db.some_schema.some_table\n  ...\n</code></pre>"},{"location":"concepts/tests/#automatic-test-generation","title":"Automatic test generation","text":"<p>Creating tests manually can be repetitive and error-prone, which is why Vulcan also provides a way to automate this process using the <code>create_test</code> command.</p> <p>This command can generate a complete test for a given model, as long as the tables of its upstream models exist in the project's data warehouse and are already populated with data.</p>"},{"location":"concepts/tests/#example_1","title":"Example","text":"<p>In this example, we'll show how to generate a test for <code>vulcan_example.incremental_model</code>, which is another model provided as part of the <code>vulcan init</code> command and is defined as follows:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Firstly, we need to specify the input data for the upstream model <code>vulcan_example.seed_model</code>. The <code>create_test</code> command starts by executing a user-supplied query against the project's data warehouse to fetch this data.</p> <p>For instance, the following query will return three rows from the table corresponding to the model <code>vulcan_example.seed_model</code>:</p> <pre><code>SELECT * FROM vulcan_example.seed_model LIMIT 3\n</code></pre> <p>Next, notice that <code>vulcan_example.incremental_model</code> contains a filter which references the <code>@start_date</code> and <code>@end_date</code> macro variables.</p> <p>To make the generated test deterministic and thus ensure that it will always succeed, we need to define these variables and modify the above query to constrain <code>event_date</code> accordingly.</p> <p>If we set <code>@start_date</code> to <code>'2020-01-01'</code> and <code>@end_date</code> to <code>'2020-01-04'</code>, the above query needs to be changed to:</p> <pre><code>SELECT * FROM vulcan_example.seed_model WHERE event_date BETWEEN '2020-01-01' AND '2020-01-04' LIMIT 3\n</code></pre> <p>Finally, combining it with the proper macro variable definitions, we can compute the expected output for the model's query in order to generate the complete test.</p> <p>This can be achieved using the following command:</p> <pre><code>$ vulcan create_test vulcan_example.incremental_model --query vulcan_example.seed_model \"SELECT * FROM vulcan_example.seed_model WHERE event_date BETWEEN '2020-01-01' AND '2020-01-04' LIMIT 3\" --var start '2020-01-01' --var end '2020-01-04'\n</code></pre> <p>Running this creates the following new test, located at <code>tests/test_incremental_model.yaml</code>:</p> <pre><code>test_incremental_model:\n  model: vulcan_example.incremental_model\n  inputs:\n    vulcan_example.seed_model:\n    - id: 1\n      item_id: 2\n      event_date: 2020-01-01\n    - id: 2\n      item_id: 1\n      event_date: 2020-01-01\n    - id: 3\n      item_id: 3\n      event_date: 2020-01-03\n  outputs:\n    query:\n    - id: 1\n      item_id: 2\n      event_date: 2020-01-01\n    - id: 2\n      item_id: 1\n      event_date: 2020-01-01\n    - id: 3\n      item_id: 3\n      event_date: 2020-01-03\n  vars:\n    start: '2020-01-01'\n    end: '2020-01-04'\n</code></pre> <p>As you can see, we now have two passing tests. Hooray!</p> <pre><code>$ vulcan test\n.\n----------------------------------------------------------------------\nRan 2 tests in 0.024s\n\nOK\n</code></pre>"},{"location":"concepts/tests/#using-a-different-testing-connection","title":"Using a different testing connection","text":"<p>The testing connection can be changed for a given test. This may be useful when, e.g., the model being tested cannot be correctly transpiled to the dialect of the default testing engine.</p> <p>The following example demonstrates this by modifying <code>test_example_full_model</code>, so that it runs against a single-threaded local Spark process, defined as the <code>test_connection</code> of the <code>spark_testing</code> gateway in the project's <code>config.yaml</code> file:</p> <pre><code>gateways:\n  local:\n    connection:\n      type: duckdb\n      database: db.db\n  spark_testing:\n    test_connection:\n      type: spark\n      config:\n        # Run Spark locally with one worker thread\n        \"spark.master\": \"local\"\n\n        # Move data under /tmp so that it is only temporarily persisted\n        \"spark.sql.warehouse.dir\": \"/tmp/data_dir\"\n        \"spark.driver.extraJavaOptions\": \"-Dderby.system.home=/tmp/derby_dir\"\n\ndefault_gateway: local\n\nmodel_defaults:\n  dialect: duckdb\n</code></pre> <p>The test would then be updated as follows:</p> <pre><code>test_example_full_model:\n  gateway: spark_testing\n  # ... the other test attributes remain the same\n</code></pre>"},{"location":"concepts/tests/#running-tests","title":"Running tests","text":"<p>Tests run automatically every time a new plan is created, but they can also be executed on demand as described in the following sections.</p>"},{"location":"concepts/tests/#testing-using-the-cli","title":"Testing using the CLI","text":"<p>You can execute tests on demand using the <code>vulcan test</code> command as follows:</p> <pre><code>$ vulcan test\n.\n----------------------------------------------------------------------\nRan 1 test in 0.005s\n\nOK\n</code></pre> <p>The command returns a non-zero exit code if there are any failures, and reports them in the standard error stream:</p> <pre><code>$ vulcan test\nF\n======================================================================\nFAIL: test_example_full_model (test/tests/test_full_model.yaml)\n----------------------------------------------------------------------\nAssertionError: Data mismatch (exp: expected, act: actual)\n\n  num_orders\n         exp  act\n0        3.0  2.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.012s\n\nFAILED (failures=1)\n</code></pre> <p>Note: when there are many differing columns, the corresponding dataframe will be truncated by default. In order to fully display them, use the <code>-v</code> (verbose) option of the <code>vulcan test</code> command.</p> <p>To run a specific model test, pass in the suite file name followed by <code>::</code> and the name of the test:</p> <pre><code>$ vulcan test tests/test_full_model.yaml::test_example_full_model\n</code></pre> <p>You can also run tests that match a pattern or substring using a glob pathname expansion syntax:</p> <pre><code>$ vulcan test tests/test_*\n</code></pre>"},{"location":"concepts/tests/#testing-using-notebooks","title":"Testing using notebooks","text":"<p>You can execute tests on demand using the <code>%run_test</code> notebook magic as follows:</p> <pre><code># This import will register all needed notebook magics\nIn [1]: import vulcan\n        %run_test\n\n        ----------------------------------------------------------------------\n        Ran 1 test in 0.018s\n\n        OK\n</code></pre> <p>The <code>%run_test</code> magic supports the same options as the corresponding CLI command.</p>"},{"location":"concepts/tests/#troubleshooting-issues","title":"Troubleshooting issues","text":"<p>When executing unit tests, Vulcan creates input fixtures as views within the testing connection.</p> <p>These fixtures are dropped by default after the execution completes, but it is possible to preserve them using the <code>--preserve-fixtures</code> option available in both the <code>vulcan test</code> CLI command and the <code>%run_test</code> notebook magic.</p> <p>This can be helpful when debugging a test failure, because for example it's possible to query the fixture views directly and verify that they are defined correctly.</p> <p>Note</p> <p>By default, the views that are necessary to run a unit test are created within a new, unique schema, whose name looks like <code>vulcan_test_&lt;random_ID&gt;</code>. To specify a custom name for this schema, set the <code>&lt;test_name&gt;.schema</code> test attribute.</p>"},{"location":"concepts/tests/#type-mismatches","title":"Type mismatches","text":"<p>It's not always possible to correctly interpret certain values in a unit test without additional context. For example, a YAML dictionary can be used to represent both a <code>STRUCT</code> and a <code>MAP</code> value in SQL.</p> <p>To avoid this ambiguity, Vulcan needs to know the columns' types. By default, it will try to infer these types based on the model definitions, but they can also be explicitly specified:</p> <ul> <li>in the <code>external_models.yaml</code> file (for external models)</li> <li>using the <code>columns</code> model property</li> <li>using the <code>columns</code> attribute of the unit test</li> </ul> <p>If none of these options work, consider using a SQL query to generate the data.</p>"},{"location":"concepts/tests/#unit-test-structure","title":"Unit test structure","text":""},{"location":"concepts/tests/#test_name","title":"<code>&lt;test_name&gt;</code>","text":"<p>The unique name of the test.</p>"},{"location":"concepts/tests/#test_namemodel","title":"<code>&lt;test_name&gt;.model</code>","text":"<p>The name of the model being tested. This model must be defined in the project's <code>models/</code> folder.</p>"},{"location":"concepts/tests/#test_namedescription","title":"<code>&lt;test_name&gt;.description</code>","text":"<p>An optional description of the test, which can be used to provide additional context.</p>"},{"location":"concepts/tests/#test_nameschema","title":"<code>&lt;test_name&gt;.schema</code>","text":"<p>The name of the schema that will contain the views that are necessary to run this unit test.</p>"},{"location":"concepts/tests/#test_namegateway","title":"<code>&lt;test_name&gt;.gateway</code>","text":"<p>The gateway whose <code>test_connection</code> will be used to run this test. If not specified, the default gateway is used.</p>"},{"location":"concepts/tests/#test_nameinputs","title":"<code>&lt;test_name&gt;.inputs</code>","text":"<p>The inputs that will be used to test the target model. If the model has no dependencies, this can be omitted.</p>"},{"location":"concepts/tests/#test_nameinputsupstream_model","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;</code>","text":"<p>A model that the target model depends on.</p>"},{"location":"concepts/tests/#test_nameinputsupstream_modelrows","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>","text":"<p>The rows of the upstream model, defined as an array of dictionaries that map columns to their values:</p> <pre><code>    &lt;upstream_model&gt;:\n      rows:\n        - &lt;column_name&gt;: &lt;column_value&gt;\n        ...\n</code></pre> <p>If <code>rows</code> is the only key under <code>&lt;upstream_model&gt;</code>, then it can be omitted:</p> <pre><code>    &lt;upstream_model&gt;:\n      - &lt;column_name&gt;: &lt;column_value&gt;\n      ...\n</code></pre> <p>When the input format is <code>csv</code>, the data can be specified inline under <code>rows</code> :</p> <pre><code>    &lt;upstream_model&gt;:\n      rows: |\n        &lt;column1_name&gt;,&lt;column2_name&gt;\n        &lt;row1_value&gt;,&lt;row1_value&gt;\n        &lt;row2_value&gt;,&lt;row2_value&gt;\n</code></pre>"},{"location":"concepts/tests/#test_nameinputsupstream_modelformat","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.format</code>","text":"<p>The optional <code>format</code> key allows for control over how the input data is loaded.</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n</code></pre> <p>Currently, the following formats are supported: <code>yaml</code> (default), <code>csv</code>.</p>"},{"location":"concepts/tests/#test_nameinputsupstream_modelcsv_settings","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.csv_settings</code>","text":"<p>When the<code>format</code> is CSV, you can control the behaviour of data loading under <code>csv_settings</code>:</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n      csv_settings: \n        sep: \"#\"\n        skip_blank_lines: true\n      rows: |\n        &lt;column1_name&gt;#&lt;column2_name&gt;\n        &lt;row1_value&gt;#&lt;row1_value&gt;\n        &lt;row2_value&gt;#&lt;row2_value&gt;\n</code></pre> <p>Learn more about the supported CSV settings.</p>"},{"location":"concepts/tests/#test_nameinputsupstream_modelpath","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.path</code>","text":"<p>The optional <code>path</code> key specifies the pathname of the data to be loaded.</p> <pre><code>    &lt;upstream_model&gt;:\n      path: filepath/test_data.yaml\n</code></pre>"},{"location":"concepts/tests/#test_nameinputsupstream_modelcolumns","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.columns</code>","text":"<p>An optional dictionary that maps columns to their types:</p> <pre><code>    &lt;upstream_model&gt;:\n      columns:\n        &lt;column_name&gt;: &lt;column_type&gt;\n        ...\n</code></pre> <p>This can be used to help Vulcan interpret the row values correctly in the context of SQL.</p> <p>Any number of columns may be omitted from this mapping, in which case their types will be inferred on a best-effort basis. Explicitly casting the corresponding columns in the model's query will enable Vulcan to infer their types more accurately.</p>"},{"location":"concepts/tests/#test_nameinputsupstream_modelquery","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the input rows:</p> <pre><code>    &lt;upstream_model&gt;:\n      query: &lt;sql_query&gt;\n</code></pre> <p>This provides more control over how the input data must be interpreted.</p> <p>The <code>query</code> key can't be used together with the <code>rows</code> key.</p>"},{"location":"concepts/tests/#test_nameoutputs","title":"<code>&lt;test_name&gt;.outputs</code>","text":"<p>The target model's expected outputs.</p> <p>Note: the columns in each row of an expected output must appear in the same relative order as they are selected in the corresponding query.</p>"},{"location":"concepts/tests/#test_nameoutputspartial","title":"<code>&lt;test_name&gt;.outputs.partial</code>","text":"<p>A boolean flag that indicates whether only a subset of the output columns will be tested. When set to <code>true</code>, only the columns referenced in the corresponding expected rows will be tested.</p> <p>See also: Omitting columns.</p>"},{"location":"concepts/tests/#test_nameoutputsquery","title":"<code>&lt;test_name&gt;.outputs.query</code>","text":"<p>The expected output of the target model's query. This is optional, as long as <code>&lt;test_name&gt;.outputs.ctes</code> is present.</p>"},{"location":"concepts/tests/#test_nameoutputsquerypartial","title":"<code>&lt;test_name&gt;.outputs.query.partial</code>","text":"<p>Same as <code>&lt;test_name&gt;.outputs.partial</code>, but applies only to the output of the target model's query.</p>"},{"location":"concepts/tests/#test_nameoutputsqueryrows","title":"<code>&lt;test_name&gt;.outputs.query.rows</code>","text":"<p>The expected rows of the target model's query.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>.</p>"},{"location":"concepts/tests/#test_nameoutputsqueryquery","title":"<code>&lt;test_name&gt;.outputs.query.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the expected rows for the target model's query.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>.</p>"},{"location":"concepts/tests/#test_nameoutputsctes","title":"<code>&lt;test_name&gt;.outputs.ctes</code>","text":"<p>The expected output per each individual top-level Common Table Expression (CTE) defined in the target model's query. This is optional, as long as <code>&lt;test_name&gt;.outputs.query</code> is present.</p>"},{"location":"concepts/tests/#test_nameoutputsctescte_name","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;</code>","text":"<p>The expected output of the CTE with name <code>&lt;cte_name&gt;</code>.</p>"},{"location":"concepts/tests/#test_nameoutputsctescte_namepartial","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.partial</code>","text":"<p>Same as <code>&lt;test_name&gt;.outputs.partial</code>, but applies only to the output of the CTE with name <code>&lt;cte_name&gt;</code>.</p>"},{"location":"concepts/tests/#test_nameoutputsctescte_namerows","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.rows</code>","text":"<p>The expected rows of the CTE with name <code>&lt;cte_name&gt;</code>.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>.</p>"},{"location":"concepts/tests/#test_nameoutputsctescte_namequery","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the expected rows for the CTE with name <code>&lt;cte_name</code>.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>.</p>"},{"location":"concepts/tests/#test_namevars","title":"<code>&lt;test_name&gt;.vars</code>","text":"<p>An optional dictionary that assigns values to macro variables:</p> <pre><code>  vars:\n    start: 2022-01-01\n    end: 2022-01-01\n    execution_time: 2022-01-01\n    &lt;macro_variable_name&gt;: &lt;macro_variable_value&gt;\n</code></pre> <p>There are three special macro variables: <code>start</code>, <code>end</code>, and <code>execution_time</code>. If these are set, they will override the corresponding date macros of the target model. For example, <code>@execution_ds</code> will render to <code>2022-01-01</code> if <code>execution_time</code> is set to this value.</p> <p>Additionally, SQL expressions like <code>CURRENT_DATE</code> and <code>CURRENT_TIMESTAMP</code> will produce the same datetime value as <code>execution_time</code>, when it is set.</p>"},{"location":"concepts/architecture/serialization/","title":"Serialization","text":""},{"location":"concepts/architecture/serialization/#serialization","title":"Serialization","text":"<p>Vulcan executes Python code through macros and Python models. Each Python model is stored as a standalone snapshot, which includes all of the Python code necessary to generate it.</p>"},{"location":"concepts/architecture/serialization/#serialization-format","title":"Serialization format","text":"<p>Rather than using Python's <code>pickle</code> format, Vulcan has its own serialization format. This is because <code>pickle</code> is not compatible across Python versions, and would, for example, prevent you from developing on Python 3.9 and then running Python 3.10 in production.</p> <p>Instead, Vulcan stores the string representation of your Python implementation and then re-evaluates it. Given a custom Python function or macro, Vulcan reads the Abstract Syntax Tree (AST) of the function and converts that into a string representation, along with all dependencies and global variables. For more information, refer to snapshot fingerprinting.</p>"},{"location":"concepts/architecture/serialization/#limitations","title":"Limitations","text":"<p>Vulcan only serializes the Python code you write and does not include libraries, which means the module of your code must match your Vulcan config path. In addition, any references to libraries will be converted to imports, so you must ensure that any libraries you are using are installed everywhere that Vulcan is running.</p>"},{"location":"concepts/architecture/snapshots/","title":"Snapshots","text":""},{"location":"concepts/architecture/snapshots/#snapshots","title":"Snapshots","text":"<p>A snapshot is a record of a model at a given time. Along with a copy of the model, a snapshot contains everything needed to evaluate the model and render its query. This allows Vulcan to have a consistent view of your project's history and its data as the project and its models evolve and change. Since model queries can have macros, each snapshot stores a copy of all macro definitions and global variables at the time the snapshot is taken. Additionally, snapshots store the intervals of time for which they have data.</p>"},{"location":"concepts/architecture/snapshots/#fingerprinting","title":"Fingerprinting","text":"<p>Snapshots have unique fingerprints that are derived from their models. Vulcan uses these fingerprints to determine when existing tables can be reused, or whether a backfill is needed as a model's query has changed.</p> <p>Because Vulcan can understand SQL with SQLGlot, it can generate fingerprints such that superficial changes to a model, such as applying formatting to its query, will not return a new fingerprint.</p>"},{"location":"concepts/architecture/snapshots/#change-categories","title":"Change categories","text":"<p>Refer to change categories.</p>"},{"location":"concepts/macros/jinja_macros/","title":"Jinja macros","text":""},{"location":"concepts/macros/jinja_macros/#jinja-macros","title":"Jinja macros","text":"<p>Vulcan supports macros from the Jinja templating system.</p> <p>Jinja's macro approach is pure string substitution. Unlike Vulcan macros, they assemble SQL query text without building a semantic representation.</p> <p>NOTE: Vulcan projects support the standard Jinja function library only - they do not support dbt-specific jinja functions like <code>{{ ref() }}</code>. dbt-specific functions are allowed in dbt projects being run with the Vulcan adapter.</p>"},{"location":"concepts/macros/jinja_macros/#basics","title":"Basics","text":"<p>Jinja uses curly braces <code>{}</code> to differentiate macro from non-macro text. It uses the second character after the left brace to determine what the text inside the braces will do.</p> <p>The three curly brace symbols are:</p> <ul> <li><code>{{...}}</code> creates Jinja expressions. Expressions are replaced by text that is incorporated into the rendered SQL query; they can contain macro variables and functions.</li> <li><code>{%...%}</code> creates Jinja statements. Statements give instructions to Jinja, such as setting variable values, control flow with <code>if</code>, <code>for</code> loops, and defining macro functions.</li> <li><code>{#...#}</code> creates Jinja comments. These comments will not be included in the rendered SQL query.</li> </ul> <p>Since Jinja strings are not syntactically valid SQL expressions and cannot be parsed as such, the model query must be wrapped in a special <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code> block in order for Vulcan to detect it:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>Similarly, to use Jinja expressions as part of statements that should be evaluated before or after the model query, the <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code> block should be used:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"concepts/macros/jinja_macros/#vulcan-predefined-variables","title":"Vulcan predefined variables","text":"<p>Vulcan provides multiple predefined macro variables you may reference in jinja code.</p> <p>Some predefined variables provide information about the Vulcan project itself, like the <code>runtime_stage</code> and <code>this_model</code> variables.</p> <p>Other predefined variables are temporal, like <code>start_ds</code> and <code>execution_date</code>. They are used to build incremental model queries and are only available in incremental model kinds.</p> <p>Access predefined macro variables by passing their unquoted name in curly braces. For example, this demonstrates how to access the <code>start_ds</code> and <code>end_ds</code> variables:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Because the two macro variables return string values, we must surround the curly braces with single quotes <code>'</code>. Other macro variables, such as <code>start_epoch</code>, return numeric values and do not require the single quotes.</p> <p>The <code>gateway</code> variable uses a slightly different syntax than other predefined variables because it is a function call. Instead of the bare name <code>{{ gateway }}</code>, it must include parentheses: <code>{{ gateway() }}</code>.</p>"},{"location":"concepts/macros/jinja_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports two kinds of user-defined macro variables: global and local.</p> <p>Global macro variables are defined in the project configuration file and can be accessed in any project model.</p> <p>Local macro variables are defined in a model definition and can only be accessed in that model.</p>"},{"location":"concepts/macros/jinja_macros/#global-variables","title":"Global variables","text":"<p>Learn more about defining global variables in the Vulcan macros documentation.</p> <p>Access global variable values in a model definition using the <code>{{ var() }}</code> jinja function. The function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, a model would access a global variable named <code>int_var</code> like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>A default value can be passed as a second argument to the <code>{{ var() }}</code> jinja function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> if no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre>"},{"location":"concepts/macros/jinja_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key. Learn more about defining gateway variables in the Vulcan macros documentation.</p> <p>Access gateway variables in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the configuration file's root <code>variables</code> key.</p>"},{"location":"concepts/macros/jinja_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Blueprint variables can be accessed using the <code>{{ blueprint_var() }}</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>{{ var() }}</code>).</p>"},{"location":"concepts/macros/jinja_macros/#local-variables","title":"Local variables","text":"<p>Define your own variables with the Jinja statement <code>{% set ... %}</code>. For example, we could specify the name of the <code>num_orders</code> column in the <code>vulcan_example.full_model</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>Note that the Jinja set statement is written after the <code>MODEL</code> statement and before the SQL query.</p> <p>Jinja variables can be string, integer, or float data types. They can also be an iterable data structure, such as a list, tuple, or dictionary. Each of these data types and structures supports multiple Python methods, such as the <code>upper()</code> method for strings.</p>"},{"location":"concepts/macros/jinja_macros/#macro-operators","title":"Macro operators","text":""},{"location":"concepts/macros/jinja_macros/#control-flow-operators","title":"Control flow operators","text":""},{"location":"concepts/macros/jinja_macros/#for-loops","title":"for loops","text":"<p>For loops let you iterate over a collection of items to condense repetitive code and easily change the values used by the code.</p> <p>Jinja for loops begin with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>. This example demonstrates creating indicator variables with <code>CASE WHEN</code> using a Jinja for loop:</p> <pre><code>SELECT\n  {% for vehicle_type in ['car', 'truck', 'bus']}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>Note that the <code>vehicle_type</code> values are quoted in the list <code>['car', 'truck', 'bus']</code>. Jinja removes those quotes during processing, so the reference <code>'{{ vehicle_type }}</code> in the <code>CASE WHEN</code> statement must be in quotes. The reference <code>vehicle_{{ vehicle_type }}</code> does not require quotes.</p> <p>Also note that a comma is present at the end of the <code>CASE WHEN</code> line. Trailing commas are not valid SQL and would normally require special handling, but Vulcan's semantic understanding of the query allows it to identify and remove the offending comma.</p> <p>The example renders to this after Vulcan processing:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>In general, it is a best practice to define lists of values separately from their use. We could do that like this:</p> <pre><code>{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types }\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>The rendered query would be the same as before.</p>"},{"location":"concepts/macros/jinja_macros/#if","title":"if","text":"<p>if statements allow you to take an action (or not) based on some condition.</p> <p>Jinja if statements begin with <code>{% if ... %}</code> and end with <code>{% endif %}</code>. The starting <code>if</code> statement must contain code that evaluates to <code>True</code> or <code>False</code>. For example, all of <code>True</code>, <code>1 + 1 == 2</code>, and <code>'a' in ['a', 'b']</code> evaluate to <code>True</code>.</p> <p>As an example, you might want a model to only include a column if the model was being run for testing purposes. We can do that by setting a variable indicating whether it's a testing run that determines whether the query includes <code>testing_column</code>:</p> <pre><code>{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n</code></pre> <p>Because <code>testing</code> is <code>True</code>, the rendered query would be:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"concepts/macros/jinja_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Jinja macro functions should be placed in <code>.sql</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.sql</code> file, or they can be distributed across multiple files.</p> <p>Jinja macro functions are defined with the <code>{% macro %}</code> and <code>{% endmacro %}</code> statements. The macro function name and arguments are specified in the <code>{% macro %}</code> statement.</p> <p>For example, a macro function named <code>print_text</code> that takes no arguments could be defined with:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>This macro function would be called in a SQL model with <code>{{ print_text() }}</code>, which would be substituted with <code>text</code>\" in the rendered query.</p> <p>Macro function arguments are placed in the parentheses next to the macro name. For example, this macro generates a SQL column with an alias based on the arguments <code>expression</code> and <code>alias</code>:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>We might call this macro function in a SQL query like this:</p> <pre><code>SELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Note that both argument values are quoted in the call <code>alias('item_id', 'item_id2')</code> but are not quoted in the rendered query. During the rendering process, Vulcan uses its semantic understanding of the query to build the rendered text - it recognizes that the first argument is a column name and that column aliases are unquoted by default.</p> <p>In that example, the SQL query selects the column <code>item_id</code> with the alias <code>item_id2</code>. If instead we wanted to select the string <code>'item_id'</code> with the name <code>item_id2</code>, we would pass the <code>expression</code> argument with double quotes around it: <code>\"'item_id'\"</code>:</p> <pre><code>SELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes around <code>\"'item_id'\"</code> signal to Vulcan that it is not a column name.</p> <p>Some SQL dialects interpret double and single quotes differently. We could replace the rendered single quoted <code>'item_id'</code> with double quoted <code>\"item_id\"</code> in the previous example by switching the placement of quotes in the macro function call. Instead of <code>alias(\"'item_id'\", 'item_id2')</code> we would use <code>alias('\"item_id\"', 'item_id2')</code>.</p>"},{"location":"concepts/macros/jinja_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both the Jinja and Vulcan macro systems. We strongly recommend using only one system in a single model - if both are present, they may fail or behave in unintuitive ways.</p> <p>Predefined Vulcan macro variables can be used in a query containing user-defined Jinja variables and functions. However, predefined variables passed as arguments to a user-defined Jinja macro function must use the Jinja curly brace syntax <code>{{ start_ds }}</code> instead of the Vulcan macro <code>@</code> prefix syntax <code>@start_ds</code>. Note that curly brace syntax may require quoting to generate the equivalent of the <code>@</code> syntax.</p>"},{"location":"concepts/macros/macro_variables/","title":"Macro variables","text":""},{"location":"concepts/macros/macro_variables/#macro-variables","title":"Macro variables","text":"<p>Macro variables are placeholders whose values are substituted in when the macro is rendered.</p> <p>They enable dynamic macro behavior - for example, a date parameter's value might be based on when the macro was run.</p> <p>Note</p> <p>This page discusses Vulcan's built-in macro variables. Learn more about custom, user-defined macro variables on the Vulcan macros page.</p>"},{"location":"concepts/macros/macro_variables/#example","title":"Example","text":"<p>Consider a SQL query that filters by date in the <code>WHERE</code> clause.</p> <p>Instead of manually changing the date each time the model is run, you can use a macro variable to make the date dynamic. With the dynamic approach, the date changes automatically based on when the query is run.</p> <p>This query filters for rows where column <code>my_date</code> is after '2023-01-01':</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>To make this query's date dynamic you could use the predefined Vulcan macro variable <code>@execution_ds</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan that <code>@execution_ds</code> is a macro variable that requires substitution before the SQL is executed.</p> <p>The macro variable <code>@execution_ds</code> is predefined, so its value will be automatically set by Vulcan based on when the execution started. If the model was executed on February 1, 2023 the rendered query would be:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>This example used one of Vulcan's predefined variables, but you can also define your own macro variables.</p> <p>We describe Vulcan's predefined variables below; user-defined macro variables are discussed in the Vulcan macros and Jinja macros pages.</p>"},{"location":"concepts/macros/macro_variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan comes with predefined variables that can be used in your queries. They are automatically set by the Vulcan runtime.</p> <p>Most predefined variables are related to time and use a combination of prefixes (start, end, etc.) and postfixes (date, ds, ts, etc.). They are described in the next section; other predefined variables are discussed in the following section.</p>"},{"location":"concepts/macros/macro_variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses the python datetime module for handling dates and times. It uses the standard Unix epoch start of 1970-01-01.</p> <p>Important</p> <p>Predefined variables with a time component always use the UTC time zone.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes:</p> <ul> <li>start - The inclusive starting interval of a model run</li> <li>end - The inclusive end interval of a model run</li> <li>execution - The timestamp of when the execution started</li> </ul> <p>Postfixes:</p> <ul> <li>dt - A python datetime object that converts into a native SQL <code>TIMESTAMP</code> (or SQL engine equivalent)</li> <li>dtntz - A python datetime object that converts into a native SQL <code>TIMESTAMP WITHOUT TIME ZONE</code> (or SQL engine equivalent)</li> <li>date - A python date object that converts into a native SQL <code>DATE</code></li> <li>ds - A date string with the format: '%Y-%m-%d'</li> <li>ts - An ISO 8601 datetime formatted string: '%Y-%m-%d %H:%M:%S'</li> <li>tstz - An ISO 8601 datetime formatted string with timezone: '%Y-%m-%d %H:%M:%S%z'</li> <li>hour - An integer representing the hour of the day, with values 0-23</li> <li>epoch - An integer representing seconds since Unix epoch</li> <li>millis - An integer representing milliseconds since Unix epoch</li> </ul> <p>All predefined temporal macro variables:</p> <ul> <li> <p>dt</p> <ul> <li>@start_dt</li> <li>@end_dt</li> <li>@execution_dt</li> </ul> </li> <li> <p>dtntz</p> <ul> <li>@start_dtntz</li> <li>@end_dtntz</li> <li>@execution_dtntz</li> </ul> </li> <li> <p>date</p> <ul> <li>@start_date</li> <li>@end_date</li> <li>@execution_date</li> </ul> </li> <li> <p>ds</p> <ul> <li>@start_ds</li> <li>@end_ds</li> <li>@execution_ds</li> </ul> </li> <li> <p>ts</p> <ul> <li>@start_ts</li> <li>@end_ts</li> <li>@execution_ts</li> </ul> </li> <li> <p>tstz</p> <ul> <li>@start_tstz</li> <li>@end_tstz</li> <li>@execution_tstz</li> </ul> </li> <li> <p>hour</p> <ul> <li>@start_hour</li> <li>@end_hour</li> <li>@execution_hour</li> </ul> </li> <li> <p>epoch</p> <ul> <li>@start_epoch</li> <li>@end_epoch</li> <li>@execution_epoch</li> </ul> </li> <li> <p>millis</p> <ul> <li>@start_millis</li> <li>@end_millis</li> <li>@execution_millis</li> </ul> </li> </ul>"},{"location":"concepts/macros/macro_variables/#runtime-variables","title":"Runtime variables","text":"<p>Vulcan provides additional predefined variables used to modify model behavior based on information available at runtime.</p> <ul> <li>@runtime_stage - A string value denoting the current stage of the Vulcan runtime. Typically used in models to conditionally execute pre/post-statements (learn more here). It returns one of these values:<ul> <li>'loading' - The project is being loaded into Vulcan's runtime context.</li> <li>'creating' - The model tables are being created for the first time. The data may be inserted during table creation.</li> <li>'evaluating' - The model query logic is evaluated, and the data is inserted into the existing model table.</li> <li>'promoting' - The model is being promoted in the target environment (view created during virtual layer update).</li> <li>'demoting' - The model is being demoted in the target environment (view dropped during virtual layer update).</li> <li>'auditing' - The audit is being run.</li> <li>'testing' - The model query logic is being evaluated in the context of a unit test.</li> </ul> </li> <li>@gateway - A string value containing the name of the current gateway.</li> <li>@this_model - The physical table name that the model's view selects from. Typically used to create generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</li> <li>@model_kind_name - A string value containing the name of the current model kind. Intended to be used in scenarios where you need to control the physical properties in model defaults.</li> </ul> <p>Embedding variables in strings</p> <p>Macro variable references sometimes use the curly brace syntax <code>@{variable}</code>, which serves a different purpose than the regular <code>@variable</code> syntax.</p> <p>The curly brace syntax tells Vulcan that the rendered string should be treated as an identifier, instead of simply replacing the macro variable value.</p> <p>For example, if <code>variable</code> is defined as <code>@DEF(</code>variable<code>, foo.bar)</code>, then <code>@variable</code> produces <code>foo.bar</code>, while <code>@{variable}</code> produces <code>\"foo.bar\"</code>. This is because Vulcan converts <code>foo.bar</code> into an identifier, using double quotes to correctly include the <code>.</code> character in the identifier name.</p> <p>In practice, <code>@{variable}</code> is most commonly used to interpolate a value within an identifier, e.g., <code>@{variable}_suffix</code>, whereas <code>@variable</code> is used to do plain substitutions for string literals.</p> <p>Learn more above.</p>"},{"location":"concepts/macros/macro_variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>The following variables are also available in <code>before_all</code> and <code>after_all</code> statements, as well as in macros invoked within them.</p> <ul> <li>@this_env - A string value containing the name of the current environment.</li> <li>@schemas - A list of the schema names of the virtual layer of the current environment.</li> <li>@views - A list of the view names of the virtual layer of the current environment.</li> </ul>"},{"location":"concepts/macros/overview/","title":"Overview","text":""},{"location":"concepts/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language. It does not natively have features like variables or control flow logic (if-then, for loops) that allow SQL commands to behave differently in different situations.</p> <p>However, data pipelines are dynamic and need different behavior depending on context. SQL is made dynamic with macros.</p> <p>Vulcan supports two macro systems: Vulcan macros and the Jinja templating system.</p> <p>Learn more about macros in Vulcan:</p> <ul> <li>Pre-defined macro variables available in both macro systems</li> <li>Vulcan macros</li> <li>Jinja macros</li> </ul>"},{"location":"concepts/macros/vulcan_macros/","title":"Vulcan macros","text":""},{"location":"concepts/macros/vulcan_macros/#vulcan-macros","title":"Vulcan macros","text":""},{"location":"concepts/macros/vulcan_macros/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros behave differently than those of templating systems like Jinja.</p> <p>Macro systems are based on string substitution. The macro system scans code files, identifies special characters that signify macro content, and replaces the macro elements with other text.</p> <p>In a general sense, that is the entire functionality of templating systems. They have tools that provide control flow logic (if-then) and other functionality, but that functionality is solely to support substituting in the correct strings.</p> <p>Templating systems are intentionally agnostic to the programming language being templated, and most of them work for everything from blog posts to HTML to SQL.</p> <p>In contrast, Vulcan macros are designed specifically for generating SQL code. They have semantic understanding of the SQL code being created by analyzing it with the Python sqlglot library, and they allow use of Python code so users can tidily implement sophisticated macro logic.</p>"},{"location":"concepts/macros/vulcan_macros/#vulcan-macro-approach","title":"Vulcan macro approach","text":"<p>This section describes how Vulcan macros work under the hood. Feel free to skip over this section and return if and when it is useful. This information is not required to use Vulcan macros, but it will be useful for debugging any macros exhibiting puzzling behavior.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</li> <li>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</li> <li>Macro functions, both Vulcan's and user-defined</li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"concepts/macros/vulcan_macros/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></li> <li>Identifiers, which reference database objects like column, table, alias, and function names<ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"concepts/macros/vulcan_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint and local.</p> <p>Global and gateway macro variables are defined in the project configuration file and can be accessed in any project model. Blueprint and macro variables are defined in a model definition and can only be accessed in that model.</p> <p>Macro variables with the same name may be specified at any or all of the global, gateway, blueprint and local levels. When variables are specified at multiple levels, the value of the most specific level takes precedence. For example, the value of a local variable takes precedence over the value of a blueprint or gateway variable with the same name, and the value of a gateway variable takes precedence over the value of a global variable.</p>"},{"location":"concepts/macros/vulcan_macros/#global-variables","title":"Global variables","text":"<p>Global variables are defined in the project configuration file <code>variables</code> key.</p> <p>Global variable values may be any of the following data types or lists or dictionaries containing these types: <code>int</code>, <code>float</code>, <code>bool</code>, <code>str</code>.</p> <p>Access global variable values in a model definition using the <code>@&lt;VAR_NAME&gt;</code> macro or the <code>@VAR()</code> macro function. The latter function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"concepts/macros/vulcan_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"concepts/macros/vulcan_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"concepts/macros/vulcan_macros/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"concepts/macros/vulcan_macros/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system has multiple operators that allow different forms of dynamic behavior in models.</p>"},{"location":"concepts/macros/vulcan_macros/#each","title":"@EACH","text":"<p><code>@EACH</code> is used to transform a list of items by applying a function to each of them, analogous to a <code>for</code> loop.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>Vulcan macros support placing macro values at the end of a column name using <code>column_@x</code>.</p> <p>However, if you wish to substitute the variable anywhere else in the identifier, you need to use the more explicit curly brace syntax <code>@{}</code> to avoid ambiguity. For example, these are valid uses: <code>@{x}_column</code> or <code>my_@{x}_column</code>.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"concepts/macros/vulcan_macros/#if","title":"@IF","text":"<p>Vulcan's <code>@IF</code> macro allows components of a SQL query to change based on the result of a logical condition.</p> <p>It includes three elements:</p> <ol> <li>A logical condition that evaluates to <code>TRUE</code> or <code>FALSE</code></li> <li>A value to return if the condition is <code>TRUE</code></li> <li>A value to return if the condition is <code>FALSE</code> [optional]</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</li> <li>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</li> <li>Between: <code>[number] BETWEEN [low number] AND [high number]</code></li> <li>Membership: <code>[item] IN ([comma-separated list of items])</code></li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li><code>'a' = 'a'</code></li> <li><code>'a' != 'b'</code></li> <li><code>0 &lt; 1</code></li> <li><code>1 &gt;= 1</code></li> <li><code>2 BETWEEN 1 AND 3</code></li> <li><code>'a' IN ('a', 'b')</code></li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"concepts/macros/vulcan_macros/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"concepts/macros/vulcan_macros/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"concepts/macros/vulcan_macros/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The relation/table whose columns are being selected</li> <li><code>alias</code> (optional): The alias of the relation (if it has one)</li> <li><code>exclude</code> (optional): A list of columns to exclude</li> <li><code>prefix</code> (optional): A string to use as a prefix for all selected column names</li> <li><code>suffix</code> (optional): A string to use as a suffix for all selected column names</li> <li><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</li> <li>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</li> <li>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</li> <li>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></li> <li>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</li> <li><code>my_column</code> is present in the query</li> </ul>"},{"location":"concepts/macros/vulcan_macros/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by: 1. <code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type) 2. Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column 3. Concatenating the column values after steps (1) and (2) 4. Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</li> <li><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>lat1</code>: Latitude of the first point</li> <li><code>lon1</code>: Longitude of the first point</li> <li><code>lat2</code>: Latitude of the second point</li> <li><code>lon2</code>: Longitude of the second point</li> <li><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>column</code>: The column to pivot</li> <li><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</li> <li><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</li> <li><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></li> <li><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></li> <li><code>prefix</code> (optional): A prefix to use for all aliases</li> <li><code>suffix</code> (optional): A suffix to use for all aliases</li> <li><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></li> <li><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></li> <li><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</li> <li><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The table or CTE name to deduplicate</li> <li><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</li> <li><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</li> <li><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</li> <li><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>. - For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</p>"},{"location":"concepts/macros/vulcan_macros/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</li> <li>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li><code>template</code> - The string template to render into an AST node</li> <li><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></li> <li><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></li> <li><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li><code>@WITH</code>: common table expression <code>WITH</code> clause</li> <li><code>@JOIN</code>: table <code>JOIN</code> clause(s)</li> <li><code>@WHERE</code>: filtering <code>WHERE</code> clause</li> <li><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</li> <li><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</li> <li><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</li> <li><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"concepts/macros/vulcan_macros/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"concepts/macros/vulcan_macros/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"concepts/macros/vulcan_macros/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Vulcan supports user-defined macro functions written in two languages - SQL and Python:</p> <ul> <li>SQL macro functions must use the Jinja templating system.</li> <li>Python macro functions use the SQLGlot library to allow more complex operations than macro variables and operators provide alone.</li> </ul>"},{"location":"concepts/macros/vulcan_macros/#python-macro-functions","title":"Python macro functions","text":""},{"location":"concepts/macros/vulcan_macros/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"concepts/macros/vulcan_macros/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"concepts/macros/vulcan_macros/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"concepts/macros/vulcan_macros/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"concepts/macros/vulcan_macros/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li><code>@add_args(1)</code> returns 1</li> <li><code>@add_args(1, 2)</code> returns 3</li> <li><code>@add_args(1, 2, 3)</code> returns 6</li> </ul>"},{"location":"concepts/macros/vulcan_macros/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li>Numbers are parsed into <code>Literal</code> expressions</li> <li>Quoted strings are parsed into <code>Literal</code> expressions</li> <li>Unquoted strings are parsed into <code>Column</code> expressions</li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"concepts/macros/vulcan_macros/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></li> <li>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in the <code>dev</code> environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in any environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</li> <li>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</li> <li>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"concepts/macros/vulcan_macros/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"concepts/macros/vulcan_macros/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"concepts/macros/vulcan_macros/#typed-macros","title":"Typed Macros","text":"<p>Typed macros in Vulcan bring the power of type hints from Python, enhancing readability, maintainability, and usability of your SQL macros. These macros enable developers to specify expected types for arguments, making the macros more intuitive and less error-prone.</p>"},{"location":"concepts/macros/vulcan_macros/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"concepts/macros/vulcan_macros/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"concepts/macros/vulcan_macros/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</li> <li><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li><code>exp.Table</code></li> <li><code>exp.Column</code></li> <li><code>exp.Literal</code></li> <li><code>exp.Identifier</code></li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"concepts/macros/vulcan_macros/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p> <p>See examples of the coercion function in action in the test suite here.</p>"},{"location":"concepts/macros/vulcan_macros/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"concepts/macros/vulcan_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan and Jinja macro systems. We strongly recommend using only one system in a model - if both are present, they may fail or behave in unintuitive ways.</p>"},{"location":"concepts/metrics/definition/","title":"Definition","text":""},{"location":"concepts/metrics/definition/#definition","title":"Definition","text":"<p>Metrics are defined in SQL files in the <code>metrics/</code> directory of your Vulcan project. A single file can contain multiple metric definitions.</p> <p>A metric is defined with the function <code>METRIC()</code> and must include the <code>name</code> and <code>expression</code> keys. The <code>name</code> is case insensitive and must be unique, and the <code>expression</code> contains the SQL code used to calculate the metric.</p>"},{"location":"concepts/metrics/definition/#sql-expression","title":"SQL expression","text":"<p>The expression field can be any SQL statement that contains an aggregation function (e.g., <code>MAX</code>, <code>SUM</code>). This example uses the <code>COUNT</code> function:</p> <pre><code>METRIC (\n  name unique_account_ids,\n  expression COUNT(DISTINCT silver.accounts.account_id)\n);\n</code></pre> <p>All columns referenced in the expression should be fully qualified with the model name. For example, if a model name is <code>a.b</code>, a metric referencing column <code>c</code> in that model should refer to it as <code>a.b.c</code>.</p> <p>The example above refers to the <code>account_id</code> column in model <code>silver.accounts</code> using <code>silver.accounts.account_id</code>.</p>"},{"location":"concepts/metrics/definition/#automatic-joins","title":"Automatic joins","text":"<p>Metrics can refer to multiple tables and will use the model grains and references to automatically join them together. <code>grains</code> specify a model's key column(s) that uniquely identify the model's rows and <code>references</code> specify column(s) that other tables may join to.</p> <p>For example, a Vulcan project might contain the <code>prod.users</code> and <code>prod.searches</code> models with the following <code>MODEL</code> DDLs.</p> <p>The <code>prod.users</code> model has a grain of <code>user_id</code>, meaning that its rows are uniquely identified by the <code>user_id</code> column:</p> <pre><code>MODEL (\n  name prod.users,\n  grain user_id\n);\n</code></pre> <p>The <code>prod.searches</code> model specifies <code>user_id</code> in its <code>references</code> key, signaling that other models may join to its <code>user_id</code> column:</p> <pre><code>MODEL (\n  name prod.searches,\n  grain search_id,\n  references user_id,\n);\n</code></pre> <p>Because those models specify their grain and references, Vulcan can correctly generate code for a metric that uses columns from both models.</p> <p>In this example, <code>canadian_searchers</code> sums searches from users located in Canada:</p> <pre><code>METRIC (\n  name canadian_searchers,\n  expression SUM(IF(prod.users.country = 'CAD', prod.searches.num_searches, 0)),\n);\n</code></pre> <p>Because the <code>prod.users.country</code> and <code>prod.searches.num_searches</code> models have specified their grains/references, Vulcan can automatically do the correct join between them.</p>"},{"location":"concepts/metrics/definition/#derived-metrics","title":"Derived metrics","text":"<p>Metrics can perform additional operations/calculations with other metrics.</p> <p>In this example, the third metric <code>clicks_per_search</code> is calculated by dividing the first metric <code>total_searches</code> by the second metric <code>total_clicks</code>:</p> <pre><code>METRIC (\n  name total_searches,\n  expression SUM(prod.searchers.num_searches)\n);\n\nMETRIC (\n  name total_clicks,\n  expression SUM(prod.clickers.num_clicks)\n);\n\nMETRIC (\n  name clicks_per_search,\n  expression total_clicks / total_searches -- Calculated from the other two metrics\n);\n</code></pre>"},{"location":"concepts/metrics/definition/#properties","title":"Properties","text":"<p>The <code>METRIC</code> definition supports the following keys. The <code>name</code> and <code>expression</code> keys are required.</p>"},{"location":"concepts/metrics/definition/#name","title":"name","text":"<ul> <li>The name of the metric. This name is case insensitive and must be unique in a project.</li> </ul>"},{"location":"concepts/metrics/definition/#expression","title":"expression","text":"<ul> <li>A SQL expression consisting of an aggregation, a formula consisting of other metrics, or a combination of both.</li> </ul>"},{"location":"concepts/metrics/definition/#description","title":"description","text":"<ul> <li>The description of the metric.</li> </ul>"},{"location":"concepts/metrics/definition/#owner","title":"owner","text":"<ul> <li>The owner of the metric. Used for organizational and governance purposes.</li> </ul>"},{"location":"concepts/metrics/definition/#dialect","title":"dialect","text":"<ul> <li>The dialect that the expression is written in. It is recommended to leave this empty and rely on the project's default dialect.</li> </ul>"},{"location":"concepts/metrics/overview/","title":"Overview","text":""},{"location":"concepts/metrics/overview/#overview","title":"Overview","text":"<p>Metrics are currently in a prototype phase and not meant for production use.</p> <p>Vulcan provides a framework for defining and working with metrics (also known as a semantic layer). Metrics are arbitrary SQL functions that perform aggregations for use in analytics, data science, or machine learning.</p> <p>A semantic layer is valuable because it provides a consistent definition and language for metrics. For example, if an executive asks \"How many active users are there?\" the answer could differ depending on who they ask or what dashboard they look at. The tables, aggregations, and joins needed to correctly calculate the answer could be complex and implemented differently (or incorrectly) by different people.</p> <p>Metrics are defined in a Vulcan project metrics directory, and they are selected by name in a model query. Vulcan uses its semantic understanding of the query to determine the metric's role in the query, determine the appropriate SQL operations to calculate the metric, and add them to the query code submitted to the SQL engine.</p>"},{"location":"concepts/metrics/overview/#example","title":"Example","text":""},{"location":"concepts/metrics/overview/#definition","title":"Definition","text":"<p>Metrics are defined using Vulcan's SQL-based metrics definition language.</p> <p>The following is an example metric definition. Note the following aspects:</p> <ul> <li>The metrics <code>expression</code> can be any aggregate SQL function (<code>COUNT</code> in this example)</li> <li>The columns referenced in <code>expression</code> are fully qualified (<code>sushi.customers.status</code>)</li> <li>The metric can reference multiple models as long as their grains/references are properly specified (<code>expression</code> uses both the <code>sushi.customers</code> and <code>sushi.orders</code> models)</li> </ul> <pre><code>METRIC (\n  name total_orders_from_active_customers,\n  expression COUNT(IF(sushi.customers.status = 'ACTIVE', sushi.orders.id, NULL))\n);\n</code></pre>"},{"location":"concepts/metrics/overview/#querying","title":"Querying","text":"<p>Vulcan models access metrics in their query's <code>SELECT</code> clause with the <code>METRIC</code> function and the metric name.</p> <p>For example, this model query selects the <code>total_orders_from_active_customer</code> metric. Because it is a simple query that solely selects a metric and its grouping column, it can select from the special table <code>__semantic.__table</code>:</p> <pre><code>SELECT\n  ds,\n  METRIC(total_orders_from_active_customers), -- METRIC function\nFROM __semantic.__table  -- special table for simple metric queries\nGROUP BY ds\n</code></pre> <p>When that model query is run, Vulcan uses its semantic understanding of the query and metrics definitions to generate the code that is actually executed by the SQL engine:</p> <pre><code>SELECT\n  __table.ds AS ds,\n  __table.total_orders_from_active_customers AS total_orders_from_active_customers\nFROM (\n  SELECT\n    sushi__orders.ds,\n    COUNT(CASE WHEN sushi__customers.status = 'ACTIVE' THEN sushi__orders.id ELSE NULL END) AS total_orders_from_active_customers\n  FROM sushi.orders AS sushi__orders\n  LEFT JOIN sushi.customers AS sushi__customers\n    ON sushi__orders.customer_id = sushi__customers.customer_id\n  GROUP BY\n    sushi__orders.ds\n) AS __table\n</code></pre> <p>Vulcan automatically generates the correct join to use values from both the <code>sushi.orders</code> and <code>sushi.customers</code> tables.</p>"},{"location":"concepts/models/external_models/","title":"External models","text":""},{"location":"concepts/models/external_models/#external-models","title":"External models","text":"<p>Vulcan model queries may reference \"external\" tables that are created and managed outside the Vulcan project. For example, a model might ingest data from a third party's read-only data system.</p> <p>Vulcan does not manage external tables, but it can use information about the tables' columns and data types to make features more useful. For example, column information allows column-level lineage to include external tables' columns.</p> <p>Vulcan stores external tables' column information as <code>EXTERNAL</code> models.</p>"},{"location":"concepts/models/external_models/#external-models-are-not-run","title":"External models are not run","text":"<p><code>EXTERNAL</code> models consist solely of an external table's column information, so there is no query for Vulcan to run.</p> <p>Vulcan has no information about the data contained in the table represented by an <code>EXTERNAL</code> model. The table could be altered or have all its data deleted, and Vulcan will not detect it. All Vulcan knows about the table is that it contains the columns specified in the <code>EXTERNAL</code> model's file (more information below).</p> <p>Vulcan will not take any actions based on an <code>EXTERNAL</code> model - its actions are solely determined by the model whose query selects from the <code>EXTERNAL</code> model.</p> <p>The querying model's <code>kind</code>, <code>cron</code>, and previously loaded time intervals determine when Vulcan will query the <code>EXTERNAL</code> model.</p>"},{"location":"concepts/models/external_models/#generating-an-external-models-schema-file","title":"Generating an external models schema file","text":"<p>External models can be defined in the <code>external_models.yaml</code> file in the Vulcan project's root folder. The alternative name for this file is <code>schema.yaml</code>.</p> <p>You can create this file by either writing the YAML by hand or allowing Vulcan to fetch information about external tables with the <code>create_external_models</code> CLI command.</p> <p>Consider this example model that queries an external table <code>external_db.external_table</code>:</p> <pre><code>MODEL (\n  name my_db.my_table,\n  kind FULL\n);\n\nSELECT\n  *\nFROM\n  external_db.external_table;\n</code></pre> <p>The following sections demonstrate how to create an external model containing <code>external_db.external_table</code>'s column information.</p> <p>All of a Vulcan project's external models are defined in a single <code>external_models.yaml</code> file, so the files created below might also include column information for other external models.</p> <p>Alternatively, additional external models can also be defined in the external_models/ folder.</p>"},{"location":"concepts/models/external_models/#using-cli","title":"Using CLI","text":"<p>Instead of creating the <code>external_models.yaml</code> file manually, Vulcan can generate it for you with the create_external_models CLI command.</p> <p>The command identifies all external tables referenced in your Vulcan project, fetches their column information from the SQL engine's metadata, and then stores the information in the <code>external_models.yaml</code> file.</p> <p>If Vulcan does not have access to an external table's metadata, the table will be omitted from the file and Vulcan will issue a warning.</p> <p><code>create_external_models</code> solely queries SQL engine metadata and does not query external tables themselves.</p>"},{"location":"concepts/models/external_models/#gateway-specific-external-models","title":"Gateway-specific external models","text":"<p>In some use-cases such as isolated systems with multiple gateways, there are external models that only exist on a certain gateway.</p> <p>Gateway names are case-insensitive in external model configurations. You can specify the gateway name using any case (e.g., <code>gateway: dev</code>, <code>gateway: DEV</code>, <code>gateway: Dev</code>) and Vulcan will handle the matching correctly.</p> <p>Consider the following model that queries an external table with a dynamic database based on the current gateway:</p> <pre><code>MODEL (\n  name my_db.my_table,\n  kind FULL\n);\n\nSELECT\n  *\nFROM\n  @{gateway}_db.external_table;\n</code></pre> <p>This table will be named differently depending on which <code>--gateway</code> Vulcan is run with (learn more about the curly brace <code>@{gateway}</code> syntax here).</p> <p>For example:</p> <ul> <li><code>vulcan --gateway dev plan</code> - Vulcan will try to query <code>dev_db.external_table</code></li> <li><code>vulcan --gateway prod plan</code> - Vulcan will try to query <code>prod_db.external_table</code></li> </ul> <p>To ensure Vulcan can look up the correct schema when the relevant gateway is set, run <code>create_external_models</code> with the <code>--gateway</code> argument. For example:</p> <ul> <li><code>vulcan --gateway dev create_external_models</code></li> </ul> <p>This will set <code>gateway: dev</code> on the external model and ensure that it is only loaded when the current gateway is set to <code>dev</code>.</p>"},{"location":"concepts/models/external_models/#writing-yaml-by-hand","title":"Writing YAML by hand","text":"<p>This example demonstrates the structure of a <code>external_models.yaml</code> file:</p> <pre><code>- name: external_db.external_table\n  description: An external table\n  columns:\n    column_a: int\n    column_b: text\n- name: external_db.some_other_external_table\n  description: Another external table\n  columns:\n    column_c: bool\n    column_d: float\n- name: external_db.gateway_specific_external_table\n  description: Another external table that only exists when the gateway is set to \"test\"\n  gateway: test  # Case-insensitive - could also be \"TEST\", \"Test\", etc.\n  columns:\n    column_e: int\n    column_f: varchar\n</code></pre> <p>It contains each <code>EXTERNAL</code> model's name, an optional description, an optional gateway and each of the external table's columns' name and data type.</p> <p>The file can be constructed by hand using a standard text editor or IDE.</p>"},{"location":"concepts/models/external_models/#using-the-external_models-directory","title":"Using the <code>external_models</code> directory","text":"<p>Sometimes, Vulcan cannot infer the structure of a model and you need to add it manually.</p> <p>However, since <code>vulcan create_external_models</code> replaces the <code>external_models.yaml</code> file, any manual changes you made to that file will be overwritten.</p> <p>The solution is to create the manual model definition files in the <code>external_models/</code> directory, like so:</p> <pre><code>external_models.yaml\nexternal_models/more_external_models.yaml\nexternal_models/even_more_external_models.yaml\n</code></pre> <p>Files in the <code>external_models</code> directory must be <code>.yaml</code> files that follow the same structure as the <code>external_models.yaml</code> file.</p> <p>When Vulcan loads the definitions, it will first load the models defined in <code>external_models.yaml</code> (or <code>schema.yaml</code>) and  any models found in <code>external_models/*.yaml</code>.</p> <p>Therefore, you can use <code>vulcan create_external_models</code> to manage the <code>external_models.yaml</code> file and then put any models that need to be defined manually inside the <code>external_models/</code> directory.</p>"},{"location":"concepts/models/external_models/#external-audits","title":"External Audits","text":"<p>It is possible to define audits on external models. This can be useful to check the data quality of upstream dependencies before your internal models evaluate.</p> <p>This example shows an external model with two audits.</p> <pre><code>- name: raw.demographics\n  description: Table containing demographics information\n  audits:\n    - name: not_null\n      columns: \"[customer_id]\"\n    - name: accepted_range\n      column: zip\n      min_v: \"'00000'\"\n      max_v: \"'99999'\"\n  columns:\n    customer_id: int\n    zip: text\n</code></pre>"},{"location":"concepts/models/managed_models/","title":"Managed models","text":""},{"location":"concepts/models/managed_models/#managed-models","title":"Managed models","text":"<p>Unlike normal tables where the user is responsible for managing the data within the table, some database engines have a concept of a table where the engine itself ensures that the data within the table is up to date. These tables are typically based on a query that reads from other tables within the database. Each time these other tables are updated, the database will ensure that the managed table reflects the changes without the user having to do anything special (such as issue a <code>REFRESH</code> command).</p> <p>Under the hood, each supported database engine achieves this in a slightly different way but most of them have background processes that run and automatically keep the tables up to date, within the parameters you define when you create the table.</p> <p>For supported engines, we expose this functionality through Managed models. This indicates to Vulcan that the underlying database engine will ensure that the data remains up to date and all Vulcan needs to do is maintain the schema.</p> <p>Due to this, managed models would typically be built off an External Model rather than another Vulcan model. Since Vulcan already ensures that models it's tracking are kept up to date, the main benefit of managed models comes when they read from external tables that arent tracked by Vulcan.</p> <p>Not supported in Python models</p> <p>Python models do not support the <code>MANAGED</code> model kind - use a SQL model isntead.</p>"},{"location":"concepts/models/managed_models/#difference-from-materialized-views","title":"Difference from materialized views","text":"<p>The difference between an Managed model and a materialized view is down to semantics and in some engines there is no difference.</p> <p>Vulcan has support for materialized views already. However, depending on the engine, these are subject to some limitations, such as:</p> <ul> <li>A Materialized View query can only be derived from a single base table</li> <li>The Materialized View is not automatically maintained by the engine. To refresh the data, a <code>REFRESH MATERIALIZED VIEW</code> or equivalent command must be issued</li> </ul> <p>Managed models are different in that:</p> <ul> <li>The engine updates the table data automatically when a base table changes</li> <li>When performing updates, the engine has a semantic understanding of the query and can decide if an incremental or full refresh should be applied</li> <li>There is no need to issue manual <code>REFRESH</code> commands. The engine maintains the table transparently in a background process</li> </ul>"},{"location":"concepts/models/managed_models/#lifecycle-in-vulcan","title":"Lifecycle in Vulcan","text":"<p>Managed models follow the same lifecycle as other models:</p> <ul> <li>Creating a Virtual Environment creates a pointer to the current model snapshot</li> <li>Modifying the model causes a new snapshot to be created</li> <li>Any upstream changes cause a new snapshot to be created</li> <li>The model can be deployed and rolled back via the usual pointer swap mechanism</li> <li>Once the TTL expires, model snapshots are cleaned up</li> </ul> <p>However, there is usually extra vendor-imposed costs associated with Managed models. For example, Snowflake has additional costs for Dynamic Tables.</p> <p>Therefore, we try to not create managed tables unnecessarily. For example, in forward-only plans we just create a normal table to preview the changes and only re-create the managed table on deployment to prod.</p> <p>Warning</p> <p>Due to the use of normal tables for dev previews, it is possible to write a query that uses features that are available to normal tables in the target engine but not managed tables. This could result in a scenario where a plan works in a dev environment but fails when deployed to production.</p> <p>We believe the cost savings are worth it, however please reach out if this causes problems for you.</p>"},{"location":"concepts/models/managed_models/#supported-engines","title":"Supported Engines","text":"<p>Vulcan supports managed models in the following database engines:</p> Engine Implementatation Snowflake Dynamic Table <p>To define a managed model, you can use the <code>MANAGED</code> model Kind.</p>"},{"location":"concepts/models/managed_models/#snowflake","title":"Snowflake","text":"<p>Managed Models are in Snowflake are implemented as Dynamic Tables.</p> <p>Here is an example of a Vulcan model that will result in a dynamic table being created:</p> <pre><code>MODEL (\n  name db.events,\n  kind MANAGED,\n  physical_properties (\n    warehouse = datalake,\n    target_lag = '2 minutes',\n    data_retention_time_in_days = 2\n  )\n);\n\nSELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>results in:</p> <pre><code>CREATE OR REPLACE DYNAMIC TABLE db.events\n  WAREHOUSE = \"datalake\",\n  TARGET_LAG = '2 minutes'\n  DATA_RETENTION_TIME_IN_DAYS = 2\nAS SELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>Note</p> <p>Vulcan will not create intervals and run this model for each interval, so there is no need to add a WHERE clause with date filters like you would for a normal incremental model. How the data in this model is refreshed is completely up to Snowflake.</p>"},{"location":"concepts/models/managed_models/#table-properties","title":"Table properties","text":"<p>Dynamic Tables have some properties that affect things like how often the data is refreshed by Snowflake, when the initial data is populated, how long data is retained for etc. The list of available properties is located in the Snowflake documentation.</p> <p>In Vulcan, these properties are set on the model definition.</p> <p>The following Dynamic Table properties are set on the model <code>physical_properties</code>:</p> Snowflake Property Required Notes target_lag Y warehouse N In Snowflake, this is a required property. However, if not specified, then Vulcan will use the result of <code>select current_warehouse()</code>. refresh_mode N initialize N data_retention_time_in_days N max_data_extension_time_in_days N <p>The following Dynamic Table properties can be set directly on the model:</p> Snowflake Property Required Notes cluster by N <code>clustered_by</code> is a standard model property, so set <code>clustered_by</code> on the model to add a <code>CLUSTER BY</code> clause to the Dynamic Table"},{"location":"concepts/models/model_kinds/","title":"Model kinds","text":""},{"location":"concepts/models/model_kinds/#model-kinds","title":"Model kinds","text":"<p>This page describes the kinds of models Vulcan supports, which determine how the data for a model is loaded.</p> <p>Find information about all model kind configuration parameters in the model configuration reference page.</p>"},{"location":"concepts/models/model_kinds/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p>Models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are computed incrementally based on a time range. This is an optimal choice for datasets in which records are captured over time and represent immutable facts such as events, logs, or transactions. Using this kind for appropriate datasets typically results in significant cost and time savings.</p> <p>Only missing time intervals are processed during each execution for <code>INCREMENTAL_BY_TIME_RANGE</code> models. This is in contrast to the FULL model kind, where the entire dataset is recomputed every time the model is executed.</p> <p>An <code>INCREMENTAL_BY_TIME_RANGE</code> model has two requirements that other models do not: it must know which column contains the time data it will use to filter the data by time range, and it must contain a <code>WHERE</code> clause that filters the upstream data by time.</p> <p>The name of the column containing time data is specified in the model's <code>MODEL</code> DDL. It is specified in the DDL <code>kind</code> specification's <code>time_column</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_TIME_RANGE</code> model that stores time data in the \"event_date\" column:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date -- This model's time information is stored in the `event_date` column\n  )\n);\n</code></pre> <p> In addition to specifying a time column in the <code>MODEL</code> DDL, the model's query must contain a <code>WHERE</code> clause that filters the upstream records by time range. Vulcan provides special macros that represent the start and end of the time range being processed: <code>@start_date</code> / <code>@end_date</code> and <code>@start_ds</code> / <code>@end_ds</code>. Refer to Macros for more information.</p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>This is borrowed from the full walkthrough: Incremental by Time Range</p> <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incrementals_demo,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    -- How does this model kind behave?\n    --   DELETE by time range, then INSERT\n    time_column transaction_date,\n\n    -- How do I handle late-arriving data?\n    --   Handle late-arriving events for the past 2 (2*1) days based on cron\n    --   interval. Each time it runs, it will process today, yesterday, and\n    --   the day before yesterday.\n    lookback 2,\n  ),\n\n  -- Don't backfill data before this date\n  start '2024-10-25',\n\n  -- What schedule should I run these at?\n  --   Daily at Midnight UTC\n  cron '@daily',\n\n  -- Good documentation for the primary key\n  grain transaction_id,\n\n  -- How do I test this data?\n  --   Validate that the `transaction_id` primary key values are both unique\n  --   and non-null. Data audit tests only run for the processed intervals,\n  --   not for the entire table.\n  -- audits (\n  --   UNIQUE_VALUES(columns = (transaction_id)),\n  --   NOT_NULL(columns = (transaction_id))\n  -- )\n);\n\nWITH sales_data AS (\n  SELECT\n    transaction_id,\n    product_id,\n    customer_id,\n    transaction_amount,\n    -- How do I account for UTC vs. PST (California baby) timestamps?\n    --   Make sure all time columns are in UTC and convert them to PST in the\n    --   presentation layer downstream.\n    transaction_timestamp,\n    payment_method,\n    currency\n  FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data\n  -- How do I make this run fast and only process the necessary intervals?\n  --   Use our date macros that will automatically run the necessary intervals.\n  --   Because Vulcan manages state, it will know what needs to run each time\n  --   you invoke `vulcan run`.\n  WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B\n  -- Include usage data from the 30 days before the interval\n  WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n)\n\nSELECT\n  s.transaction_id,\n  s.product_id,\n  s.customer_id,\n  s.transaction_amount,\n  -- Extract the date from the timestamp to partition by day\n  DATE(s.transaction_timestamp) as transaction_date,\n  -- Convert timestamp to PST using a SQL function in the presentation layer for end users\n  DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,\n  s.payment_method,\n  s.currency,\n  -- Product usage metrics\n  p.last_usage_date,\n  p.usage_count,\n  p.feature_utilization_score,\n  p.user_segment,\n  -- Derived metrics\n  CASE\n    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.8 THEN 'Power User'\n    WHEN p.usage_count &gt; 50 THEN 'Regular User'\n    WHEN p.usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type,\n  -- Time since last usage\n  DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage\nFROM sales_data s\nLEFT JOIN product_usage p\n  ON s.product_id = p.product_id\n  AND s.customer_id = p.customer_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>50975949</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (\n  `transaction_id` STRING,\n  `product_id` STRING,\n  `customer_id` STRING,\n  `transaction_amount` NUMERIC,\n  `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),\n  `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),\n  `payment_method` STRING,\n  `currency` STRING,\n  `last_usage_date` TIMESTAMP,\n  `usage_count` INT64,\n  `feature_utilization_score` FLOAT64,\n  `user_segment` STRING,\n  `user_type` STRING OPTIONS (description='Derived metrics'),\n  `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')\n  )\n  PARTITION BY `transaction_date`\n</code></pre> <p>Vulcan will validate the SQL before processing data (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps).</p> <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND\n    FALSE\n),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)\n    ) AND\n    FALSE\n)\n\nSELECT\n  `s`.`transaction_id` AS `transaction_id`,\n  `s`.`product_id` AS `product_id`,\n  `s`.`customer_id` AS `customer_id`,\n  CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n  DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n  DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n  `s`.`payment_method` AS `payment_method`,\n  `s`.`currency` AS `currency`,\n  `p`.`last_usage_date` AS `last_usage_date`,\n  `p`.`usage_count` AS `usage_count`,\n  `p`.`feature_utilization_score` AS `feature_utilization_score`,\n  `p`.`user_segment` AS `user_segment`,\n  CASE\n    WHEN `p`.`feature_utilization_score` &gt; 0.8 AND `p`.`usage_count` &gt; 100 THEN 'Power User'\n    WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n    WHEN `p`.`usage_count` IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END AS `user_type`,\n  DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\nFROM `sales_data` AS `s`\nLEFT JOIN `product_usage` AS `p`\n  ON `p`.`customer_id` = `s`.`customer_id` AND\n  `p`.`product_id` = `s`.`product_id`\nWHERE FALSE\nLIMIT 0\n</code></pre> <p>Vulcan will merge data into the empty table.</p> <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `transaction_id`,\n      `product_id`,\n      `customer_id`,\n      `transaction_amount`,\n      `transaction_timestamp`,\n      `payment_method`,\n      `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_id`,\n      `customer_id`,\n      `last_usage_date`,\n      `usage_count`,\n      `feature_utilization_score`,\n      `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  )\n\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`usage_count` &gt; 100 AND `p`.`feature_utilization_score` &gt; 0.8 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n      END AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p`\n      ON `s`.`product_id` = `p`.`product_id`\n      AND `s`.`customer_id` = `p`.`customer_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN\n  INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n  VALUES (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer to pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`\n</code></pre> <p>Important</p> <p>A model's <code>time_column</code> should be in the UTC time zone to ensure correct interaction with Vulcan's scheduler and predefined macro variables.</p> <p>This requirement aligns with the data engineering best practice of converting datetime/timestamp columns to UTC as soon as they are ingested into the data system and only converting them to local timezones when they exit the system for downstream uses. The <code>cron_tz</code> flag does not change this requirement.</p> <p>Placing all timezone conversion code in the system's first/last transformation models prevents inadvertent timezone-related errors as data flows between models.</p> <p>If a model must use a different timezone, parameters like lookback, allow_partials, and cron with offset time can be used to try to account for misalignment between the model's timezone and the UTC timezone used by Vulcan.</p> <p>This example implements a complete <code>INCREMENTAL_BY_TIME_RANGE</code> model that specifies the time column name <code>event_date</code> in the <code>MODEL</code> DDL and includes a SQL <code>WHERE</code> clause to filter records by time range:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  )\n);\n\nSELECT\n  event_date::TEXT as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\nWHERE\n  event_date BETWEEN @start_ds AND @end_ds;\n</code></pre>"},{"location":"concepts/models/model_kinds/#time-column","title":"Time column","text":"<p>Vulcan needs to know which column in the model's output represents the timestamp or date associated with each record.</p> <p>Important</p> <p>The <code>time_column</code> variable should be in the UTC time zone - learn more above.</p> <p>The time column is used to determine which records will be overwritten during data restatement and provides a partition key for engines that support partitioning (such as Apache Spark). The name of the time column is specified in the <code>MODEL</code> DDL <code>kind</code> specification:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date -- This model's time information is stored in the `event_date` column\n  )\n);\n</code></pre> <p>By default, Vulcan assumes the time column is in the <code>%Y-%m-%d</code> format. For other formats, the default can be overridden with a formatting string: </p><pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (event_date, '%Y-%m-%d')\n  )\n);\n</code></pre><p></p> <p>Note</p> <p>The time format should be defined using the same SQL dialect as the one used to define the model's query.</p> <p>Vulcan also uses the time column to automatically append a time range filter to the model's query at runtime, which prevents records that are not part of the target interval from being stored. This is a safety mechanism that prevents unintentionally overwriting unrelated records when handling late-arriving data.</p> <p>The required filter you write in the model query's <code>WHERE</code> clause filters the input data as it is read from upstream tables, reducing the amount of data processed by the model. The automatically appended time range filter is applied to the model query's output data to prevent data leakage.</p> <p>Consider the following model definition, which specifies a <code>WHERE</code> clause filter with the <code>receipt_date</code> column. The model's <code>time_column</code> is a different column <code>event_date</code>, whose filter is automatically added to the model query. This approach is useful when an upstream model's time column is different from the model's time column:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date -- `event_date` is model's time column\n  )\n);\n\nSELECT\n  event_date::TEXT as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\nWHERE\n  receipt_date BETWEEN @start_ds AND @end_ds; -- Filter is based on the user-supplied `receipt_date` column\n</code></pre> <p>At runtime, Vulcan will automatically modify the model's query to look like this: </p><pre><code>SELECT\n  event_date::TEXT as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\nWHERE\n  receipt_date BETWEEN @start_ds AND @end_ds\n  AND event_date BETWEEN @start_ds AND @end_ds; -- `event_date` time column filter automatically added by Vulcan\n</code></pre><p></p>"},{"location":"concepts/models/model_kinds/#partitioning","title":"Partitioning","text":"<p>By default, we ensure that the <code>time_column</code> is part of the partitioned_by property of the model so that it forms part of the partition key and allows the database engine to do partition pruning. If it is not explicitly listed in the Model definition, we will automatically add it.</p> <p>However, this may be undesirable if you want to exclusively partition on another column or you want to partition on something like <code>month(time_column)</code> but the engine you're using doesnt support partitioning based on expressions.</p> <p>To opt out of this behaviour, you can set <code>partition_by_time_column false</code> like so:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date,\n    partition_by_time_column false\n  ),\n  partitioned_by (other_col) -- event_date will no longer be automatically added here and the partition key will just be 'other_col'\n);\n</code></pre>"},{"location":"concepts/models/model_kinds/#idempotency","title":"Idempotency","text":"<p>We recommend making sure incremental by time range model queries are idempotent to prevent unexpected results during data restatement.</p> <p>Note, however, that upstream models and tables can impact a model's idempotency. For example, referencing an upstream model of kind FULL in the model query automatically causes the model to be non-idempotent because its data could change on every model execution.</p>"},{"location":"concepts/models/model_kinds/#materialization-strategy","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE by time column partition Databricks INSERT OVERWRITE by time column partition Snowflake DELETE by time range, then INSERT BigQuery DELETE by time range, then INSERT Redshift DELETE by time range, then INSERT Postgres DELETE by time range, then INSERT DuckDB DELETE by time range, then INSERT"},{"location":"concepts/models/model_kinds/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p>Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are computed incrementally based on a key.</p> <p>They insert or update rows based on these rules:</p> <ul> <li>If a key in newly loaded data is not present in the model table, the new data row is inserted.</li> <li>If a key in newly loaded data is already present in the model table, the existing row is updated with the new data.</li> <li>If a key is present in the model table but not present in the newly loaded data, its row is not modified and remains in the model table.</li> </ul> <p>Prevent duplicated keys</p> <p>If you do not want duplicated keys in the model table, you must ensure the model query does not return rows with duplicate keys.</p> <p>Vulcan does not automatically detect or prevent duplicates.</p> <p>This kind is a good fit for datasets that have the following traits:</p> <ul> <li>Each record has a unique key associated with it.</li> <li>There is at most one record associated with each unique key.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their keys match.</li> </ul> <p>A Slowly Changing Dimension (SCD) is one approach that fits this description well. See the SCD Type 2 model kind for a specific model kind for SCD Type 2 models.</p> <p>The name of the unique key column must be provided as part of the <code>MODEL</code> DDL, as in this example: </p><pre><code>MODEL (\n  name db.employees,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key name\n  )\n);\n\nSELECT\n  name::TEXT as name,\n  title::TEXT as title,\n  salary::INT as salary\nFROM raw_employees;\n</code></pre><p></p> <p>Composite keys are also supported: </p><pre><code>MODEL (\n  name db.employees,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (first_name, last_name)\n  )\n);\n</code></pre><p></p> <p><code>INCREMENTAL_BY_UNIQUE_KEY</code> model kinds can also filter upstream records by time range using a SQL <code>WHERE</code> clause and the <code>@start_date</code>, <code>@end_date</code> or other macro variables (similar to the INCREMENTAL_BY_TIME_RANGE kind). Note that Vulcan macro time variables are in the UTC time zone. </p><pre><code>SELECT\n  name::TEXT as name,\n  title::TEXT as title,\n  salary::INT as salary\nFROM raw_employee_events\nWHERE\n  event_date BETWEEN @start_date AND @end_date;\n</code></pre><p></p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incremental_by_unique_key_example,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key id\n  ),\n  start '2020-01-01',\n  cron '@daily',\n);\n\nSELECT\n  id,\n  item_id,\n  event_date\nFROM demo.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>1161945221</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>FALSE LIMIT 0</code> in the <code>WHERE</code> statement and the placeholder dates).</p> <pre><code>SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE (`seed_model`.`event_date` &lt;= CAST('1970-01-01' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('1970-01-01' AS DATE)) AND FALSE LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE `seed_model`.`event_date` &lt;= CAST('2024-10-30' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('2020-01-01' AS DATE)) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incremental_by_unique_key_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221`\n</code></pre> <p>Note: Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are inherently non-idempotent, which should be taken into consideration during data restatement. As a result, partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated.</p>"},{"location":"concepts/models/model_kinds/#unique-key-expressions","title":"Unique Key Expressions","text":"<p>The <code>unique_key</code> values can either be column names or SQL expressions. For example, if you wanted to create a key that is based on the coalesce of a value then you could do the following:</p> <pre><code>MODEL (\n  name db.employees,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key COALESCE(\"name\", '')\n  )\n);\n</code></pre>"},{"location":"concepts/models/model_kinds/#when-matched-expression","title":"When Matched Expression","text":"<p>The logic to use when updating columns when a match occurs (the source and target match on the given keys) by default updates all the columns. This can be overriden with custom logic like below:</p> <pre><code>MODEL (\n  name db.employees,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key name,\n    when_matched (\n      WHEN MATCHED THEN UPDATE SET target.salary = COALESCE(source.salary, target.salary)\n    )\n  )\n);\n</code></pre> <p>The <code>source</code> and <code>target</code> aliases are required when using the <code>when_matched</code> expression in order to distinguish between the source and target columns.</p> <p>Multiple <code>WHEN MATCHED</code> expressions can also be provided. Ex:</p> <pre><code>MODEL (\n  name db.employees,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key name,\n    when_matched (\n      WHEN MATCHED AND source.value IS NULL THEN UPDATE SET target.salary = COALESCE(source.salary, target.salary)\n      WHEN MATCHED THEN UPDATE SET target.title = COALESCE(source.title, target.title)\n    )\n  )\n);\n</code></pre> <p>Note: <code>when_matched</code> is only available on engines that support the <code>MERGE</code> statement. Currently supported engines include:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> </ul> <p>In Redshift's case, to enable the use of the native <code>MERGE</code> statement, you need to pass the <code>enable_merge</code> flag in the connection and set it to <code>true</code>. It is disabled by default.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      enable_merge: true\n</code></pre> <p>Redshift supports only the <code>UPDATE</code> or <code>DELETE</code> actions for the <code>WHEN MATCHED</code> clause and does not allow multiple <code>WHEN MATCHED</code> expressions. For further information, refer to the Redshift documentation.</p>"},{"location":"concepts/models/model_kinds/#merge-filter-expression","title":"Merge Filter Expression","text":"<p>The <code>MERGE</code> statement typically induces a full table scan of the existing table, which can be problematic with large data volumes.</p> <p>Prevent a full table scan by passing filtering conditions to the <code>merge_filter</code> parameter.</p> <p>The <code>merge_filter</code> accepts a single or a conjunction of predicates to be used in the <code>ON</code> clause of the <code>MERGE</code> operation:</p> <pre><code>MODEL (\n  name db.employee_contracts,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key id,\n    merge_filter source._operation IS NULL AND target.contract_date &gt; dateadd(day, -7, current_date)\n  )\n);\n</code></pre> <p>Similar to <code>when_matched</code>, the <code>source</code> and <code>target</code> aliases are used to distinguish between the source and target tables.</p> <p>If an existing dbt project uses the incremental_predicates functionality, Vulcan will automatically convert them into the equivalent <code>merge_filter</code> specification.</p>"},{"location":"concepts/models/model_kinds/#materialization-strategy_1","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are materialized using the following strategies:</p> Engine Strategy Spark not supported Databricks MERGE ON unique key Snowflake MERGE ON unique key BigQuery MERGE ON unique key Redshift MERGE ON unique key Postgres MERGE ON unique key DuckDB DELETE ON matched + INSERT new rows"},{"location":"concepts/models/model_kinds/#full","title":"FULL","text":"<p>Models of the <code>FULL</code> kind cause the dataset associated with a model to be fully refreshed (rewritten) upon each model evaluation.</p> <p>The <code>FULL</code> model kind is somewhat easier to use than incremental kinds due to the lack of special settings or additional query considerations. This makes it suitable for smaller datasets, where recomputing data from scratch is relatively cheap and doesn't require preservation of processing history. However, using this kind with datasets containing a large volume of records will result in significant runtime and compute costs.</p> <p>This kind can be a good fit for aggregate tables that lack a temporal dimension. For aggregate tables with a temporal dimension, consider the INCREMENTAL_BY_TIME_RANGE kind instead.</p> <p>This example specifies a <code>FULL</code> model kind: </p><pre><code>MODEL (\n  name db.salary_by_title_agg,\n  kind FULL\n);\n\nSELECT\n  title,\n  AVG(salary)\nFROM db.employees\nGROUP BY title;\n</code></pre><p></p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.full_model_example,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM demo.incremental_model\nGROUP BY\n  item_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>2345651858</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` (`item_id` INT64, `num_orders` INT64)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>WHERE FALSE</code> and <code>LIMIT 0</code>).</p> <pre><code>SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nWHERE FALSE\nGROUP BY `incremental_model`.`item_id` LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` AS\nSELECT CAST(`item_id` AS INT64) AS `item_id`, CAST(`num_orders` AS INT64) AS `num_orders`\nFROM (SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nGROUP BY `incremental_model`.`item_id`) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`full_model_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858`\n</code></pre>"},{"location":"concepts/models/model_kinds/#materialization-strategy_2","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>FULL</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE Databricks INSERT OVERWRITE Snowflake CREATE OR REPLACE TABLE BigQuery CREATE OR REPLACE TABLE Redshift DROP TABLE, CREATE TABLE, INSERT Postgres DROP TABLE, CREATE TABLE, INSERT DuckDB CREATE OR REPLACE TABLE"},{"location":"concepts/models/model_kinds/#view","title":"VIEW","text":"<p>The model kinds described so far cause the output of a model query to be materialized and stored in a physical table.</p> <p>The <code>VIEW</code> kind is different, because no data is actually written during model execution. Instead, a non-materialized view (or \"virtual table\") is created or replaced based on the model's query.</p> <p>Note: <code>VIEW</code> is the default model kind if kind is not specified.</p> <p>Note: Python models do not support the <code>VIEW</code> model kind - use a SQL model instead.</p> <p>Note: With this kind, the model's query is evaluated every time the model is referenced in a downstream query. This may incur undesirable compute cost and time in cases where the model's query is compute-intensive, or when the model is referenced in many downstream queries.</p> <p>This example specifies a <code>VIEW</code> model kind: </p><pre><code>MODEL (\n  name db.highest_salary,\n  kind VIEW\n);\n\nSELECT\n  MAX(salary)\nFROM db.employees;\n</code></pre><p></p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.example_view,\n  kind VIEW,\n  cron '@daily',\n);\n\nSELECT\n  'hello there' as a_column\n</code></pre> <p>Vulcan will execute this SQL to create a versioned view in the physical layer. Note that the view's version fingerprint, <code>1024042926</code>, is part of the view name.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n(`a_column`) AS SELECT 'hello there' AS `a_column`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned view in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`example_view` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n</code></pre>"},{"location":"concepts/models/model_kinds/#materialized-views","title":"Materialized Views","text":"<p>The <code>VIEW</code> model kind can be configured to represent a materialized view by setting the <code>materialized</code> flag to <code>true</code>: </p><pre><code>MODEL (\n  name db.highest_salary,\n  kind VIEW (\n    materialized true\n  )\n);\n</code></pre><p></p> <p>Note: This flag only applies to engines that support materialized views and is ignored by other engines. Supported engines include:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Snowflake</li> </ul> <p>During the evaluation of a model of this kind, the view will be replaced or recreated only if the model's query rendered during evaluation does not match the query used during the previous view creation for this model, or if the target view does not exist. Thus, views are recreated only when necessary in order to realize all the benefits provided by materialized views.</p>"},{"location":"concepts/models/model_kinds/#embedded","title":"EMBEDDED","text":"<p>Embedded models are a way to share common logic between different models of other kinds.</p> <p>There are no data assets (tables or views) associated with <code>EMBEDDED</code> models in the data warehouse. Instead, an <code>EMBEDDED</code> model's query is injected directly into the query of each downstream model that references it, as a subquery.</p> <p>Note: Python models do not support the <code>EMBEDDED</code> model kind - use a SQL model instead.</p> <p>This example specifies a <code>EMBEDDED</code> model kind: </p><pre><code>MODEL (\n  name db.unique_employees,\n  kind EMBEDDED\n);\n\nSELECT DISTINCT\n  name\nFROM db.employees;\n</code></pre><p></p>"},{"location":"concepts/models/model_kinds/#seed","title":"SEED","text":"<p>The <code>SEED</code> model kind is used to specify seed models for using static CSV datasets in your Vulcan project.</p> <p>Notes:</p> <ul> <li>Seed models are loaded only once unless the SQL model and/or seed file is updated.</li> <li>Python models do not support the <code>SEED</code> model kind - use a SQL model instead.</li> </ul> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.seed_example,\n  kind SEED (\n    path '../../seeds/seed_example.csv'\n  ),\n  columns (\n    id INT64,\n    item_id INT64,\n    event_date DATE\n  ),\n  grain (id, event_date)\n)\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>3038173937</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will upload the seed as a temp table in the physical layer.</p> <pre><code>vulcan-public-demo.vulcan__demo.__temp_demo__seed_example__3038173937_9kzbpld7\n</code></pre> <p>Vulcan will create a versioned table in the physical layer from the temp table.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `id`, `item_id`, `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`) AS `_subquery`\n</code></pre> <p>Vulcan will drop the temp table in the physical layer.</p> <pre><code>DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`seed_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937`\n</code></pre>"},{"location":"concepts/models/model_kinds/#scd-type-2","title":"SCD Type 2","text":"<p>SCD Type 2 is a model kind that supports slowly changing dimensions (SCDs) in your Vulcan project. SCDs are a common pattern in data warehousing that allow you to track changes to records over time.</p> <p>Vulcan achieves this by adding a <code>valid_from</code> and <code>valid_to</code> column to your model. The <code>valid_from</code> column is the timestamp that the record became valid (inclusive) and the <code>valid_to</code> column is the timestamp that the record became invalid (exclusive). The <code>valid_to</code> column is set to <code>NULL</code> for the latest record.</p> <p>Therefore, you can use these models to not only tell you what the latest value is for a given record but also what the values were anytime in the past. Note that maintaining this history does come at a cost of increased storage and compute and this may not be a good fit for sources that change frequently since the history could get very large.</p> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss, so data restatement is disabled for models of this kind by default.</p> <p>There are two ways to tracking changes: By Time (Recommended) or By Column.</p>"},{"location":"concepts/models/model_kinds/#scd-type-2-by-time-recommended","title":"SCD Type 2 By Time (Recommended)","text":"<p>SCD Type 2 By Time supports sourcing from tables that have an \"Updated At\" timestamp defined in the table that tells you when a given record was last updated. This is the recommended way since this \"Updated At\" gives you a precise time when the record was last updated and therefore improves the accuracy of the SCD Type 2 table that is produced.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_TIME</code> model kind: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n  )\n);\n\nSELECT\n  id::INT,\n  name::STRING,\n  price::DOUBLE,\n  updated_at::TIMESTAMP\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p> <p>The <code>updated_at</code> column name can also be changed by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    updated_at_name my_updated_at -- Name for `updated_at` column\n  )\n);\n\nSELECT\n  id,\n  name,\n  price,\n  my_updated_at\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  my_updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"concepts/models/model_kinds/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>SCD Type 2 By Column supports sourcing from tables that do not have an \"Updated At\" timestamp defined in the table. Instead, it will check the columns defined in the <code>columns</code> field to see if their value has changed and if so it will record the <code>valid_from</code> time as the execution time when the change was detected.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_COLUMN</code> model kind: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key id,\n    columns [name, price]\n  )\n);\n\nSELECT\n  id::INT,\n  name::STRING,\n  price::DOUBLE,\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"concepts/models/model_kinds/#change-column-names","title":"Change Column Names","text":"<p>Vulcan will automatically add the <code>valid_from</code> and <code>valid_to</code> columns to your table. If you would like to specify the names of these columns you can do so by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    valid_from_name my_valid_from, -- Name for `valid_from` column\n    valid_to_name my_valid_to -- Name for `valid_to` column\n  )\n);\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  my_valid_from TIMESTAMP,\n  my_valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"concepts/models/model_kinds/#deletes","title":"Deletes","text":"<p>A hard delete is when a record no longer exists in the source table. When this happens,</p> <p>If <code>invalidate_hard_deletes</code> is set to <code>false</code> (default):</p> <ul> <li><code>valid_to</code> column will continue to be set to <code>NULL</code> (therefore still considered \"valid\")</li> <li>If the record is added back, then the <code>valid_to</code> column will be set to the <code>valid_from</code> of the new record.</li> </ul> <p>When a record is added back, the new record will be inserted into the table with <code>valid_from</code> set to:</p> <ul> <li>SCD_TYPE_2_BY_TIME: the largest of either the <code>updated_at</code> timestamp of the new record or the <code>valid_from</code> timestamp of the deleted record in the SCD Type 2 table</li> <li>SCD_TYPE_2_BY_COLUMN: the <code>execution_time</code> when the record was detected again</li> </ul> <p>If <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> <ul> <li><code>valid_to</code> column will be set to the time when the Vulcan run started that detected the missing record (called <code>execution_time</code>).</li> <li>If the record is added back, then the <code>valid_to</code> column will remain unchanged.</li> </ul> <p>One way to think about <code>invalidate_hard_deletes</code> is that, if <code>invalidate_hard_deletes</code> is set to <code>true</code>, deletes are most accurately tracked in the SCD Type 2 table since it records when the delete occurred. As a result though, you can have gaps between records if the there is a gap of time between when it was deleted and added back. If you would prefer to not have gaps, and a result consider missing records in source as still \"valid\", then you can leave the default value or set <code>invalidate_hard_deletes</code> to <code>false</code>.</p>"},{"location":"concepts/models/model_kinds/#example-of-scd-type-2-by-time-in-action","title":"Example of SCD Type 2 By Time in Action","text":"<p>Lets say that you started with the following data in your source table and <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> ID Name Price Updated At 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 2 Cheeseburger 8.99 2020-01-01 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Chocolate Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 4 Chocolate Milkshake 3.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL <p>Note: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 00:00:00</code> meaning if you queried the table during that time range then you would not see <code>Cheeseburger</code> in the menu. This is the most accurate representation of the menu based on the source data provided. If <code>Cheeseburger</code> were added back to the menu with it's original updated at timestamp of <code>2020-01-01 00:00:00</code> then the <code>valid_from</code> timestamp of the new record would have been <code>2020-01-02 11:00:00</code> resulting in no period of time where the item was deleted. Since in this case the updated at timestamp did not change it is likely the item was removed in error and this again most accurately represents the menu based on the source data.</p>"},{"location":"concepts/models/model_kinds/#example-of-scd-type-2-by-column-in-action","title":"Example of SCD Type 2 By Column in Action","text":"<p>Lets say that you started with the following data in your source table and <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> ID Name Price 1 Chicken Sandwich 10.99 2 Cheeseburger 8.99 3 French Fries 4.99 <p>We configure the SCD Type 2 By Column model to check the columns <code>Name</code> and <code>Price</code> for changes</p> <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 12.99 3 French Fries 4.99 4 Milkshake 3.99 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 14.99 2 Cheeseburger 8.99 3 French Fries 4.99 4 Chocolate Milkshake 3.99 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-03 11:00:00</code>, Target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 2020-01-03 11:00:00 1 Chicken Sandwich 14.99 2020-01-03 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 11:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 2020-01-03 11:00:00 4 Chocolate Milkshake 3.99 2020-01-03 11:00:00 NULL <p>Note: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 11:00:00</code> meaning if you queried the table during that time range then you would not see <code>Cheeseburger</code> in the menu. This is the most accurate representation of the menu based on the source data provided.</p>"},{"location":"concepts/models/model_kinds/#shared-configuration-options","title":"Shared Configuration Options","text":"Name Description Type unique_key Unique key used for identifying rows between source and target List of strings or string valid_from_name The name of the <code>valid_from</code> column to create in the target table. Default: <code>valid_from</code> string valid_to_name The name of the <code>valid_to</code> column to create in the target table. Default: <code>valid_to</code> string invalidate_hard_deletes If set to <code>true</code>, when a record is missing from the source table it will be marked as invalid. Default: <code>false</code> bool batch_size The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int <p>Important</p> <p>If using BigQuery, the default data type of the valid_from/valid_to columns is DATETIME. If you want to use TIMESTAMP, you can specify the data type in the model definition.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    time_data_type TIMESTAMP\n  )\n);\n</code></pre> <p>This could likely be used on other engines to change the expected data type but has only been tested on BigQuery.</p>"},{"location":"concepts/models/model_kinds/#scd-type-2-by-time-configuration-options","title":"SCD Type 2 By Time Configuration Options","text":"Name Description Type updated_at_name The name of the column containing a timestamp to check for new or updated records. Default: <code>updated_at</code> string updated_at_as_valid_from By default, for new rows <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code>. This changes the behavior to set it to the valid of <code>updated_at</code> when the row is inserted. Default: <code>false</code> bool"},{"location":"concepts/models/model_kinds/#scd-type-2-by-column-configuration-options","title":"SCD Type 2 By Column Configuration Options","text":"Name Description Type columns The name of the columns to check for changes. <code>*</code> to represent that all columns should be checked. List of strings or string execution_time_as_valid_from By default, when the model is first loaded <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code> and future new rows will have <code>execution_time</code> of when the pipeline ran. This changes the behavior to always use <code>execution_time</code>. Default: <code>false</code> bool updated_at_name If sourcing from a table that includes as timestamp to use as valid_from, set this property to that column. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int"},{"location":"concepts/models/model_kinds/#processing-source-table-with-historical-data","title":"Processing Source Table with Historical Data","text":"<p>The most common case for SCD Type 2 is creating history for a table that it doesn't have it already.  In the example of the restaurant menu, the menu just tells you what is offered right now, but you want to know what was offered over time. In this case, the default setting of <code>None</code> for <code>batch_size</code> is the best option.</p> <p>Another use case though is processing a source table that already has history in it.  A common example of this is a \"daily snapshot\" table that is created by a source system that takes a snapshot of the data at the end of each day. If your source table has historical records, like a \"daily snapshot\" table, then set <code>batch_size</code> to <code>1</code> to process each interval (each day if a <code>@daily</code> cron) in sequential order. That way the historical records will be properly captured in the SCD Type 2 table.</p>"},{"location":"concepts/models/model_kinds/#example-source-from-daily-snapshot-table","title":"Example - Source from Daily Snapshot Table","text":"<pre><code>MODEL (\n    name db.table,\n    kind SCD_TYPE_2_BY_COLUMN (\n        unique_key id,\n        columns [some_value],\n        updated_at_name ds,\n        batch_size 1\n    ),\n    start '2025-01-01',\n    cron '@daily'\n);\nSELECT\n    id,\n    some_value,\n    ds\nFROM\n    source_table\nWHERE\n    ds between @start_ds and @end_ds\n</code></pre> <p>This will process each day of the source table in sequential order (if more than one day to process), checking <code>some_value</code> column to see if it changed. If it did change, <code>valid_from</code> will be set to match the <code>ds</code> column (except for first value which would be <code>1970-01-01 00:00:00</code>).</p> <p>If the source data was the following:</p> id some_value ds 1 1 2025-01-01 1 2 2025-01-02 1 3 2025-01-03 1 3 2025-01-04 <p>Then the resulting SCD Type 2 table would be:</p> id some_value ds valid_from valid_to 1 1 2025-01-01 1970-01-01 00:00:00 2025-01-02 00:00:00 1 2 2025-01-02 2025-01-02 00:00:00 2025-01-03 00:00:00 1 3 2025-01-03 2025-01-03 00:00:00 NULL"},{"location":"concepts/models/model_kinds/#querying-scd-type-2-models","title":"Querying SCD Type 2 Models","text":""},{"location":"concepts/models/model_kinds/#querying-the-current-version-of-a-record","title":"Querying the current version of a record","text":"<p>Although SCD Type 2 models support history, it is still very easy to query for just the latest version of a record. Simply query the model as you would any other table. For example, if you wanted to query the latest version of the <code>menu_items</code> table you would simply run:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  valid_to IS NULL;\n</code></pre> <p>One could also create a view on top of the SCD Type 2 model that creates a new <code>is_current</code> column to make it easy for consumers to identify the current record.</p> <pre><code>SELECT\n  *,\n  valid_to IS NULL AS is_current\nFROM\n  menu_items;\n</code></pre>"},{"location":"concepts/models/model_kinds/#querying-for-a-specific-version-of-a-record-at-a-give-point-in-time","title":"Querying for a specific version of a record at a give point in time","text":"<p>If you wanted to query the <code>menu_items</code> table as it was on <code>2020-01-02 01:00:00</code> you would simply run:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  id = 1\n  AND '2020-01-02 01:00:00' &gt;= valid_from\n  AND '2020-01-02 01:00:00' &lt; COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>Example in a join:</p> <pre><code>SELECT\n  *\nFROM\n  orders\nINNER JOIN\n  menu_items\n  ON orders.menu_item_id = menu_items.id\n  AND orders.created_at &gt;= menu_items.valid_from\n  AND orders.created_at &lt; COALESCE(menu_items.valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>A view can be created to do the <code>COALESCE</code> automatically. This, combined with the <code>is_current</code> flag, makes it easier to query for a specific version of a record.</p> <pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP)) AS valid_to\n  valid_to IS NULL AS is_current,\nFROM\n  menu_items;\n</code></pre> <p>Furthermore if you want to make it so users can use <code>BETWEEN</code> when querying by making <code>valid_to</code> inclusive you can do the following: </p><pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2200-01-01 00:00:00+00:00' AS TIMESTAMP)) - INTERVAL 1 SECOND AS valid_to\n  valid_to IS NULL AS is_current,\n</code></pre><p></p> <p>Note: The precision of the timestamps in this example is second so I subtract 1 second. Make sure to subtract a value equal to the precision of your timestamps.</p>"},{"location":"concepts/models/model_kinds/#querying-for-deleted-records","title":"Querying for deleted records","text":"<p>One way to identify deleted records is to query for records that do not have a <code>valid_to</code> record of <code>NULL</code>. For example, if you wanted to query for all deleted ids in the <code>menu_items</code> table you would simply run:</p> <pre><code>SELECT\n  id,\n  MAX(CASE WHEN valid_to IS NULL THEN 0 ELSE 1 END) AS is_deleted\nFROM\n  menu_items\nGROUP BY\n  id\n</code></pre>"},{"location":"concepts/models/model_kinds/#reset-scd-type-2-model-clearing-history","title":"Reset SCD Type 2 Model (clearing history)","text":"<p>SCD Type 2 models are designed by default to protect the data that has been captured because it is not possible to recreate the history once it has been lost. However, there are cases where you may want to clear the history and start fresh. For this use use case you will want to start by setting <code>disable_restatement</code> to <code>false</code> in the model definition.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    disable_restatement false\n  )\n);\n</code></pre> <p>Plan/apply this change to production. Then you will want to restate the model.</p> <pre><code>vulcan plan --restate-model db.menu_items\n</code></pre> <p>Warning</p> <p>This will remove the historical data on the model which in most situations cannot be recovered.</p> <p>Once complete you will want to remove <code>disable_restatement</code> on the model definition which will set it back to <code>true</code> and prevent accidental data loss.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n  )\n);\n</code></pre> <p>Plan/apply this change to production.</p>"},{"location":"concepts/models/model_kinds/#external","title":"EXTERNAL","text":"<p>The EXTERNAL model kind is used to specify external models that store metadata about external tables. External models are special; they are not specified in .sql files like the other model kinds. They are optional but useful for propagating column and type information for external tables queried in your Vulcan project.</p>"},{"location":"concepts/models/model_kinds/#managed","title":"MANAGED","text":"<p>Warning</p> <p>Managed models are still under development and the API / semantics may change as support for more engines is added</p> <p>Note: Python models do not support the <code>MANAGED</code> model kind - use a SQL model instead.</p> <p>The <code>MANAGED</code> model kind is used to create models where the underlying database engine manages the data lifecycle.</p> <p>These models don't get updated with new intervals or refreshed when <code>vulcan run</code> is called. Responsibility for keeping the data up to date falls on the engine.</p> <p>You can control how the engine creates the managed model by using the <code>physical_properties</code> to pass engine-specific parameters for adapter to use when issuing commands to the underlying database.</p> <p>Due to there being no standard, each vendor has a different implementation with different semantics and different configuration parameters. Therefore, <code>MANAGED</code> models are not as portable between database engines as other Vulcan model types. In addition, due to their black-box nature, Vulcan has limited visibility into the integrity and state of the model.</p> <p>We would recommend using standard Vulcan model types in the first instance. However, if you do need to use Managed models, you still gain other Vulcan benefits like the ability to use them in virtual environments.</p> <p>See Managed Models for more information on which engines are supported and which properties are available.</p>"},{"location":"concepts/models/model_kinds/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Models of the <code>INCREMENTAL_BY_PARTITION</code> kind are computed incrementally based on partition. A set of columns defines the model's partitioning key, and a partition is the group of rows with the same partitioning key value.</p> <p>Should you use this model kind?</p> <p>Any model kind can use a partitioned table by specifying the <code>partitioned_by</code> key in the <code>MODEL</code> DDL.</p> <p>The \"partition\" in <code>INCREMENTAL_BY_PARTITION</code> is about how the data is loaded when the model runs.</p> <p><code>INCREMENTAL_BY_PARTITION</code> models are inherently non-idempotent, so restatements and other actions can cause data loss. This makes them more complex to manage than other model kinds.</p> <p>In most scenarios, an <code>INCREMENTAL_BY_TIME_RANGE</code> model can meet your needs and will be easier to manage. The <code>INCREMENTAL_BY_PARTITION</code> model kind should only be used when the data must be loaded by partition (usually for performance reasons).</p> <p>This model kind is designed for the scenario where data rows should be loaded and updated as a group based on their shared value for the partitioning key.</p> <p>It may be used with any SQL engine. Vulcan will automatically create partitioned tables on engines that support explicit table partitioning (e.g., BigQuery, Databricks).</p> <p>New rows are loaded based on their partitioning key value:</p> <ul> <li>If a partitioning key in newly loaded data is not present in the model table, the new partitioning key and its data rows are inserted.</li> <li>If a partitioning key in newly loaded data is already present in the model table, all the partitioning key's existing data rows in the model table are replaced with the partitioning key's data rows in the newly loaded data.</li> <li>If a partitioning key is present in the model table but not present in the newly loaded data, the partitioning key's existing data rows are not modified and remain in the model table.</li> </ul> <p>This kind should only be used for datasets that have the following traits:</p> <ul> <li>The dataset's records can be grouped by a partitioning key.</li> <li>Each record has a partitioning key associated with it.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their partitioning keys match.</li> <li>All existing records associated with a given partitioning key can be removed or overwritten when any new record has the partitioning key value.</li> </ul> <p>The column defining the partitioning key is specified in the model's <code>MODEL</code> DDL <code>partitioned_by</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_PARTITION</code> model whose partition key is the row's value for the <code>region</code> column:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by region,\n);\n</code></pre> <p>Compound partition keys are also supported, such as <code>region</code> and <code>department</code>:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by (region, department),\n);\n</code></pre> <p>Date and/or timestamp column expressions are also supported (varies by SQL engine). This BigQuery example's partition key is based on the month each row's <code>event_date</code> occurred:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by DATETIME_TRUNC(event_date, MONTH)\n);\n</code></pre> <p>Only full restatements supported</p> <p>Partial data restatements are used to reprocess part of a table's data (usually a limited time range).</p> <p>Partial data restatement is not supported for <code>INCREMENTAL_BY_PARTITION</code> models. If you restate an <code>INCREMENTAL_BY_PARTITION</code> model, its entire table will be recreated from scratch.</p> <p>Restating <code>INCREMENTAL_BY_PARTITION</code> models may lead to data loss and should be performed with care.</p>"},{"location":"concepts/models/model_kinds/#example","title":"Example","text":"<p>This is a fuller example of how you would use this model kind in practice. It limits the number of partitions to backfill based on time range in the <code>partitions_to_update</code> CTE.</p> <pre><code>MODEL (\n  name demo.incremental_by_partition_demo,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by user_segment,\n);\n\n-- This is the source of truth for what partitions need to be updated and will join to the product usage data\n-- This could be an INCREMENTAL_BY_TIME_RANGE model that reads in the user_segment values last updated in the past 30 days to reduce scope\n-- Use this strategy to reduce full restatements\nWITH partitions_to_update AS (\n  SELECT DISTINCT\n    user_segment\n  FROM demo.incremental_by_time_range_demo  -- upstream table tracking which user segments to update\n  WHERE last_updated_at BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage\n  WHERE user_segment IN (SELECT user_segment FROM partitions_to_update) -- partition filter applied here\n)\n\nSELECT\n  product_id,\n  customer_id,\n  last_usage_date,\n  usage_count,\n  feature_utilization_score,\n  user_segment,\n  CASE\n    WHEN usage_count &gt; 100 AND feature_utilization_score &gt; 0.7 THEN 'Power User'\n    WHEN usage_count &gt; 50 THEN 'Regular User'\n    WHEN usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type\nFROM product_usage\n</code></pre> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss.</p>"},{"location":"concepts/models/model_kinds/#materialization-strategy_3","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_PARTITION</code> kind are materialized using the following strategies:</p> Engine Strategy Databricks REPLACE WHERE by partitioning key Spark INSERT OVERWRITE by partitioning key Snowflake DELETE by partitioning key, then INSERT BigQuery DELETE by partitioning key, then INSERT Redshift DELETE by partitioning key, then INSERT Postgres DELETE by partitioning key, then INSERT DuckDB DELETE by partitioning key, then INSERT"},{"location":"concepts/models/model_kinds/#incremental_unmanaged","title":"INCREMENTAL_UNMANAGED","text":"<p>The <code>INCREMENTAL_UNMANAGED</code> model kind exists to support append-only tables. It's \"unmanaged\" in the sense that Vulcan doesnt try to manage how the data is loaded. Vulcan will just run your query on the configured cadence and append whatever it gets into the table.</p> <p>Should you use this model kind?</p> <p>Some patterns for data management, such as Data Vault, may rely on append-only tables. In this situation, <code>INCREMENTAL_UNMANAGED</code> is the correct type to use.</p> <p>In most other situations, you probably want <code>INCREMENTAL_BY_TIME_RANGE</code> or <code>INCREMENTAL_BY_UNIQUE_KEY</code> because they give you much more control over how the data is loaded.</p> <p>Usage of the <code>INCREMENTAL_UNMANAGED</code> model kind is straightforward:</p> <pre><code>MODEL (\n  name db.events,\n  kind INCREMENTAL_UNMANAGED,\n);\n</code></pre> <p>Since it's unmanaged, it doesnt support the <code>batch_size</code> and <code>batch_concurrency</code> properties to control how data is loaded like the other incremental model types do.</p> <p>Only full restatements supported</p> <p>Similar to <code>INCREMENTAL_BY_PARTITION</code>, attempting to restate an <code>INCREMENTAL_UNMANAGED</code> model will trigger a full restatement. That is, the model will be rebuilt from scratch rather than from a time slice you specify.</p> <p>This is because an append-only table is inherently non-idempotent. Restating <code>INCREMENTAL_UNMANAGED</code> models may lead to data loss and should be performed with care.</p>"},{"location":"concepts/models/overview/","title":"Overview","text":""},{"location":"concepts/models/overview/#overview","title":"Overview","text":"<p>Models are made up of metadata and queries that create tables and views, which can be used by other models or even outside of Vulcan. They are defined in the <code>models/</code> directory of your Vulcan project and live in <code>.sql</code> files.</p> <p>Vulcan will automatically determine the relationships among and lineage of your models by parsing SQL, so you don't have to worry about manually configuring dependencies.</p>"},{"location":"concepts/models/overview/#example","title":"Example","text":"<p>The following is an example of a model defined in SQL. Note the following aspects:</p> <ul> <li>Models can include descriptive information as comments, such as the first line.</li> <li>The first non-comment statement in the file is the <code>MODEL</code> DDL.</li> <li>The last non-comment statement is a <code>SELECT</code> query containing the logic that transforms the data.</li> </ul> <pre><code>-- Customer revenue computed and stored daily.\nMODEL (\n  name sushi.customer_total_revenue,\n  owner toby,\n  cron '@daily',\n  grain customer_id\n);\n\nSELECT\n  o.customer_id::TEXT,\n  SUM(o.amount)::DOUBLE AS revenue\nFROM sushi.orders AS o\nGROUP BY o.customer_id;\n</code></pre>"},{"location":"concepts/models/overview/#conventions","title":"Conventions","text":"<p>Vulcan attempts to infer as much as possible about your pipelines through SQL alone to reduce the cognitive overhead of switching to another format such as YAML.</p> <p>One way it does this is by inferring a model's column names and data types from its SQL query. Disable this behavior for a model by manually specifying its column names and types in the <code>columns</code> model property.</p> <p>The <code>SELECT</code> expression of a model must follow certain conventions for Vulcan to detect the necessary metadata to operate.</p>"},{"location":"concepts/models/overview/#unique-column-names","title":"Unique column names","text":"<p>The final <code>SELECT</code> of a model's query must contain unique column names.</p>"},{"location":"concepts/models/overview/#explicit-types","title":"Explicit types","text":"<p>Vulcan encourages explicit type casting in the final <code>SELECT</code> of a model's query. It is considered a best practice to prevent unexpected types in the schema of a model's table.</p> <p>Vulcan uses the postgres <code>x::int</code> syntax for casting; the casts are automatically transpiled to the appropriate format for the execution engine.</p> <pre><code>WITH cte AS (\n  SELECT 1 AS foo -- don't need to cast here\n)\nSELECT foo::int -- need to cast here because it's in the final select statement\n</code></pre>"},{"location":"concepts/models/overview/#inferrable-names","title":"Inferrable names","text":"<p>The final <code>SELECT</code> of a model's query must have inferrable names or aliases.</p> <p>Explicit aliases are recommended, but not required. The Vulcan formatter will automatically add aliases to columns without them when the model SQL is rendered.</p> <p>This example demonstrates non-inferrable, inferrable, and explicit aliases:</p> <pre><code>SELECT\n  1, -- not inferrable\n  x + 1, -- not inferrable\n  SUM(x), -- not inferrable\n  x, -- inferrable as x\n  x::int, -- inferrable as x\n  x + 1 AS x, -- explicitly x\n  SUM(x) as x, -- explicitly x\n</code></pre>"},{"location":"concepts/models/overview/#model-description-and-comments","title":"Model description and comments","text":"<p>Model files may contain SQL comments in a format supported in the model's SQL dialect. (Comments begin with <code>--</code> or are gated by <code>/*</code> and <code>*/</code> in most dialects.)</p> <p>Some SQL engines support registering comments as metadata associated with a table or view. They may support table-level comments (e.g., \"Revenue data for each customer\") and/or column-level comments (e.g., \"Customer's unique ID\").</p> <p>Vulcan will automatically register comments if the engine supports it and the connection's <code>register_comments</code> configuration is <code>true</code> (<code>true</code> by default). Engines vary in their support for comments - see tables below.</p>"},{"location":"concepts/models/overview/#model-comment","title":"Model comment","text":"<p>Vulcan will register a comment specified before the <code>MODEL</code> DDL block as the table comment in the underlying SQL engine. If the <code>MODEL</code> DDL <code>description</code> field is also specified, Vulcan will register it with the engine instead.</p>"},{"location":"concepts/models/overview/#explicit-column-comments","title":"Explicit column comments","text":"<p>You may explicitly specify column comments in the <code>MODEL</code> DDL <code>column_descriptions</code> field.</p> <p>Specify them as a dictionary of key/value pairs separated by an equals sign <code>=</code>, where the column name is the key and the column comment is the value. For example:</p> <pre><code>MODEL (\n  name sushi.customer_total_revenue,\n  cron '@daily',\n  column_descriptions (\n    id = 'This is the ID column comment'\n  )\n);\n</code></pre> <p>If the <code>column_descriptions</code> key is present, Vulcan will not detect and register inline column comments from the model query.</p>"},{"location":"concepts/models/overview/#inline-column-comments","title":"Inline column comments","text":"<p>If the <code>column_descriptions</code> key is not present in the <code>MODEL</code> definition, Vulcan will automatically detect comments in a query's column selections and register each column's final comment in the underlying SQL engine.</p> <p>For example, the physical table created for the following model definition would have:</p> <ol> <li>The value of its <code>MODEL</code> DDL <code>description</code> field, \"Revenue data for each customer\", registered as a table comment in the SQL engine</li> <li>The comment on the <code>customer_id</code> column definition, \"Customer's unique ID\", registered as a column comment for the table's <code>customer_id</code> column</li> <li>The second comment on the <code>revenue</code> column definition, \"Revenue from customer orders\", registered as a column comment for the table's <code>revenue</code> column</li> </ol> <pre><code>-- The MODEL DDL 'description' field is present, so this comment will not be registered with the SQL engine\nMODEL (\n  name sushi.customer_total_revenue,\n  owner toby,\n  cron '@daily',\n  grain customer_id,\n  description 'Revenue data for each customer'\n);\n\nSELECT\n  o.customer_id::TEXT, -- Customer's unique ID\n  -- This comment will not be registered because another `revenue` comment is present\n  SUM(o.amount)::DOUBLE AS revenue -- Revenue from customer orders\nFROM sushi.orders AS o\nGROUP BY o.customer_id;\n</code></pre>"},{"location":"concepts/models/overview/#python-models","title":"Python models","text":"<p>Python models are not parsed like SQL models, so column comments cannot be inferred from the model definition's inline comments.</p> <p>Instead, specify them in the <code>@model</code> decorator's <code>column_descriptions</code> key. Specify them in a dictionary whose keys are column names and values are the columns' comments. Vulcan will error if a column name is present that is not also in the <code>columns</code> key.</p> <p>For example:</p> <pre><code>from vulcan import ExecutionContext, model\n\n@model(\n    \"my_model.name\",\n    columns={\n        \"column_name\": \"int\",\n    },\n    column_descriptions={\n        \"column_name\": \"The `column_name` column comment\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n</code></pre>"},{"location":"concepts/models/overview/#comment-registration-by-object-type","title":"Comment registration by object type","text":"<p>Only some tables/views have comments registered:</p> <ul> <li>Temporary tables are not registered</li> <li>Non-temporary tables and views in the physical layer (i.e., the schema named <code>vulcan__[project schema name]</code>) are registered</li> <li>Views in non-prod environments are not registered</li> <li>Views in the <code>prod</code> environment are registered</li> </ul> <p>Some engines automatically pass comments from physical tables through to views that select from them. In those engines, views may display comments even if Vulcan did not explicitly register them.</p>"},{"location":"concepts/models/overview/#engine-comment-support","title":"Engine comment support","text":"<p>Engines vary in their support for comments and their method(s) of registering comments. Engines may support one or both registration methods: in the <code>CREATE</code> command that creates the object or with specific post-creation commands.</p> <p>In the former method, column comments are embedded in the <code>CREATE</code> schema definition - for example: <code>CREATE TABLE my_table (my_col INTEGER COMMENT 'comment on my_col') COMMENT 'comment on my_table'</code>. This means that all table and column comments can be registered in a single command.</p> <p>In the latter method, separate commands are required for every comment. This may result in many commands: one for the table comment and one for each column comment. In some scenarios, Vulcan is not able to use the former <code>CREATE</code> method and must issue separate commands. Because Vulcan must use different methods in different situations and engines vary in their support of the methods, comments may not be registered for all objects.</p> <p>This table lists each engine's support for <code>TABLE</code> and <code>VIEW</code> object comments:</p> Engine <code>TABLE</code> comments <code>VIEW</code> comments Athena N N BigQuery Y Y ClickHouse Y Y Databricks Y Y DuckDB &lt;=0.9 N N DuckDB &gt;=0.10 Y Y MySQL Y Y MSSQL N N Postgres Y Y GCP Postgres Y Y Redshift Y N Snowflake Y Y Spark Y Y Trino Y Y"},{"location":"concepts/models/overview/#model-properties","title":"Model properties","text":"<p>The <code>MODEL</code> DDL statement takes various properties, which are used for both metadata and controlling behavior.</p> <p>Learn more about these properties and their default values in the model configuration reference.</p>"},{"location":"concepts/models/overview/#name","title":"name","text":"<p>Name specifies the name of the model. This name represents the production view name that the model outputs, so it generally takes the form of <code>\"schema\".\"view_name\"</code>. The name of a model must be unique in a Vulcan project.</p> <p>When models are used in non-production environments, Vulcan automatically prefixes the names. For example, consider a model named <code>\"sushi\".\"customers\"</code>. In production its view is named <code>\"sushi\".\"customers\"</code>, and in dev its view is named <code>\"sushi__dev\".\"customers\"</code>.</p> <p>Name is required and must be unique, unless name inference is enabled.</p>"},{"location":"concepts/models/overview/#project","title":"project","text":"Project specifies the name of the project the model belongs to. Used in multi-repo Vulcan deployments."},{"location":"concepts/models/overview/#kind","title":"kind","text":"Kind specifies what kind a model is. A model's kind determines how it is computed and stored. The default kind is <code>VIEW</code> for SQL models, which means a view is created and your query is run each time that view is accessed. On the other hand, the default kind for Python models is <code>FULL</code>, which means that a table is created and the Python code is executed each time the model is evaluated. See below for properties that apply to incremental model kinds."},{"location":"concepts/models/overview/#audits","title":"audits","text":"Audits specifies which audits should run after the model is evaluated."},{"location":"concepts/models/overview/#dialect","title":"dialect","text":"Dialect defines the SQL dialect of the model. By default, this uses the dialect in the configuration file <code>model_defaults</code> <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed."},{"location":"concepts/models/overview/#owner","title":"owner","text":"Owner specifies who the main point of contact is for the model. It is an important field for organizations that have many data collaborators."},{"location":"concepts/models/overview/#stamp","title":"stamp","text":"An optional arbitrary string sequence used to create a new model version without changing the functional components of the definition."},{"location":"concepts/models/overview/#tags","title":"tags","text":"Tags are one or more labels used to organize your models."},{"location":"concepts/models/overview/#cron","title":"cron","text":"Cron is used to schedule when your model processes or refreshes data. It accepts a cron expression or any of <code>@hourly</code>, <code>@daily</code>, <code>@weekly</code>, or <code>@monthly</code>. All times are assumed to be UTC timezone by default."},{"location":"concepts/models/overview/#cron_tz","title":"cron_tz","text":"Cron timezone is used to specify the timezone of the cron. This is only used for scheduling and does not affect the intervals processed in an incremental model. For example, if a model is <code>@daily</code> with cron_tz <code>America/Los_Angeles</code>, it will run every day 12AM pacific time, however the <code>start</code> and <code>end</code> variables passed to the incremental model will represent the UTC date boundaries."},{"location":"concepts/models/overview/#interval_unit","title":"interval_unit","text":"<p>Interval unit determines the temporal granularity with which time intervals are calculated for the model.</p> <p>By default, the interval unit is automatically derived from the <code>cron</code> expression and does not need to be specified.</p> <p>Supported values are: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, and <code>five_minute</code>.</p>"},{"location":"concepts/models/overview/#relationship-to-cron","title":"Relationship to <code>cron</code>","text":"<p>The Vulcan scheduler needs two temporal pieces of information from a model: specific times when the model should run and the finest temporal granularity with which the data is processed or stored. The <code>interval_unit</code> specifies that granularity.</p> <p>If a model's <code>cron</code> parameter is a frequency like <code>@daily</code>, the run times and <code>interval_unit</code> are simple to determine: the model is ready to run at the start of the day, and its <code>interval_unit</code> is <code>day</code>. Similarly, a <code>cron</code> of <code>@hourly</code> is ready to run at the start of each hour, and its <code>interval_unit</code> is <code>hour</code>.</p> <p>If <code>cron</code> is specified with a cron expression, however, Vulcan uses a more complex approach to derive the <code>interval_unit</code>.</p> <p>A cron expression can generate complex time intervals, so Vulcan does not parse it directly. Instead, it:</p> <ol> <li>Generates the next five run times from the cron expression (relative to the time of calculation)</li> <li>Calculates the duration of the intervals between those five values</li> <li>Determines the model's <code>interval_unit</code> as the largest interval unit value that is less than or equal to the minimum duration from (2)</li> </ol> <p>For example, consider a cron expression corresponding to \"run every 43 minutes.\" Its <code>interval_unit</code> is <code>half_hour</code> because that is the largest <code>interval_unit</code> value shorter than 43 minutes. If the cron expression is \"run every 67 minutes\", its <code>interval_unit</code> is <code>hour</code> given the same logic.</p> <p>However, <code>interval_unit</code> does not have to be inferred from <code>cron</code> - you can specify it explicitly to customize how your backfill occurs.</p>"},{"location":"concepts/models/overview/#specifying-interval_unit","title":"Specifying <code>interval_unit</code>","text":"<p>Models often run on a regular cadence, where the same amount of time passes between each run and the same time length of data is processed in each run.</p> <p>For example, a model might run at midnight every day (1 run per day) to process the previous day's data (1 day's worth of data per run). The length of time between runs and the time length of data processed in each run are both 1 day (or both 2 days if you miss a run).</p> <p>However, the run cadence length and processed data length do not have to be the same.</p> <p>Consider a model that runs every day at 7:30am and processes data up until 7am today. The model's <code>cron</code> is a cron expression for \"run every day at 7:30am,\" from which Vulcan infers an <code>interval_unit</code> of <code>day</code>.</p> <p>What will happen when this model runs? First, Vulcan will identify the most recent completed interval. The <code>interval_unit</code> was inferred to be <code>day</code>, so the last complete interval was yesterday. Vulcan will not include any of today's data between 12:00am and 7:00am in the run.</p> <p>To include today's data, manually specify an <code>interval_unit</code> of <code>hour</code>. When the model runs at 7:30am, Vulcan will identify the most recent completed <code>hour</code> interval as 6:00-7:00am and include data through that interval in the backfill.</p> <pre><code>MODEL (\n    name vulcan_example.up_until_7,\n    kind INCREMENTAL_BY_TIME_RANGE (\n      time_column date_column,\n    ),\n    start '2024-11-01',\n    cron '30 7 * * *', -- cron expression for \"every day at 7:30am\"\n    interval_unit 'hour', -- backfill up until the most recently completed hour (rather than day)\n  );\n</code></pre> <p>Caution: complex use case</p> <p>The example below is a complex use case that uses the <code>allow_partials</code> configuration option. We recommend that you do NOT use this option unless absolutely necessary.</p> <p>When partials are allowed, you will not be able to determine the cause of missing data. A pipeline problem and a correctly executed partial backfill both result in missing data, so you may not be able to differentiate the two.</p> <p>Overall, you risk sharing incomplete/incorrect data even when Vulcan runs successfully. Learn more on the Tobiko blog.</p> <p>This section configures a model that:</p> <ul> <li>Runs every hour</li> <li>Processes data for the last two days on every run</li> <li>Processes the data that has accumulated so far today on every run</li> </ul> <p>Configuring this model requires letting Vulcan process partially completed intervals by setting the model configuration <code>allow_partials True</code>.</p> <p>The data for partial intervals is only temporary - Vulcan will reprocess the entire interval once it is complete.</p> <pre><code>MODEL (\n    name vulcan_example.demo,\n    kind INCREMENTAL_BY_TIME_RANGE (\n      time_column date_column,\n      lookback 2, -- 2 days of late-arriving data to backfill\n    ),\n    start '2024-11-01',\n    cron '@hourly', -- run model hourly, not tied to the interval_unit\n    allow_partials true, -- allow partial intervals so today's data is processed in each run\n    interval_unit 'day', -- finest granularity of data to be time bucketed\n);\n</code></pre> <p>The <code>lookback</code> is calculated in days because the model's <code>interval_unit</code> is specified as <code>day</code>.</p>"},{"location":"concepts/models/overview/#start","title":"start","text":"Start is used to determine the earliest time needed to process the model. It can be an absolute date/time (<code>2022-01-01</code>), or a relative one (<code>1 year ago</code>)."},{"location":"concepts/models/overview/#end","title":"end","text":"End is used to determine the latest time needed to process the model. It can be an absolute date/time (<code>2022-01-01</code>), or a relative one (<code>1 year ago</code>)."},{"location":"concepts/models/overview/#description","title":"description","text":"Optional description of the model. Automatically registered as a table description/comment with the underlying SQL engine (if supported by the engine)."},{"location":"concepts/models/overview/#column_descriptions","title":"column_descriptions","text":"Optional dictionary of key/value pairs. Automatically registered as column descriptions/comments with the underlying SQL engine (if supported by the engine). If not present, inline comments will automatically be registered."},{"location":"concepts/models/overview/#grain","title":"grain","text":"A model's grain is the column or combination of columns that uniquely identify a row in the results returned by the model's query. If the grain is set, Vulcan tools like <code>table_diff</code> are simpler to run because they automatically use the model grain for parameters that would otherwise need to be specified manually."},{"location":"concepts/models/overview/#grains","title":"grains","text":"A model can define multiple grains if it has more than one unique key or combination of keys."},{"location":"concepts/models/overview/#references","title":"references","text":"<p>References are non-unique columns or combinations of columns that identify a join relationship to another model.</p> <p>For example, a model could define a reference <code>account_id</code>, which would indicate that it can now automatically join to any model with an <code>account_id</code> grain. It cannot safely join to a table with an <code>account_id</code> reference because references are not unique and doing so would constitute a many-to-many join.</p> <p>Sometimes columns are named differently, in that case you can alias column names to a common entity name. For example <code>guest_id AS account_id</code> would allow a model with the column guest_id to join to a model with the grain account_id.</p>"},{"location":"concepts/models/overview/#depends_on","title":"depends_on","text":"Depends on explicitly specifies the models on which the model depends, in addition to the ones automatically inferred by from the model code."},{"location":"concepts/models/overview/#table_format","title":"table_format","text":"<p>Table format is an optional property for engines that support table formats like <code>iceberg</code> and <code>hive</code> where the physical file format is configurable. The intention is to define the table type using <code>table_format</code> and then the on-disk format of the files within the table using <code>storage_format</code>.</p> <p>Note that this property only implemented for engines that allow the <code>table_format</code> to be configured independently of the <code>storage_format</code>.</p>"},{"location":"concepts/models/overview/#storage_format","title":"storage_format","text":"Storage format is a property for engines such as Spark or Hive that support storage formats such as <code>parquet</code> and <code>orc</code>. Note that some engines dont make a distinction between <code>table_format</code> and <code>storage_format</code>, in which case <code>storage_format</code> is used and <code>table_format</code> is ignored."},{"location":"concepts/models/overview/#partitioned_by","title":"partitioned_by","text":"<p>Partitioned by plays two roles. For most model kinds, it is an optional property for engines that support table partitioning such as Spark or BigQuery.</p> <p>For the <code>INCREMENTAL_BY_PARTITION</code> model kind, it defines the partition key used to incrementally load data.</p> <p>It can specify a multi-column partition key or modify a date column for partitioning. For example, in BigQuery you could partition by day by extracting the day component of a timestamp column <code>event_ts</code> with <code>partitioned_by TIMESTAMP_TRUNC(event_ts, DAY)</code>.</p>"},{"location":"concepts/models/overview/#clustered_by","title":"clustered_by","text":"Clustered by is an optional property for engines such as Bigquery that support clustering."},{"location":"concepts/models/overview/#columns","title":"columns","text":"<p>By default, Vulcan infers a model's column names and types from its SQL query. Disable that behavior by manually specifying all column names and data types in the model's <code>columns</code> property.</p> <p>WARNING: Vulcan may exhibit unexpected behavior if the <code>columns</code> property includes columns not returned by the query, omits columns returned by the query, or specifies data types other than the ones returned by the query.</p> <p>For example, this shows a seed model definition that includes the <code>columns</code> key. It specifies the data types for all columns in the file: the <code>holiday_name</code> column is data type <code>VARCHAR</code> and the <code>holiday_date</code> column is data type <code>DATE</code>.</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv'\n  ),\n  columns (\n    holiday_name VARCHAR,\n    holiday_date DATE\n  )\n);\n</code></pre> <p>NOTE: Specifying column names and data types is required for Python models that return DataFrames.</p>"},{"location":"concepts/models/overview/#physical_properties","title":"physical_properties","text":"<p>Previously named <code>table_properties</code></p> <p>Physical properties is a key-value mapping of arbitrary properties that are applied to the model table / view in the physical layer. Note the partitioning details and <code>creatable_type</code> which overrides the kind of model/view created. In this case it creates a <code>TRANSIENT TABLE</code>. While <code>creatable_type</code> is generic, other properties are adapter specific so check the engine documentation for those. For example:</p> <pre><code>MODEL (\n  ...,\n  physical_properties (\n    partition_expiration_days = 7,\n    require_partition_filter = true,\n    creatable_type = TRANSIENT\n  )\n);\n</code></pre>"},{"location":"concepts/models/overview/#virtual_properties","title":"virtual_properties","text":"<p>Virtual properties is a key-value mapping of arbitrary properties that are applied to the model view in the virtual layer. Note the partitioning details and <code>creatable_type</code> which overrides the kind of model/view created. In this case it creates a <code>SECURE VIEW</code>. While <code>creatable_type</code> is generic, other properties are adapter specific so check the engine documentation for those. For example:</p> <pre><code>MODEL (\n  ...,\n  virtual_properties (\n    creatable_type = SECURE,\n    labels = [('test-label', 'label-value')]\n  )\n);\n</code></pre>"},{"location":"concepts/models/overview/#session_properties","title":"session_properties","text":"Session properties is a key-value mapping of arbitrary properties specific to the target engine that are applied to the engine session."},{"location":"concepts/models/overview/#allow_partials","title":"allow_partials","text":"<p>Indicates that this model can be executed for partial (incomplete) data intervals.</p> <p>By default, each model processes only complete intervals to prevent common errors caused by partial data. The size of the interval is determined by the model's interval_unit.</p> <p>Setting <code>allow_partials</code> to <code>true</code> overrides this behavior, indicating that the model may process a segment of input data that is missing some of the data points.</p> <p>NOTE: To force the model to run every time, set <code>allow_partials</code> to <code>true</code> and use the <code>--ignore-cron</code> argument: <code>vulcan run --ignore-cron</code>. Simply setting <code>allow_partials</code> to <code>true</code> does not guarantee that the model will run on every <code>vulcan run</code> command invocation. The model\u2019s configured <code>cron</code> schedule is still respected, even when partial intervals are allowed. </p> <p>Similarly, using <code>--ignore-cron</code> without setting <code>allow_partials</code> to <code>true</code> does not guarantee the model will run every time. Depending on the time of day, the interval might not be complete and ready for execution, even when ignoring the <code>cron</code> schedule. Therefore, both are required to ensure that the model runs on every <code>vulcan run</code> invocation.</p>"},{"location":"concepts/models/overview/#enabled","title":"enabled","text":"Whether the model is enabled. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to ignore this model when loading the project."},{"location":"concepts/models/overview/#physical_version","title":"physical_version","text":"<p>Pins the version of this model's physical table to the given value.</p> <p>NOTE: This can only be set for forward-only models.</p>"},{"location":"concepts/models/overview/#gateway","title":"gateway","text":"Specifies the gateway to use for the execution of this model. When not specified, the default gateway is used."},{"location":"concepts/models/overview/#optimize_query","title":"optimize_query","text":"Whether the model's query should be optimized. All SQL models are optimized by default. Setting this to <code>false</code> causes Vulcan to disable query canonicalization &amp; simplification. This should be turned off only if the optimized query leads to errors such as surpassing text limit. <p>Warning</p> <p>Turning off the optimizer may prevent column-level lineage from working for the affected model and its descendants, unless all columns in the model's query are qualified and it contains no star projections (e.g. <code>SELECT *</code>).</p>"},{"location":"concepts/models/overview/#validate_query","title":"validate_query","text":"Whether the model's query will be validated at compile time. This attribute is <code>false</code> by default. Setting it to <code>true</code> causes Vulcan to raise an error instead of emitting warnings. This will display invalid columns in your SQL statements along with models containing <code>SELECT *</code> that cannot be automatically expanded to list out all columns. This ensures SQL is verified locally before time and money are spent running the SQL in your data warehouse. <p>Warning</p> <p>This flag is deprecated as of v.0.159.7+ in favor of the linter. To preserve validation during compilation, the built-in rules that check for correctness should be configured to error severity.</p>"},{"location":"concepts/models/overview/#ignored_rules","title":"ignored_rules","text":"Specifies which linter rules should be ignored/excluded for this model."},{"location":"concepts/models/overview/#formatting","title":"formatting","text":"Whether the model will be formatted. All models are formatted by default. Setting this to <code>false</code> causes Vulcan to ignore this model during <code>vulcan format</code>."},{"location":"concepts/models/overview/#incremental-model-properties","title":"Incremental Model Properties","text":"<p>These properties can be specified in an incremental model's <code>kind</code> definition.</p> <p>Some properties are only available in specific model kinds - see the model configuration reference for more information and a complete list of each <code>kind</code>'s properties.</p>"},{"location":"concepts/models/overview/#time_column","title":"time_column","text":"<p>Time column is a required property for incremental models. It is used to determine which records to overwrite when doing an incremental insert. Time column can have an optional format string specified in the SQL dialect of the model.</p> <p>Engines that support partitioning, such as Spark and BigQuery, use the time column as the model's partition key. Multi-column partitions or modifications to columns can be specified with the <code>partitioned_by</code> property.</p> <p>Important</p> <p>The <code>time_column</code> variable should be in the UTC time zone - learn more here.</p>"},{"location":"concepts/models/overview/#batch_size","title":"batch_size","text":"<p>Batch size is used to backfill incremental data when the number of intervals to backfill is too large for the engine to execute in a single pass. It allows you to process sets of intervals in batches small enough to execute on your system. The <code>batch_size</code> parameter determines the maximum number of <code>interval_unit</code>s of data to run in a single job.</p> <p>For example, consider a model with an <code>@hourly</code> <code>cron</code> that has not run in 3 days. Because its <code>cron</code> is <code>@hourly</code>, its <code>interval_unit</code> is <code>hour</code>.</p> <p>First, let's calculate the total number of outstanding intervals to backfill: 3 days of unprocessed data * 24 hours/day = 72 <code>hour</code> intervals.</p> <p>Now we can calculate the number of jobs for different <code>batch_size</code> values with this formula:</p> <p>Number of Intervals / <code>batch_size</code> = Number of jobs to run</p> <p>Let's look at the number of jobs for a few different <code>batch_size</code> values:   - <code>batch_size</code> not specified: scheduler will spawn 1 job that processes all 72 intervals (Vulcan's default behavior)   - <code>batch_size</code> of 1: scheduler will spawn [72 <code>hour</code> intervals / 1 interval per job] = 72 jobs   - <code>batch_size</code> of 12: scheduler will spawn [72 <code>hour</code> intervals / 12 intervals per job] = 6 jobs</p>"},{"location":"concepts/models/overview/#batch_concurrency","title":"batch_concurrency","text":"The maximum number of batches that can run concurrently for this model. If not specified, the concurrency is only constrained by the number of concurrent tasks set in the connection settings."},{"location":"concepts/models/overview/#lookback","title":"lookback","text":"<p>Lookback is used with incremental by time range and incremental by unique key models to capture late-arriving data. It allows the model to access data points not in the time interval currently being processed.</p> <p>It must be a positive integer and specifies how many <code>interval_unit</code>s intervals before the current interval the model should include.</p> <p>For example, consider a model with cron <code>@daily</code> (<code>interval_unit</code> <code>day</code>). If the model specified a <code>lookback</code> of 7, Vulcan would include the 7 days prior to the time interval being processed. A model with cron <code>@weekly</code> and <code>lookback</code> of 7 would include the 7 weeks prior to the time interval being processed.</p> <p>Or consider a model whose cron expression is \"run every 6 hours\" (<code>0 */6 * * *</code>). Vulcan calculates its <code>interval_unit</code> as <code>hour</code>. The <code>lookback</code> value is calculated in <code>interval_units</code>, so a <code>lookback</code> of 1 would include the 1 hour prior to the time interval being processed.</p>"},{"location":"concepts/models/overview/#forward_only","title":"forward_only","text":"Set this to true to indicate that all changes to this model should be forward-only."},{"location":"concepts/models/overview/#on_destructive_change","title":"on_destructive_change","text":"<p>What should happen when a change to a forward-only model or incremental model in a forward-only plan causes a destructive modification to the table schema (i.e., requires dropping an existing column or modifying column constraints in ways that could cause data loss).</p> <p>Vulcan checks for destructive changes at plan time based on the model definition and run time based on the model's underlying physical tables.</p> <p>Must be one of the following values: <code>allow</code>, <code>warn</code>, <code>error</code> (default), or <code>ignore</code>.</p>"},{"location":"concepts/models/overview/#on_additive_change","title":"on_additive_change","text":"<p>What should happen when a change to a forward-only model or incremental model in a forward-only plan causes an additive modification to the table schema (i.e., adding new columns, modifying column data types in compatible ways, ect.).</p> <p>Vulcan checks for additive changes at plan time based on the model definition and run time based on the model's underlying physical tables.</p> <p>Must be one of the following values: <code>allow</code> (default), <code>warn</code>, <code>error</code>, or <code>ignore</code>.</p>"},{"location":"concepts/models/overview/#disable_restatement","title":"disable_restatement","text":"Set this to true to indicate that data restatement is disabled for this model."},{"location":"concepts/models/overview/#auto_restatement_cron","title":"auto_restatement_cron","text":"<p>A cron expression that determines when Vulcan should automatically restate this model. Restatement means re-evaluating either a number of last intervals (controlled by <code>auto_restatement_intervals</code>) for model kinds that support it or the entire model for model kinds that don't. Downstream models that depend on this model will also be restated. The auto-restatement is only applied when running the <code>vulcan run</code> command against the production environment.</p> <p>A common use case for auto-restatement is to periodically re-evaluate a model (less frequently than the model's cron) to account for late-arriving data or dimension changes. However, relying on this feature is generally not recommended, as it often indicates an underlying issue with the data model or dependency chain. Instead, users should prefer setting the <code>lookback</code> property to handle late-arriving data more effectively.</p> <p>Unlike the <code>lookback</code> property, which only controls the time range of data scanned, auto-restatement rewrites all previously processed data for this model in the target table.</p> <p>For model kinds that don't support <code>auto_restatement_intervals</code> the table will be re-created from scratch.</p> <p>Models with <code>disable_restatement</code> set to <code>true</code> will not be restated automatically even if this property is set.</p> <p>NOTE: Models with this property set can only be previewed in development environments, which means that the data computed in those environments will not be reused in production.</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  cron '@daily',\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key key,\n    auto_restatement_cron '@weekly',\n  )\n);\n</code></pre>"},{"location":"concepts/models/overview/#auto_restatement_intervals","title":"auto_restatement_intervals","text":"<p>The number of last intervals to restate automatically. This is only applied in conjunction with <code>auto_restatement_cron</code>.</p> <p>If not specified, the entire model will be restated.</p> <p>This property is only supported for the <code>INCREMENTAL_BY_TIME_RANGE</code> model kind.</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  cron '@daily',\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    auto_restatement_cron '@weekly',\n    auto_restatement_intervals 7, -- automatically restate the last 7 days of data\n  )\n);\n</code></pre>"},{"location":"concepts/models/overview/#macros","title":"Macros","text":"<p>Macros can be used for passing in parameterized arguments such as dates, as well as for making SQL less repetitive. By default, Vulcan provides several predefined macro variables that can be used. Macros are used by prefixing with the <code>@</code> symbol. For more information, refer to macros.</p>"},{"location":"concepts/models/overview/#statements","title":"Statements","text":"<p>Models can have additional statements that run before and/or after the main query. They can be useful for loading things such as UDFs or cleaning up after a model query has run.</p> <p>In general, pre-statements statements should only be used for preparing the main query. They should not be used for creating or altering tables, as this could lead to unpredictable behavior if multiple models are running simultaneously.</p> <pre><code>MODEL (\n...\n);\n\n-- Additional statements preparing for main query\nADD JAR s3://special_udf.jar;\nCREATE TEMPORARY FUNCTION UDF AS 'my.jar.udf';\n\nSELECT UDF(x)::int AS x\nFROM y;\n</code></pre> <p>Additional statements can also be provided after the main query, in which case they will run after each evaluation of the SELECT query. Note that the model query must end with a semi-colon prior to the post-statements.</p> <pre><code>MODEL (\n...\n);\n\n...\n\nSELECT UDF(x)::int AS x\nFROM y;\n\n-- Cleanup statements\nDROP TABLE temp_table;\n</code></pre>"},{"location":"concepts/models/python_models/","title":"Python models","text":""},{"location":"concepts/models/python_models/#python-models","title":"Python models","text":"<p>Although SQL is a powerful tool, some use cases are better handled by Python. For example, Python may be a better option in pipelines that involve machine learning, interacting with external APIs, or complex business logic that cannot be expressed in SQL.</p> <p>Vulcan has first-class support for models defined in Python; there are no restrictions on what can be done in the Python model as long as it returns a Pandas or Spark DataFrame instance.</p> <p>Unsupported model kinds</p> <p>Python models do not support these model kinds - use a SQL model instead.</p> <ul> <li><code>VIEW</code></li> <li><code>SEED</code></li> <li><code>MANAGED</code></li> <li><code>EMBEDDED</code></li> </ul>"},{"location":"concepts/models/python_models/#definition","title":"Definition","text":"<p>To create a Python model, add a new file with the <code>*.py</code> extension to the <code>models/</code> directory. Inside the file, define a function named <code>execute</code>. For example:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"my_model.name\",\n    columns={\n        \"column_name\": \"int\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n</code></pre> <p>The <code>execute</code> function is wrapped with the <code>@model</code> decorator, which is used to capture the model's metadata (similar to the <code>MODEL</code> DDL statement in SQL models).</p> <p>Because Vulcan creates tables before evaluating models, the schema of the output DataFrame is a required argument. The <code>@model</code> argument <code>columns</code> contains a dictionary of column names to types.</p> <p>The function takes an <code>ExecutionContext</code> that is able to run queries and to retrieve the current time interval that is being processed, along with arbitrary key-value arguments passed in at runtime. The function can either return a Pandas, PySpark, Bigframe, or Snowpark Dataframe instance.</p> <p>If the function output is too large, it can also be returned in chunks using Python generators.</p>"},{"location":"concepts/models/python_models/#model-specification","title":"<code>@model</code> specification","text":"<p>The arguments provided in the <code>@model</code> specification have the same names as those provided in a SQL model's <code>MODEL</code> DDL.</p> <p>Python model <code>kind</code>s are specified with a Python dictionary containing the kind's name and arguments. All model kind arguments are listed in the models configuration reference page.</p> <p>The model <code>kind</code> dictionary must contain a <code>name</code> key whose value is a member of the <code>ModelKindName</code> enum class. The <code>ModelKindName</code> class must be imported at the beginning of the model definition file before being used in the <code>@model</code> specification.</p> <p>Supported <code>kind</code> dictionary <code>name</code> values are:</p> <ul> <li><code>ModelKindName.VIEW</code></li> <li><code>ModelKindName.FULL</code></li> <li><code>ModelKindName.SEED</code></li> <li><code>ModelKindName.INCREMENTAL_BY_TIME_RANGE</code></li> <li><code>ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</code></li> <li><code>ModelKindName.INCREMENTAL_BY_PARTITION</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_TIME</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_COLUMN</code></li> <li><code>ModelKindName.EMBEDDED</code></li> <li><code>ModelKindName.CUSTOM</code></li> <li><code>ModelKindName.MANAGED</code></li> <li><code>ModelKindName.EXTERNAL</code></li> </ul> <p>This example demonstrates how to specify an incremental by time range model kind in Python:</p> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan.core.model.kind import ModelKindName\n\n@model(\n    \"docs_example.incremental_model\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"model_time_column\"\n    )\n)\n</code></pre>"},{"location":"concepts/models/python_models/#execution-context","title":"Execution context","text":"<p>Python models can do anything you want, but it is strongly recommended for all models to be idempotent. Python models can fetch data from upstream models or even data outside of Vulcan.</p> <p>Given an execution <code>ExecutionContext</code> \"context\", you can fetch a DataFrame with the <code>fetchdf</code> method:</p> <pre><code>df = context.fetchdf(\"SELECT * FROM my_table\")\n</code></pre>"},{"location":"concepts/models/python_models/#optional-prepost-statements","title":"Optional pre/post-statements","text":"<p>Optional pre/post-statements allow you to execute SQL commands before and after a model runs, respectively.</p> <p>For example, pre/post-statements might modify settings or create indexes. However, be careful not to run any statement that could conflict with the execution of another statement if models run concurrently, such as creating a physical table.</p> <p>You can set the <code>pre_statements</code> and <code>post_statements</code> arguments to a list of SQL strings, SQLGlot expressions, or macro calls to define the model's pre/post-statements.</p> <p>Project-level defaults: You can also define pre/post-statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"db.test_model\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    pre_statements=[\n        \"SET GLOBAL parameter = 'value';\",\n        exp.Cache(this=exp.table_(\"x\"), expression=exp.select(\"1\")),\n    ],\n    post_statements=[\"@CREATE_INDEX(@this_model, id)\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>The previous example's <code>post_statements</code> called user-defined Vulcan macro <code>@CREATE_INDEX(@this_model, id)</code>.</p> <p>We could define the <code>CREATE_INDEX</code> macro in the project's <code>macros</code> directory like this. The macro creates a table index on a single column, conditional on the runtime stage being <code>creating</code> (table creation time).</p> <pre><code>@macro()\ndef create_index(\n    evaluator: MacroEvaluator,\n    model_name: str,\n    column: str,\n):\n    if evaluator.runtime_stage == \"creating\":\n        return f\"CREATE INDEX idx ON {model_name}({column});\"\n    return None\n</code></pre> <p>Alternatively, pre- and post-statements can be issued with the Vulcan <code>fetchdf</code> method described above.</p> <p>Pre-statements may be specified anywhere in the function body before it <code>return</code>s or <code>yield</code>s. Post-statements must execute after the function completes, so instead of <code>return</code>ing a value the function must <code>yield</code> the value. The post-statement must be specified after the <code>yield</code>.</p> <p>This example function includes both pre- and post-statements:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    # pre-statement\n    context.engine_adapter.execute(\"SET GLOBAL parameter = 'value';\")\n\n    # post-statement requires using `yield` instead of `return`\n    yield pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n\n    # post-statement\n    context.engine_adapter.execute(\"CREATE INDEX idx ON example.pre_post_statements (id);\")\n</code></pre>"},{"location":"concepts/models/python_models/#optional-on-virtual-update-statements","title":"Optional on-virtual-update statements","text":"<p>The optional on-virtual-update statements allow you to execute SQL commands after the completion of the Virtual Update.</p> <p>These can be used, for example, to grant privileges on views of the virtual layer.</p> <p>Similar to pre/post-statements you can set the <code>on_virtual_update</code> argument in the <code>@model</code> decorator to a list of SQL strings, SQLGlot expressions, or macro calls.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project (including Python models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"db.test_model\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    on_virtual_update=[\"GRANT SELECT ON VIEW @this_model TO ROLE dev_role\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>Note</p> <p>Table resolution for these statements occurs at the virtual layer. This means that table names, including <code>@this_model</code> macro, are resolved to their qualified view names. For instance, when running the plan in an environment named <code>dev</code>, <code>db.test_model</code> and <code>@this_model</code> would resolve to <code>db__dev.test_model</code> and not to the physical table name.</p>"},{"location":"concepts/models/python_models/#dependencies","title":"Dependencies","text":"<p>In order to fetch data from an upstream model, you first get the table name using <code>context</code>'s <code>resolve_table</code> method. This returns the appropriate table name for the current runtime environment:</p> <pre><code>table = context.resolve_table(\"docs_example.upstream_model\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>The <code>resolve_table</code> method will automatically add the referenced model to the Python model's dependencies.</p> <p>The only other way to set dependencies of models in Python models is to define them explicitly in the <code>@model</code> decorator using the keyword <code>depends_on</code>. The dependencies defined in the model decorator take precedence over any dynamic references inside the function.</p> <p>In this example, only <code>upstream_dependency</code> will be captured, while <code>another_dependency</code> will be ignored:</p> <pre><code>@model(\n    \"my_model.with_explicit_dependencies\",\n    depends_on=[\"docs_example.upstream_dependency\"], # captured\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    # ignored due to @model dependency \"upstream_dependency\"\n    context.resolve_table(\"docs_example.another_dependency\")\n</code></pre> <p>User-defined global variables or blueprint variables can also be used in <code>resolve_table</code> calls, as shown in the following example (similarly for <code>blueprint_var()</code>):</p> <pre><code>@model(\n    \"@schema_name.test_model2\",\n    kind=\"FULL\",\n    columns={\"id\": \"INT\"},\n)\ndef execute(context, **kwargs):\n    table = context.resolve_table(f\"{context.var('schema_name')}.test_model1\")\n    select_query = exp.select(\"*\").from_(table)\n    return context.fetchdf(select_query)\n</code></pre>"},{"location":"concepts/models/python_models/#returning-empty-dataframes","title":"Returning empty dataframes","text":"<p>Python models may not return an empty dataframe.</p> <p>If your model could possibly return an empty dataframe, conditionally <code>yield</code> the dataframe or an empty generator instead of <code>return</code>ing:</p> <pre><code>@model(\n    \"my_model.empty_df\"\n)\ndef execute(\n    context: ExecutionContext,\n) -&gt; pd.DataFrame:\n\n    [...code creating df...]\n\n    if df.empty:\n        yield from ()\n    else:\n        yield df\n</code></pre>"},{"location":"concepts/models/python_models/#user-defined-variables","title":"User-defined variables","text":"<p>User-defined global variables can be accessed from within the Python model with the <code>context.var</code> method.</p> <p>For example, this model access the user-defined variables <code>var</code> and <code>var_with_default</code>. It specifies a default value of <code>default_value</code> if <code>variable_with_default</code> resolves to a missing value.</p> <pre><code>@model(\n    \"my_model.name\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    var_value = context.var(\"var\")\n    var_with_default_value = context.var(\"var_with_default\", \"default_value\")\n    ...\n</code></pre> <p>Alternatively, you can access global variables via <code>execute</code> function arguments, where the name of the argument corresponds to the name of a variable key.</p> <p>For example, this model specifies <code>my_var</code> as an argument to the <code>execute</code> method. The model code can reference the <code>my_var</code> object directly:</p> <pre><code>@model(\n    \"my_model.name\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    my_var: Optional[str] = None,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    my_var_plus1 = my_var + 1\n    ...\n</code></pre> <p>Make sure the argument has a default value if it's possible for the variable to be missing.</p> <p>Note that arguments must be specified explicitly - variables cannot be accessed using <code>kwargs</code>.</p>"},{"location":"concepts/models/python_models/#python-model-blueprinting","title":"Python model blueprinting","text":"<p>A Python model can also serve as a template for creating multiple models, or blueprints, by specifying a list of key-value dicts in the <code>blueprints</code> property. In order to achieve this, the model's name must be parameterized with a variable that exists in this mapping.</p> <p>For instance, the following model will result into two new models, each using the corresponding mapping in the <code>blueprints</code> property:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"@{customer}.some_table\",\n    kind=\"FULL\",\n    blueprints=[\n        {\"customer\": \"customer1\", \"field_a\": \"x\", \"field_b\": \"y\"},\n        {\"customer\": \"customer2\", \"field_a\": \"z\", \"field_b\": \"w\"},\n    ],\n    columns={\n        \"field_a\": \"text\",\n        \"field_b\": \"text\",\n        \"customer\": \"text\",\n    },\n)\ndef entrypoint(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        {\n            \"field_a\": [context.blueprint_var(\"field_a\")],\n            \"field_b\": [context.blueprint_var(\"field_b\")],\n            \"customer\": [context.blueprint_var(\"customer\")],\n        }\n    )\n</code></pre> <p>Note the use of curly brace syntax <code>@{customer}</code> in the model name above. It is used to ensure Vulcan can combine the macro variable into the model name identifier correctly - learn more here.</p> <p>Blueprint variable mappings can also be constructed dynamically, e.g., by using a macro: <code>blueprints=\"@gen_blueprints()\"</code>. This is useful in cases where the <code>blueprints</code> list needs to be sourced from external sources, such as CSV files.</p> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@EACH(@values, x -&gt; (customer := schema_@x))\",\n    ...\n)\n...\n</code></pre>"},{"location":"concepts/models/python_models/#using-macros-in-model-properties","title":"Using macros in model properties","text":"<p>Python models support macro variables in model properties. However, special care must be taken when the macro variable appears within a string.</p> <p>For example when using macro variables inside cron expressions, you need to wrap the entire expression in quotes and prefix it with <code>@</code> to ensure proper parsing:</p> <pre><code># Correct: Wrap the cron expression containing a macro variable\n@model(\n    \"my_model\",\n    cron=\"@'*/@{mins} * * * *'\",  # Note the @'...' syntax\n    ...\n)\n\n# This also works with blueprint variables\n@model(\n    \"@{customer}.scheduled_model\",\n    cron=\"@'0 @{hour} * * *'\",\n    blueprints=[\n        {\"customer\": \"customer_1\", \"hour\": 2}, # Runs at 2 AM\n        {\"customer\": \"customer_2\", \"hour\": 8}, # Runs at 8 AM\n    ],\n    ...\n)\n</code></pre> <p>This is necessary because cron expressions often use <code>@</code> for aliases (like <code>@daily</code>, <code>@hourly</code>), which can conflict with Vulcan's macro syntax.</p>"},{"location":"concepts/models/python_models/#examples","title":"Examples","text":""},{"location":"concepts/models/python_models/#basic","title":"Basic","text":"<p>The following is an example of a Python model returning a static Pandas DataFrame.</p> <p>Note: All of the metadata field names are the same as those in the SQL <code>MODEL</code> DDL.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom sqlglot.expressions import to_column\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"docs_example.basic\",\n    owner=\"janet\",\n    cron=\"@daily\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    column_descriptions={\n        \"id\": \"Unique ID\",\n        \"name\": \"Name corresponding to the ID\",\n    },\n    audits=[\n        (\"not_null\", {\"columns\": [to_column(\"id\")]}),\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre>"},{"location":"concepts/models/python_models/#sql-query-and-pandas","title":"SQL Query and Pandas","text":"<p>The following is a more complex example that queries an upstream model and outputs a Pandas DataFrame:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"docs_example.sql_pandas\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's name and register it as a dependency\n    table = context.resolve_table(\"upstream_model\")\n\n    # fetch data from the model as a pandas DataFrame\n    # if the engine is spark, this returns a spark DataFrame\n    df = context.fetchdf(f\"SELECT id, name FROM {table}\")\n\n    # do some pandas stuff\n    df[id] += 1\n    return df\n</code></pre>"},{"location":"concepts/models/python_models/#pyspark","title":"PySpark","text":"<p>This example demonstrates using the PySpark DataFrame API. If you use Spark, the DataFrame API is preferred to Pandas since it allows you to compute in a distributed fashion.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom pyspark.sql import DataFrame, functions\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"docs_example.pyspark\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n        \"country\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # get the upstream model's name and register it as a dependency\n    table = context.resolve_table(\"upstream_model\")\n\n    # use the spark DataFrame api to add the country column\n    df = context.spark.table(table).withColumn(\"country\", functions.lit(\"USA\"))\n\n    # returns the pyspark DataFrame directly, so no data is computed locally\n    return df\n</code></pre>"},{"location":"concepts/models/python_models/#snowpark","title":"Snowpark","text":"<p>This example demonstrates using the Snowpark DataFrame API. If you use Snowflake, the DataFrame API is preferred to Pandas since it allows you to compute in a distributed fashion.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom snowflake.snowpark.dataframe import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"docs_example.snowpark\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n        \"country\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # returns the snowpark DataFrame directly, so no data is computed locally\n    df = context.snowpark.create_dataframe([[1, \"a\", \"usa\"], [2, \"b\", \"cad\"]], schema=[\"id\", \"name\", \"country\"])\n    df = df.filter(df.id &gt; 1)\n    return df\n</code></pre>"},{"location":"concepts/models/python_models/#bigframe","title":"Bigframe","text":"<p>This example demonstrates using the Bigframe DataFrame API. If you use Bigquery, the Bigframe API is preferred to Pandas as all computation is done in Bigquery.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nfrom bigframes.pandas import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n\ndef get_bucket(num: int):\n    if not num:\n        return \"NA\"\n    boundary = 10\n    return \"at_or_above_10\" if num &gt;= boundary else \"below_10\"\n\n\n@model(\n    \"mart.wiki\",\n    columns={\n        \"title\": \"text\",\n        \"views\": \"int\",\n        \"bucket\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # Create a remote function to be used in the Bigframe DataFrame\n    remote_get_bucket = context.bigframe.remote_function([int], str)(get_bucket)\n\n    # Returns the Bigframe DataFrame handle, no data is computed locally\n    df = context.bigframe.read_gbq(\"bigquery-samples.wikipedia_pageviews.200809h\")\n\n    df = (\n        # This runs entirely on the BigQuery engine lazily\n        df[df.title.str.contains(r\"[Gg]oogle\")]\n        .groupby([\"title\"], as_index=False)[\"views\"]\n        .sum(numeric_only=True)\n        .sort_values(\"views\", ascending=False)\n    )\n\n    return df.assign(bucket=df[\"views\"].apply(remote_get_bucket))\n</code></pre>"},{"location":"concepts/models/python_models/#batching","title":"Batching","text":"<p>If the output of a Python model is very large and you cannot use Spark, it may be helpful to split the output into multiple batches.</p> <p>With Pandas or other single machine DataFrame libraries, all data is stored in memory. Instead of returning a single DataFrame instance, you can return multiple instances using the Python generator API. This minimizes the memory footprint by reducing the size of data loaded into memory at any given time.</p> <p>This examples uses the Python generator <code>yield</code> to batch the model output:</p> <pre><code>@model(\n    \"docs_example.batching\",\n    columns={\n        \"id\": \"int\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's table name\n    table = context.resolve_table(\"upstream_model\")\n\n    for i in range(3):\n        # run 3 queries to get chunks of data and not run out of memory\n        df = context.fetchdf(f\"SELECT id from {table} WHERE id = {i}\")\n        yield df\n</code></pre>"},{"location":"concepts/models/python_models/#serialization","title":"Serialization","text":"<p>Vulcan executes Python code locally where Vulcan is running by using our custom serialization framework.</p>"},{"location":"concepts/models/seed_models/","title":"Seed models","text":""},{"location":"concepts/models/seed_models/#seed-models","title":"Seed models","text":"<p>A <code>SEED</code> is a special kind of model in which data is sourced from a static dataset defined as a CSV file (rather than from a data source accessed via SQL or Python). The CSV files themselves are a part of your Vulcan project.</p> <p>Since seeds are also models in Vulcan, they capitalize on all the same benefits that SQL or Python models provide:</p> <ul> <li>A physical table is created in the data warehouse, which reflects the contents of the seed's CSV file.</li> <li>Seed models can be referenced in downstream models in the same way as other models.</li> <li>Changes to CSV files are captured during planning and versioned using the same fingerprinting mechanism.</li> <li>Environment isolation also applies to seed models.</li> </ul> <p>Seed models are a good fit for static datasets that change infrequently or not at all. Examples of such datasets include:</p> <ul> <li>Names of national holidays and their dates</li> <li>A static list of identifiers that should be excluded</li> </ul> <p>Not supported in Python models</p> <p>Python models do not support the <code>SEED</code> model kind - use a SQL model instead.</p>"},{"location":"concepts/models/seed_models/#creating-a-seed-model","title":"Creating a seed model","text":"<p>Similar to SQL models, <code>SEED</code> models are defined in files with the <code>.sql</code> extension in the <code>models/</code> directory of the Vulcan project.</p> <p>Use the special kind <code>SEED</code> in the <code>MODEL</code> definition to indicate that the model is a seed model:</p> <p></p><pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv'\n  )\n);\n</code></pre> The <code>path</code> attribute contains the path to the seed's CSV file relative to the path of the model's <code>.sql</code> file. If you want to specify a path relative to the root of the Vulcan project, use the <code>$root</code> marker (see Markers).<p></p> <p>If your seed file has special quoting rules or delimiters, you can pass settings to Pandas' <code>read_csv</code> function with the <code>csv_settings</code> dictionary (all supported settings here):</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv',\n    csv_settings (\n      delimiter = \"|\"\n    )\n  )\n);\n</code></pre> <p>The physical table with the seed CSV's content is created using column types inferred by Pandas. Alternatively, you can manually specify the dataset schema as part of the <code>MODEL</code> definition: </p><pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv',\n    csv_settings (\n      delimiter = \"|\"\n    )\n  ),\n  columns (\n    name VARCHAR,\n    date DATE\n  )\n);\n</code></pre> Note: The dataset schema provided in the definition takes precedence over column names defined in the header of the CSV file. This means that the order in which columns are provided in the <code>MODEL</code> definition must match the order of columns in the CSV file.<p></p>"},{"location":"concepts/models/seed_models/#markers","title":"Markers","text":"<p>The <code>$root</code> marker can be used in the <code>path</code> attribute to indicate that the CSV file path is relative to the root of the Vulcan project:</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path '$root/seeds/national_holidays.csv'\n  )\n);\n</code></pre> <p>This is useful when you want to keep all seed CSV files in a top-level directory such as <code>seeds/</code> but don't want to keep track of or manage a bunch of relative paths.</p>"},{"location":"concepts/models/seed_models/#encoding","title":"Encoding","text":"<p>Vulcan expects seed files to be encoded according to the UTF-8 standard. Using a different encoding may lead to unexpected behavior.</p>"},{"location":"concepts/models/seed_models/#example","title":"Example","text":"<p>In this example, we use the model definition from the previous section saved in the <code>models/national_holidays.sql</code> file of the Vulcan project.</p> <p>We also add the seed CSV file itself in the <code>models/</code> directory as a CSV file named <code>national_holidays.csv</code> with the following contents:</p> <pre><code>name,date\nNew Year,2023-01-01\nChristmas,2023-12-25\n</code></pre> <p>When we run the <code>vulcan plan</code> command, the new seed model is automatically detected: </p><pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 test_db.national_holidays\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 test_db.national_holidays: (2023-02-16, 2023-02-16)\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank for the beginning of history:\nApply - Backfill Tables [y/n]: y\n\nAll model batches have been executed successfully\n\ntest_db.national_holidays \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n</code></pre><p></p> <p>Applying the plan created a new table <code>test_db.national_holidays</code>.</p> <p>You can now run a custom query against the table with <code>vulcan fetchdf</code>: </p><pre><code>$ vulcan fetchdf \"SELECT * FROM test_db.national_holidays\"\n\n        name        date\n0   New Year  2023-01-01\n1  Christmas  2023-12-25\n</code></pre><p></p> <p>Changes to the seed CSV file get picked up when the <code>vulcan plan</code> command is run: </p><pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 test_db.national_holidays\n---\n\n+++\n\n@@ -1,3 +1,4 @@\n\n name,date\n New Year,2023-01-01\n Christmas,2023-12-25\n+Independence Day,2023-07-04\nDirectly Modified: test_db.national_holidays\n[1] [Breaking] Backfill test_db.national_holidays and indirectly modified children\n[2] [Non-breaking] Backfill test_db.national_holidays but not indirectly modified children: 1\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 test_db.national_holidays: (2023-02-16, 2023-02-16)\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank for the beginning of history:\nApply - Backfill Tables [y/n]: y\n\nAll model batches have been executed successfully\n\ntest_db.national_holidays \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n</code></pre><p></p>"},{"location":"concepts/models/seed_models/#pre-and-post-statements","title":"Pre- and post-statements","text":"<p>Seed models also support pre- and post-statements, which are evaluated before inserting the seed's content and after, respectively.</p> <p>Below is an example that only involves pre-statements:</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv'\n  )\n);\n\nALTER SESSION SET TIMEZONE = 'UTC';\n</code></pre> <p>To add post-statements, you should use the special <code>@INSERT_SEED()</code> macro to separate pre- and post-statements:</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv'\n  )\n);\n\n-- These are pre-statements\nALTER SESSION SET TIMEZONE = 'UTC';\n\n@INSERT_SEED();\n\n-- These are post-statements\nALTER SESSION SET TIMEZONE = 'PST';\n</code></pre>"},{"location":"concepts/models/seed_models/#on-virtual-update-statements","title":"On-virtual-update statements","text":"<p>Seed models also support on-virtual-update statements, which are executed after the completion of the Virtual Update.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project (including seed models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>These must be enclosed within an <code>ON_VIRTUAL_UPDATE_BEGIN;</code> ...; <code>ON_VIRTUAL_UPDATE_END;</code> block:</p> <pre><code>MODEL (\n  name test_db.national_holidays,\n  kind SEED (\n    path 'national_holidays.csv'\n  )\n);\n\nON_VIRTUAL_UPDATE_BEGIN;\nGRANT SELECT ON VIEW @this_model TO ROLE dev_role;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE admin_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Jinja expressions can also be used within them, as demonstrated in the example above. These expressions must be properly nested within a <code>JINJA_STATEMENT_BEGIN;</code> and <code>JINJA_END;</code> block.</p> <p>Note</p> <p>Table resolution for these statements occurs at the virtual layer. This means that table names, including <code>@this_model</code> macro, are resolved to their qualified view names. For instance, when running the plan in an environment named <code>dev</code>, <code>db.customers</code> and <code>@this_model</code> would resolve to <code>db__dev.customers</code> and not to the physical table name.</p>"},{"location":"concepts/models/sql_models/","title":"SQL models","text":""},{"location":"concepts/models/sql_models/#sql-models","title":"SQL models","text":"<p>SQL models are the main type of models used by Vulcan. These models can be defined using either SQL or Python that generates SQL.</p>"},{"location":"concepts/models/sql_models/#sql-based-definition","title":"SQL-based definition","text":"<p>The SQL-based definition of SQL models is the most common one, and consists of the following sections:</p> <ul> <li>The <code>MODEL</code> DDL</li> <li>Optional pre-statements</li> <li>A single query</li> <li>Optional post-statements</li> <li>Optional on-virtual-update-statements</li> </ul> <p>These models are designed to look and feel like you're simply using SQL, but they can be customized for advanced use cases.</p> <p>To create a SQL-based model, add a new file with the <code>.sql</code> suffix into the <code>models/</code> directory (or a subdirectory of <code>models/</code>) within your Vulcan project. Although the name of the file doesn't matter, it is customary to use the model's name (without the schema) as the file name. For example, the file containing the model <code>vulcan_example.seed_model</code> would be named <code>seed_model.sql</code>.</p>"},{"location":"concepts/models/sql_models/#example","title":"Example","text":"<pre><code>-- This is the MODEL DDL, where you specify model metadata and configuration information.\nMODEL (\n  name db.customers,\n  kind FULL,\n);\n\n/*\n  Optional pre-statements that will run before the model's query.\n  You should NOT do things that cause side effects that could error out when\n  executed concurrently with other statements, such as creating physical tables.\n*/\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\n/*\n  This is the single query that defines the model's logic.\n  Although it is not required, it is considered best practice to explicitly\n  specify the type for each one of the model's columns through casting.\n*/\nSELECT\n  r.id::INT,\n  r.name::TEXT,\n  c.country::TEXT\nFROM raw.restaurants AS r\nJOIN countries AS c\n  ON r.id = c.restaurant_id;\n\n/*\n  Optional post-statements that will run after the model's query.\n  You should NOT do things that cause side effects that could error out when\n  executed concurrently with other statements, such as creating physical tables.\n*/\nUNCACHE TABLE countries;\n</code></pre>"},{"location":"concepts/models/sql_models/#model-ddl","title":"<code>MODEL</code> DDL","text":"<p>The <code>MODEL</code> DDL is used to specify metadata about the model such as its name, kind, owner, cron, and others. This should be the first statement in your SQL-based model's file.</p> <p>Refer to <code>MODEL</code> properties for the full list of allowed properties.</p>"},{"location":"concepts/models/sql_models/#optional-prepost-statements","title":"Optional pre/post-statements","text":"<p>Optional pre/post-statements allow you to execute SQL commands before and after a model runs, respectively.</p> <p>For example, pre/post-statements might modify settings or create a table index. However, be careful not to run any statement that could conflict with the execution of another model if they are run concurrently, such as creating a physical table.</p> <p>Pre/post-statements are just standard SQL commands located before/after the model query. They must end with a semi-colon, and the model query must end with a semi-colon if a post-statement is present. The example above contains both pre- and post-statements.</p> <p>Project-level defaults: You can also define pre/post-statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>Warning</p> <p>Pre/post-statements are evaluated twice: when a model's table is created and when its query logic is evaluated. Executing statements more than once can have unintended side-effects, so you can conditionally execute them based on Vulcan's runtime stage.</p> <p>The pre/post-statements in the example above will run twice because they are not conditioned on runtime stage.</p> <p>We can condition the post-statement to only run after the model query is evaluated using the <code>@IF</code> macro operator and <code>@runtime_stage</code> macro variable like this:</p> <pre><code>MODEL (\n  name db.customers,\n  kind FULL,\n);\n\n[...same as example above...]\n\n@IF(\n  @runtime_stage = 'evaluating',\n  UNCACHE TABLE countries\n);\n</code></pre> <p>Note that the SQL command <code>UNCACHE TABLE countries</code> inside the <code>@IF()</code> macro does not end with a semi-colon. Instead, the semi-colon comes after the <code>@IF()</code> macro's closing parenthesis.</p>"},{"location":"concepts/models/sql_models/#optional-on-virtual-update-statements","title":"Optional on-virtual-update statements","text":"<p>The optional on-virtual-update statements allow you to execute SQL commands after the completion of the Virtual Update.</p> <p>These can be used, for example, to grant privileges on views of the virtual layer.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>These SQL statements must be enclosed within an <code>ON_VIRTUAL_UPDATE_BEGIN;</code> ...; <code>ON_VIRTUAL_UPDATE_END;</code> block like this:</p> <pre><code>MODEL (\n  name db.customers,\n  kind FULL\n);\n\nSELECT\n  r.id::INT\nFROM raw.restaurants AS r;\n\nON_VIRTUAL_UPDATE_BEGIN;\nGRANT SELECT ON VIEW @this_model TO ROLE role_name;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE admin;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Jinja expressions can also be used within them, as demonstrated in the example above. These expressions must be properly nested within a <code>JINJA_STATEMENT_BEGIN;</code> and <code>JINJA_END;</code> block.</p> <p>Note</p> <p>Table resolution for these statements occurs at the virtual layer. This means that table names, including <code>@this_model</code> macro, are resolved to their qualified view names. For instance, when running the plan in an environment named <code>dev</code>, <code>db.customers</code> and <code>@this_model</code> would resolve to <code>db__dev.customers</code> and not to the physical table name.</p>"},{"location":"concepts/models/sql_models/#the-model-query","title":"The model query","text":"<p>The model must contain a standalone query, which can be a single <code>SELECT</code> expression, or multiple <code>SELECT</code> expressions combined with the <code>UNION</code>, <code>INTERSECT</code>, or <code>EXCEPT</code> operators. The result of this query will be used to populate the model's table or view.</p>"},{"location":"concepts/models/sql_models/#sql-model-blueprinting","title":"SQL model blueprinting","text":"<p>A SQL model can also serve as a template for creating multiple models, or blueprints, by specifying a list of key-value mappings in the <code>blueprints</code> property. In order to achieve this, the model's name must be parameterized with a variable that exists in this mapping.</p> <p>For instance, the following model will result into two new models, each using the corresponding mapping in the <code>blueprints</code> property:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z, field_b := w)\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b\nFROM @customer.some_source\n</code></pre> <p>The two models produced from this template are:</p> <pre><code>-- This uses the first variable mapping\nMODEL (\n  name customer1.some_table,\n  kind FULL\n);\n\nSELECT\n  'x',\n  y AS field_b\nFROM customer1.some_source\n\n-- This uses the second variable mapping\nMODEL (\n  name customer2.some_table,\n  kind FULL\n);\n\nSELECT\n  'z',\n  w AS field_b\nFROM customer2.some_source\n</code></pre> <p>Note the use of curly brace syntax <code>@{field_b} AS field_b</code> in the model query above. It is used to tell Vulcan that the rendered variable value should be treated as a SQL identifier instead of a string literal.</p> <p>You can see the different behavior in the first rendered model. <code>@field_a</code> is resolved to the string literal <code>'x'</code> (with single quotes) and <code>@{field_b}</code> is resolved to the identifier <code>y</code> (without quotes). Learn more about the curly brace syntax here.</p> <p>Blueprint variable mappings can also be constructed dynamically, e.g., by using a macro: <code>blueprints @gen_blueprints()</code>. This is useful in cases where the <code>blueprints</code> list needs to be sourced from external sources, such as CSV files.</p> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints @EACH(@values, x -&gt; (customer := schema_@x)),\n);\n\nSELECT\n  1 AS c\n</code></pre>"},{"location":"concepts/models/sql_models/#python-based-definition","title":"Python-based definition","text":"<p>The Python-based definition of SQL models consists of a single python function, decorated with Vulcan's <code>@model</code> decorator. The decorator is required to have the <code>is_sql</code> keyword argument set to <code>True</code> to distinguish it from Python models that return DataFrame instances.</p> <p>This function's return value serves as the model's query, and it must be either a SQL string or a SQLGlot expression. The <code>@model</code> decorator is used to define the model's metadata and, optionally its pre/post-statements or on-virtual-update-statements that are also in the form of SQL strings or SQLGlot expressions.</p> <p>Defining a SQL model using Python can be beneficial in cases where its query is too complex to express cleanly in SQL, for example due to having many dynamic components that would require heavy use of macros. Since Python-based models generate SQL, they support the same features as regular SQL models, such as column-level lineage.</p> <p>To create a Python-based model, add a new file with the <code>.py</code> suffix into the <code>models/</code> directory (or a subdirectory of <code>models/</code>) within your Vulcan project. The file naming conventions of Python-based models are similar to those of SQL-based models. Inside this file, define a function named <code>entrypoint</code> with a single <code>evaluator</code> argument, as shown in the example below.</p>"},{"location":"concepts/models/sql_models/#example_1","title":"Example","text":"<p>The following example demonstrates how the above <code>db.customers</code> model can be defined as a Python-based model using SQLGlot's <code>Expression</code> builder methods:</p> <pre><code>from sqlglot import exp\n\nfrom vulcan.core.model import model\nfrom vulcan.core.macros import MacroEvaluator\n\n@model(\n    \"db.customers\",\n    is_sql=True,\n    kind=\"FULL\",\n    pre_statements=[\"CACHE TABLE countries AS SELECT * FROM raw.countries\"],\n    post_statements=[\"UNCACHE TABLE countries\"],\n    on_virtual_update=[\"GRANT SELECT ON VIEW @this_model TO ROLE dev_role\"],\n)\ndef entrypoint(evaluator: MacroEvaluator) -&gt; str | exp.Expression:\n    return (\n        exp.select(\"r.id::int\", \"r.name::text\", \"c.country::text\")\n        .from_(\"raw.restaurants as r\")\n        .join(\"countries as c\", on=\"r.id = c.restaurant_id\")\n    )\n</code></pre> <p>One could also define this model by simply returning a string that contained the SQL query of the SQL-based example. Strings used as pre/post-statements or return values in Python-based models will be parsed into SQLGlot expressions, which means that Vulcan will still be able to understand them semantically and thus provide information such as column-level lineage.</p> <p>Note</p> <p>Since python models have access to the macro evaluation context (<code>MacroEvaluator</code>), they can also access model schemas through its <code>columns_to_types</code> method.</p>"},{"location":"concepts/models/sql_models/#model-decorator","title":"<code>@model</code> decorator","text":"<p>The <code>@model</code> decorator is the Python equivalent of the <code>MODEL</code> DDL.</p> <p>In addition to model metadata and configuration information, one can also set the keyword arguments <code>pre_statements</code>, <code>post_statements</code> and <code>on_virtual_update</code> to a list of SQL strings and/or SQLGlot expressions to define the pre/post-statements and on-virtual-update-statements of the model, respectively.</p> <p>Note</p> <p>All of the metadata property field names are the same as those in the <code>MODEL</code> DDL.</p>"},{"location":"concepts/models/sql_models/#python-model-blueprinting","title":"Python model blueprinting","text":"<p>A Python-based SQL model can also serve as a template for creating multiple models, or blueprints, by specifying a list of key-value dicts in the <code>blueprints</code> property. In order to achieve this, the model's name must be parameterized with a variable that exists in this mapping.</p> <p>For instance, the following model will result into two new models, each using the corresponding mapping in the <code>blueprints</code> property:</p> <pre><code>from sqlglot import exp\n\nfrom vulcan.core.model import model\nfrom vulcan.core.macros import MacroEvaluator\n\n@model(\n    \"@{customer}.some_table\",\n    is_sql=True,\n    kind=\"FULL\",\n    blueprints=[\n        {\"customer\": \"customer1\", \"field_a\": \"x\", \"field_b\": \"y\"},\n        {\"customer\": \"customer2\", \"field_a\": \"z\", \"field_b\": \"w\"},\n    ],\n)\ndef entrypoint(evaluator: MacroEvaluator) -&gt; str | exp.Expression:\n    field_a = evaluator.blueprint_var(\"field_a\")\n    field_b = evaluator.blueprint_var(\"field_b\")\n    customer = evaluator.blueprint_var(\"customer\")\n\n    return exp.select(field_a, field_b).from_(f\"{customer}.some_source\")\n</code></pre> <p>The two models produced from this template are the same as in the example for SQL-based blueprinting.</p> <p>Blueprint variable mappings can also be constructed dynamically, e.g., by using a macro: <code>blueprints=\"@gen_blueprints()\"</code>. This is useful in cases where the <code>blueprints</code> list needs to be sourced from external sources, such as CSV files.</p> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    is_sql=True,\n    blueprints=\"@EACH(@values, x -&gt; (customer := schema_@x))\",\n    ...\n)\n...\n</code></pre>"},{"location":"concepts/models/sql_models/#automatic-dependencies","title":"Automatic dependencies","text":"<p>Vulcan parses your SQL, so it understands what the code does and how it relates to other models. There is no need for you to manually specify dependencies to other models with special tags or commands.</p> <p>For example, consider a model with this query:</p> <pre><code>SELECT employees.id\nFROM employees\nJOIN countries\n  ON employees.id = countries.employee_id\n</code></pre> <p>Vulcan will detect that the model depends on both <code>employees</code> and <code>countries</code>. When executing this model, it will ensure that <code>employees</code> and <code>countries</code> are executed first.</p> <p>External dependencies not defined in Vulcan are also supported. Vulcan can either depend on them implicitly through the order in which they are executed, or through signals.</p> <p>Although automatic dependency detection works most of the time, there may be specific cases for which you want to define dependencies manually. You can do so in the <code>MODEL</code> DDL with the dependencies property.</p>"},{"location":"concepts/models/sql_models/#conventions","title":"Conventions","text":"<p>Vulcan encourages explicitly specifying the data types of a model's columns through casting. This allows Vulcan to understand the data types in your models, and it prevents incorrect type inference. Vulcan supports the casting format <code>&lt;column name&gt;::&lt;data type&gt;</code> in models of any SQL dialect.</p>"},{"location":"concepts/models/sql_models/#explicit-selects","title":"Explicit SELECTs","text":"<p>Although <code>SELECT *</code> is convenient, it is dangerous because a model's results can change due to external factors (e.g., an upstream source adding or removing a column). In general, we encourage listing out every column you need or using <code>create_external_models</code> to capture the schema of an external data source.</p> <p>If you select from an external source, <code>SELECT *</code> will prevent Vulcan from performing some optimization steps and from determining upstream column-level lineage. Use an <code>external</code> model kind to enable optimizations and upstream column-level lineage for external sources.</p>"},{"location":"concepts/models/sql_models/#encoding","title":"Encoding","text":"<p>Vulcan expects files containing SQL models to be encoded according to the UTF-8 standard. Using a different encoding may lead to unexpected behavior.</p>"},{"location":"concepts/models/sql_models/#transpilation","title":"Transpilation","text":"<p>Vulcan leverages SQLGlot to parse and transpile SQL. Therefore, you can write your SQL in any supported dialect and transpile it into another supported dialect.</p> <p>You can also use advanced syntax that may not be available in your engine of choice. For example, <code>x::int</code> is equivalent to <code>CAST(x as INT)</code>, but is only supported in some dialects. SQLGlot allows you to use this feature regardless of what engine you're using.</p> <p>Additionally, you won't have to worry about minor formatting differences such as trailing commas, as SQLGlot will remove them at parse time.</p>"},{"location":"concepts/models/sql_models/#macros","title":"Macros","text":"<p>Although standard SQL is very powerful, complex data systems often require running SQL queries with dynamic components such as date filters. For example, you may want to change the date ranges in a <code>between</code> statement so that you can get the latest batch of data. Vulcan provides these dates automatically through macro variables.</p> <p>Additionally, large queries can be difficult to read and maintain. In order to make queries more compact, Vulcan supports a powerful macro syntax as well as Jinja, allowing you to write macros that make your SQL queries easier to manage.</p>"},{"location":"examples/incremental_time_full_walkthrough/","title":"Incremental by Time Range","text":""},{"location":"examples/incremental_time_full_walkthrough/#incremental-by-time-range","title":"Incremental by Time Range","text":"<p>Vulcan incremental models are a powerful feature that come in many flavors and configurations so you can fine tune your query performance and scheduled runs exactly how you want with a plethora of guardrails.</p> <p>However, we recognize with all this power comes a responsibility to make sure you\u2019re equipped to succeed confidently.</p> <p>We\u2019re going to walk you through a clear story problem step by step. The end outcome is for you to feel confident with this new workflow to:</p> <ul> <li>Build a mental model for how to solve data transformation problems with Vulcan incremental models</li> <li>Know which configs to update and why</li> <li>Run a sequence of <code>vulcan</code> commands and know exactly what\u2019s running and why</li> <li>Understand the tradeoffs between different approaches and make the right decisions for your use case</li> <li>Save precious time and money running your data transformation pipelines</li> </ul>"},{"location":"examples/incremental_time_full_walkthrough/#story-problem","title":"Story Problem","text":"<p>I am a data engineer working for a company selling software directly to customers. I have sales data with millions of transactions per day, and I want to add dimensions from other raw sources to better understand what sales/product trends are happening.</p> <p>So I have two raw data sources like this:</p> <ul> <li>Source A: raw sales data is extracted and loaded into my data warehouse (think: BigQuery, Snowflake, Databricks, etc.) hourly</li> <li>Source B: product usage data from a backend database (think: Postgres) is extracted and loaded into my data warehouse daily</li> </ul> <p>On first impression, this looks like a piece of cake. However, as I reflect on what success looks like for this to be built AND maintained well, there\u2019s a lot of problems to solve for. Don\u2019t worry, we answer all these questions at the end.</p> <ul> <li>How do I handle late-arriving data?</li> <li>How do I account for UTC vs. PST (California) timestamps, do I convert them?</li> <li>What schedule should I run these at?</li> <li>How do I test this data?</li> <li>How do I make this run fast and only the intervals necessary (read: partitions)?</li> <li>How do I make patch changes when an edge case error occurs with incorrect data from months ago?</li> <li>What do unit tests look and feel like for this?</li> <li>How do I prevent data gaps with unprocessed or incomplete intervals?</li> <li>Am I okay processing incomplete intervals (think: allow partials)?</li> <li>What tradeoffs am I willing to make for fresh data?</li> <li>How to make this not feel so complex during development?</li> <li>How do I know Vulcan is behaving how I want it to behave?</li> </ul>"},{"location":"examples/incremental_time_full_walkthrough/#development-workflow","title":"Development Workflow","text":"<p>You\u2019ll be following this general sequence of actions when working with Vulcan:</p> <ol> <li><code>vulcan plan dev</code>: create a dev environment for your new SQL model</li> <li><code>vulcan fetchdf</code>: preview data in dev</li> <li><code>vulcan create_external_models</code>: automatically generate documentation for raw source tables' column-level lineage</li> <li><code>vulcan plan</code>: promote model from dev to prod</li> <li><code>vulcan plan dev --forward-only</code>: make more code changes and only process new data going forward with those code changes; leave historical data alone</li> <li><code>vulcan fetchdf</code>: preview data in dev</li> <li><code>vulcan create_test</code>: automatically generate unit tests</li> <li><code>vulcan test</code>: run those unit tests</li> <li><code>vulcan plan</code>: promote dev to prod</li> </ol> <p>Note: If this is the first time you're running Vulcan, I recommend following the CLI Quickstart first and then coming back to this example.</p>"},{"location":"examples/incremental_time_full_walkthrough/#setup","title":"Setup","text":"<p>Let\u2019s start with some demo data coupled with an existing Vulcan project with models already in production.</p> <p>I recommend not reading too much into the exact contents of this data outside of timestamps and primary/foreign keys. All of this is fabricated for the purposes of this guide.</p> <p>We have data like the below that gets ingested into our data warehouse on a daily basis.</p> Raw product usage data product_id customer_id last_usage_date usage_count feature_utilization_score user_segment PROD-101 CUST-001 2024-10-25 23:45:00+00:00 120 0.85 enterprise PROD-103 CUST-001 2024-10-27 12:30:00+00:00 95 0.75 enterprise PROD-102 CUST-002 2024-10-25 15:15:00+00:00 150 0.92 enterprise PROD-103 CUST-002 2024-10-26 14:20:00+00:00 80 0.68 enterprise PROD-101 CUST-003 2024-10-25 18:30:00+00:00 45 0.45 professional PROD-102 CUST-003 2024-10-27 19:45:00+00:00 30 0.35 professional PROD-103 CUST-004 2024-10-25 21:20:00+00:00 15 0.25 starter PROD-102 CUST-005 2024-10-25 23:10:00+00:00 5 0.15 starter PROD-102 CUST-006 2024-10-26 15:30:00+00:00 110 0.88 enterprise PROD-101 CUST-007 2024-10-26 17:45:00+00:00 60 0.55 professional PROD-103 CUST-008 2024-10-26 22:20:00+00:00 25 0.30 starter PROD-101 CUST-009 2024-10-27 05:15:00+00:00 75 0.65 professional PROD-102 CUST-010 2024-10-27 08:40:00+00:00 3 0.10 starter Raw sales data transaction_id product_id customer_id transaction_amount transaction_timestamp payment_method currency TX-001 PROD-101 CUST-001 99.99 2024-10-25 08:30:00+00:00 credit_card USD TX-002 PROD-102 CUST-002 149.99 2024-10-25 09:45:00+00:00 paypal USD TX-003 PROD-101 CUST-003 99.99 2024-10-25 15:20:00+00:00 credit_card USD TX-004 PROD-103 CUST-004 299.99 2024-10-25 18:10:00+00:00 credit_card USD TX-005 PROD-102 CUST-005 149.99 2024-10-25 21:30:00+00:00 debit_card USD TX-006 PROD-101 CUST-001 99.99 2024-10-26 03:15:00+00:00 credit_card USD TX-007 PROD-103 CUST-002 299.99 2024-10-26 07:45:00+00:00 paypal USD TX-008 PROD-102 CUST-006 149.99 2024-10-26 11:20:00+00:00 credit_card USD TX-009 PROD-101 CUST-007 99.99 2024-10-26 14:30:00+00:00 debit_card USD TX-010 PROD-103 CUST-008 299.99 2024-10-26 19:45:00+00:00 credit_card USD TX-011 PROD-101 CUST-009 99.99 2024-10-27 02:30:00+00:00 paypal USD TX-012 PROD-102 CUST-010 149.99 2024-10-27 05:15:00+00:00 credit_card USD TX-013 PROD-103 CUST-001 299.99 2024-10-27 08:40:00+00:00 credit_card USD TX-014 PROD-101 CUST-002 99.99 2024-10-27 13:25:00+00:00 debit_card USD TX-015 PROD-102 CUST-003 149.99 2024-10-27 16:50:00+00:00 credit_card USD Code to load the data into BigQuery <p>If you want to follow along, here are BigQuery SQL queries to make it easier for you! Just run them directly in the query console. Feel free to adjust for your data warehouse.</p> <pre><code>-- Create the product_usage table with appropriate schema\nCREATE OR REPLACE TABLE `vulcan-public-demo.tcloud_raw_data.product_usage` (\n    product_id STRING NOT NULL,\n    customer_id STRING NOT NULL,\n    last_usage_date TIMESTAMP NOT NULL,\n    usage_count INT64 NOT NULL,\n    feature_utilization_score FLOAT64 NOT NULL,\n    user_segment STRING NOT NULL,\n);\n\n-- Insert the data\nINSERT INTO `vulcan-public-demo.tcloud_raw_data.product_usage`\n(product_id, customer_id, last_usage_date, usage_count, feature_utilization_score, user_segment)\nVALUES\n    ('PROD-101', 'CUST-001', TIMESTAMP '2024-10-25 23:45:00+00:00', 120, 0.85, 'enterprise'),\n    ('PROD-103', 'CUST-001', TIMESTAMP '2024-10-27 12:30:00+00:00', 95, 0.75, 'enterprise'),\n    ('PROD-102', 'CUST-002', TIMESTAMP '2024-10-25 15:15:00+00:00', 150, 0.92, 'enterprise'),\n    ('PROD-103', 'CUST-002', TIMESTAMP '2024-10-26 14:20:00+00:00', 80, 0.68, 'enterprise'),\n    ('PROD-101', 'CUST-003', TIMESTAMP '2024-10-25 18:30:00+00:00', 45, 0.45, 'professional'),\n    ('PROD-102', 'CUST-003', TIMESTAMP '2024-10-27 19:45:00+00:00', 30, 0.35, 'professional'),\n    ('PROD-103', 'CUST-004', TIMESTAMP '2024-10-25 21:20:00+00:00', 15, 0.25, 'starter'),\n    ('PROD-102', 'CUST-005', TIMESTAMP '2024-10-25 23:10:00+00:00', 5, 0.15, 'starter'),\n    ('PROD-102', 'CUST-006', TIMESTAMP '2024-10-26 15:30:00+00:00', 110, 0.88, 'enterprise'),\n    ('PROD-101', 'CUST-007', TIMESTAMP '2024-10-26 17:45:00+00:00', 60, 0.55, 'professional'),\n    ('PROD-103', 'CUST-008', TIMESTAMP '2024-10-26 22:20:00+00:00', 25, 0.30, 'starter'),\n    ('PROD-101', 'CUST-009', TIMESTAMP '2024-10-27 05:15:00+00:00', 75, 0.65, 'professional'),\n    ('PROD-102', 'CUST-010', TIMESTAMP '2024-10-27 08:40:00+00:00', 3, 0.10, 'starter');\n</code></pre> <pre><code>--Create the sales table with appropriate schema\nCREATE OR REPLACE TABLE `vulcan-public-demo.tcloud_raw_data.sales` (\n    transaction_id STRING NOT NULL,\n    product_id STRING NOT NULL,\n    customer_id STRING NOT NULL,\n    transaction_amount NUMERIC(10,2) NOT NULL,\n    transaction_timestamp TIMESTAMP NOT NULL,\n    payment_method STRING,\n    currency STRING,\n);\n\n-- Then, insert the data\nINSERT INTO `vulcan-public-demo.tcloud_raw_data.sales`\n(transaction_id, product_id, customer_id, transaction_amount, transaction_timestamp, payment_method, currency)\nVALUES\n    ('TX-001', 'PROD-101', 'CUST-001', 99.99, TIMESTAMP '2024-10-25 08:30:00+00:00', 'credit_card', 'USD'),\n    ('TX-002', 'PROD-102', 'CUST-002', 149.99, TIMESTAMP '2024-10-25 09:45:00+00:00', 'paypal', 'USD'),\n    ('TX-003', 'PROD-101', 'CUST-003', 99.99, TIMESTAMP '2024-10-25 15:20:00+00:00', 'credit_card', 'USD'),\n    ('TX-004', 'PROD-103', 'CUST-004', 299.99, TIMESTAMP '2024-10-25 18:10:00+00:00', 'credit_card', 'USD'),\n    ('TX-005', 'PROD-102', 'CUST-005', 149.99, TIMESTAMP '2024-10-25 21:30:00+00:00', 'debit_card', 'USD'),\n    ('TX-006', 'PROD-101', 'CUST-001', 99.99, TIMESTAMP '2024-10-26 03:15:00+00:00', 'credit_card', 'USD'),\n    ('TX-007', 'PROD-103', 'CUST-002', 299.99, TIMESTAMP '2024-10-26 07:45:00+00:00', 'paypal', 'USD'),\n    ('TX-008', 'PROD-102', 'CUST-006', 149.99, TIMESTAMP '2024-10-26 11:20:00+00:00', 'credit_card', 'USD'),\n    ('TX-009', 'PROD-101', 'CUST-007', 99.99, TIMESTAMP '2024-10-26 14:30:00+00:00', 'debit_card', 'USD'),\n    ('TX-010', 'PROD-103', 'CUST-008', 299.99, TIMESTAMP '2024-10-26 19:45:00+00:00', 'credit_card', 'USD'),\n    ('TX-011', 'PROD-101', 'CUST-009', 99.99, TIMESTAMP '2024-10-27 02:30:00+00:00', 'paypal', 'USD'),\n    ('TX-012', 'PROD-102', 'CUST-010', 149.99, TIMESTAMP '2024-10-27 05:15:00+00:00', 'credit_card', 'USD'),\n    ('TX-013', 'PROD-103', 'CUST-001', 299.99, TIMESTAMP '2024-10-27 08:40:00+00:00', 'credit_card', 'USD'),\n    ('TX-014', 'PROD-101', 'CUST-002', 99.99, TIMESTAMP '2024-10-27 13:25:00+00:00', 'debit_card', 'USD'),\n    ('TX-015', 'PROD-102', 'CUST-003', 149.99, TIMESTAMP '2024-10-27 16:50:00+00:00', 'credit_card', 'USD');\n</code></pre>"},{"location":"examples/incremental_time_full_walkthrough/#model-configuration","title":"Model Configuration","text":"<p>I can answer some of the questions above by walking through the model's config, coupled with the business logic/code I prepared ahead of time.</p> <p>You can see this code in a Vulcan project context here.</p> <pre><code>MODEL (\n  name demo.incrementals_demo,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    -- How does this model kind behave?\n    --   DELETE by time range, then INSERT\n    time_column transaction_date,\n\n    -- How do I handle late-arriving data?\n    --   Handle late-arriving events for the past 2 (2*1) days based on cron\n    --   interval. Each time it runs, it will process today, yesterday, and\n    --   the day before yesterday.\n    lookback 2,\n  ),\n\n  -- Don't backfill data before this date\n  start '2024-10-25',\n\n  -- What schedule should I run these at?\n  --   Daily at Midnight UTC\n  cron '@daily',\n\n  -- Good documentation for the primary key\n  grain transaction_id,\n\n  -- How do I test this data?\n  --   Validate that the `transaction_id` primary key values are both unique\n  --   and non-null. Data audit tests only run for the processed intervals,\n  --   not for the entire table.\n  audits (\n    UNIQUE_VALUES(columns = (transaction_id)),\n    NOT_NULL(columns = (transaction_id))\n  )\n);\n\nWITH sales_data AS (\n  SELECT\n    transaction_id,\n    product_id,\n    customer_id,\n    transaction_amount,\n    -- How do I account for UTC vs. PST (California baby) timestamps?\n    --   Make sure all time columns are in UTC and convert them to PST in the\n    --   presentation layer downstream.\n    transaction_timestamp,\n    payment_method,\n    currency\n  FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data\n  -- How do I make this run fast and only process the necessary intervals?\n  --   Use our date macros that will automatically run the necessary intervals.\n  --   Because Vulcan manages state, it will know what needs to run each time\n  --   you invoke `vulcan run`.\n  WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B\n  -- Include usage data from the 30 days before the interval\n  WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n)\n\nSELECT\n  s.transaction_id,\n  s.product_id,\n  s.customer_id,\n  s.transaction_amount,\n  -- Extract the date from the timestamp to partition by day\n  DATE(s.transaction_timestamp) as transaction_date,\n  -- Convert timestamp to PST using a SQL function in the presentation layer for end users\n  DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,\n  s.payment_method,\n  s.currency,\n  -- Product usage metrics\n  p.last_usage_date,\n  p.usage_count,\n  p.feature_utilization_score,\n  p.user_segment,\n  -- Derived metrics\n  CASE\n    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.8 THEN 'Power User'\n    WHEN p.usage_count &gt; 50 THEN 'Regular User'\n    WHEN p.usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type,\n  -- Time since last usage\n  DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage\nFROM sales_data s\nLEFT JOIN product_usage p\n  ON s.product_id = p.product_id\n  AND s.customer_id = p.customer_id\n</code></pre>"},{"location":"examples/incremental_time_full_walkthrough/#creating-the-model","title":"Creating the model","text":"<p>I\u2019m creating this model for the first time against an existing Vulcan project that already has data in production. So let\u2019s run this in a <code>dev</code> environment.</p> <p>Run this command to add this incremental model to a <code>dev</code> environment:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Note: Using <code>vulcan</code> version <code>0.132.1</code> at the time of writing</p> <p>Keep pressing enter on the date prompts, as we want to backfill all of history since 2024-10-25.</p> <pre><code>(venv) \u2717 vulcan plan dev\n======================================================================\nSuccessfully Ran 2 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 demo__dev.incrementals_demo\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 demo__dev.incrementals_demo: 2024-10-25 - 2024-11-04\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until now:\nApply - Backfill Tables [y/n]: y\n[1/1] demo__dev.incrementals_demo evaluated in 6.97s\nEvaluating models \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:06\n\n\nAll model batches have been executed successfully\n\nVirtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:01\n\nThe target environment has been updated successfully\n</code></pre> <p>Now I\u2019m thinking to myself \"what exact SQL queries are running to make sure this is behaving as I expect?\"</p> <p>This sequence of queries is exactly what\u2019s happening in the query engine. Click on the toggles to see the SQL queries.</p> Create an empty table with the proper schema that\u2019s also versioned (ex: <code>__50975949</code>) <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (\n  `transaction_id` STRING,\n  `product_id` STRING,\n  `customer_id` STRING,\n  `transaction_amount` NUMERIC,\n  `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),\n  `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),\n  `payment_method` STRING,\n  `currency` STRING,\n  `last_usage_date` TIMESTAMP,\n  `usage_count` INT64,\n  `feature_utilization_score` FLOAT64,\n  `user_segment` STRING,\n  `user_type` STRING OPTIONS (description='Derived metrics'),\n  `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')\n  )\n  PARTITION BY `transaction_date`\n</code></pre> Validate the SQL before processing data (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps) <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND\n    FALSE\n),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)\n    ) AND\n    FALSE\n)\n\nSELECT\n  `s`.`transaction_id` AS `transaction_id`,\n  `s`.`product_id` AS `product_id`,\n  `s`.`customer_id` AS `customer_id`,\n  CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n  DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n  DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n  `s`.`payment_method` AS `payment_method`,\n  `s`.`currency` AS `currency`,\n  `p`.`last_usage_date` AS `last_usage_date`,\n  `p`.`usage_count` AS `usage_count`,\n  `p`.`feature_utilization_score` AS `feature_utilization_score`,\n  `p`.`user_segment` AS `user_segment`,\n  CASE\n    WHEN `p`.`feature_utilization_score` &gt; 0.8 AND `p`.`usage_count` &gt; 100 THEN 'Power User'\n    WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n    WHEN `p`.`usage_count` IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END AS `user_type`,\n  DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\nFROM `sales_data` AS `s`\nLEFT JOIN `product_usage` AS `p`\n  ON `p`.`customer_id` = `s`.`customer_id` AND\n  `p`.`product_id` = `s`.`product_id`\nWHERE FALSE\nLIMIT 0\n</code></pre> Merge data into empty table <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `transaction_id`,\n      `product_id`,\n      `customer_id`,\n      `transaction_amount`,\n      `transaction_timestamp`,\n      `payment_method`,\n      `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_id`,\n      `customer_id`,\n      `last_usage_date`,\n      `usage_count`,\n      `feature_utilization_score`,\n      `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  )\n\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`usage_count` &gt; 100 AND `p`.`feature_utilization_score` &gt; 0.8 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n      END AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p`\n      ON `s`.`product_id` = `p`.`product_id`\n      AND `s`.`customer_id` = `p`.`customer_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN\n  INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n  VALUES (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n</code></pre> Run data audits to test if <code>transaction_id</code> is unique and not null (SQL is automatically generated) <p><code>UNIQUE_VALUES()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n  SELECT *\n  FROM (\n    SELECT\n      ROW_NUMBER() OVER (\n        PARTITION BY (`transaction_id`) O\n        RDER BY (`transaction_id`)\n      ) AS `rank_`\n    FROM (\n      SELECT *\n      FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `demo__incrementals_demo__50975949`\n      WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-05' AS DATE)\n    ) AS `_q_0`\n  WHERE TRUE\n) AS `_q_1`\nWHERE `rank_` &gt; 1\n) AS `audit`\n</code></pre><p></p> <p><code>NOT_NULL()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n  SELECT *\n  FROM (\n    SELECT *\n    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `demo__incrementals_demo__50975949`\n    WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-05' AS DATE)\n  ) AS `_q_0`\nWHERE\n  `transaction_id` IS NULL\n  AND TRUE\n) AS `audit`\n</code></pre><p></p> Create development schema based on the name of the plan dev environment <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> Create a view in the virtual layer to officially query this new table. <p>Don\u2019t worry, you won\u2019t get view performance penalties - modern query engines employ pushdown predicate to query the base table directly example.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`\n</code></pre> <p>Now let\u2019s make sure the look and feel is what I want. Let\u2019s query the new <code>dev</code> table:</p> <pre><code>vulcan fetchdf \"select * from demo__dev.incrementals_demo limit 5\"\n</code></pre> <pre><code>(.venv) \u2717 vulcan fetchdf \"select * from demo__dev.incrementals_demo limit 5\"\n\n  transaction_id product_id customer_id transaction_amount transaction_date  ... usage_count feature_utilization_score  user_segment     user_type  days_since_last_usage\n0         TX-010   PROD-103    CUST-008      299.990000000       2024-10-26  ...          25                      0.30       starter    Light User                      0\n1         TX-008   PROD-102    CUST-006      149.990000000       2024-10-26  ...         110                      0.88    enterprise    Power User                      0\n2         TX-006   PROD-101    CUST-001       99.990000000       2024-10-26  ...         120                      0.85    enterprise    Power User                      0\n3         TX-009   PROD-101    CUST-007       99.990000000       2024-10-26  ...          60                      0.55  professional  Regular User                      0\n4         TX-007   PROD-103    CUST-002      299.990000000       2024-10-26  ...          80                      0.68    enterprise  Regular User                      0\n\n[5 rows x 14 columns]\n</code></pre>"},{"location":"examples/incremental_time_full_walkthrough/#track-column-level-lineage","title":"Track Column Level Lineage","text":"<p>Now that I have a solid start to my development, I want to document and visualize how this transformation logic works without manually writing a bunch of <code>yaml</code> for the next hour.</p> <p>Thankfully, I don\u2019t have to. I\u2019ll get an automatically generated <code>external_models.yaml</code> file that will parse my <code>incrementals_demo.sql</code> model and query BigQuery metadata to get all columns AND their data types. All of it neatly formatted.</p> <p>Run this command:</p> <pre><code>vulcan create_external_models\n</code></pre> <pre><code># external_models.yaml\n- name: '`vulcan-public-demo`.`tcloud_raw_data`.`product_usage`'\n  columns:\n    product_id: STRING\n    customer_id: STRING\n    last_usage_date: TIMESTAMP\n    usage_count: INT64\n    feature_utilization_score: FLOAT64\n    user_segment: STRING\n- name: '`vulcan-public-demo`.`tcloud_raw_data`.`sales`'\n  columns:\n    transaction_id: STRING\n    product_id: STRING\n    customer_id: STRING\n    transaction_amount: NUMERIC(10,2)\n    transaction_timestamp: TIMESTAMP\n    payment_method: STRING\n    currency: STRING\n</code></pre> <p>Now, when I run the command below in my terminal and click on the link it will open up my browser to show the column level lineage I know and love.</p> <pre><code>vulcan ui\n</code></pre> <pre><code>(venv) \u2717 vulcan ui\nINFO:     Started server process [89705]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n</code></pre> <p></p> <p>When I click on a column in <code>demo.incrementals_demo</code>, it will trace the column to the source!</p> <p></p> <p>Now, typically, I will promote these changes to production using Vulcan\u2019s open source GitHub CICD bot as shown in this demo pull request, but to keep this guide simpler, let\u2019s run <code>vulcan plan</code> directly.</p> <p>This is where I feel the claim \u201cdata transformation without the waste\u201d feels tangible. I did all this great work in my dev environment, and I\u2019m used to reprocessing and duplicating storage in production. However, by default Vulcan will bypass all that and create new views to point to the same physical tables created in <code>dev</code>! You can see for yourself in the query history.</p> <pre><code>(venv) \u2717 vulcan plan\n======================================================================\nSuccessfully Ran 2 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Added:\n    \u251c\u2500\u2500 demo.incrementals_demo\n    \u251c\u2500\u2500 tcloud_raw_data.product_usage\n    \u2514\u2500\u2500 tcloud_raw_data.sales\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:02\n\nThe target environment has been updated successfully\n</code></pre> Create production schema if it does not exist <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo`\n</code></pre> Create a production version of the view. This is where Vulcan reuses the hard work you\u2019ve already done. <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__3076101542`\n</code></pre> Run data audits to test if <code>transaction_id</code> is unique and not null (SQL is automatically generated) <p>Made you look! No need to rerun audits we already passed in dev.</p>"},{"location":"examples/incremental_time_full_walkthrough/#making-changes","title":"Making Changes","text":"<p>Alright, it feels pretty neat to go through this workflow, but now comes the part that represents the majority of my job as a data engineer:</p> <ul> <li>Making changes</li> <li>Testing those changes</li> <li>Promoting those changes safely and confidently to production</li> </ul> <p>Let\u2019s say I want to change my code's definition of a power user but ONLY going forward because we want to broaden our definition. However, I still want to retain how we defined power users historically.</p> <p>At first glance, this is a very surgical operation that can feel intimidating with custom <code>DML</code> operations, but thankfully Vulcan has a native way to solve this problem.</p> <p>First, I make the change to decrease the threshold in my SQL logic:</p> <pre><code>CASE\n  WHEN p.usage_count &gt; 50 AND p.feature_utilization_score &gt; 0.5 THEN 'Power User'\n</code></pre> <p>Unlike last time, I run <code>vulcan plan dev --forward-only</code> with the <code>--forward-only</code> flag, which tells Vulcan it should not run the changed model against all the existing data.</p> <p>In the terminal output, I can see the change displayed like before, but I see some new date prompts.</p> <p>I leave the effective date prompt blank because I do not want to reprocess historical data in <code>prod</code> - I only want to apply this new business logic going forward.</p> <p>However, I do want to preview the new business logic in my <code>dev</code> environment before pushing to <code>prod</code>. Because I have configured Vulcan to create previews for forward-only models in my <code>config.yaml</code> file, Vulcan has created a temporary copy of the <code>prod</code> table in my <code>dev</code> environment, so I can test the new logic on historical data.</p> <p>I specify the beginning of the preview's historical data window as <code>2024-10-27</code> in the preview start date prompt, and I specify the end of the window as now by leaving the preview end date prompt blank.</p> <pre><code>vulcan plan dev --forward-only\n</code></pre> <pre><code>(venv) \u279c  vulcan-demos git:(incremental-demo) \u2717 vulcan plan dev --forward-only\n======================================================================\nSuccessfully Ran 2 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `dev` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 demo__dev.incrementals_demo\n---\n\n+++\n\n@@ -57,7 +57,7 @@\n\n   p.feature_utilization_score,\n   p.user_segment,\n   CASE\n-    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.6\n+    WHEN p.usage_count &gt; 50 AND p.feature_utilization_score &gt; 0.5\n     THEN 'Power User'\n     WHEN p.usage_count &gt; 50\n     THEN 'Regular User'\nDirectly Modified: demo__dev.incrementals_demo (Forward-only)\nEnter the effective date (eg. '1 year', '2020-01-01') to apply forward-only changes retroactively or blank to only apply them going forward once changes\nare deployed to prod:\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 demo__dev.incrementals_demo: 2024-11-07 - 2024-11-07 (preview)\nEnter the preview start date (eg. '1 year', '2020-01-01') or blank to backfill to preview starting from yesterday: 2024-10-27\nEnter the preview end date (eg. '1 month ago', '2020-01-01') or blank to preview up until '2024-11-08 00:00:00':\nApply - Preview Tables [y/n]: y\n[1/1] demo__dev.incrementals_demo evaluated in 6.18s\nEvaluating models \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:06\n\n\nAll model batches have been executed successfully\n\nVirtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:01\n\nThe target environment has been updated successfully\n</code></pre> Create another empty table with the proper schema that\u2019s also versioned (ex: <code>__2896326998__dev__schema_migration_source</code>). <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev__schema_migration_source` (\n  `transaction_id` STRING, `product_id` STRING, `customer_id` STRING, `transaction_amount` NUMERIC, `transaction_date` DATE,\n  `transaction_timestamp_pst` DATETIME, `payment_method` STRING, `currency` STRING, `last_usage_date` TIMESTAMP, `usage_count` INT64,\n  `feature_utilization_score` FLOAT64, `user_segment` STRING, `user_type` STRING, `days_since_last_usage` INT64\n)\nPARTITION BY `transaction_date`\n</code></pre> Validate new SQL (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps) <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP)\n    AND `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP))\n    AND FALSE\n  ),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP)\n    AND `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP))\n    AND FALSE\n  )\n  SELECT\n    `s`.`transaction_id` AS `transaction_id`,\n    `s`.`product_id` AS `product_id`,\n    `s`.`customer_id` AS `customer_id`,\n    CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n    DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n    DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n    `s`.`payment_method` AS `payment_method`,\n    `s`.`currency` AS `currency`,\n    `p`.`last_usage_date` AS `last_usage_date`,\n    `p`.`usage_count` AS `usage_count`,\n    `p`.`feature_utilization_score` AS `feature_utilization_score`,\n    `p`.`user_segment` AS `user_segment`,\n    CASE\n      WHEN `p`.`feature_utilization_score` &gt; 0.5 AND `p`.`usage_count` &gt; 50 THEN 'Power User'\n      WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n      WHEN `p`.`usage_count` IS NULL THEN 'New User'\n      ELSE 'Light User'\n    END AS `user_type`,\n    DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n  FROM `sales_data` AS `s`\n  LEFT JOIN `product_usage` AS `p` ON\n    `p`.`customer_id` = `s`.`customer_id`\n    AND `p`.`product_id` = `s`.`product_id`\n  WHERE FALSE\n  LIMIT 0\n</code></pre> Create a CLONE of the table in the <code>preview</code> process so that we work with physical data for these specific backfill date ranges. <p>This will NOT be reused when deployed to prod.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev`\nCLONE `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__843752089`\n</code></pre> Inspect metadata for this newly versioned table we\u2019re creating, so we can properly track it from its journey from dev to prod eventually. <p>This query examines the table's <code>INFORMATION_SCHEMA</code> metadata about column names and types to confirm for Vulcan\u2019s state that objects exist as expected.</p> <p>Since other actors could hypothetically touch/modify the project's tables, Vulcan doesn\u2019t ever reuse this info because it could have changed. That\u2019s why we see this query executed so many times in the logs.</p> <pre><code>WITH `clustering_info` AS (\n  SELECT\n    `table_catalog`,\n    `table_schema`,\n    `table_name`,\n    STRING_AGG(`column_name` ORDER BY `clustering_ordinal_position`) AS `clustering_key`\n  FROM `vulcan-public-demo`.`vulcan__demo`.`INFORMATION_SCHEMA`.`COLUMNS`\n  WHERE `clustering_ordinal_position` IS NOT NULL\n  GROUP BY 1, 2, 3\n)\nSELECT\n  `table_catalog` AS `catalog`,\n  `table_name` AS `name`,\n  `table_schema` AS `schema_name`,\n  CASE\n    WHEN `table_type` = 'BASE TABLE' THEN 'TABLE'\n    WHEN `table_type` = 'CLONE' THEN 'TABLE'\n    WHEN `table_type` = 'EXTERNAL' THEN 'TABLE'\n    WHEN `table_type` = 'SNAPSHOT' THEN 'TABLE'\n    WHEN `table_type` = 'VIEW' THEN 'VIEW'\n    WHEN `table_type` = 'MATERIALIZED VIEW' THEN 'MATERIALIZED_VIEW'\n    ELSE `table_type` END\n  AS `type`,\n  `ci`.`clustering_key` AS `clustering_key`\nFROM `vulcan-public-demo`.`vulcan__demo`.`INFORMATION_SCHEMA`.`TABLES`\nLEFT JOIN `clustering_info` AS `ci` USING (`table_catalog`, `table_schema`, `table_name`)\nWHERE `table_name` IN ('demo__incrementals_demo__2896326998__dev')\n</code></pre> Inspect metadata to track journey for the migration source schema <pre><code>WITH `clustering_info` AS (\n  SELECT\n    `table_catalog`,\n    `table_schema`,\n    `table_name`,\n    STRING_AGG(`column_name` ORDER BY `clustering_ordinal_position`) AS `clustering_key`\n  FROM `vulcan-public-demo`.`vulcan__demo`.`INFORMATION_SCHEMA`.`COLUMNS`\n  WHERE `clustering_ordinal_position` IS NOT NULL\n  GROUP BY 1, 2, 3\n)\nSELECT\n  `table_catalog` AS `catalog`,\n  `table_name` AS `name`,\n  `table_schema` AS `schema_name`,\n  CASE\n    WHEN `table_type` = 'BASE TABLE' THEN 'TABLE'\n    WHEN `table_type` = 'CLONE' THEN 'TABLE'\n    WHEN `table_type` = 'EXTERNAL' THEN 'TABLE'\n    WHEN `table_type` = 'SNAPSHOT' THEN 'TABLE'\n    WHEN `table_type` = 'VIEW' THEN 'VIEW'\n    WHEN `table_type` = 'MATERIALIZED VIEW' THEN 'MATERIALIZED_VIEW'\n    ELSE `table_type`\n    END\n  AS `type`,\n  `ci`.`clustering_key` AS `clustering_key`\nFROM `vulcan-public-demo`.`vulcan__demo`.`INFORMATION_SCHEMA`.`TABLES`\nLEFT JOIN `clustering_info` AS `ci` USING (`table_catalog`, `table_schema`, `table_name`)\nWHERE `table_name` IN ('demo__incrementals_demo__2896326998__dev__schema_migration_source')\n</code></pre> Drop the migration source table because we have the metadata we need now for proper state tracking <pre><code>DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev__schema_migration_source`\n</code></pre> Merge data into empty table for only the intervals I care about: 2024-10-27 to 'up until now' <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `sales`.`transaction_id` AS `transaction_id`,\n      `sales`.`product_id` AS `product_id`,\n      `sales`.`customer_id` AS `customer_id`,\n      `sales`.`transaction_amount` AS `transaction_amount`,\n      `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n      `sales`.`payment_method` AS `payment_method`,\n      `sales`.`currency` AS `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE\n      `sales`.`transaction_timestamp` &lt;= CAST('2024-11-07 23:59:59.999999+00:00' AS TIMESTAMP)\n      AND `sales`.`transaction_timestamp` &gt;= CAST('2024-10-27 00:00:00+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_usage`.`product_id` AS `product_id`,\n      `product_usage`.`customer_id` AS `customer_id`,\n      `product_usage`.`last_usage_date` AS `last_usage_date`,\n      `product_usage`.`usage_count` AS `usage_count`,\n      `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n      `product_usage`.`user_segment` AS `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE\n      `product_usage`.`last_usage_date` &lt;= CAST('2024-11-07 23:59:59.999999+00:00' AS TIMESTAMP)\n      AND `product_usage`.`last_usage_date` &gt;= CAST('2024-09-27 00:00:00+00:00' AS TIMESTAMP)\n  )\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`feature_utilization_score` &gt; 0.5 AND `p`.`usage_count` &gt; 50 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n        END\n      AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p` ON\n      `p`.`customer_id` = `s`.`customer_id`\n      AND `p`.`product_id` = `s`.`product_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-27' AS DATE) AND CAST('2024-11-07' AS DATE)\n) AS `__MERGE_SOURCE__\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-27' AS DATE) AND CAST('2024-11-07' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN INSERT (\n  `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n  `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n  `days_since_last_usage`)\n  VALUES (`transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n  `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n  `days_since_last_usage`)\n</code></pre> Run data audits to test if <code>transaction_id</code> is unique and not null. <p>SQL is automatically generated for the preview data range in scope: 2024-10-27 to \u201cup until now\u201d.</p> <p><code>UNIQUE_VALUES()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n  SELECT *\n  FROM (\n    SELECT ROW_NUMBER() OVER (\n      PARTITION BY (`transaction_id`)\n      ORDER BY (`transaction_id`)\n      ) AS `rank_`\n    FROM (\n      SELECT *\n      FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev` AS `demo__incrementals_demo__2896326998__dev`\n      WHERE `transaction_date` BETWEEN CAST('2024-10-27' AS DATE) AND CAST('2024-11-08' AS DATE)\n    ) AS `_q_0`\n  WHERE TRUE\n  ) AS `_q_1`\nWHERE `rank_` &gt; 1\n) AS `audit`\n</code></pre><p></p> <p><code>NOT_NULL()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n    SELECT *\n    FROM (\n      SELECT *\n      FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev` AS `demo__incrementals_demo__2896326998__dev`\n    WHERE `transaction_date` BETWEEN CAST('2024-10-27' AS DATE) AND CAST('2024-11-08' AS DATE)\n  ) AS `_q_0`\nWHERE\n  (`transaction_id`) IS NULL\n  AND TRUE\n) AS `audit`\n</code></pre><p></p> Create development schema based on the name of the plan dev environment <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> Create a view in the virtual layer to officially query this new table version <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998__dev`\n</code></pre> <p>Now I\u2019m getting exactly what I expect when I preview the data.</p> <ul> <li>Backfill (reprocess) the new definition of power user on and after 2024-10-27 in <code>dev</code> only</li> <li>See the new power user definition apply from 2024-10-27 to now</li> <li>Retain the old definition of power user before 2024-10-27 to preview the comparison</li> </ul> <p>An experience I\u2019d manually do outside of my transformation workflow with a cobbling of python scripts and ad hoc SQL is now both clear and predictable and tracked in Vulcan\u2019s state history.</p> <pre><code>vulcan fetchdf \"select * from demo__dev.incrementals_demo where usage_count&gt;=50\"\n</code></pre> <pre><code>(venv) \u2717 vulcan fetchdf \"select * from demo__dev.incrementals_demo where usage_count&gt;=50\"\n\n  transaction_id product_id customer_id transaction_amount  ... feature_utilization_score  user_segment     user_type days_since_last_usage\n0         TX-002   PROD-102    CUST-002      149.990000000  ...                      0.92    enterprise    Power User                     0\n1         TX-001   PROD-101    CUST-001       99.990000000  ...                      0.85    enterprise    Power User                     0\n2         TX-008   PROD-102    CUST-006      149.990000000  ...                      0.88    enterprise    Power User                     0\n3         TX-006   PROD-101    CUST-001       99.990000000  ...                      0.85    enterprise    Power User                     0\n4         TX-007   PROD-103    CUST-002      299.990000000  ...                      0.68    enterprise  Regular User                     0\n5         TX-009   PROD-101    CUST-007       99.990000000  ...                      0.55  professional  Regular User                     0\n6         TX-011   PROD-101    CUST-009       99.990000000  ...                      0.65  professional    Power User                     0\n7         TX-013   PROD-103    CUST-001      299.990000000  ...                      0.75    enterprise    Power User                     0\n\n[8 rows x 14 columns]\n</code></pre> <p>Now, here, I may think through this question during development:</p> <ul> <li>What if I don\u2019t like the data results during the preview part of my <code>vulcan plan dev --forward-only</code>?<ul> <li>I update my code changes, go through the above workflow again, and preview data for a specific date range whether for a regular <code>vulcan plan dev</code> or <code>vulcan plan dev --forward-only</code></li> </ul> </li> </ul>"},{"location":"examples/incremental_time_full_walkthrough/#adding-unit-tests","title":"Adding Unit Tests","text":"<p>Data audits are great, but they only verify basic things like primary key integrity. They don\u2019t validate my SQL logic is doing exactly what I want.</p> <p>I know Vulcan has unit tests, but the quiet part out loud is that I dislike writing so much <code>yaml</code> by hand. Thankfully, I don\u2019t have to.</p> <p>I can use the <code>vulcan create_test</code> command to generate the unit test configuration file for me, using SQL queries to select and store the data the tests will run on.</p> <pre><code>vulcan create_test demo.incrementals_demo \\\n--query vulcan-public-demo.tcloud_raw_data.product_usage \"select * from vulcan-public-demo.tcloud_raw_data.product_usage where customer_id='CUST-001'\" \\\n--query vulcan-public-demo.tcloud_raw_data.sales \"select * from vulcan-public-demo.tcloud_raw_data.sales where customer_id='CUST-001'\" \\\n--var start_dt '2024-10-25' \\\n--var end_dt '2024-10-27'\n</code></pre> <p>It\u2019ll create a unit test configuration file automatically like the below based on live queried data called <code>test_incrementals_demo.yaml</code>. I can then modify this file to my liking.</p> Unit test configuration file <pre><code>test_incrementals_demo:\n  model: demo.incrementals_demo\n  inputs:\n    '`vulcan-public-demo`.`tcloud_raw_data`.`product_usage`':\n    - product_id: PROD-101\n      customer_id: CUST-001\n      last_usage_date: 2024-10-25 23:45:00+00:00\n      usage_count: 120\n      feature_utilization_score: 0.85\n      user_segment: enterprise\n    - product_id: PROD-103\n      customer_id: CUST-001\n      last_usage_date: 2024-10-27 12:30:00+00:00\n      usage_count: 95\n      feature_utilization_score: 0.75\n      user_segment: enterprise\n    '`vulcan-public-demo`.`tcloud_raw_data`.`sales`':\n    - transaction_id: TX-013\n      product_id: PROD-103\n      customer_id: CUST-001\n      transaction_amount: '299.990000000'\n      transaction_timestamp: 2024-10-27 08:40:00+00:00\n      payment_method: credit_card\n      currency: USD\n    - transaction_id: TX-006\n      product_id: PROD-101\n      customer_id: CUST-001\n      transaction_amount: '99.990000000'\n      transaction_timestamp: 2024-10-26 03:15:00+00:00\n      payment_method: credit_card\n      currency: USD\n    - transaction_id: TX-001\n      product_id: PROD-101\n      customer_id: CUST-001\n      transaction_amount: '99.990000000'\n      transaction_timestamp: 2024-10-25 08:30:00+00:00\n      payment_method: credit_card\n      currency: USD\n  outputs:\n    query:\n    - transaction_id: TX-006\n      product_id: PROD-101\n      customer_id: CUST-001\n      transaction_amount: 99.99\n      transaction_date: 2024-10-25\n      transaction_timestamp_pst: 2024-10-25 20:15:00\n      payment_method: credit_card\n      currency: USD\n      last_usage_date: 2024-10-25 16:45:00-07:00\n      usage_count: 120\n      feature_utilization_score: 0.85\n      user_segment: enterprise\n      user_type: Power User\n      days_since_last_usage: 0\n    - transaction_id: TX-001\n      product_id: PROD-101\n      customer_id: CUST-001\n      transaction_amount: 99.99\n      transaction_date: 2024-10-25\n      transaction_timestamp_pst: 2024-10-25 01:30:00\n      payment_method: credit_card\n      currency: USD\n      last_usage_date: 2024-10-25 16:45:00-07:00\n      usage_count: 120\n      feature_utilization_score: 0.85\n      user_segment: enterprise\n      user_type: Power User\n      days_since_last_usage: 0\n  vars:\n    start_dt: '2024-10-25'\n    end_dt: '2024-10-27'\n</code></pre> <p>Now, when I run <code>vulcan test</code> I run all my unit tests for free on my local machine.</p> <p>Vulcan runs these unit test fixtures directly in duckdb in-memory by transpiling your specific database\u2019s SQL syntax into the same meaning via SQLGlot. That\u2019s why it runs so fast!</p> I can also run my unit tests against my main query engine to test things like UDFs or if there\u2019s very specific SQL functions that do not neatly transpile to duckdb. Example test connection. <pre><code>gateways:\n  bigquery:\n    connection:\n      concurrent_tasks: 24\n      register_comments: true\n      type: bigquery\n      method: service-account-json\n      keyfile_json: {{ env_var('GOOGLE_VULCAN_CREDENTIALS') }}\n      project: vulcan-public-demo\n    test_connection:\n      concurrent_tasks: 24\n      register_comments: true\n      type: bigquery\n      method: service-account-json\n      keyfile_json: {{ env_var('GOOGLE_VULCAN_CREDENTIALS') }}\n      project: vulcan-public-demo\n</code></pre> <pre><code>(venv) \u2717 vulcan test\n...\n----------------------------------------------------------------------\nRan 3 tests in 0.090s\n\nOK\n</code></pre>"},{"location":"examples/incremental_time_full_walkthrough/#promoting-changes-to-production","title":"Promoting Changes to Production","text":"<p>Now that I\u2019ve done all this great work, how do I get this promoted into production?</p> <p>Typically, I will open a pull request combined with the Vulcan GitHub CI/CD bot as I mentioned earlier in this guide. But to keep it simple, I\u2019ll run <code>vulcan plan</code> as I did above.</p> <p>This time because it\u2019s promoting a forward-only dev model into prod, it\u2019s a virtual update to the SQL definition.</p> <p>We run a bunch of metadata queries to version tables. More queries (read: 15/15 in the progress bar) are run in this forward-only model promotion to track schema evolution, if it appears, between the old and new schema.</p> <p>Next time it\u2019s run, it\u2019ll backfill new data with this new definition of \u2018Power User\u2019.</p> <pre><code>vulcan plan\n</code></pre> <pre><code>(venv) \u279c  vulcan-demos git:(incremental-demo) \u2717 vulcan plan\n======================================================================\nSuccessfully Ran 3 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 demo.incrementals_demo\n---\n\n+++\n\n@@ -57,7 +57,7 @@\n\n   p.feature_utilization_score,\n   p.user_segment,\n   CASE\n-    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.6\n+    WHEN p.usage_count &gt; 50 AND p.feature_utilization_score &gt; 0.5\n     THEN 'Power User'\n     WHEN p.usage_count &gt; 50\n     THEN 'Regular User'\nDirectly Modified: demo.incrementals_demo (Forward-only)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:02\n\nThe target environment has been updated successfully\n</code></pre> Create production schema if it does not exist <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo`\n</code></pre> Create a production version of the view. This is where Vulcan reuses the hard work you\u2019ve already done. No need to rerun audits. <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo`.`incrementals_demo` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__2896326998`\n</code></pre> <p>Now, when a cron job runs on a schedule Vulcan will track that midnight UTC has passed for a full day before running new intervals to backfill in this SQL model. Note: it will skip backfilling this model if a full day interval has not passed.</p> <p>The run will look and feel like the below as an example.</p> <pre><code>vulcan run --select-model \"demo.incrementals_demo\"\n</code></pre> <pre><code>(venv) \u2717 vulcan run --select-model \"demo.incrementals_demo\"\n[1/1] demo.incrementals_demo evaluated in 8.40s\nEvaluating models \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:08\n\n\nAll model batches have been executed successfully\n\nRun finished for environment 'prod'\n</code></pre> Merge data into empty table for only the intervals I have not backfilled since last running this command <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__922005762` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `sales`.`transaction_id` AS `transaction_id`,\n      `sales`.`product_id` AS `product_id`,\n      `sales`.`customer_id` AS `customer_id`,\n      `sales`.`transaction_amount` AS `transaction_amount`,\n      `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n      `sales`.`payment_method` AS `payment_method`,\n      `sales`.`currency` AS `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE\n      `sales`.`transaction_timestamp` &lt;= CAST('2024-11-07 23:59:59.999999+00:00' AS TIMESTAMP)\n      AND `sales`.`transaction_timestamp` &gt;= CAST('2024-11-03 00:00:00+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_usage`.`product_id` AS `product_id`,\n      `product_usage`.`customer_id` AS `customer_id`,\n      `product_usage`.`last_usage_date` AS `last_usage_date`,\n      `product_usage`.`usage_count` AS `usage_count`,\n      `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n      `product_usage`.`user_segment` AS `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE\n      `product_usage`.`last_usage_date` &lt;= CAST('2024-11-07 23:59:59.999999+00:00' AS TIMESTAMP)\n      AND `product_usage`.`last_usage_date` &gt;= CAST('2024-10-04 00:00:00+00:00' AS TIMESTAMP)\n  )\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`feature_utilization_score` &gt; 0.6 AND `p`.`usage_count` &gt; 60 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n        END\n      AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p` ON\n      `p`.`customer_id` = `s`.`customer_id`\n      AND `p`.`product_id` = `s`.`product_id`\n    ) AS `_subquery`\n  WHERE\n    `transaction_date` BETWEEN CAST('2024-11-03' AS DATE)\n    AND CAST('2024-11-07' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-11-03' AS DATE) AND CAST('2024-11-07' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`)\n    VALUES (`transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`)\n</code></pre> Run data audits to test if transaction_id is unique and not null. SQL is automatically generated. <p><code>UNIQUE_VALUES()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n  SELECT *\n  FROM (\n    SELECT\n      ROW_NUMBER() OVER (\n        PARTITION BY (`transaction_id`)\n        ORDER BY (`transaction_id`)\n      ) AS `rank_`\n    FROM (\n      SELECT *\n      FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__922005762` AS `demo__incrementals_demo__922005762`\n    WHERE `transaction_date` BETWEEN CAST('2024-11-03' AS DATE) AND CAST('2024-11-08' AS DATE)\n    ) AS `_q_0`\n  WHERE TRUE\n  ) AS `_q_1`\nWHERE `rank_` &gt; 1\n) AS `audit`\n</code></pre><p></p> <p><code>NOT_NULL()</code> audit </p><pre><code>SELECT\n  COUNT(*)\nFROM (\n  SELECT *\n  FROM (\n    SELECT *\n    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__922005762` AS `demo__incrementals_demo__922005762`\n    WHERE `transaction_date` BETWEEN CAST('2024-11-03' AS DATE) AND CAST('2024-11-08' AS DATE)\n  ) AS `_q_0`\nWHERE\n  (`transaction_id`) IS NULL\n  AND TRUE\n) AS `audit`\n</code></pre><p></p>"},{"location":"examples/incremental_time_full_walkthrough/#summary","title":"Summary","text":"<p>I went through a full workflow for an intimidating problem, and it feels really good knowing what goes on behind the scenes when I run these Vulcan commands. For those coming from other transformation frameworks like dbt, this is a new way to work.</p> <p>It respects data as infrastructure vs. things to rebuild many times over each time you change something. I hope you feel equipped AND confident to start using Vulcan and especially incremental models today!</p> <p>I\u2019ll make it convenient for you in making sure we answered all the pertinent questions.</p> <ul> <li>How do I handle late arriving data?<ul> <li>Use the <code>lookback</code> config.</li> </ul> </li> <li>How do I account for UTC vs. PST (California baby) timestamps, do I convert them?<ul> <li>See the SQL logic for how everything is in UTC by default for safe and reliable processing and then convert the presentation timestamp to PST for downstream tools (ex: business intelligence, data sharing)</li> </ul> </li> <li>What schedule should I run these at?<ul> <li>Daily is a default as we don\u2019t want to show incomplete intervals when merging product and sales information. You can go as low as 5 minutes.</li> </ul> </li> <li>How do I test this data?<ul> <li>Unit tests for code, audits for data</li> </ul> </li> <li>How do I make this run fast and only the intervals necessary (read: partitions)?<ul> <li>Vulcan macros work by default to run and test only the intervals necessary because it manages state.</li> <li>No <code>max(timestamp)</code> acrobatics.</li> </ul> </li> <li>How do I make patch changes when an edge case error occurs with incorrect data for specific time ranges?<ul> <li>Make code changes and backfill only what\u2019s necessary safely in dev before promoting to prod.</li> <li>Retain history AND correct changes for specific time ranges.</li> <li>Check out the forward-only example above and notice you can make retroactive changes to prod.</li> </ul> </li> <li>What do unit tests look and feel like for this?<ul> <li>See automatic unit test creation above. No manual <code>yaml</code> handwriting!</li> </ul> </li> <li>How do I prevent data gaps with unprocessed or incomplete intervals?<ul> <li>Vulcan manages state, so it will track which intervals were backfilled vs. not.</li> <li>Even if an interval failed during a scheduled <code>vulcan run</code>, it will recognize that the next time this command is run and attempt to backfill that previously failed interval.</li> <li>No <code>max(timestamp)</code> acrobats.</li> </ul> </li> <li>Am I okay processing incomplete intervals (think: allow partials)?<ul> <li>I'm only okay with allowing partial intervals to be processed for things like logging event data, but for sales and product data, I want to make sure complete intervals are processed so end users don't confuse incomplete data with incorrect data.</li> </ul> </li> <li>What tradeoffs am I willing to make for fresh data?<ul> <li>I prefer complete data over fresh data for its own sake. Correctness matters when viewing revenue data.</li> </ul> </li> <li>How to make this not feel so complex during development?<ul> <li>Hopefully this guide helps ;)</li> </ul> </li> <li>How do I know Vulcan is behaving how I want it to behave?<ul> <li>See the queries run by Vulcan above. They\u2019re listed out exactly as listed in the query history.</li> <li>I skip listing out basic metadata queries and test connection queries like <code>SELECT 1</code> as those are more background tasks than core logic tasks.</li> </ul> </li> <li>Bonus question: How does this compare to dbt\u2019s way of handling incrementals?<ul> <li>See here for a complete comparison</li> </ul> </li> </ul>"},{"location":"examples/overview/","title":"Overview","text":""},{"location":"examples/overview/#overview","title":"Overview","text":"<p>Realistic examples are a fantastic way to understand Vulcan better.</p> <p>They allow you to tinker with a project's code and data, issuing different Vulcan commands to see what happens.</p> <p>You can reset the examples at any time, so if things get turned around you can just start over!</p> <p>This page links to a few different types of examples:</p> <ul> <li>Walkthroughs pose a specific story or task, and you follow along as we work through the story<ul> <li>Walkthroughs do not require running code, although the code is available if you would like to</li> <li>Different walkthroughs use different SQL engines, so if you want to run the code you might need to update it for your SQL engine</li> </ul> </li> <li>Projects are self-contained Vulcan projects and datasets<ul> <li>Projects generally use DuckDB so you can run them locally without installing or accessing a separate SQL engine</li> </ul> </li> </ul> <p>Tip</p> <p>If you haven't tried out Vulcan before, we recommending working through the Vulcan Quickstart before trying these examples!</p>"},{"location":"examples/overview/#walkthroughs","title":"Walkthroughs","text":"<p>Walkthroughs are easy to follow and provide lots of information in a self-contained format.</p> <ul> <li>Get the Vulcan workflow under your fingers with the Vulcan CLI Crash Course</li> <li>See the end-to-end workflow in action with the Incremental by Time Range: Full Walkthrough (BigQuery SQL engine)</li> </ul>"},{"location":"examples/overview/#projects","title":"Projects","text":"<p>Vulcan example projects are stored in the vulcan-examples Github repository. The repository's front page includes additional information about how to download the files and set up the projects.</p> <p>The two most comprehensive example projects use the Vulcan <code>sushi</code> data, based on a fictional sushi restaurant. (\"Tobiko\" is the Japanese word for flying fish roe, commonly used in sushi.)</p> <p>The <code>sushi</code> data is described in an overview notebook in the repository.</p> <p>The example repository include two versions of the <code>sushi</code> project, at different levels of complexity:</p> <ul> <li>The <code>simple</code> project contains four <code>VIEW</code> and one <code>SEED</code> model<ul> <li>The <code>VIEW</code> model kind refreshes every run, making it easy to reason about Vulcan's behavior</li> </ul> </li> <li>The <code>moderate</code> project contains five <code>INCREMENTAL_BY_TIME_RANGE</code>, one <code>FULL</code>, one <code>VIEW</code>, and one <code>SEED</code> model<ul> <li>The incremental models allow you to observe how and when new data is transformed by Vulcan</li> <li>Some models, like <code>customer_revenue_lifetime</code>, demonstrate more advanced incremental queries like customer lifetime value calculation</li> </ul> </li> </ul>"},{"location":"examples/vulcan_cli_crash_course/","title":"Vulcan CLI Crash Course","text":""},{"location":"examples/vulcan_cli_crash_course/#vulcan-cli-crash-course","title":"Vulcan CLI Crash Course","text":"<p>This doc is designed to get you intimate with a majority of the Vulcan workflows you\u2019ll use to build and maintain transformation data pipelines. The goal is to get Vulcan into muscle memory in 30 minutes or less.</p> <p>This doc is inspired by community observations, face-to-face conversations, live screenshares, and debugging sessions. This is not an exhaustive list but is rooted in lived experience.</p> <p>You can follow along in the open source GitHub repo.</p> <p>If you're new to how Vulcan uses virtual data environments, watch this quick explainer.</p> <p>Tip</p> <p>Put this page on your second monitor or in a side by side window to swiftly copy/paste into your terminal.</p>"},{"location":"examples/vulcan_cli_crash_course/#development-workflow","title":"Development Workflow","text":"<p>You\u2019ll use these commands 80% of the time because this is how you apply the changes you make to models. The workflow is:</p> <ol> <li>Make changes to your models directly in SQL and python files (pre-made in examples below)</li> <li>Plan the changes in your dev environment</li> <li>Apply the changes to your dev environment</li> <li>Audit the changes (test data quality)</li> <li>Run data diff against prod</li> <li>Apply the changes to prod</li> </ol>"},{"location":"examples/vulcan_cli_crash_course/#preview-apply-and-audit-changes-in-dev","title":"Preview, Apply, and Audit Changes in <code>dev</code>","text":"<p>You can make changes quickly and confidently through one simple command: <code>vulcan plan dev</code></p> <ul> <li>Plan the changes in your dev environment.</li> <li>Apply the changes to your dev environment by entering <code>y</code> at the prompt.</li> <li>Audit the changes (test data quality). This happens automatically when you apply the changes to dev.</li> </ul> <p>Note: If you run this without making any changes, Vulcan will prompt you to make changes or use the <code>--include-unmodified</code> flag like this <code>vulcan plan dev --include-unmodified</code>. We recommend you make changes first before running this command to avoid creating a lot of noise in your dev environment with extraneous virtual layer views.</p> VulcanTobiko Cloud <pre><code>vulcan plan dev\n</code></pre> <pre><code>vulcan plan &lt;environment&gt;\n</code></pre> <p>If you want to move faster, you can add the <code>--auto-apply</code> flag to skip the manual prompt and apply the plan. You should do this when you're familiar with the plan output, and don't need to see tiny changes in the diff output before applying the plan.</p> <pre><code>vulcan plan &lt;environment&gt; --auto-apply\n</code></pre> <pre><code>tcloud vulcan plan dev\n</code></pre> <pre><code>tcloud vulcan plan &lt;environment&gt;\n</code></pre> <p>If you want to move faster, you can add the <code>--auto-apply</code> flag to skip the manual prompt and apply the plan. You should do this when you're familiar with the plan output, and don't need to see tiny changes in the diff output before applying the plan.</p> <pre><code>tcloud vulcan plan &lt;environment&gt; --auto-apply\n</code></pre> Example Output <p>I made a breaking change to <code>incremental_model</code> and <code>full_model</code>.</p> <p>Vulcan:</p> <ul> <li>Showed me the models impacted by the changes.</li> <li>Showed me the changes that will be made to the models.</li> <li>Showed me the models that need to be backfilled.</li> <li>Prompted me to apply the changes to <code>dev</code>.</li> <li>Showed me the audit failures that raise as warnings.</li> <li>Updated the physical layer to validate the SQL.</li> <li>Executed the model batches by inserting the data into the physical layer.</li> <li>Updated the virtual layer's view pointers to reflect the changes.</li> </ul> <pre><code>&gt; vulcan plan dev\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.incremental_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.full_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.view_model\n\n---\n\n+++\n\n@@ -9,7 +9,8 @@\n\n SELECT\n   item_id,\n   COUNT(DISTINCT id) AS num_orders,\n-  6 AS new_column\n+  new_column\n FROM vulcan_example.incremental_model\n GROUP BY\n-  item_id\n+  item_id,\n+  new_column\n\nDirectly Modified: vulcan_example__dev.full_model (Breaking)\n\n---\n\n+++\n\n@@ -15,7 +15,7 @@\n\n   id,\n   item_id,\n   event_date,\n-  5 AS new_column\n+  7 AS new_column\n FROM vulcan_example.seed_model\n WHERE\n   event_date BETWEEN @start_date AND @end_date\n\nDirectly Modified: vulcan_example__dev.incremental_model (Breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.view_model (Indirect Breaking)\nModels needing backfill:\n\u251c\u2500\u2500 vulcan_example__dev.full_model: [full refresh]\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: [2020-01-01 - 2025-04-16]\n\u2514\u2500\u2500 vulcan_example__dev.view_model: [recreate view]\nApply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1]  vulcan_example__dev.incremental_model               [insert 2020-01-01 - 2025-04-16]                 0.03s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0% \u2022 pending \u2022 0:00:00\nvulcan_example__dev.incremental_model .\n[WARNING] vulcan_example__dev.full_model: 'assert_positive_order_ids' audit error: 2 rows failed. Learn more in logs:\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/logs/vulcan_2025_04_18_10_33_43.log\n[1/1]  vulcan_example__dev.full_model                      [full refresh, audits \u274c1]                       0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 33.3% \u2022 1/3 \u2022 0:00:00\nvulcan_example__dev.full_model .\n[WARNING] vulcan_example__dev.view_model: 'assert_positive_order_ids' audit error: 2 rows failed. Learn more in logs:\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/logs/vulcan_2025_04_18_10_33_43.log\n[1/1]  vulcan_example__dev.view_model                      [recreate view, audits \u27142 \u274c1]                   0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#run-data-diff-against-prod","title":"Run Data Diff Against Prod","text":"<p>Run data diff against prod. This is a good way to verify the changes are behaving as expected after applying them to <code>dev</code>.</p> <p>To make this easier and faster, you can run data diff against all models in the environment impacted by plan changes applied using the <code>-m '*'</code> flag example below. No need to specify the model name! Read more about options here.</p> VulcanTobiko Cloud <pre><code>vulcan table_diff prod:dev vulcan_example.full_model --show-sample\n</code></pre> <pre><code>vulcan table_diff &lt;environment&gt;:&lt;environment&gt; &lt;model_name&gt; --show-sample\n</code></pre> <pre><code>vulcan table_diff prod:dev -m '*' --show-sample\n</code></pre> <pre><code>tcloud vulcan table_diff prod:dev vulcan_example.full_model --show-sample\n</code></pre> <pre><code>tcloud vulcan table_diff &lt;environment&gt;:&lt;environment&gt; &lt;model_name&gt; --show-sample\n</code></pre> <pre><code>tcloud vulcan table_diff prod:dev -m '*' --show-sample\n</code></pre> Example Output <p>I compare the <code>prod</code> and <code>dev</code> environments for <code>vulcan_example.full_model</code>.</p> <ul> <li>Verified environments and models to diff along with the join on grain configured.</li> <li>Showed me schema diffs between the environments.</li> <li>Showed me row count diffs between the environments.</li> <li>Showed me common rows stats between the environments.</li> <li>Showed me sample data differences between the environments.</li> <li>This is where your human judgement comes in to verify the changes are behaving as expected.</li> </ul> <p>Model definition: </p><pre><code>-- models/full_model.sql\nMODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id, -- grain is optional BUT necessary for table diffs to work correctly. It's your primary key that is unique and not null.\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\n  new_column\nFROM\n    vulcan_example.incremental_model\nGROUP BY item_id, new_column\n</code></pre><p></p> <p>Table diff: </p><pre><code>&gt; vulcan table_diff prod:dev vulcan_example.full_model --show-sample\nTable Diff\n\u251c\u2500\u2500 Model:\n\u2502   \u2514\u2500\u2500 vulcan_example.full_model\n\u251c\u2500\u2500 Environment:\n\u2502   \u251c\u2500\u2500 Source: prod\n\u2502   \u2514\u2500\u2500 Target: dev\n\u251c\u2500\u2500 Tables:\n\u2502   \u251c\u2500\u2500 Source: db.vulcan_example.full_model\n\u2502   \u2514\u2500\u2500 Target: db.vulcan_example__dev.full_model\n\u2514\u2500\u2500 Join On:\n    \u2514\u2500\u2500 item_id\n\nSchema Diff Between 'PROD' and 'DEV' environments for model 'vulcan_example.full_model':\n\u2514\u2500\u2500 Schemas match\n\n\nRow Counts:\n\u2514\u2500\u2500  PARTIAL MATCH: 5 rows (100.0%)\n\nCOMMON ROWS column comparison stats:\n            pct_match\nnum_orders      100.0\nnew_column        0.0\n\n\nCOMMON ROWS sample data differences:\nColumn: new_column\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 item_id \u2503 PROD \u2503 DEV \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 -11     \u2502 5    \u2502 7   \u2502\n\u2502 -3      \u2502 5    \u2502 7   \u2502\n\u2502 1       \u2502 5    \u2502 7   \u2502\n\u2502 3       \u2502 5    \u2502 7   \u2502\n\u2502 9       \u2502 5    \u2502 7   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p>"},{"location":"examples/vulcan_cli_crash_course/#apply-changes-to-prod","title":"Apply Changes to Prod","text":"<p>After you feel confident about the changes, apply them to <code>prod</code>.</p> <p>Apply the changes to prod</p> <p>We recommend only applying changes to <code>prod</code> using CI/CD as best practice. For learning purposes and hot fixes, you can manually apply the changes to prod by entering <code>y</code> at the prompt.</p> VulcanTobiko Cloud <pre><code>vulcan plan\n</code></pre> <pre><code>tcloud vulcan plan\n</code></pre> Example Output <p>After I feel confident about the changes, I apply them to <code>prod</code>.</p> <p>Vulcan:</p> <ul> <li>Showed me the models impacted by the changes.</li> <li>Showed me the changes that will be made to the models.</li> <li>Showed me the models that need to be backfilled. None in this case as it was already backfilled earlier in <code>dev</code>.</li> <li>Prompted me to apply the changes to <code>prod</code>.</li> <li>Showed me physical layer and execution steps are skipped as the changes were already applied to <code>dev</code>.</li> <li>Updated the virtual layer view pointers to reflect the changes.</li> </ul> <pre><code>&gt; vulcan plan\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.view_model\n\n---\n\n+++\n\n@@ -9,7 +9,8 @@\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\n-  5 AS new_column\n+  new_column\nFROM vulcan_example.incremental_model\nGROUP BY\n-  item_id\n+  item_id,\n+  new_column\n\nDirectly Modified: vulcan_example.full_model (Breaking)\n\n---\n\n+++\n\n@@ -15,7 +15,7 @@\n\n  id,\n  item_id,\n  event_date,\n-  5 AS new_column\n+  7 AS new_column\nFROM vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n\nDirectly Modified: vulcan_example.incremental_model (Breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.view_model (Indirect Breaking)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#enhanced-testing-workflow","title":"Enhanced Testing Workflow","text":"<p>You'll use these commands to validate your changes are behaving as expected. Audits (data tests) are a great first step, and you'll want to grow from there to feel confident about your pipelines. The workflow is as follows:</p> <ol> <li>Create and audit external models outside of Vulcan's control (ex: data loaded in by Fivetran, Airbyte, etc.)</li> <li>Automatically generate unit tests for your models</li> <li>Ad hoc query the data directly in the CLI</li> <li>Lint your models to catch known syntax errors</li> </ol>"},{"location":"examples/vulcan_cli_crash_course/#create-and-audit-external-models","title":"Create and Audit External Models","text":"<p>Sometimes models <code>SELECT</code> from tables/views that are outside of Vulcan's control. Vulcan can automatically parse their fully qualified names from model definitions (ex: <code>bigquery-public-data</code>.<code>ga4_obfuscated_sample_ecommerce</code>.<code>events_20210131</code>) and determine their full schemas and column data types.</p> <p>These \"external model\" schemas are used for column level lineage. You can also add audits to test data quality. If an audit fails, Vulcan prevents downstream models from wastefully running.</p> VulcanTobiko Cloud <pre><code>vulcan create_external_models\n</code></pre> <pre><code>tcloud vulcan create_external_models\n</code></pre> Example Output <p>Note: this is an example from a separate Tobiko Cloud project, so you can't follow along in the Github repo.</p> <ul> <li>Generated external models from the <code>bigquery-public-data</code>.<code>ga4_obfuscated_sample_ecommerce</code>.<code>events_20210131</code> table parsed in the model's SQL.</li> <li>Added an audit to the external model to ensure <code>event_date</code> is not NULL.</li> <li>Viewed a plan preview of the changes that will be made for the external model.</li> </ul> models/external_model_example.sql<pre><code>MODEL (\n  name tcloud_demo.external_model\n);\n\nSELECT\n  event_date,\n  event_timestamp,\n  event_name,\n  event_params,\n  event_previous_timestamp,\n  event_value_in_usd,\n  event_bundle_sequence_id,\n  event_server_timestamp_offset,\n  user_id,\n  user_pseudo_id,\n  privacy_info,\n  user_properties,\n  user_first_touch_timestamp,\n  user_ltv,\n  device,\n  geo,\n  app_info,\n  traffic_source,\n  stream_id,\n  platform,\n  event_dimensions,\n  ecommerce\n/*   items */\nFROM bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131 -- I fully qualified the external table name and vulcan will automatically create the external model\n</code></pre> <p><code>vulcan create_external_models</code> output file:</p> external_models.yaml<pre><code>- name: '`bigquery-public-data`.`ga4_obfuscated_sample_ecommerce`.`events_20210131`'\n  audits: # I added this audit manually to the external model YAML file\n    - name: not_null\n      columns: \"[event_date]\"\n  columns:\n    event_date: STRING\n    event_timestamp: INT64\n    event_name: STRING\n    event_params: ARRAY&lt;STRUCT&lt;key STRING, value STRUCT&lt;string_value STRING, int_value\n      INT64, float_value FLOAT64, double_value FLOAT64&gt;&gt;&gt;\n    event_previous_timestamp: INT64\n    event_value_in_usd: FLOAT64\n    event_bundle_sequence_id: INT64\n    event_server_timestamp_offset: INT64\n    user_id: STRING\n    user_pseudo_id: STRING\n    privacy_info: STRUCT&lt;analytics_storage INT64, ads_storage INT64, uses_transient_token\n      STRING&gt;\n    user_properties: ARRAY&lt;STRUCT&lt;key INT64, value STRUCT&lt;string_value INT64, int_value\n      INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64&gt;&gt;&gt;\n    user_first_touch_timestamp: INT64\n    user_ltv: STRUCT&lt;revenue FLOAT64, currency STRING&gt;\n    device: STRUCT&lt;category STRING, mobile_brand_name STRING, mobile_model_name STRING,\n      mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system\n      STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64,\n      language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64,\n      web_info STRUCT&lt;browser STRING, browser_version STRING&gt;&gt;\n    geo: STRUCT&lt;continent STRING, sub_continent STRING, country STRING, region STRING,\n      city STRING, metro STRING&gt;\n    app_info: STRUCT&lt;id STRING, version STRING, install_store STRING, firebase_app_id\n      STRING, install_source STRING&gt;\n    traffic_source: STRUCT&lt;medium STRING, name STRING, source STRING&gt;\n    stream_id: INT64\n    platform: STRING\n    event_dimensions: STRUCT&lt;hostname STRING&gt;\n    ecommerce: STRUCT&lt;total_item_quantity INT64, purchase_revenue_in_usd FLOAT64,\n      purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64,\n      shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64,\n      tax_value FLOAT64, unique_items INT64, transaction_id STRING&gt;\n    items: ARRAY&lt;STRUCT&lt;item_id STRING, item_name STRING, item_brand STRING, item_variant\n      STRING, item_category STRING, item_category2 STRING, item_category3 STRING,\n      item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64,\n      quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd\n      FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id\n      STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING,\n      promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot\n      STRING&gt;&gt;\n  gateway: public-demo\n</code></pre> <pre><code>&gt; vulcan plan dev_sung\nDifferences from the `dev_sung` environment:\n\nModels:\n\u2514\u2500\u2500 Metadata Updated:\n    \u2514\u2500\u2500 \"bigquery-public-data\".ga4_obfuscated_sample_ecommerce__dev_sung.events_20210131\n\n---\n\n+++\n\n@@ -29,5 +29,6 @@\n\n    ecommerce STRUCT&lt;total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64,\ntax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING&gt;,\n    items ARRAY&lt;STRUCT&lt;item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING,\nprice_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING,\nitem_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING&gt;&gt;\n  ),\n+  audits (not_null('columns' = [event_date])),\n  gateway `public-demo`\n)\n\nMetadata Updated: \"bigquery-public-data\".ga4_obfuscated_sample_ecommerce__dev_sung.events_20210131\nModels needing backfill:\n\u2514\u2500\u2500 \"bigquery-public-data\".ga4_obfuscated_sample_ecommerce__dev_sung.events_20210131: [full refresh]\nApply - Backfill Tables [y/n]:\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#automatically-generate-unit-tests","title":"Automatically Generate Unit Tests","text":"<p>You can ensure business logic is working as expected by running your models against static sample data.</p> <p>Unit tests run before a plan is applied automatically. This is great for testing complex business logic (ex: <code>CASE WHEN</code> conditions) before you backfill data. No need to write them manually, either!</p> VulcanTobiko Cloud <p>Create a unit test based on 5 rows from the upstream <code>vulcan_example.incremental_model</code>.</p> <pre><code>vulcan create_test vulcan_example.full_model \\\n  --query vulcan_example.incremental_model \\\n  \"select * from vulcan_example.incremental_model limit 5\"\n</code></pre> <pre><code>vulcan create_test &lt;model_name&gt; \\\n  --query &lt;model_name upstream&gt; \\\n  \"select * from &lt;model_name upstream&gt; limit 5\"\n</code></pre> <pre><code>tcloud vulcan create_test demo.stg_payments \\\n  --query demo.seed_raw_payments \\\n  \"select * from demo.seed_raw_payments limit 5\"\n</code></pre> <pre><code>tcloud vulcan create_test &lt;model_name&gt; \\\n  --query &lt;model_name upstream&gt; \\\n  \"select * from &lt;model_name upstream&gt; limit 5\"\n</code></pre> Example Output <p>Vulcan:</p> <ul> <li>Generated unit tests for the <code>vulcan_example.full_model</code> model by live querying the data.</li> <li>Ran the tests and they passed locally in DuckDB.</li> <li>If you're using a cloud data warehouse, this will transpile your SQL syntax to its equivalent in duckdb.</li> <li>This runs fast and free on your local machine.</li> </ul> <p>Generated test definition file:</p> tests/test_full_model.yaml<pre><code>test_full_model:\n  model: '\"db\".\"vulcan_example\".\"full_model\"'\n  inputs:\n    '\"db\".\"vulcan_example\".\"incremental_model\"':\n    - id: -11\n      item_id: -11\n      event_date: 2020-01-01\n      new_column: 7\n    - id: 1\n      item_id: 1\n      event_date: 2020-01-01\n      new_column: 7\n    - id: 3\n      item_id: 3\n      event_date: 2020-01-03\n      new_column: 7\n    - id: 4\n      item_id: 1\n      event_date: 2020-01-04\n      new_column: 7\n    - id: 5\n      item_id: 1\n      event_date: 2020-01-05\n      new_column: 7\n  outputs:\n    query:\n    - item_id: 3\n      num_orders: 1\n      new_column: 7\n    - item_id: 1\n      num_orders: 3\n      new_column: 7\n    - item_id: -11\n      num_orders: 1\n      new_column: 7\n</code></pre> <p>Manually execute tests with <code>vulcan test</code>:</p> <pre><code>(demo) \u279c  demo git:(main) \u2717 vulcan test\n.\n----------------------------------------------------------------------\nRan 1 test in 0.053s\n\nOK\n</code></pre> <pre><code># what do we see if the test fails?\n(demo) \u279c  demo git:(main) \u2717 vulcan test\nF\n======================================================================\nFAIL: test_full_model (/Users/sung/Desktop/git_repos/vulcan-cli-revamp/tests/test_full_model.yaml)\nNone\n----------------------------------------------------------------------\nAssertionError: Data mismatch (exp: expected, act: actual)\n\n  new_column\n        exp  act\n0        0.0  7.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.020s\n\nFAILED (failures=1)\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#run-ad-hoc-queries","title":"Run Ad-Hoc Queries","text":"<p>You can run live queries directly from the CLI. This is great to validate the look and feel of your changes without context switching to your query console.</p> <p>Pro tip: run this after <code>vulcan table_diff</code> to get a full picture of your changes.</p> VulcanTobiko Cloud <pre><code>vulcan fetchdf \"select * from vulcan_example__dev.full_model limit 5\"\n</code></pre> <pre><code># construct arbitrary query\nvulcan fetchdf \"select * from &lt;schema__environment&gt;.&lt;model_name&gt; limit 5\" # double underscore in schema name is important. Not needed for prod.\n</code></pre> <pre><code>tcloud vulcan fetchdf \"select * from vulcan_example__dev.full_model limit 5\"\n</code></pre> <pre><code># construct arbitrary query\ntcloud vulcan fetchdf \"select * from &lt;schema__environment&gt;.&lt;model_name&gt; limit 5\" # double underscore in schema name is important. Not needed for prod.\n</code></pre> Example Output <pre><code>item_id  num_orders  new_column\n0        9           1           7\n1      -11           1           7\n2        3           1           7\n3       -3           1           7\n4        1           4           7\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#linting","title":"Linting","text":"<p>If enabled, linting runs automatically during development. The linting rules can be overridden per model, too.</p> <p>This is a great way to catch SQL issues before wasting runtime in your data warehouse. It runs automatically, or you can run it manually to proactively check for any issues.</p> VulcanTobiko Cloud <pre><code>vulcan lint\n</code></pre> <pre><code>tcloud vulcan lint\n</code></pre> Example Output <p>You add linting rules in your <code>config.yaml</code> file.</p> config.yaml<pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      database: db.db\n\ndefault_gateway: duckdb\n\nmodel_defaults:\n  dialect: duckdb\n  start: 2025-03-26\n\nlinter:\n  enabled: true\n  rules: [\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"] # raise errors for these rules\n  warn_rules: [\"noselectstar\", \"nomissingaudits\"]\n  # ignored_rules: [\"noselectstar\"]\n</code></pre> <pre><code>&gt; vulcan lint\n[WARNING] Linter warnings for /Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/lint_warn.sql:\n- noselectstar: Query should not contain SELECT * on its outer most projections, even if it can be\nexpanded.\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_partition.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for /Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/seed_model.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_unique_key.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_model.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#debugging-workflow","title":"Debugging Workflow","text":"<p>You'll use these commands as needed to validate that your changes are behaving as expected. This is great to get more details beyond the defaults above. The workflow is as follows:</p> <ol> <li>Render the model to verify the SQL is looking as expected.</li> <li>Run Vulcan in verbose mode so you can verify its behavior.</li> <li>View the logs easily in your terminal.</li> </ol>"},{"location":"examples/vulcan_cli_crash_course/#render-your-sql-changes","title":"Render your SQL Changes","text":"<p>This is a great way to verify that your model's SQL is looking as expected before applying the changes. It is especially important if you're migrating from one query engine to another (ex: postgres to databricks).</p> VulcanTobiko Cloud <pre><code>vulcan render vulcan_example.incremental_model\n</code></pre> <pre><code>vulcan render vulcan_example.incremental_model --dialect databricks\n</code></pre> <pre><code>vulcan render &lt;model_name&gt; --dialect &lt;dialect&gt;\n</code></pre> <pre><code>tcloud vulcan render vulcan_example.incremental_model\n</code></pre> <pre><code>tcloud vulcan render vulcan_example.incremental_model --dialect databricks\n</code></pre> <pre><code>tcloud vulcan render &lt;model_name&gt; --dialect &lt;dialect&gt;\n</code></pre> Example Output <p>Model definition:</p> models/incremental_model.sql<pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\n  7 as new_column\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Vulcan returns the full SQL code in the default or target dialect.</p> <pre><code>&gt; vulcan render vulcan_example.incremental_model\n-- rendered sql in default dialect\nSELECT\n  \"seed_model\".\"id\" AS \"id\",\n  \"seed_model\".\"item_id\" AS \"item_id\",\n  \"seed_model\".\"event_date\" AS \"event_date\",\n  7 AS \"new_column\"\nFROM \"db\".\"vulcan__vulcan_example\".\"vulcan_example__seed_model__3294646944\" AS \"seed_model\" /*\ndb.vulcan_example.seed_model */\nWHERE\n  \"seed_model\".\"event_date\" &lt;= CAST('1970-01-01' AS DATE) -- placeholder dates for date macros\n  AND \"seed_model\".\"event_date\" &gt;= CAST('1970-01-01' AS DATE)\n</code></pre> <pre><code>&gt; vulcan render vulcan_example.incremental_model --dialect databricks\n-- rendered sql in databricks dialect\nSELECT\n  `seed_model`.`id` AS `id`,\n  `seed_model`.`item_id` AS `item_id`,\n  `seed_model`.`event_date` AS `event_date`,\n  7 AS `new_column`\nFROM `db`.`vulcan__vulcan_example`.`vulcan_example__seed_model__3294646944` AS `seed_model` /*\ndb.vulcan_example.seed_model */\nWHERE\n  `seed_model`.`event_date` &lt;= CAST('1970-01-01' AS DATE)\n  AND `seed_model`.`event_date` &gt;= CAST('1970-01-01' AS DATE)\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#apply-plan-changes-in-verbose-mode","title":"Apply Plan Changes in Verbose Mode","text":"<p>Verbose mode lets you see detailed operations in the physical and virtual layers. This is useful to see exactly what Vulcan is doing every step. After, you can copy/paste the fully qualified table/view name into your query console to validate the data (if that's your preference).</p> VulcanTobiko Cloud <pre><code>vulcan plan dev -vv\n</code></pre> <pre><code>vulcan plan &lt;environment&gt; -vv\n</code></pre> <pre><code>tcloud vulcan plan dev -vv\n</code></pre> <pre><code>tcloud vulcan plan &lt;environment&gt; -vv\n</code></pre> Example Output <pre><code>&gt; vulcan plan dev -vv\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_partition.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for /Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/seed_model.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_unique_key.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n[WARNING] Linter warnings for\n/Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_model.sql:\n- nomissingaudits: Model `audits` must be configured to test data quality.\n\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 db.vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 db.vulcan_example__dev.full_model\n    \u2514\u2500\u2500 db.vulcan_example__dev.view_model\n\n---\n\n+++\n\n@@ -15,7 +15,7 @@\n\n  id,\n  item_id,\n  event_date,\n-  9 AS new_column\n+  7 AS new_column\nFROM vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n\nDirectly Modified: db.vulcan_example__dev.incremental_model (Breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u251c\u2500\u2500 db.vulcan_example__dev.full_model (Breaking)\n    \u2514\u2500\u2500 db.vulcan_example__dev.view_model (Indirect Breaking)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\ndb.vulcan_example__dev.incremental_model  updated # you'll notice that it's updated vs. promoted because we changed the existing view definition\ndb.vulcan_example__dev.full_model         updated\ndb.vulcan_example__dev.view_model         updated\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#view-logs-easily","title":"View Logs Easily","text":"<p>Each time you perform a Vulcan command, it creates a log file in the <code>logs</code> directory. You can view them by manually navigating to the correct file name with latest timestamp or with this simple shell command.</p> <p>This is useful to see the exact queries that were executed to apply your changes. Admittedly, this is outside of native functionality, but it's a quick and easy way to view logs.</p> <pre><code># install this open source tool that enhances the default `cat` command\n# https://github.com/sharkdp/bat\nbrew install bat # installation command if using homebrew\n</code></pre> <pre><code>bat --theme='ansi' $(ls -t logs/ | head -n 1 | sed 's/^/logs\\//')\n</code></pre> <ul> <li>In simple terms this command works like this: \"Show me the contents of the newest log file in the <code>logs/</code> directory, with nice formatting and syntax highlighting.\u201d</li> <li>press <code>q</code> to quit out of big files in the terminal</li> </ul> Example Output <p>This is the log file for the <code>vulcan plan dev</code> command. If you want to see the log file directly, you can click on the file path in the output to open it in your code editor.</p> <pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      \u2502 File: logs/vulcan_2025_04_18_12_34_35.log\n\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  1   \u2502 2025-04-18 12:34:35,715 - MainThread - vulcan.core.config.connection - INFO - Creating new D\n      \u2502 uckDB adapter for data files: {'db.db'} (connection.py:319)\n  2   \u2502 2025-04-18 12:34:35,951 - MainThread - vulcan.core.console - WARNING - Linter warnings for /\n      \u2502 Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_partition.sql:\n  3   \u2502  - nomissingaudits: Model `audits` must be configured to test data quality. (console.py:1848)\n  4   \u2502 2025-04-18 12:34:35,953 - MainThread - vulcan.core.console - WARNING - Linter warnings for /\n      \u2502 Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/seed_model.sql:\n  5   \u2502  - nomissingaudits: Model `audits` must be configured to test data quality. (console.py:1848)\n  6   \u2502 2025-04-18 12:34:35,953 - MainThread - vulcan.core.console - WARNING - Linter warnings for /\n      \u2502 Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_by_unique_key.sql:\n  7   \u2502  - nomissingaudits: Model `audits` must be configured to test data quality. (console.py:1848)\n  8   \u2502 2025-04-18 12:34:35,953 - MainThread - vulcan.core.console - WARNING - Linter warnings for /\n      \u2502 Users/sung/Desktop/git_repos/vulcan-cli-revamp/models/incremental_model.sql:\n  9   \u2502  - nomissingaudits: Model `audits` must be configured to test data quality. (console.py:1848)\n  10  \u2502 2025-04-18 12:34:35,954 - MainThread - vulcan.core.config.connection - INFO - Using existing\n      \u2502  DuckDB adapter due to overlapping data file: db.db (connection.py:309)\n  11  \u2502 2025-04-18 12:34:37,071 - MainThread - vulcan.core.snapshot.evaluator - INFO - Listing data\n      \u2502 objects in schema db.vulcan__vulcan_example (evaluator.py:338)\n  12  \u2502 2025-04-18 12:34:37,072 - MainThread - vulcan.core.engine_adapter.base - INFO - Executing SQ\n      \u2502 L: SELECT CURRENT_CATALOG() (base.py:2128)\n  13  \u2502 2025-04-18 12:34:37,072 - MainThread - vulcan.core.engine_adapter.base - INFO - Executing SQ\n      \u2502 L: SELECT CURRENT_CATALOG() (base.py:2128)\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#run-on-production-schedule","title":"Run on Production Schedule","text":"<p>Vulcan schedules your transformation on a per-model basis in proper DAG order. This makes it easy to configure how often each step in your pipeline runs to backfill data.</p> <p>Vulcan won't schedule models whose upstream models are late or failed, and they will rerun from point of failure by default!</p> <p>Example scenario and model DAG:</p> <p><code>stg_transactions</code>(cron: <code>@hourly</code>) -&gt; <code>fct_transcations</code>(cron: <code>@daily</code>). All times in UTC.</p> <ol> <li><code>stg_transactions</code> runs hourly</li> <li><code>fct_transcations</code> runs at 12am UTC if <code>stg_transactions</code> is fresh and updated since its most recent hour interval</li> <li>If <code>stg_transactions</code> failed from 11pm-11:59:59pm, it will prevent <code>fct_transcations</code> from running and put it in a <code>pending</code> state</li> <li>If <code>fct_transactions</code> is <code>pending</code> past its full interval (1 full day), it will be put in a <code>late</code> state</li> <li>Once <code>stg_transactions</code> runs successfully either from a retry or a fix from a pull request, <code>fct_transactions</code> will rerun from the point of failure. This is true even if <code>fct_transactions</code> has been <code>late</code> for several days.</li> </ol> <p>Note: <code>pending</code> and <code>late</code> states are only supported in Tobiko Cloud. In Vulcan, it will only understand if the model is ready or not ready to execute without mention of these states.</p> <p>If you're using open source Vulcan, you can run this command in your orchestrator (ex: Dagster, GitHub Actions, etc.) every 5 minutes or at your lowest model cron schedule (ex: every 1 hour). Don't worry! It will only run executions that need to be run.</p> <p>If you're using Tobiko Cloud, this configures automatically without additional configuration.</p>"},{"location":"examples/vulcan_cli_crash_course/#run-models","title":"Run Models","text":"<p>This command is intended be run on a schedule. It will skip the physical and virtual layer updates and simply execute the model batches.</p> VulcanTobiko Cloud <pre><code>vulcan run\n</code></pre> <pre><code>tcloud vulcan run\n</code></pre> Example Output <p>This is what it looks like if models are ready to run.</p> <pre><code>&gt; vulcan run\n[1/1] vulcan_example.incremental_model               [insert 2025-04-17 - 2025-04-17]\n0.01s\n[1/1] vulcan_example.incremental_unique_model        [insert/update rows]\n0.01s\n[1/1] vulcan_example_v3.incremental_partition_model  [insert partitions]\n0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.0% \u2022 2/5 \u2022 0:00:00\nvulcan_example_v3.incremental_partition_model .\n[WARNING] vulcan_example.full_model: 'assert_positive_order_ids' audit error: 2 rows failed. Learn\nmore in logs: /Users/sung/Desktop/git_repos/vulcan-cli-revamp/logs/vulcan_2025_04_18_12_48_35.log\n[1/1] vulcan_example.full_model                      [full refresh, audits \u274c1]\n0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501 80.0% \u2022 4/5 \u2022 0:00:00\nvulcan_example.view_model .\n[WARNING] vulcan_example.view_model: 'assert_positive_order_ids' audit error: 2 rows failed. Learn\nmore in logs: /Users/sung/Desktop/git_repos/vulcan-cli-revamp/logs/vulcan_2025_04_18_12_48_35.log\n[1/1] vulcan_example.view_model                      [recreate view, audits \u27142 \u274c1]\n0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 5/5 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nRun finished for environment 'prod'\n</code></pre> <p>This is what it looks like if no models are ready to run.</p> <pre><code>&gt; vulcan run\nNo models are ready to run. Please wait until a model `cron` interval has elapsed.\n\nNext run will be ready at 2025-04-18 05:00PM PDT (2025-04-19 12:00AM UTC).\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#run-models-with-incomplete-intervals-warning","title":"Run Models with Incomplete Intervals (Warning)","text":"<p>You can run models that execute backfills each time you invoke a <code>run</code>, whether ad hoc or on a schedule.</p> <p>Run Models with Incomplete Intervals</p> <p>This only applies to incremental models that have <code>allow_partials</code> set to <code>true</code>. This is generally not recommended for production environments as you risk shipping incomplete data which will be perceived as broken data.</p> VulcanTobiko Cloud <pre><code>vulcan run --ignore-cron\n</code></pre> <pre><code>tcloud vulcan run --ignore-cron\n</code></pre> Example Output <p>Model definition: </p>models/incremental_model.sql<pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date),\n  audits( UNIQUE_VALUES(columns = (\n      id,\n  )), NOT_NULL(columns = (\n      id,\n      event_date\n  ))),\n  allow_partials true\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\n  16 as new_column\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre><p></p> <pre><code>&gt; vulcan run --ignore-cron\n[1/1] vulcan_example.incremental_model  [insert 2025-04-19 - 2025-04-19, audits \u27142] 0.05s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nRun finished for environment 'prod'\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#forward-only-development-workflow","title":"Forward-Only Development Workflow","text":"<p>This is an advanced workflow and specifically designed for large incremental models (ex: &gt; 200 million rows) that take a long time to run even during development. It solves for:</p> <ul> <li>Transforming data with schema evolution in <code>struct</code> and nested <code>array</code> data types.</li> <li>Retaining history of a calculated column and applying a new calculation to new rows going forward.</li> <li>Retain history of a column with complex conditional <code>CASE WHEN</code> logic and apply new conditions to new rows going forward.</li> </ul> <p>When you modify a forward-only model and apply the plan to <code>prod</code> after the dev workflow, it will NOT backfill historical data. It will only execute model batches for new intervals going forward in time (i.e., only for new rows).</p> <p>If you want to see a full walkthrough, go here.</p> VulcanTobiko Cloud <pre><code>vulcan plan dev --forward-only\n</code></pre> <pre><code>vulcan plan &lt;environment&gt; --forward-only\n</code></pre> <pre><code>tcloud vulcan plan dev --forward-only\n</code></pre> <pre><code>tcloud vulcan plan &lt;environment&gt; --forward-only\n</code></pre> Example Output <ul> <li>I applied a change to a new column</li> <li>It impacts 2 downstream models</li> <li>I enforced a forward-only plan to avoid backfilling historical data for the incremental model (ex: <code>preview</code> language in the CLI output)</li> <li>I previewed the changes in a clone of the incremental impacted (clones will NOT be reused in production) along with the full and view models (these are NOT clones).</li> </ul> <pre><code>&gt; vulcan plan dev\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 vulcan_example__dev.view_model\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n---\n\n+++\n\n@@ -16,7 +16,7 @@\n\n  id,\n  item_id,\n  event_date,\n-  9 AS new_column\n+  10 AS new_column\nFROM vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n\nDirectly Modified: vulcan_example__dev.incremental_model (Forward-only)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u251c\u2500\u2500 vulcan_example__dev.full_model (Forward-only)\n    \u2514\u2500\u2500 vulcan_example__dev.view_model (Forward-only)\nModels needing backfill:\n\u251c\u2500\u2500 vulcan_example__dev.full_model: [full refresh] (preview)\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: [2025-04-17 - 2025-04-17] (preview)\n\u2514\u2500\u2500 vulcan_example__dev.view_model: [recreate view] (preview)\nApply - Preview Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example__dev.incremental_model  [insert 2025-04-17 - 2025-04-17]                0.01s\n[1/1] vulcan_example__dev.full_model         [full refresh, audits \u27141]                       0.01s\n[1/1] vulcan_example__dev.view_model         [recreate view, audits \u27143]                      0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>When the plan is applied to <code>prod</code>, it will only execute model batches for new intervals (new rows). This will NOT re-use <code>preview</code> models (backfilled data) in development.</p> <pre><code>&gt; vulcan plan\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 vulcan_example.view_model\n    \u2514\u2500\u2500 vulcan_example.full_model\n\n---\n\n+++\n\n@@ -9,13 +9,14 @@\n\n    disable_restatement FALSE,\n    on_destructive_change 'ERROR'\n  ),\n-  grains ((id, event_date))\n+  grains ((id, event_date)),\n+  allow_partials TRUE\n)\nSELECT\n  id,\n  item_id,\n  event_date,\n-  7 AS new_column\n+  10 AS new_column\nFROM vulcan_example.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n\nDirectly Modified: vulcan_example.incremental_model (Forward-only)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u251c\u2500\u2500 vulcan_example.full_model (Forward-only)\n    \u2514\u2500\u2500 vulcan_example.view_model (Forward-only)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre>"},{"location":"examples/vulcan_cli_crash_course/#miscellaneous","title":"Miscellaneous","text":"<p>If you notice you have a lot of old development schemas/data, you can clean them up with the following command. This process runs automatically during the <code>vulcan run</code> command. This defaults to deleting data older than 7 days.</p> VulcanTobiko Cloud <pre><code>vulcan janitor\n</code></pre> <pre><code>tcloud vulcan janitor\n</code></pre>"},{"location":"faq/faq/","title":"FAQ","text":""},{"location":"faq/faq/#faq","title":"FAQ","text":""},{"location":"faq/faq/#general","title":"General","text":"What is Vulcan? <p>Vulcan is an open source data transformation framework that brings the best practices of DevOps to data teams. It enables data engineers, scientists, and analysts to efficiently run and deploy data transformations written in SQL or Python.</p> <p>It is created and maintained by Tobiko Data, a company founded by data leaders from Airbnb, Apple, and Netflix.</p> <p>Check out the quickstart guide to see it in action.</p> What is Vulcan used for? <p>Vulcan is used to manage and execute data transformations - the process of converting raw data into a form useful for making business decisions.</p> What problems does Vulcan solve? <p>Problem: organizing, maintaining, and changing data transformation code in SQL or Python</p> <p>Solutions:</p> <ul> <li>Identify dependencies among data transformation models and determine the order in which they should run</li> <li>Run data audits and unit tests to prevent unintended side effects from code changes</li> <li>Implement best practices from the DevOps paradigm, such as development environments and continuous integration/continuous development (CI/CD)</li> <li>Execute transformations written in one SQL dialect on an engine/database that runs a different SQL dialect (SQL transpilation)</li> </ul> <p></p> <p>Problem: understanding a complex set of data transformations</p> <p>Solutions:</p> <ul> <li>Determine and display the flow of data through data transformation models</li> <li>Trace which columns in a table contribute to a column in another table (column-level lineage)</li> </ul> <p></p> <p>Problem: inefficient, unnecessarily expensive data transformations</p> <p>Solutions:</p> <ul> <li>Understand the impacts of a code change on the codebase and underlying data tables without running the code</li> <li>Efficiently deploy code changes by only running the transformations impacted by the changes</li> <li>Safely promote transformations executed in a development environment to production so computations aren\u2019t needlessly re-executed</li> </ul> <p></p> <p>Problem: complex business requirements and data transformations</p> <p>Solutions:</p> <ul> <li>Easily and safely implement incremental data loading</li> <li>Perform complex data transformations or operations with Python models (e.g., machine learning models, geocoding)</li> </ul> <p></p> <p>...and more!</p> What is semantic understanding of SQL? <p>Semantic understanding is the result of analyzing SQL code to determine what it does at a granular level. Vulcan uses the free, open-source Python library SQLGlot to parse the SQL code and build the semantic understanding.</p> <p>Semantic understanding allows Vulcan to do things like transpilation (executing one SQL dialect on an engine running another dialect) and preventing incremental loading queries from duplicating data.</p> Does Vulcan work like Terraform? <p>Vulcan was inspired by Terraform, but its commands are not equivalent.</p> <p>Terraform's \"plan\" approach compares a local configuration to a remote configuration and determines what actions are needed to synchronize the two. Similarly, Vulcan compares the state of local project files (such as SQL models) to an environment and determines the actions needed to synchronize them.</p> <p>However, the commands to create and apply a plan are different. In Terraform, the \"plan\" command generates a plan and saves it to file. The \"apply\" command reads a plan file and applies it.</p> <p>In Vulcan, the <code>vulcan plan</code> command generates a plan, runs any unit tests, and prompts the user to apply the plan. There is no \"apply\" command in Vulcan.</p>"},{"location":"faq/faq/#getting-started","title":"Getting Started","text":"How do I install Vulcan? <p>Vulcan is a Python library. After ensuring you have an appropriate Python runtime, install it with <code>pip</code>.</p> How do I use Vulcan? <p>Vulcan has three interfaces: command line, Jupyter or Databricks notebook, and graphical user interface.</p> <p>The quickstart guide demonstrates an example project in each of the interfaces.</p>"},{"location":"faq/faq/#usage","title":"Usage","text":"Why does Vulcan create schemas? <p>Vulcan creates schemas for two reasons:</p> <ul> <li>Vulcan stores state/metadata information about a project in the <code>vulcan</code> schema. This schema is created in the project's default gateway, or you can specify a different location.</li> <li>Vulcan uses Virtual Data Environments to prevent duplicative computation whenever possible, and stores environment-specific objects in separate schemas by default.</li> </ul> <p>How Virtual Data Environments work:</p> <p>Virtual Data Environments work by maintaining a virtual layer of views that users interact with when building models and a physical layer of tables that stores the actual data.</p> <p>Each Vulcan environment consists of a collection of views. When changes are promoted from one environment to another (e.g., dev to prod), Vulcan determines whether the data in an underlying physical table is equivalent between the environments. If it is, Vulcan simply modifies the environment's view to pull from a different underlying physical table instead of redoing the computations that have already occurred.</p> <p>Vulcan creates schemas for both the physical and virtual layers. The physical layer is stored in a schema named <code>vulcan__[project name]</code>. For example, the quickstart example's physical layer is stored in the <code>vulcan__vulcan_example</code> schema.</p> <p>The virtual layers are stored in one schema per environment. All Vulcan projects contain a <code>prod</code> environment by default - its virtual layer is stored in the project name schema (e.g., <code>vulcan_example</code> for the quickstart). Other environments' virtual layers are stored in schemas of the form <code>[project name]__[environment name]</code>. For example, the quickstart example <code>dev</code> environment's virtual layer is in the <code>vulcan_example__dev</code> schema.</p> <p>The Vulcan janitor automatically deletes unused environment schemas. It determines whether an environment schema should be deleted based on the elapsed time since the <code>vulcan plan [environment name]</code> command was successfully executed for the environment. If that time is greater than the environment time to live (default value of 1 week), the environment schema is deleted.</p> <p>Vulcan's default behavior is appropriate for most deployments, but you can override where Vulcan creates physical tables and views with schema configuration options.</p> What's the difference between a <code>test</code> and an <code>audit</code>? <p>A Vulcan <code>test</code> is analogous to a \"unit test\" in software engineering. It tests code based on known inputs and outputs. In Vulcan, the inputs and outputs are specified in a YAML file, and Vulcan automatically runs them when <code>vulcan plan</code> is executed.</p> <p>Writing YAML is annoying and error-prone, so Vulcan's <code>create_test</code> command allows you to automatically generate YAML test files based on queries of existing data tables.</p> <p>A Vulcan <code>audit</code> validates that transformed data meet some criteria. For example, an <code>audit</code> might verify that a column contains no <code>NULL</code> values or has no duplicated values. Vulcan automatically runs audits when a <code>vulcan plan</code> is executed and the plan is applied or when <code>vulcan run</code> is executed.</p> <p>When the <code>vulcan plan</code> command is executed, Vulcan <code>test</code>s run before any model's code is executed. A Vulcan model's <code>audit</code>s run after the model's code is executed to validate the data output by the model.</p> How does a model know when to run? <p>A Vulcan model determines when to run based on its <code>cron</code> parameter and how much time has elapsed since its previous run.</p> <p>Models are not aware of upstream data updates and do not run based on what has happened in an upstream data source.</p> <p></p> What is the model <code>cron</code> parameter? <p>Vulcan does not fully refresh models every time a project is run. Instead, you specify how frequently each model should run with its <code>cron</code> parameter (defaults to daily).</p> <p>When you execute <code>vulcan run</code>, Vulcan compares each model's <code>cron</code> value to its record of when the model was last run. If enough time has elapsed it will run the model, otherwise it does nothing.</p> <p>For example, consider a model whose <code>cron</code> is daily. The first time you execute <code>vulcan run</code> today the model will run. If you execute <code>vulcan run</code> again, Vulcan will detect that the model has already run today and will not re-run the model.</p> What's the difference between <code>vulcan plan</code> and <code>vulcan run</code>? <p>During project development, there are two things in play: the current state of your project files and the existing states of each environment you have.</p> <p>Vulcan\u2019s <code>plan</code> command is the primary tool for understanding the effects of changes you make to your project. If your project files have changed or are different from the state of an environment, you execute <code>vulcan plan [environment name]</code> to synchronize the environment's state with your project files. <code>vulcan plan</code> will generate a summary of the actions needed to implement the changes, automatically run unit tests, and prompt you to <code>apply</code> the plan and implement the changes.</p> <p>If your project files have not changed, you execute <code>vulcan run</code> to run your project's models and audits.</p> <p><code>vulcan run</code> does not use models, macros, or audits from your local project files. Everything it executes is based on the model, macro, and audit versions currently promoted in the target environment. Those versions are stored in the metadata Vulcan captures about the state of your environment.</p> <p>A sensible approach to executing <code>vulcan run</code> is to use Linux\u2019s <code>cron</code> tool to execute <code>vulcan run</code> on a cadence at least as frequent as your briefest Vulcan model <code>cron</code> parameter. For example, if your most frequent model\u2019s <code>cron</code> is hour, your <code>cron</code> tool should execute <code>vulcan run</code> at least every hour.</p> What are start date and end date for? <p>Vulcan uses the \"intervals\" approach to determine the date ranges that should be included in an incremental by time model query. It divides time into disjoint intervals and tracks which intervals have ever been processed.</p> <p>Start date plays two separate roles in Vulcan. In an incremental model configuration, the <code>start</code> parameter tells Vulcan the first date that should be included in the model's set of time intervals.</p> <p>Start date and end date also play a role as parameters for Vulcan commands like <code>plan</code> and <code>run</code>. In this context, start and end tell Vulcan that only certain time intervals should be included when executing the command. For example, you might process only a few intervals to iterate quickly during development before processing all of time when deploying to production.</p> How do I reprocess data I already transformed? <p>Sometimes you need to reprocess data that has already been loaded and transformed. In Vulcan, you do that with restatement plans.</p> <p>Specify the <code>plan</code> command's <code>--restate-model</code> option and the model name(s) you want to reprocess. Applying the plan will reprocess those models and all models downstream from them. You can use the <code>--start</code> and <code>--end</code> options to limit the reprocessing to a specific date range.</p> How do I reuse an existing table instead of creating a new one? <p>Sometimes a table is too large to completely rebuild for a breaking change, so you need to reuse the existing table. This is done with forward-only plans. Create one by adding the <code>--forward-only</code> option to the <code>plan</code> command: <code>vulcan plan [environment name] --forward-only</code>.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused. All changes made as part of a forward-only plan automatically get a forward-only category assigned to them - they can't be mixed together with regular breaking/non-breaking changes.</p> <p>You can retroactively apply the forward-only plan's changes to existing data in the production environment with <code>plan</code>'s <code>--effective-from</code> option.</p> How can I force a model to run now? <p>Ensure that the model's <code>allow_partials</code> attribute is set to <code>true</code> and execute the <code>run</code> command with the <code>--ignore-cron</code> option: <code>vulcan run --ignore-cron</code>.</p> <p>See the documentation for allow_partials to understand the rationale behind this.</p>"},{"location":"faq/faq/#databasesengines","title":"Databases/Engines","text":"What databases/engines does Vulcan work with? <p>See this page for the list of currently supported engines.</p>"},{"location":"faq/faq/#scheduling","title":"Scheduling","text":"How do I run Vulcan models on a schedule? <p>You can run Vulcan models using the built-in scheduler.</p> <p>The built-in scheduler uses each model's <code>cron</code> parameter to determine when the model should run - see the question about <code>cron</code> above for more information.</p> <p>The built-in scheduler works by executing the command <code>vulcan run</code>. A sensible approach to running on your project on a schedule is to use Linux's <code>cron</code> tool to execute <code>vulcan run</code> on a cadence at least as frequent as your briefest Vulcan model <code>cron</code> parameter. For example, if your most frequent model's <code>cron</code> is hour, the <code>cron</code> tool should execute <code>vulcan run</code> at least every hour.</p> How do I use Vulcan with Airflow? <p>You can integrate Vulcan with Airflow by running <code>vulcan run</code> commands in Airflow tasks. See the scheduling guide for more information.</p> How do I use Vulcan with Dagster? <p>You can integrate Vulcan with Dagster by running <code>vulcan run</code> commands in Dagster ops. See the scheduling guide for more information.</p>"},{"location":"faq/faq/#warnings-and-errors","title":"Warnings and Errors","text":"Why did I get the warning 'Query cannot be optimized due to missing schema(s) for model(s): [...]'? <p>Vulcan uses its knowledge of table schema (column names and data types) to optimize model queries and create column-level lineage. Vulcan does not have schema knowledge for data sources outside the project and will generate this warning when a model selects from one.</p> <p>You can resolve this by creating an <code>external</code> model for each external data source. The <code>vulcan create_external_models</code> command captures schema information for external data sources and stores them in the project's <code>schema.yml</code> file. You can create the file manually instead, if desired.</p> Why did I get the error 'Table \"xxx\" must match the schema's nesting level: 3'? <p>Vulcan throws this error when a model\u2019s name does not include a schema. Model names must be of the form <code>schema.table</code> or <code>catalog.schema.table</code>.</p>"},{"location":"faq/faq/#how-is-this-different-from-dbt","title":"How is this different from dbt?","text":"Terminology differences? <ul> <li>dbt \u201cmaterializations\u201d are analogous to model <code>kinds</code> in Vulcan</li> <li>dbt seeds are a model kind in Vulcan</li> <li>dbt\u2019s \u201ctests\u201d are called <code>audits</code> in Vulcan because they are auditing the contents of data that already exists. Vulcan <code>tests</code> are equivalent to \u201cunit tests\u201d in software engineering - they evaluate the correctness of code based on known inputs and outputs.</li> <li><code>dbt build</code> is analogous to <code>vulcan run</code></li> </ul> Workflow differences? <p>dbt workflow</p> <ul> <li>Configure your project and set up one database connection target for each environment you will use during development</li> <li>Create, configure, and modify models, seeds, tests, and other project components</li> <li>Execute <code>dbt build</code> (or its constituent parts <code>dbt run</code>, <code>dbt seed</code>, etc.) to evaluate and test the project components</li> <li>Execute <code>dbt build</code> (or its constituent parts <code>dbt run</code>, <code>dbt seed</code>, etc.) on a schedule to ingest and transform new data</li> </ul> <p>Vulcan workflow</p> <ul> <li>Configure your project and set up a project database (using DuckDB locally or a database connection)</li> <li>Create, configure, and modify models, audits, tests, and other project components</li> <li>Execute <code>vulcan plan [environment name]</code> to:<ul> <li>Generate a summary of the differences between your project files and the environment and whether each change is <code>breaking</code>. The <code>plan</code> includes a list of the actions needed to implement the changes and automatically runs the project's unit <code>test</code>s.</li> <li>Optionally apply the plan to implement the actions and run the project's <code>audit</code>s.</li> </ul> </li> <li>Execute <code>vulcan run</code> on a schedule to ingest and transform new data</li> </ul> Differences in running models? <p>dbt projects are executed with the commands <code>dbt run</code> (models only) or <code>dbt build</code> (models, tests, snapshots).</p> <p>In Vulcan, the execution depends on whether the project\u2019s contents have been modified since the last execution:</p> <ul> <li>If they have been modified, the <code>vulcan plan</code> command both:<ol> <li>Generates a summary of the actions that will occur to implement the code changes and</li> <li>Prompts the user to \"apply\" the plan and execute those actions.</li> </ol> </li> <li> <p>If they have not been modified, the <code>vulcan run</code> command will evaluate the project models and run the audits. Vulcan determines which project models should be executed based on their <code>cron</code> configuration parameter.</p> <p>For example, if a model\u2019s <code>cron</code> is <code>daily</code> then <code>vulcan run</code> will only execute the model once per day. If you issue <code>vulcan run</code> the first time on a day the model will execute; if you issue <code>vulcan run</code> again nothing will happen because the model shouldn\u2019t be executed again until tomorrow.</p> </li> </ul> Differences in state management? <p>dbt</p> <p>By default, dbt runs/builds are independent and have no knowledge of previous runs/builds. This knowledge is called \u201cstate\u201d (as in \u201cthe state of things\u201d).</p> <p>dbt has the ability to store/maintain state with the <code>state</code> selector method and the <code>defer</code> feature. dbt stores state information in <code>artifacts</code> like the manifest JSON file and reads the files at runtime.</p> <p>The dbt documentation \u201cCaveats to state comparison\u201d page comments on those features: \u201cThe state: selection method is a powerful feature, with a lot of underlying complexity.\u201d</p> <p>Vulcan</p> <p>Vulcan always maintains state about the project structure, contents, and past runs. State information enables powerful Vulcan features like virtual data environments and easy incremental loads.</p> <p>State information is stored by default - you do not need to take any action to maintain or to use it when executing models. As the dbt caveats page says, state information is powerful but complex. Vulcan handles that complexity for you so you don't need to worry about the underlying mechanics.</p> <p>Vulcan stores state information in database tables. By default, it stores this information in the same database/connection where your project models run. You can specify a different database/connection if you would prefer to store state information somewhere else. We recommend using a separate connection for storing state in production deployments.</p> <p>Vulcan adds information to the state tables via transactions, and some databases like BigQuery are not optimized to execute transactions. Changing the state connection to another database like PostgreSQL can alleviate performance issues you may encounter due to state transactions.</p> How do I get column-level lineage for my dbt project? <p>Vulcan can run dbt projects with its dbt adapter. After configuring the dbt project to work with Vulcan, you can view the column-level lineage in the Vulcan browser UI:</p> <p></p> Do I have to run Python models in my SQL engine? <p>No! Vulcan executes Python models wherever Vulcan is running, and there are no restrictions on what they can do as long as they return a Pandas or Spark DataFrame instance.</p> <p>If the data is too large to fit in memory, the model can process it in batches or execute the transformations on an external Spark cluster.</p> How do incremental models determine which dates to ingest? <p>dbt uses the \"most recent record\" approach to determine which dates should be included in an incremental load. It works by querying the existing data for the most recent date it contains, then ingesting all records after that date from the source system in a single query.</p> <p>Vulcan uses the \"intervals\" approach instead. It divides time into disjoint intervals based on a model's <code>cron</code> parameter then records which intervals have ever been processed. It ingests source records from only unprocessed intervals. The intervals approach enables features like loading in batches.</p> How do I run an append only model in Vulcan? <p>Vulcan does not support append-only models as implemented in dbt. You can achieve a similar outcome by defining a time column and using an incremental by time range model or specifying a unique key and using an incremental by unique key model.</p>"},{"location":"faq/faq/#company","title":"Company","text":"How does Tobiko Data make money? <ul> <li>Tobiko Cloud: learn more here</li> <li>Enterprise Github Actions CI/CD App (in development)<ul> <li>Advanced version of open source CI/CD bot</li> </ul> </li> <li>Providing hands-on support for companies' Vulcan projects</li> </ul>"},{"location":"guides/configuration/","title":"Configuration guide","text":""},{"location":"guides/configuration/#configuration-guide","title":"Configuration guide","text":"<p>Vulcan's behavior is determined by three things: a project's files (e.g., models), user actions (e.g., creating a <code>plan</code>), and how Vulcan is configured.</p> <p>This page describes how Vulcan configuration works and discusses the aspects of Vulcan behavior that can be modified via configuration.</p> <p>The configuration reference page contains concise lists of all configuration parameters and their default values.</p>"},{"location":"guides/configuration/#configuration-files","title":"Configuration files","text":"<p>NOTE: Vulcan project configurations have the following two requirements:</p> <ol> <li>A <code>config.yaml</code> or <code>config.py</code> file must be present in the project's folder.</li> <li>That configuration file must contain a default SQL dialect for the project's models in the <code>model_defaults</code> <code>dialect</code> key.</li> </ol> <p>Vulcan configuration parameters can be set as environment variables, in a configuration file in the <code>~/.vulcan</code> folder, and in the configuration file within a project folder.</p> <p>The sources have the following order of precedence:</p> <ol> <li>Environment variable (e.g., <code>VULCAN__MODEL_DEFAULTS__DIALECT</code>). [HIGHEST PRECEDENCE]</li> <li><code>config.yaml</code> or <code>config.py</code> in the <code>~/.vulcan</code> folder.</li> <li><code>config.yaml</code> or <code>config.py</code> in a project folder. [LOWEST PRECEDENCE]</li> </ol> <p>Note</p> <p>To relocate the <code>.vulcan</code> folder, set the <code>VULCAN_HOME</code> environment variable to your preferred directory path.</p>"},{"location":"guides/configuration/#file-type","title":"File type","text":"<p>You can specify a Vulcan configuration in either YAML or Python.</p> <p>YAML configuration is simpler, and we recommend it for most projects. Python configuration is more complex, but it enables functionality that YAML does not support.</p> <p>Because Python configuration files are evaluated by Python when Vulcan reads them, they support dynamic parameters based on the computational environment in which Vulcan is running.</p> <p>For example, Python configuration files enable use of third-party secrets managers for storing passwords and other sensitive information. They also support user-specific parameters such as automatically setting project defaults based on which user account is running Vulcan.</p>"},{"location":"guides/configuration/#yaml","title":"YAML","text":"<p>YAML configuration files consist of configuration keys and values. Strings are not quoted, and some keys are \"dictionaries\" that contain one or more sub-keys.</p> <p>For example, the <code>default_gateway</code> key specifies the default gateway Vulcan should use when executing commands. It takes a single, unquoted gateway name as its value:</p> <pre><code>default_gateway: local\n</code></pre> <p>In contrast, the <code>gateways</code> key takes dictionaries as values, and each gateway dictionary contains one or more connection dictionaries. This example specifies the <code>my_gateway</code> gateway with a Snowflake <code>connection</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>Gateway dictionaries can contain multiple connection dictionaries if different Vulcan components should use different connections (e.g., Vulcan <code>test</code>s should run in a different database than Vulcan <code>plan</code>s). See the gateways section for more information on gateway configuration.</p>"},{"location":"guides/configuration/#python","title":"Python","text":"<p>Python configuration files consist of statements that import Vulcan configuration classes and a configuration specification using those classes.</p> <p>At minimum, a Python configuration file must:</p> <ol> <li>Create an object of the Vulcan <code>Config</code> class named <code>config</code></li> <li>Specify that object's <code>model_defaults</code> argument with a <code>ModelDefaultsConfig()</code> object specifying the default SQL dialect for the project's models</li> </ol> <p>For example, this minimal configuration specifies a default SQL dialect of <code>duckdb</code> and uses the default values for all other configuration parameters:</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n)\n</code></pre> <p>Python configuration files may optionally define additional configuration objects and switch between the configurations when issuing <code>vulcan</code> commands. For example, if a configuration file contained a second configuration object <code>my_second_config</code>, you could create a plan using that config with <code>vulcan --config my_second_config plan</code>.</p> <p>Different <code>Config</code> arguments accept different object types. Some, such as <code>model_defaults</code>, take Vulcan configuration objects. Others, such as <code>default_gateway</code>, take strings or other Python object types like dictionaries.</p> <p>Vulcan's Python configuration components are documented in the <code>vulcan.core.config</code> module's API documentation.</p> <p>The <code>config</code> sub-module API documentation describes the individual classes used for the relevant <code>Config</code> arguments:</p> <ul> <li>Model defaults configuration: <code>ModelDefaultsConfig()</code></li> <li>Gateway configuration: <code>GatewayConfig()</code><ul> <li>Connection configuration (separate classes for each supported database/engine)</li> <li>Scheduler configuration (separate classes for each supported scheduler)</li> </ul> </li> <li>Plan change categorization configuration: <code>CategorizerConfig()</code></li> <li>User configuration: <code>User()</code></li> <li>Notification configuration (separate classes for each notification target)</li> </ul> <p>See the notifications guide for more information about user and notification specification.</p>"},{"location":"guides/configuration/#environment-variables","title":"Environment variables","text":"<p>All software runs within a system environment that stores information as \"environment variables.\"</p> <p>Vulcan can access environment variables during configuration, which enables approaches like storing passwords/secrets outside the configuration file and changing configuration parameters dynamically based on which user is running Vulcan.</p> <p>You can specify environment variables in the configuration file or by storing them in a <code>.env</code> file.</p>"},{"location":"guides/configuration/#env-files","title":".env files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory. This provides a convenient way to manage environment variables without having to set them in your shell.</p> <p>Create a <code>.env</code> file in your project root with key-value pairs:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override specific Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>See the overrides section for a detailed explanation of how these are defined.</p> <p>The rest of the <code>.env</code> file variables can be used in your configuration files with <code>{{ env_var('VARIABLE_NAME') }}</code> syntax in YAML or accessed via <code>os.environ['VARIABLE_NAME']</code> in Python.</p>"},{"location":"guides/configuration/#custom-dot-env-file-location-and-name","title":"Custom dot env file location and name","text":"<p>By default, Vulcan loads <code>.env</code> files from each project directory. However, you can specify a custom path using the <code>--dotenv</code> CLI flag directly when running a command:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag is a global option and must be placed before the subcommand (e.g. <code>plan</code>, <code>run</code>), not after.</p> <p>Alternatively, you can export the <code>VULCAN_DOTENV_PATH</code> environment variable once, to persist a custom path across all subsequent commands in your shell session:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\nvulcan run\n</code></pre> <p>Important considerations: - Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information - Vulcan will only load the <code>.env</code> file if it exists in the project directory (unless a custom path is specified) - When using a custom path, that specific file takes precedence over any <code>.env</code> file in the project directory.</p>"},{"location":"guides/configuration/#configuration-file","title":"Configuration file","text":"<p>This section demonstrates using environment variables in YAML and Python configuration files.</p> <p>The examples specify a Snowflake connection whose password is stored in an environment variable <code>SNOWFLAKE_PW</code>.</p> YAMLPython <p>Specify environment variables in a YAML configuration with the syntax <code>{{ env_var('&lt;ENVIRONMENT VARIABLE NAME&gt;') }}</code>. Note that the environment variable name is contained in single quotes.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: {{ env_var('SNOWFLAKE_PW') }}\n      account: &lt;account&gt;\n</code></pre> <p>Python accesses environment variables via the <code>os</code> library's <code>environ</code> dictionary.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>import os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=os.environ['SNOWFLAKE_PW'],\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#default-target-environment","title":"Default target environment","text":"<p>The Vulcan <code>plan</code> command acts on the <code>prod</code> environment by default (i.e., <code>vulcan plan</code> is equivalent to <code>vulcan plan prod</code>).</p> <p>In some organizations, users never run plans directly against <code>prod</code> - they do all Vulcan work in a development environment unique to them. In a standard Vulcan configuration, this means they need to include their development environment name every time they issue the <code>plan</code> command (e.g., <code>vulcan plan dev_tony</code>).</p> <p>If your organization works like this, it may be convenient to change the <code>plan</code> command's default environment from <code>prod</code> to each user's development environment. That way people can issue <code>vulcan plan</code> without typing the environment name every time.</p> <p>The Vulcan configuration <code>user()</code> function returns the name of the user currently logged in and running Vulcan. It retrieves the username from system environment variables like <code>USER</code> on MacOS/Linux or <code>USERNAME</code> on Windows.</p> <p>Call <code>user()</code> inside Jinja curly braces with the syntax <code>{{ user() }}</code>, which allows you to combine the user name with a prefix or suffix.</p> <p>The example configuration below constructs the environment name by appending the username to the end of the string <code>dev_</code>. If the user running Vulcan is <code>tony</code>, the default target environment when they run Vulcan will be <code>dev_tony</code>. In other words, <code>vulcan plan</code> will be equivalent to <code>vulcan plan dev_tony</code>.</p> YAMLPython <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <pre><code>default_target_environment: dev_{{ user() }}\n</code></pre> <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <p>Retrieve the username with the <code>getpass.getuser()</code> function, and combine it with <code>dev_</code> in a Python f-string.</p> <pre><code>import getpass\nimport os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(),\n        ),\n    },\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre>"},{"location":"guides/configuration/#overrides","title":"Overrides","text":"<p>Environment variables have the highest precedence among configuration methods, as noted above. They will automatically override configuration file specifications if they follow a specific naming structure.</p> <p>The structure is based on the names of the configuration fields, with double underscores <code>__</code> between the field names. The environment variable name must begin with <code>VULCAN__</code>, followed by the YAML field names starting at the root and moving downward in the hierarchy.</p> <p>For example, we can override the password specified in a Snowflake connection. This is the YAML specification contained in our configuration file, which specifies a password <code>dummy_pw</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: dummy_pw\n      account: &lt;account&gt;\n</code></pre> <p>We can override the <code>dummy_pw</code> value with the true password <code>real_pw</code> by creating the environment variable. This example demonstrates creating the variable with the bash <code>export</code> function:</p> <pre><code>$ export VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre> <p>After the initial string <code>VULCAN__</code>, the environment variable name components move down the key hierarchy in the YAML specification: <code>GATEWAYS</code> \u2192 <code>MY_GATEWAY</code> \u2192 <code>CONNECTION</code> \u2192 <code>PASSWORD</code>.</p>"},{"location":"guides/configuration/#configuration-types","title":"Configuration types","text":"<p>A Vulcan project configuration is hierarchical and consists of root level parameters within which other parameters are defined.</p> <p>Conceptually, we can group the root level parameters into the following types. Each type links to its table of parameters in the Vulcan configuration reference page:</p> <ol> <li>Project - configuration options for Vulcan project directories.</li> <li>Environment - configuration options for Vulcan environment creation/promotion, physical table schemas, and view schemas.</li> <li>Gateways - configuration options for how Vulcan should connect to the data warehouse, state backend, and scheduler.</li> <li>Gateway/connection defaults - configuration options for what should happen when gateways or connections are not all explicitly specified.</li> <li>Model defaults - configuration options for what should happen when model-specific configurations are not explicitly specified in a model's file.</li> <li>Debug mode - configuration option for Vulcan to print and log actions and full backtraces.</li> </ol>"},{"location":"guides/configuration/#configuration-details","title":"Configuration details","text":"<p>The rest of this page provides additional detail for some of the configuration options and provides brief examples. Comprehensive lists of configuration options are at the configuration reference page.</p>"},{"location":"guides/configuration/#cache-directory","title":"Cache directory","text":"<p>By default, the Vulcan cache is stored in a <code>.cache</code> directory within your project folder. You can customize the cache location using the <code>cache_dir</code> configuration option:</p> YAMLPython <pre><code># Relative path to project directory\ncache_dir: my_custom_cache\n\n# Absolute path\ncache_dir: /tmp/vulcan_cache\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    cache_dir=\"/tmp/vulcan_cache\",\n)\n</code></pre> <p>The cache directory is automatically created if it doesn't exist. You can clear the cache using the <code>vulcan clean</code> command.</p>"},{"location":"guides/configuration/#tableview-storage-locations","title":"Table/view storage locations","text":"<p>Vulcan creates schemas, physical tables, and views in the data warehouse/engine. Learn more about why and how Vulcan creates schema in the \"Why does Vulcan create schemas?\" FAQ.</p> <p>The default Vulcan behavior described in the FAQ is appropriate for most deployments, but you can override where Vulcan creates physical tables and views with the <code>physical_schema_mapping</code>, <code>environment_suffix_target</code>, and <code>environment_catalog_mapping</code> configuration options.</p> <p>You can also override what the physical tables are called by using the <code>physical_table_naming_convention</code> option.</p> <p>These options are in the environments section of the configuration reference page.</p>"},{"location":"guides/configuration/#physical-table-schemas","title":"Physical table schemas","text":"<p>By default, Vulcan creates physical schemas for a model with a naming convention of <code>vulcan__[model schema]</code>.</p> <p>This can be overridden on a per-schema basis using the <code>physical_schema_mapping</code> option, which removes the <code>vulcan__</code> prefix and uses the regex pattern you provide to map the schemas defined in your model to their corresponding physical schemas.</p> <p>This example configuration overrides the default physical schemas for the <code>my_schema</code> model schema and any model schemas starting with <code>dev</code>:</p> YAMLPython <pre><code>physical_schema_mapping:\n  '^my_schema$': my_new_schema,\n  '^dev.*': development\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_schema_mapping={\n        \"^my_schema$\": \"my_new_schema\",\n        '^dev.*': \"development\"\n    },\n)\n</code></pre> <p>This config causes the following mapping behaviour:</p> Model name Default physical location Resolved physical location <code>my_schema.my_table</code> <code>vulcan__my_schema.table_&lt;fingerprint&gt;</code> <code>my_new_schema.table_&lt;fingerprint&gt;</code> <code>dev_schema.my_table</code> <code>vulcan__dev_schema.table_&lt;fingerprint&gt;</code> <code>development.table_&lt;fingerprint&gt;</code> <code>other.my_table</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <p>This only applies to the physical tables that Vulcan creates - the views are still created in <code>my_schema</code> (prod) or <code>my_schema__&lt;env&gt;</code>.</p>"},{"location":"guides/configuration/#disable-environment-specific-schemas","title":"Disable environment-specific schemas","text":"<p>Vulcan stores <code>prod</code> environment views in the schema in a model's name - for example, the <code>prod</code> views for a model <code>my_schema.users</code> will be located in <code>my_schema</code>.</p> <p>By default, for non-prod environments Vulcan creates a new schema that appends the environment name to the model name's schema. For example, by default the view for a model <code>my_schema.users</code> in a Vulcan environment named <code>dev</code> will be located in the schema <code>my_schema__dev</code> as <code>my_schema__dev.users</code>.</p>"},{"location":"guides/configuration/#show-at-the-table-level-instead","title":"Show at the table level instead","text":"<p>This behavior can be changed to append a suffix at the end of a table/view name instead. Appending the suffix to a table/view name means that non-prod environment views will be created in the same schema as the <code>prod</code> environment. The prod and non-prod views are differentiated by non-prod view names ending with <code>__&lt;env&gt;</code>.</p> <p>For example, if you created a <code>dev</code> environment for a project containing a model named <code>my_schema.users</code>, the model view would be created as <code>my_schema.users__dev</code> instead of the default behavior of <code>my_schema__dev.users</code>.</p> <p>Config example:</p> YAMLPython <pre><code>environment_suffix_target: table\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.TABLE,\n)\n</code></pre> <p>Default behavior</p> <p>The default behavior of appending the suffix to schemas is recommended because it leaves production with a single clean interface for accessing the views. However, if you are deploying Vulcan in an environment with tight restrictions on schema creation then this can be a useful way of reducing the number of schemas Vulcan uses.</p>"},{"location":"guides/configuration/#show-at-the-catalog-level-instead","title":"Show at the catalog level instead","text":"<p>If neither the schema (default) nor the table level are sufficient for your use case, you can indicate the environment at the catalog level instead.</p> <p>This can be useful if you have downstream BI reporting tools and you would like to point them at a development environment to test something out without renaming all the table / schema references within the report query.</p> <p>In order to achieve this, you can configure environment_suffix_target like so:</p> YAMLPython <pre><code>environment_suffix_target: catalog\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.CATALOG,\n)\n</code></pre> <p>Given the example of a model called <code>my_schema.users</code> with a default catalog of <code>warehouse</code> this will cause the following behavior:</p> <ul> <li>For the <code>prod</code> environment, the default catalog as configured in the gateway will be used. So the view will be created at <code>warehouse.my_schema.users</code></li> <li>For any other environment, eg <code>dev</code>, the environment name will be appended to the default catalog. So the view will be created at <code>warehouse__dev.my_schema.users</code></li> <li>If a model is fully qualified with a catalog already, eg <code>finance_mart.my_schema.users</code>, then the environment catalog will be based off the model catalog and not the default catalog. In this example, the view will be created at <code>finance_mart__dev.my_schema.users</code></li> </ul> <p>Caveats</p> <ul> <li>Using <code>environment_suffix_target: catalog</code> only works on engines that support querying across different catalogs. If your engine does not support cross-catalog queries then you will need to use <code>environment_suffix_target: schema</code> or <code>environment_suffix_target: table</code> instead.</li> <li>Automatic catalog creation is not supported on all engines even if they support cross-catalog queries. For engines where it is not supported, the catalogs must be managed externally from Vulcan and exist prior to invoking Vulcan.</li> </ul>"},{"location":"guides/configuration/#physical-table-naming-convention","title":"Physical table naming convention","text":"<p>Out of the box, Vulcan has the following defaults set:</p> <ul> <li><code>environment_suffix_target: schema</code></li> <li><code>physical_table_naming_convention: schema_and_table</code></li> <li>no <code>physical_schema_mapping</code> overrides, so a <code>vulcan__&lt;model schema&gt;</code> physical schema will be created for each model schema</li> </ul> <p>This means that given a catalog of <code>warehouse</code> and a model named <code>finance_mart.transaction_events_over_threshold</code>, Vulcan will create physical tables using the following convention:</p> <pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;schema&gt;__&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.finance_mart__transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre> <p>This deliberately contains some redundancy with the model schema as it's repeated at the physical layer in both the physical schema name as well as the physical table name.</p> <p>This default exists to make the physical table names portable between different configurations. If you were to define a <code>physical_schema_mapping</code> that maps all models to the same physical schema, since the model schema is included in the table name as well, there are no naming conflicts.</p>"},{"location":"guides/configuration/#table-only","title":"Table only","text":"<p>Some engines have object name length limitations which cause them to silently truncate table and view names that exceed this limit. This behaviour breaks Vulcan, so we raise a runtime error if we detect the engine would silently truncate the name of the table we are trying to create.</p> <p>Having redundancy in the physical table names does reduce the number of characters that can be utilised in model names. To increase the number of characters available to model names, you can use <code>physical_table_naming_convention</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: table_only\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.TABLE_ONLY,\n)\n</code></pre> <p>This will cause Vulcan to omit the model schema from the table name and generate physical names that look like (using the above example): </p><pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre><p></p> <p>Notice that the model schema name is no longer part of the physical table name. This allows for slightly longer model names on engines with low identifier length limits, which may be useful for your project.</p> <p>In this configuration, it is your responsibility to ensure that any schema overrides in <code>physical_schema_mapping</code> result in each model schema getting mapped to a unique physical schema.</p> <p>For example, the following configuration will cause data corruption:</p> <pre><code>physical_table_naming_convention: table_only\nphysical_schema_mapping:\n  '.*': vulcan\n</code></pre> <p>This is because every model schema is mapped to the same physical schema but the model schema name is omitted from the physical table name.</p>"},{"location":"guides/configuration/#md5-hash","title":"MD5 hash","text":"<p>If you still need more characters, you can set <code>physical_table_naming_convention: hash_md5</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: hash_md5\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.HASH_MD5,\n)\n</code></pre> <p>This will cause Vulcan generate physical names that are always 45-50 characters in length and look something like:</p> <pre><code># vulcan_md5__&lt;hash of what we would have generated using 'schema_and_table'&gt;\n\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00\n\n# or, for a dev preview\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00__dev\n</code></pre> <p>This has a downside that now it's much more difficult to determine which table corresponds to which model by just looking at the database with a SQL client. However, the table names have a predictable length so there are no longer any surprises with identfiers exceeding the max length at the physical layer.</p>"},{"location":"guides/configuration/#virtual-data-environment-modes","title":"Virtual Data Environment Modes","text":"<p>By default, Virtual Data Environments (VDE) are applied across both development and production environments. This allows Vulcan to reuse physical tables when appropriate, even when promoting from development to production.</p> <p>However, users may prefer their production environment to be non-virtual. The non-exhaustive list of reasons may include:</p> <ul> <li>Integration with third-party tools and platforms, such as data catalogs, may not work well with the virtual view layer that Vulcan imposes by default</li> <li>A desire to rely on time travel features provided by cloud data warehouses such as BigQuery, Snowflake, and Databricks</li> </ul> <p>To mitigate this, Vulcan offers an alternative 'dev-only' mode for using VDE. It can be enabled in the project configuration like so:</p> YAMLPython <pre><code>virtual_environment_mode: dev_only\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    virtual_environment_mode=\"dev_only\",\n)\n</code></pre> <p>'dev-only' mode means that VDE is applied only in development environments. While in production, model tables and views are updated directly and bypass the virtual layer. This also means that physical tables in production will be created using the original, unversioned model names. Users will still benefit from VDE and data reuse across development environments.</p> <p>Please note the following tradeoffs when enabling this mode:</p> <ul> <li>All data inserted in development environments is used only for preview and will not be reused in production</li> <li>Reverting a model to a previous version will be applied going forward and may require an explicit data restatement</li> </ul> <p>Warning</p> <p>Switching the mode for an existing project will result in a complete rebuild of all models in the project. Refer to the Table Migration Guide to migrate existing tables without rebuilding them from scratch.</p>"},{"location":"guides/configuration/#environment-view-catalogs","title":"Environment view catalogs","text":"<p>By default, Vulcan creates an environment view in the same catalog as the physical table the view points to. The physical table's catalog is determined by either the catalog specified in the model name or the default catalog defined in the connection.</p> <p>It can be desirable to create <code>prod</code> and non-prod virtual layer objects in separate catalogs instead. For example, there might be a \"prod\" catalog that contains all <code>prod</code> environment views and a separate \"dev\" catalog that contains all <code>dev</code> environment views.</p> <p>Separate prod and non-prod catalogs can also be useful if you have a CI/CD pipeline that creates environments, like the Vulcan Github Actions CI/CD Bot. You might want to store the CI/CD environment objects in a dedicated catalog since there can be many of them.</p> <p>Virtual layer only</p> <p>Note that the following setting only affects the virtual layer. If you need full segregation by catalog between environments in the physical layer as well, see the Isolated Systems Guide.</p> <p>To configure separate catalogs, provide a mapping from regex patterns to catalog names. Vulcan will compare the name of an environment to the regex patterns; when it finds a match it will store the environment's objects in the corresponding catalog.</p> <p>Vulcan evaluates the regex patterns in the order defined in the configuration; it uses the catalog for the first matching pattern. If no match is found, the catalog defined in the model or the default catalog defined on the connection will be used.</p> <p>Config example:</p> YAMLPython <pre><code>environment_catalog_mapping:\n  '^prod$': prod\n  '^dev.*': dev\n  '^analytics_repo.*': cicd\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_catalog_mapping={\n        '^prod$': 'prod',\n        '^dev.*': 'dev',\n        '^analytics_repo.*': 'cicd',\n    },\n)\n</code></pre> <p>With the example configuration above, Vulcan would evaluate environment names as follows:</p> <ul> <li>If the environment name is <code>prod</code>, the catalog will be <code>prod</code>.</li> <li>If the environment name starts with <code>dev</code>, the catalog will be <code>dev</code>.</li> <li>If the environment name starts with <code>analytics_repo</code>, the catalog will be <code>cicd</code>.</li> </ul> <p>Warning</p> <p>This feature is mutually exclusive with <code>environment_suffix_target: catalog</code> in order to prevent ambiguous mappings from being defined. Attempting to specify both <code>environment_catalog_mapping</code> and <code>environment_suffix_target: catalog</code> will raise an error on project load</p> <p>Note: This feature is only available for engines that support querying across catalogs. At the time of writing, the following engines are NOT supported:</p> <ul> <li>MySQL</li> <li>Postgres</li> <li>GCP Postgres</li> </ul>"},{"location":"guides/configuration/#regex-tips","title":"Regex Tips","text":"<ul> <li>If you are less familiar with regex, you can use a tool like regex101 to help you build your regex patterns.<ul> <li>LLMs, like ChatGPT, can help with generating regex patterns. Make sure to validate the suggestion in regex101.</li> </ul> </li> <li>If you are wanting to do an exact word match then surround it with <code>^</code> and <code>$</code> like in the example above.</li> <li>If you want a catch-all at the end of your mapping, to avoid ever using the model catalog or default catalog, then use <code>.*</code> as the pattern. This will match any environment name that hasn't already been matched.</li> </ul>"},{"location":"guides/configuration/#auto-categorize-model-changes","title":"Auto-categorize model changes","text":"<p>Vulcan compares the current state of project files to an environment when <code>vulcan plan</code> is run. It detects changes to models, which can be classified as breaking or non-breaking.</p> <p>Vulcan can  attempt to automatically categorize the changes it detects. The <code>plan.auto_categorize_changes</code> option determines whether Vulcan should attempt automatic change categorization. This option is in the plan section of the configuration reference page.</p> <p>Supported values:</p> <ul> <li><code>full</code>: Never prompt the user for input, instead fall back to the most conservative category (breaking) if the category can't be determined automatically.</li> <li><code>semi</code>: Prompt the user for input only if the change category can't be determined automatically.</li> <li><code>off</code>: Always prompt the user for input; automatic categorization will not be attempted.</li> </ul> <p>Example showing default values:</p> YAMLPython <pre><code>plan:\n  auto_categorize_changes:\n    external: full\n    python: off\n    sql: full\n    seed: full\n</code></pre> <p>The Python <code>auto_categorize_changes</code> argument takes <code>CategorizerConfig</code> object. That object's arguments take an <code>AutoCategorizationMode</code> enumeration with values of <code>AutoCategorizationMode.FULL</code>, <code>AutoCategorizationMode.SEMI</code>, or <code>AutoCategorizationMode.OFF</code>.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    AutoCategorizationMode,\n    CategorizerConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        auto_categorize_changes=CategorizerConfig(\n            external=AutoCategorizationMode.FULL,\n            python=AutoCategorizationMode.OFF,\n            sql=AutoCategorizationMode.FULL,\n            seed=AutoCategorizationMode.FULL,\n        )\n    ),\n)\n</code></pre>"},{"location":"guides/configuration/#always-comparing-against-production","title":"Always comparing against production","text":"<p>By default, Vulcan compares the current state of project files to the target <code>&lt;env&gt;</code> environment when <code>vulcan plan &lt;env&gt;</code> is run. However, a common expectation is that local changes should always be compared to the production environment.</p> <p>The <code>always_recreate_environment</code> boolean plan option can alter this behavior. When enabled, Vulcan will always attempt to compare against the production environment by recreating the target environment; If <code>prod</code> does not exist, Vulcan will fall back to comparing against the target environment.</p> <p>NOTE:: Upon succesfull plan application, changes are still promoted to the target <code>&lt;env&gt;</code> environment.</p> YAMLPython <pre><code>plan:\n    always_recreate_environment: True\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        always_recreate_environment=True,\n    ),\n)\n</code></pre>"},{"location":"guides/configuration/#change-categorization-example","title":"Change Categorization Example","text":"<p>Consider this scenario with <code>always_recreate_environment</code> enabled:</p> <ol> <li> <p>Initial state in <code>prod</code>: </p><pre><code>MODEL (name vulcan_example.test_model, kind FULL);\nSELECT 1 AS col\n</code></pre><p></p> </li> <li> <p>First (breaking) change in <code>dev</code>: </p><pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL);\nSELECT 2 AS col\n</code></pre><p></p> </li> </ol> Output plan example #1 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n+++\n\n\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n</code></pre> <ol> <li>Second (metadata) change in <code>dev</code>: <pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL, owner 'John Doe');\nSELECT 5 AS col\n</code></pre></li> </ol> Output plan example #2 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n\n+++\n\n@@ -1,8 +1,9 @@\n\nMODEL (\nname vulcan_example.test_model,\n+  owner \"John Doe\",\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n\nDirectly Modified: vulcan_example__dev.test_model (Breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.test_model: [full refresh]\n</code></pre> <p>Even though the second change should have been a metadata change (thus not requiring a backfill), it will still be classified as a breaking change because the comparison is against production instead of the previous development state. This is intentional and may cause additional backfills as more changes are accumulated.</p>"},{"location":"guides/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> configuration defines how Vulcan should connect to the data warehouse, state backend, and scheduler. These options are in the gateway section of the configuration reference page.</p> <p>Each gateway key represents a unique gateway name and configures its connections. Gateway names are case-insensitive - Vulcan automatically normalizes gateway names to lowercase during configuration validation. This means you can use any case in your configuration files (e.g., <code>MyGateway</code>, <code>mygateway</code>, <code>MYGATEWAY</code>) and they will all work correctly.</p> <p>For example, this configures the <code>my_gateway</code> gateway:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      ...\n    state_connection:\n      ...\n    test_connection:\n      ...\n    scheduler:\n      ...\n</code></pre> <p>The Python <code>gateways</code> argument takes a dictionary of gateway names and <code>GatewayConfig</code> objects. A <code>GatewayConfig</code>'s connection-related arguments take an engine-specific connection config object, and the <code>scheduler</code> argument takes a scheduler config object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    ...\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=...,\n            state_connection=...,\n            test_connection=...,\n            scheduler=...,\n        ),\n    }\n)\n</code></pre> <p>Gateways do not need to specify all four components in the example above. The gateway defaults options control what happens if they are not all specified - find more information on gateway defaults below.</p>"},{"location":"guides/configuration/#connections","title":"Connections","text":"<p>The <code>connection</code> configuration controls the data warehouse connection. These options are in the connection section of the configuration reference page.</p> <p>The allowed keys include:</p> <ul> <li>The optional <code>concurrent_tasks</code> key specifies the maximum number of concurrent tasks Vulcan will run. Default value is 4 for engines that support concurrent tasks.</li> <li>Most keys are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</li> </ul> <p>Example snowflake connection configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>A Snowflake connection is specified with a <code>SnowflakeConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#engine-connection-configuration","title":"Engine connection configuration","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Athena</li> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>Fabric</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides/configuration/#state-connection","title":"State connection","text":"<p>Configuration for the state backend connection if different from the data warehouse connection.</p> <p>The data warehouse connection is used to store Vulcan state if the <code>state_connection</code> key is not specified.</p> <p>Unlike data transformations, storing state information requires database transactions. Data warehouses aren\u2019t optimized for executing transactions, and storing state information in them can slow down your project or produce corrupted data due to simultaneous writes to the same table. Therefore, production Vulcan deployments should use a dedicated state connection.</p> <p>Note</p> <p>Using the same connection for data warehouse and state is not recommended for production deployments of Vulcan.</p> <p>The easiest and most reliable way to manage your state connection is for Tobiko Cloud to do it for you. If you'd rather handle it yourself, we list recommended and unsupported state engines below.</p> <p>Recommended state engines for production deployments:</p> <ul> <li>Postgres</li> <li>GCP Postgres</li> </ul> <p>Other state engines with fast and reliable database transactions (less tested than the recommended engines):</p> <ul> <li>DuckDB<ul> <li>With the caveat that it's a single user database so will not scale to production usage</li> </ul> </li> <li>MySQL</li> <li>MSSQL</li> </ul> <p>Unsupported state engines, even for development:</p> <ul> <li>ClickHouse</li> <li>Spark</li> <li>Trino</li> </ul> <p>This example gateway configuration uses Snowflake for the data warehouse connection and Postgres for the state backend connection:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      # snowflake credentials here\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n    state_connection:\n      # postgres credentials here\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n</code></pre> <p>A Postgres connection is specified with a <code>PostgresConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            # snowflake credentials here\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n            # postgres credentials here\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#state-schema-name","title":"State schema name","text":"<p>By default, the schema name used to store state tables is <code>vulcan</code>. This can be changed by providing the <code>state_schema</code> config key in the gateway configuration.</p> <p>Example configuration to store state information in a postgres database's <code>custom_name</code> schema:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    state_connection:\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n    state_schema: custom_name\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n            state_schema=\"custom_name\",\n        ),\n    }\n)\n</code></pre> <p>This would create all state tables in the schema <code>custom_name</code>.</p>"},{"location":"guides/configuration/#test-connection","title":"Test connection","text":"<p>Configuration for a connection used to run unit tests. An in-memory DuckDB database is used if the <code>test_connection</code> key is not specified.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    test_connection:\n      type: duckdb\n</code></pre> <p>A DuckDB connection is specified with a <code>DuckDBConnectionConfig</code> object. A <code>DuckDBConnectionConfig</code> with no arguments specified uses an in-memory DuckDB database.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            test_connection=DuckDBConnectionConfig(),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans. By default, the scheduler type is set to <code>builtin</code>, which uses the existing SQL engine to store metadata.</p> <p>These options are in the scheduler section of the configuration reference page.</p>"},{"location":"guides/configuration/#builtin","title":"Builtin","text":"<p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    scheduler:\n      type: builtin\n</code></pre> <p>A built-in scheduler is specified with a <code>BuiltInSchedulerConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    BuiltInSchedulerConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            scheduler=BuiltInSchedulerConfig(),\n        ),\n    }\n)\n</code></pre> <p>No additional configuration options are supported by this scheduler type.</p>"},{"location":"guides/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. These options are in the gateway/connection defaults section of the configuration reference page.</p> <p>The gateway specified in <code>default_gateway</code> is used when a <code>vulcan</code> command does not explicitly specify a gateway. All Vulcan CLI commands accept a gateway option after <code>vulcan</code> and before the command name; for example, <code>vulcan --gateway my_gateway plan</code>. If the option is not specified in a command call, the <code>default_gateway</code> is used.</p> <p>The three default connection types are used when some gateways in the <code>gateways</code> configuration dictionaries do not specify every connection type.</p>"},{"location":"guides/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> <p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    &lt;gateway specification&gt;\ndefault_gateway: my_gateway\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            &lt;gateway specification&gt;\n        ),\n    },\n    default_gateway=\"my_gateway\",\n)\n</code></pre>"},{"location":"guides/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> <p>Example configuration specifying a Postgres default connection, in-memory DuckDB default test connection, and builtin default scheduler:</p> YAMLPython <pre><code>default_connection:\n  type: postgres\n  host: &lt;host&gt;\n  port: &lt;port&gt;\n  user: &lt;username&gt;\n  password: &lt;password&gt;\n  database: &lt;database&gt;\ndefault_test_connection:\n  type: duckdb\ndefault_scheduler:\n  type: builtin\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PostgresConnectionConfig,\n    DuckDBConnectionConfig,\n    BuiltInSchedulerConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    default_connection=PostgresConnectionConfig(\n        host=&lt;host&gt;,\n        port=&lt;port&gt;,\n        user=&lt;username&gt;,\n        password=&lt;password&gt;,\n        database=&lt;database&gt;,\n    ),\n    default_test_connection=DuckDBConnectionConfig(),\n    default_scheduler=BuiltInSchedulerConfig(),\n)\n</code></pre>"},{"location":"guides/configuration/#models","title":"Models","text":""},{"location":"guides/configuration/#model-defaults","title":"Model defaults","text":"<p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed. Other values are set automatically unless explicitly overridden in the model definition.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference page.</p> <p>Example configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless overridden with the <code>kind</code> key. For more information on model kinds, refer to model concepts page.</p>"},{"location":"guides/configuration/#identifier-resolution","title":"Identifier resolution","text":"<p>When a SQL engine receives a query such as <code>SELECT id FROM \"some_table\"</code>, it eventually needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This process is usually referred to as identifier (or name) resolution.</p> <p>Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.</p> <p>Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it normalizes and quotes all identifiers in those queries, respecting each dialect's resolution rules.</p> <p>The \"normalization strategy\", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.</p> <p>See here to learn more about the supported normalization strategies.</p>"},{"location":"guides/configuration/#gateway-specific-model-defaults","title":"Gateway-specific model defaults","text":"<p>You can also define gateway specific <code>model_defaults</code> in the <code>gateways</code> section, which override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This allows you to tailor the behavior of models for each gateway without affecting the global <code>model_defaults</code>.</p> <p>For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.</p> <p>Gateway-specific <code>model_defaults</code> allow you to change how Vulcan performs identifier normalization by engine to align the different engines' behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway configuration overrides that global default dialect with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>That value tells Vulcan that the <code>redshift</code> gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"guides/configuration/#model-kinds","title":"Model Kinds","text":"<p>Model kinds are required in each model file's <code>MODEL</code> DDL statement. They may optionally be used to specify a default kind in the model defaults configuration key.</p> <p>All model kind specification keys are listed in the models configuration reference page.</p> <p>The <code>VIEW</code>, <code>FULL</code>, and <code>EMBEDDED</code> model kinds are specified by name only, while other models kinds require additional parameters and are provided with an array of parameters:</p> YAML <p><code>FULL</code> model only requires a name:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  kind FULL\n);\n</code></pre> <p><code>INCREMENTAL_BY_TIME_RANGE</code> requires an array specifying the model's <code>time_column</code> (which should be in the UTC time zone):</p> <pre><code>MODEL(\n  name docs_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column model_time_column\n  )\n);\n</code></pre> <p>Python model kinds are specified with model kind objects. Python model kind objects have the same arguments as their SQL counterparts, listed in the models configuration reference page.</p> <p>This example demonstrates how to specify an incremental by time range model kind in Python:</p> Python <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan.core.model.kind import ModelKindName\n\n@model(\n    \"docs_example.incremental_model\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"ds\"\n    )\n)\n</code></pre> <p>Learn more about specifying Python models at the Python models concepts page.</p>"},{"location":"guides/configuration/#model-naming","title":"Model Naming","text":"<p>The <code>model_naming</code> configuration controls if model names are inferred based on the project's directory structure. If <code>model_naming</code> is not defined or <code>infer_names</code> is set to false, the model names must be provided explicitly.</p> <p>With <code>infer_names</code> set to true, model names are inferred based on their path. For example, a model located at <code>models/catalog/schema/model.sql</code> would be named <code>catalog.schema.model</code>. However, if a name is provided in the model definition, it will take precedence over the inferred name.</p> <p>Example enabling name inference:</p> YAMLPython <pre><code>model_naming:\n  infer_names: true\n</code></pre> <pre><code>from vulcan.core.config import Config, NameInferenceConfig\n\nconfig = Config(\n    model_naming=NameInferenceConfig(\n        infer_names=True\n    )\n)\n</code></pre>"},{"location":"guides/configuration/#before_all-and-after_all-statements","title":"Before_all and after_all Statements","text":"<p>The <code>before_all</code> and <code>after_all</code> statements are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands.</p> <p>These statements can be defined in the configuration file under the <code>before_all</code> and <code>after_all</code> keys, either as a list of SQL statements or by using Vulcan macros:</p> YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\nafter_all:\n  - \"@grant_select_privileges()\"\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all = [\n        \"CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\"\n    ],\n    after_all = [\n        \"@grant_select_privileges()\",\n        \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n    ],\n)\n</code></pre>"},{"location":"guides/configuration/#examples","title":"Examples","text":"<p>These statements allow for actions to be executed before all individual model statements or after all have run, respectively. They can also simplify tasks such as granting privileges.</p>"},{"location":"guides/configuration/#example-granting-select-privileges","title":"Example: Granting Select Privileges","text":"<p>For example, rather than using an <code>on_virtual_update</code> statement in each model to grant privileges on the views of the virtual layer, a single macro can be defined and used at the end of the plan:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    if evaluator.views:\n        return [\n            f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE admin_role;\"\n            for view_name in evaluator.views\n        ]\n</code></pre> <p>By including the comment <code>/* sqlglot.meta replace=false */</code>, you further ensure that the evaluator does not replace the view name with the physical table name during rendering.</p>"},{"location":"guides/configuration/#example-granting-schema-privileges","title":"Example: Granting Schema Privileges","text":"<p>Similarly, you can define a macro to grant schema usage privileges and, as demonstrated in the configuration above, using <code>this_env</code> macro conditionally execute it only in the production environment.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    if evaluator.this_env == \"prod\" and evaluator.schemas:\n        return [\n            f\"GRANT USAGE ON SCHEMA {schema} TO admin_role;\"\n            for schema in evaluator.schemas\n        ]\n</code></pre> <p>As demonstrated in these examples, the <code>schemas</code>  and <code>views</code> are available within the macro evaluator for macros invoked within the <code>before_all</code> and <code>after_all</code> statements. Additionally, the macro <code>this_env</code> provides access to the current environment name, which can be helpful for more advanced use cases that require fine-grained control over their behaviour.</p>"},{"location":"guides/configuration/#linting","title":"Linting","text":"<p>Vulcan provides a linter that checks for potential issues in your models' code. Enable it and specify which linting rules to apply in the configuration file's <code>linter</code> key.</p> <p>Learn more about linting configuration in the linting guide.</p>"},{"location":"guides/configuration/#debug-mode","title":"Debug mode","text":"<p>To enable debug mode set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</p> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"guides/configuration/#python-library-dependencies","title":"Python library dependencies","text":"<p>Vulcan enables you to write Python models and macros which depend on third-party libraries. To ensure each run / evaluation uses the same version, you can specify versions in a <code>vulcan-requirements.lock</code> file in the root of your project.</p> <p>The vulcan.lock must be of the format <code>dep==version</code>. Only <code>==</code> is supported.</p> <p>For example:</p> <pre><code>numpy==2.1.2\npandas==2.2.3\n</code></pre> <p>This feature is only available in Tobiko Cloud.</p>"},{"location":"guides/configuration/#excluding-dependencies","title":"Excluding dependencies","text":"<p>You can exclude dependencies by prefixing the dependency with a <code>^</code>. For example:</p> <pre><code>^numpy\npandas==2.2.3\n</code></pre>"},{"location":"guides/connections/","title":"Connections guide","text":""},{"location":"guides/connections/#connections-guide","title":"Connections guide","text":""},{"location":"guides/connections/#overview","title":"Overview","text":"<p>In order to deploy models and to apply changes to them, you must configure a connection to your Data Warehouse and, optionally, connection to the database where the Vulcan state is stored. This can be done in either the <code>config.yaml</code> file in your project folder, or the one in <code>~/.vulcan</code>.</p> <p>Each connection is configured as part of a gateway which has a unique name associated with it. The gateway name can be used to select a specific combination of connection settings  when using the CLI. For example:</p> <pre><code>gateways:\n  local_db:\n    connection:\n      type: duckdb\n</code></pre> <p>Now the defined connection can be selected in the <code>vulcan plan</code> CLI command as follows:</p> <pre><code>vulcan --gateway local_db plan\n</code></pre>"},{"location":"guides/connections/#state-connection","title":"State connection","text":"<p>By default, the data warehouse connection is also used to store the Vulcan state.</p> <p>The state connection can be changed by providing different connection settings in the <code>state_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    state_connection:\n      type: duckdb\n      database: state.db\n</code></pre> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p>"},{"location":"guides/connections/#default-connection","title":"Default connection","text":"<p>Additionally, you can set a default connection by defining its configuration in the <code>default_connection</code> key:</p> <pre><code>default_connection:\n  type: duckdb\n  database: local.db\n</code></pre> <p>This connection configuration will be used if one is not provided in the target gateway.</p>"},{"location":"guides/connections/#test-connection","title":"Test connection","text":"<p>By default, when running tests, Vulcan uses an in-memory DuckDB database connection. You can override this behavior by providing connection settings in the <code>test_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    test_connection:\n      type: duckdb\n      database: test.db\n</code></pre>"},{"location":"guides/connections/#default-test-connection","title":"Default test connection","text":"<p>To configure a default test connection for all gateways use the <code>default_test_connection</code> key:</p> <pre><code>default_test_connection:\n  type: duckdb\n  database: test.db\n</code></pre>"},{"location":"guides/connections/#default-gateway","title":"Default gateway","text":"<p>To change the default gateway used by the CLI when no gateway name is provided, set the desired name in the <code>default_gateway</code> key:</p> <pre><code>default_gateway: local_db\n</code></pre>"},{"location":"guides/connections/#supported-engines","title":"Supported engines","text":"<ul> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides/custom_materializations/","title":"Custom materializations guide","text":""},{"location":"guides/custom_materializations/#custom-materializations-guide","title":"Custom materializations guide","text":"<p>Vulcan supports a variety of model kinds that reflect the most common approaches to evaluating and materializing data transformations.</p> <p>Sometimes, however, a specific use case cannot be addressed with an existing model kind. For scenarios like this, Vulcan allows users to create their own materialization implementation using Python.</p> <p>NOTE: this is an advanced feature and should only be considered if all other approaches have been exhausted. If you're at this decision point, we recommend you reach out to our team in the community slack before investing time building a custom materialization. If an existing model kind can solve your problem, we want to clarify the Vulcan documentation; if an existing kind can almost solve your problem, we want to consider modifying the kind so all Vulcan users can solve the problem as well.</p>"},{"location":"guides/custom_materializations/#background","title":"Background","text":"<p>A Vulcan model kind consists of methods for executing and managing the outputs of data transformations - collectively, these are the kind's \"materialization.\"</p> <p>Some materializations are relatively simple. For example, the SQL FULL model kind completely replaces existing data each time it is run, so its materialization boils down to executing <code>CREATE OR REPLACE [table name] AS [your model query]</code>.</p> <p>The materializations for other kinds, such as INCREMENTAL BY TIME RANGE, require additional logic to process the correct time intervals and replace/insert their results into an existing table.</p> <p>A model kind's materialization may differ based on the SQL engine executing the model. For example, PostgreSQL does not support <code>CREATE OR REPLACE TABLE</code>, so <code>FULL</code> model kinds instead <code>DROP</code> the existing table then <code>CREATE</code> a new table. Vulcan already contains the logic needed to materialize existing model kinds on all supported engines.</p>"},{"location":"guides/custom_materializations/#overview","title":"Overview","text":"<p>Custom materializations are analogous to new model kinds. Users specify them by name in a model definition's <code>MODEL</code> block, and they may accept user-specified arguments.</p> <p>A custom materialization must:</p> <ul> <li>Be written in Python code</li> <li>Be a Python class that inherits the Vulcan <code>CustomMaterialization</code> base class</li> <li>Use or override the <code>insert</code> method from the Vulcan <code>MaterializableStrategy</code> class/subclasses</li> <li>Be loaded or imported by Vulcan at runtime</li> </ul> <p>A custom materialization may:</p> <ul> <li>Use or override methods from the Vulcan <code>MaterializableStrategy</code> class/subclasses</li> <li>Use or override methods from the Vulcan <code>EngineAdapter</code> class/subclasses</li> <li>Execute arbitrary SQL code and fetch results with the engine adapter <code>execute</code> and related methods</li> </ul> <p>A custom materialization may perform arbitrary Python processing with Pandas or other libraries, but in most cases that logic should reside in a Python model instead of the materialization.</p> <p>A Vulcan project will automatically load any custom materializations present in its <code>materializations/</code> directory. Alternatively, the materialization may be bundled into a Python package and installed with standard methods.</p>"},{"location":"guides/custom_materializations/#creating-a-custom-materialization","title":"Creating a custom materialization","text":"<p>Create a new custom materialization by adding a <code>.py</code> file containing the implementation to the <code>materializations/</code> folder in the project directory. Vulcan will automatically import all Python modules in this folder at project load time and register the custom materializations. (Find more information about sharing and packaging custom materializations below.)</p> <p>A custom materialization must be a class that inherits the <code>CustomMaterialization</code> base class and provides an implementation for the <code>insert</code> method.</p> <p>For example, a minimal full-refresh custom materialization might look like the following:</p> <pre><code>from vulcan import CustomMaterialization # required\n\n# argument typing: strongly recommended but optional best practice\nfrom __future__ import annotations\nfrom vulcan import Model\nimport typing as t\nif t.TYPE_CHECKING:\n    from vulcan import QueryOrDF\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str, # \": str\" is optional argument typing\n        query_or_df: QueryOrDF,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre> <p>Let's unpack this materialization:</p> <ul> <li><code>NAME</code> - name of the custom materialization. This name is used to specify the materialization in a model definition <code>MODEL</code> block. If not specified in the custom materialization, the name of the class is used in the <code>MODEL</code> block instead.</li> <li>The <code>insert</code> method has the following arguments:<ul> <li><code>table_name</code> - the name of a target table or view into which the data should be inserted</li> <li><code>query_or_df</code> - a query (of SQLGlot expression type) or DataFrame (Pandas, PySpark, or Snowpark) instance to be inserted</li> <li><code>model</code> - the model definition object used to access model parameters and user-specified materialization arguments</li> <li><code>is_first_insert</code> - whether this is the first insert for the current version of the model (used with batched or multi-step inserts)</li> <li><code>render_kwargs</code> - a dictionary of arguments used to render the model query</li> <li><code>kwargs</code> - additional and future arguments</li> </ul> </li> <li>The <code>self.adapter</code> instance is used to interact with the target engine. It comes with a set of useful high-level APIs like <code>replace_query</code>, <code>columns</code>, and <code>table_exists</code>, but also supports executing arbitrary SQL expressions with its <code>execute</code> method.</li> </ul> <p>You can control how data objects (tables, views, etc.) are created and deleted by overriding the <code>MaterializableStrategy</code> class's <code>create</code> and <code>delete</code> methods:</p> <pre><code>from vulcan import CustomMaterialization # required\n\n# argument typing: strongly recommended but optional best practice\nfrom __future__ import annotations\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    # NAME and `insert` method code here\n    ...\n\n    def create(\n        self,\n        table_name: str,\n        model: Model,\n        is_table_deployable: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Custom table/view creation logic.\n        # Likely uses `self.adapter` methods like `create_table`, `create_view`, or `ctas`.\n\n    def delete(self, name: str, **kwargs: t.Any) -&gt; None:\n        # Custom table/view deletion logic.\n        # Likely uses `self.adapter` methods like `drop_table` or `drop_view`.\n</code></pre>"},{"location":"guides/custom_materializations/#using-a-custom-materialization","title":"Using a custom materialization","text":"<p>Specify the model kind <code>CUSTOM</code> in a model definition <code>MODEL</code> block to use the custom materialization. Specify the <code>NAME</code> from the custom materialization code in the <code>materialization</code> attribute of the <code>CUSTOM</code> kind:</p> <pre><code>MODEL (\n  name my_db.my_model,\n  kind CUSTOM (\n      materialization 'my_custom_full'\n  )\n);\n</code></pre> <p>A custom materialization may accept arguments specified in an array of key-value pairs in the <code>CUSTOM</code> kind's <code>materialization_properties</code> attribute:</p> <pre><code>MODEL (\n  name my_db.my_model,\n  kind CUSTOM (\n    materialization 'my_custom_full',\n    materialization_properties (\n      'config_key' = 'config_value'\n    )\n  )\n);\n</code></pre> <p>The custom materialization implementation accesses the <code>materialization_properties</code> via the <code>model</code> object's <code>custom_materialization_properties</code> dictionary:</p> <pre><code>class CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: QueryOrDF,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        config_value = model.custom_materialization_properties[\"config_key\"]\n        # Proceed with implementing the insertion logic.\n        # Example existing materialization for look and feel: https://github.com/TobikoData/vulcan/blob/main/vulcan/core/snapshot/evaluator.py\n</code></pre>"},{"location":"guides/custom_materializations/#extending-customkind","title":"Extending <code>CustomKind</code>","text":"<p>Warning</p> <p>This is even lower level usage that contains a bunch of extra complexity and relies on knowledge of the Vulcan internals. If you dont need this level of complexity, stick with the method described above.</p> <p>In many cases, the above usage of a custom materialization will suffice.</p> <p>However, you may still want tighter integration with Vulcan's internals:</p> <ul> <li>You may want to validate custom properties are correct before any database connections are made</li> <li>You may want to leverage existing functionality of Vulcan that relies on specific properties being present</li> </ul> <p>In this case, you can provide a subclass of <code>CustomKind</code> for Vulcan to use instead of <code>CustomKind</code> itself. During project load, Vulcan will instantiate your subclass instead of <code>CustomKind</code>.</p> <p>This allows you to run custom validators at load time rather than having to perform extra validation when <code>insert()</code> is invoked on your <code>CustomMaterialization</code>.</p> <p>You can also define standard Python <code>@property</code> methods to \"hoist\" properties declared inside <code>materialization_properties</code> to the top level on your <code>Kind</code> object. This can make using them from within your custom materialization easier.</p> <p>To extend <code>CustomKind</code>, first you define a subclass like so:</p> <pre><code>from typing_extensions import Self\nfrom pydantic import field_validator, ValidationInfo\nfrom vulcan import CustomKind\nfrom vulcan.utils.pydantic import list_of_fields_validator\nfrom vulcan.utils.errors import ConfigError\n\nclass MyCustomKind(CustomKind):\n\n    _primary_key: t.List[exp.Expression]\n\n    @model_validator(mode=\"after\")\n    def _validate_model(self) -&gt; Self:\n        self._primary_key = list_of_fields_validator(\n            self.materialization_properties.get(\"primary_key\"),\n            { \"dialect\": self.dialect }\n        )\n        if not self.primary_key:\n            raise ConfigError(\"primary_key must be specified\")\n        return self\n\n    @property\n    def primary_key(self) -&gt; t.List[exp.Expression]:\n        return self._primary_key\n</code></pre> <p>To use it within a model, we can do something like:</p> <pre><code>MODEL (\n  name my_db.my_model,\n  kind CUSTOM (\n    materialization 'my_custom_full',\n    materialization_properties (\n        primary_key = (col1, col2)\n    )\n  )\n);\n</code></pre> <p>To indicate to Vulcan that it should use the <code>MyCustomKind</code> subclass instead of <code>CustomKind</code>, specify it as a generic type parameter on your custom materialization class like so:</p> <pre><code>class CustomFullMaterialization(CustomMaterialization[MyCustomKind]):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: QueryOrDF,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        assert isinstance(model.kind, MyCustomKind)\n\n        self.adapter.merge(\n            ...,\n            unique_key=model.kind.primary_key\n        )\n</code></pre> <p>When Vulcan loads your custom materialization, it will inspect the Python type signature for generic parameters that are subclasses of <code>CustomKind</code>. If it finds one, it will instantiate your subclass when building <code>model.kind</code> instead of using the default <code>CustomKind</code> class.</p> <p>In this example, this means that:</p> <ul> <li>Validation for <code>primary_key</code> happens at load time instead of evaluation time. So if there is an issue, you can abort early rather than halfway through applying a plan.</li> <li>When your custom materialization is called to load data into tables, <code>model.kind</code> will resolve to your custom kind object so you can access the extra properties you defined without first needing to validate them / coerce them to a usable type.</li> </ul>"},{"location":"guides/custom_materializations/#sharing-custom-materializations","title":"Sharing custom materializations","text":""},{"location":"guides/custom_materializations/#copying-files","title":"Copying files","text":"<p>The simplest (but least robust) way to use a custom materialization in multiple Vulcan projects is for each project to place a copy of the materialization's Python code in its <code>materializations/</code> directory.</p> <p>If you use this approach, we strongly recommend storing the materialization code in a version-controlled repository and creating a reliable method of notifying users when it is updated.</p> <p>This approach may be appropriate for smaller organizations, but it is not robust.</p>"},{"location":"guides/custom_materializations/#python-packaging","title":"Python packaging","text":"<p>A more complex (but robust) way to use a custom materialization in multiple Vulcan projects is to create and publish a Python package containing the implementation.</p> <p>One scenario that requires Python packaging is when a Vulcan project uses Airflow or other external schedulers, and the scheduler cluster does not have the <code>materializations/</code> folder available. The cluster will use standard Python package installation methods to import the custom materialization.</p> <p>Package and expose custom materializations with the setuptools entrypoints mechanism. Once the package is installed, Vulcan will automatically load custom materializations from the entrypoint list.</p> <p>For example, if your custom materialization class is defined in the <code>my_package/my_materialization.py</code> module, you can expose it as an entrypoint in the <code>pyproject.toml</code> file as follows:</p> <pre><code>[project.entry-points.\"vulcan.materializations\"]\nmy_materialization = \"my_package.my_materialization:CustomFullMaterialization\"\n</code></pre> <p>Or in <code>setup.py</code>:</p> <pre><code>setup(\n    ...,\n    entry_points={\n        \"vulcan.materializations\": [\n            \"my_materialization = my_package.my_materialization:CustomFullMaterialization\",\n        ],\n    },\n)\n</code></pre> <p>Refer to the Vulcan Github custom_materializations example for more details on Python packaging.</p>"},{"location":"guides/customizing_vulcan/","title":"Customizing Vulcan","text":""},{"location":"guides/customizing_vulcan/#customizing-vulcan","title":"Customizing Vulcan","text":"<p>Vulcan supports the workflows used by the vast majority of data engineering teams. However, your company may have bespoke processes or tools that require special integration with Vulcan.</p> <p>Fortunately, Vulcan is an open-source Python library, so you can view its underlying code and customize it for your needs.</p> <p>Customization generally involves subclassing Vulcan classes to extend or modify their functionality.</p> <p>Caution</p> <p>Customize Vulcan with extreme caution. Errors may cause Vulcan to produce unexpected results.</p>"},{"location":"guides/customizing_vulcan/#custom-loader","title":"Custom loader","text":"<p>Loading is the process of reading project files and converting their contents into Vulcan's internal Python objects.</p> <p>The loading stage is a convenient place to customize Vulcan behavior because you can access a project's objects after they've been ingested from file but before Vulcan uses them.</p> <p>Vulcan's <code>VulcanLoader</code> class handles the loading process - customize it by subclassing it and overriding its methods.</p> <p>Python configuration only</p> <p>Custom loaders require using the Python configuration format (YAML is not supported).</p>"},{"location":"guides/customizing_vulcan/#modify-every-model","title":"Modify every model","text":"<p>One reason to customize the loading process is to do something to every model. For example, you might want to add a post-statement to every model.</p> <p>The loading process parses all model SQL statements, so new or modified SQL must be parsed by SQLGlot before being passed to a model object.</p> <p>This custom loader example adds a post-statement to every model:</p> config.py<pre><code>from vulcan.core.loader import VulcanLoader\nfrom vulcan.utils import UniqueKeyDict\nfrom vulcan.core.dialect import parse_one\nfrom vulcan.core.config import Config\n\n# New `CustomLoader` class subclasses `VulcanLoader`\nclass CustomLoader(VulcanLoader):\n    # Override VulcanLoader's `_load_models` method to access every model\n    def _load_models(\n        self,\n        macros: \"MacroRegistry\",\n        jinja_macros: \"JinjaMacroRegistry\",\n        gateway: str | None,\n        audits: UniqueKeyDict[str, \"ModelAudit\"],\n        signals: UniqueKeyDict[str, \"signal\"],\n    ) -&gt; UniqueKeyDict[str, \"Model\"]:\n        # Call VulcanLoader's normal `_load_models` method to ingest models from file and parse model SQL\n        models = super()._load_models(macros, jinja_macros, gateway, audits, signals)\n\n        new_models = {}\n        # Loop through the existing model names/objects\n        for model_name, model in models.items():\n            # Create list of existing and new post-statements\n            new_post_statements = [\n                # Existing post-statements from model object\n                *model.post_statements,\n                # New post-statement is raw SQL, so we parse it with SQLGlot's `parse_one` function.\n                # Make sure to specify the SQL dialect if different from the project default.\n                parse_one(f\"VACUUM @this_model\"),\n            ]\n            # Create a copy of the model with the `post_statements_` field updated\n            new_models[model_name] = model.copy(update={\"post_statements_\": new_post_statements})\n\n        return new_models\n\n# Pass the CustomLoader class to the Vulcan configuration object\nconfig = Config(\n    # &lt; your configuration parameters here &gt;,\n    loader=CustomLoader,\n)\n</code></pre>"},{"location":"guides/incremental_time/","title":"Incremental by time guide","text":""},{"location":"guides/incremental_time/#incremental-by-time-guide","title":"Incremental by time guide","text":"<p>Vulcan models are classified by kind. One powerful model kind is \"incremental by time range\" - this guide describes how these models work and demonstrates how to use them.</p> <p>See the models guide to learn more about working with models in general or the model kinds concepts page for an overview of the different model kinds.</p>"},{"location":"guides/incremental_time/#load-the-right-data","title":"Load the right data","text":"<p>The incremental by time approach to data loading is motivated by efficiency. It is based on the principle of only loading a given data row one time.</p> <p>Model kinds such as <code>VIEW</code> or <code>FULL</code> reload the entirety of the source system's data every time they run. In some cases, reloading all the data is not feasible. In other cases, it is an inefficient use of time and computational resources - both of which equate to money your business could spend on something else.</p> <p>Incremental by time models only load new data, drastically decreasing the computational resources required for each model run.</p>"},{"location":"guides/incremental_time/#counting-time","title":"Counting time","text":"<p>Incremental by time models work by first identifying the date range for which data should be read from the source table.</p> <p>One approach to determining the date range bases it on the most recent record timestamp observed in the data. That approach is simple to implement, but it makes three assumptions: the table already exists, there are no temporal gaps in the data, and that the query is able to run in a single pass.</p> <p>Vulcan takes a different approach by using time intervals.</p>"},{"location":"guides/incremental_time/#calculating-intervals","title":"Calculating intervals","text":"<p>The first step to using time intervals is to create the set of all intervals based on the model's start datetime and interval unit. The start datetime specifies when time \"begins\" for the model, and interval unit specifies how finely time should be divided.</p> <p>For example, consider a model with a start datetime of 12am two days ago that we are working with today at 12pm. This is illustrated in Figure 1:</p> <p></p> <p>Figure 1: Illustration of model with start datetime of 12am two days ago that we are working with today at 12pm</p> <p></p> <p>If the model's interval unit was 1 day, the model's set of intervals would have 3 entries:</p> <ul> <li>1 for two days ago</li> <li>1 for yesterday</li> <li>1 for today</li> </ul> <p>Today's interval is not yet complete because it's 12pm right now. This is illustrated in the top panel of Figure 2:</p> <p></p> <p>Figure 2: Illustration of counting intervals over a 60 hour period with interval units of 1 day and 1 hour</p> <p></p> <p>If the model's interval unit was 1 hour instead, its set of time intervals would have 60 entries:</p> <ul> <li>24 for each hour of two days ago</li> <li>24 for each hour of yesterday</li> <li>12 for each hour today from 12am to 12pm</li> </ul> <p>All intervals are complete because it is 12pm (so the 11am interval has ended). This is illustrated in the bottom panel of Figure 2.</p> <p>When we first execute and backfill the bottom model as part of a <code>vulcan plan</code> today at 12pm, Vulcan calculates its set of 60 time intervals and records that all 60 of them were backfilled. It retains this information in the Vulcan state tables for future use.</p> <p>If we <code>vulcan run</code> the model tomorrow at 12pm, Vulcan calculates the set of all intervals as:</p> <ul> <li>24 for two days ago</li> <li>24 for yesterday</li> <li>24 for today</li> <li>12 for tomorrow 12am to 12pm</li> </ul> <p>This gives a total of 84 intervals.</p> <p>It compares this set of 84 to the stored set of 60 that we already backfilled to identify the 24 un-processed intervals from 12pm yesterday to 12pm today. It then processes only those 24 intervals during today's run.</p>"},{"location":"guides/incremental_time/#running-run","title":"Running <code>run</code>","text":"<p>Vulcan has two different commands for processing data. If any model has been changed, <code>vulcan plan</code> is used to apply the change to data in a specific environment. If no models have changed, <code>vulcan run</code> is used to execute the project's models.</p> <p>Data accumulation rates and freshness requirements may differ across models. If <code>vulcan run</code> ran every model whenever the command was executed, all models would be held to the same freshness requirements as the most stringent model. This is inefficient and wastes computational resources (and money).</p> <p>Instead, you specify the <code>cron</code> parameter for each model. <code>vulcan run</code> uses each model's <code>cron</code> to determine whether that model should be executed in a given run.</p> <p>For example, if your most frequent model's <code>cron</code> is hourly, you need to execute the <code>vulcan run</code> command at least hourly (with a tool like Linux's cron). That model will run once every hour, but another model with a <code>cron</code> of daily will only run once per day when 24 hours have elapsed since its previous run.</p>"},{"location":"guides/incremental_time/#scheduling-computations","title":"Scheduling computations","text":"<p>By default, Vulcan processes all intervals that have elapsed since a model's previous run in a single job. If a model's source data is large, you may want to break the computations up into smaller jobs - this is done with the model configuration's <code>batch_size</code> parameter.</p> <p>When <code>batch_size</code> is specified, the total number of intervals to process is divided into batches of size <code>batch_size</code>, and one job is executed for each batch.</p>"},{"location":"guides/incremental_time/#model-time","title":"Model time","text":"<p>Incremental by time models require specification of a time column in their configuration. In addition, their model SQL queries should specify a <code>WHERE</code> clause that filters the data on a time range.</p> <p>This example shows an incremental by time model that could be added to the Vulcan quickstart project:</p> <pre><code>MODEL (\n  name vulcan_example.new_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (model_time_column, '%Y-%m-%d'), -- Time column `model_time_column` with format '%Y-%m-%d'\n  ),\n);\n\nSELECT\n  *\nFROM\n  vulcan_example.incremental_model\nWHERE\n  model_time_column BETWEEN @start_ds and @end_ds -- WHERE clause filters based on time\n</code></pre> <p>The model configuration specifies that the column <code>model_time_column</code> represents the time stamp for each row, and the model query contains a <code>WHERE</code> clause that uses the time column to filter the data.</p> <p>The <code>WHERE</code> clause uses the Vulcan predefined macro variables <code>@start_ds</code> and <code>@end_ds</code> to specify the date range. Vulcan automatically substitutes in the correct dates based on which intervals are being processed in a job.</p> <p>Important</p> <p>The <code>time_column</code> should be in the UTC time zone to ensure correct interaction with Vulcan's scheduler and predefined macro variables.</p> <p>In addition to the query <code>WHERE</code> clause, Vulcan prevents data leakage by automatically wrapping the query in another time-filtering <code>WHERE</code> clause using the time column in the model's configuration.</p> <p>This raises a question: if Vulcan automatically adds a time filtering <code>WHERE</code> clause, why do you need to include one in the query? Because the two filters play different roles:</p> <ul> <li>The model query <code>WHERE</code> clause filters the data read into the model</li> <li>The Vulcan wrapper <code>WHERE</code> clause filters the data output by the model</li> </ul> <p>The model query ensures that only the necessary data is processed by the model, so no resources are wasted. It also adds flexibility - if an upstream model uses a different time column than the model itself, that column can be used in addition to (or in place of) the model's time column in the query <code>WHERE</code> clause.</p> <p>The Vulcan wrapper clause prevents data leakage by ensuring the model does not return records outside the time range. This is a safety mechanism that guards against improperly specified queries.</p> <p>For some queries, the two filters are functionally duplicative, but for others they are not. There is no way for Vulcan to determine whether they are duplicative in any given instance, so the model query should always include a time-filtering <code>WHERE</code> clause.</p>"},{"location":"guides/incremental_time/#forward-only-models","title":"Forward-only models","text":"<p>Every time a model is modified, Vulcan classifies the change as \"breaking\" or \"non-breaking.\"</p> <p>Breaking changes may invalidate data for downstream models, so a new physical table is created and fully refreshed for the changed model and all models downstream of it. Non-breaking changes only affect the changed model, so only its physical table is refreshed.</p> <p>Sometimes a model's data may be so large that it is not feasible to rebuild either its own or its downstream models' physical tables. In those situations a third type of change, \"forward only,\" can be used. The name reflects that the change is only applied \"going forward\" in time.</p>"},{"location":"guides/incremental_time/#specifying-forward-only","title":"Specifying forward-only","text":"<p>Forward-only changes can be specified in two ways. First, a model can be configured as forward-only such that all changes to them are automatically classified as forward-only. This guarantees that the model's physical table will never be fully refreshed.</p> <p>This example configures the model in the previous example to be forward only:</p> <pre><code>MODEL (\n  name vulcan_example.new_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (model_time_column, '%Y-%m-%d'),\n    forward_only true -- All changes will be forward only\n  )\n);\n\nSELECT\n  *\nFROM\n  vulcan_example.incremental_model\nWHERE\n  model_time_column BETWEEN @start_ds and @end_ds\n</code></pre> <p>Alternatively, all the changes contained in a specific plan can be classified as forward-only with a flag: <code>vulcan plan --forward-only</code>. A subsequent plan that did not include the forward-only flag would fully refresh the model's physical table. Learn more about forward-only plans here.</p>"},{"location":"guides/incremental_time/#schema-changes","title":"Schema changes","text":"<p>When Vulcan processes forward-only changes to incremental models, it compares the model's new schema with the existing physical table schema to detect potential data loss or compatibility issues. Vulcan categorizes schema changes into two types:</p>"},{"location":"guides/incremental_time/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Examples include:</p> <ul> <li>Dropping a column from the model</li> <li>Renaming a column</li> <li>Modifying a column data type in a ways that could cause data loss</li> </ul> <p>Whether a specific change is destructive may differ across SQL engines based on their schema evolution capabilities.</p>"},{"location":"guides/incremental_time/#additive-changes","title":"Additive changes","text":"<p>Additive changes are any changes to the table's columns that aren't categorized as destructive. A simple example would be adding a column to a table but another would be changing a column data type to a type that is compatible (ex: INT -&gt; STRING).</p> <p>Vulcan performs schema change detection at plan time based on the model definition. If Vulcan cannot resolve all of a model's column data types at plan time, the check is performed again at run time based on the physical tables underlying the model.</p>"},{"location":"guides/incremental_time/#changes-to-forward-only-models","title":"Changes to forward-only models","text":"<p>Vulcan provides two configuration settings to control how schema changes are handled:</p> <ul> <li><code>on_destructive_change</code> - Controls behavior for destructive schema changes</li> <li><code>on_additive_change</code> - Controls behavior for additive schema changes</li> </ul>"},{"location":"guides/incremental_time/#configuration-options","title":"Configuration options","text":"<p>Both properties support four values:</p> <ul> <li><code>error</code> (default for <code>on_destructive_change</code>): Stop execution and raise an error</li> <li><code>warn</code>: Log a warning but proceed with the change</li> <li><code>allow</code> (default for <code>on_additive_change</code>): Silently proceed with the change</li> <li><code>ignore</code>: Skip the schema change check entirely for this change type</li> </ul> <p>Ignore is Dangerous</p> <p><code>ignore</code> is dangerous since it can result in error or data loss. It likely should never be used but could be useful as an \"escape-hatch\" or a way to workaround unexpected behavior.</p>"},{"location":"guides/incremental_time/#destructive-change-handling","title":"Destructive change handling","text":"<p>The <code>on_destructive_change</code> configuration setting determines what happens when Vulcan detects a destructive change. By default, Vulcan will error so no data is lost.</p> <p>This example configures a model to silently <code>allow</code> destructive changes:</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column model_time_column,\n        forward_only true,\n        on_destructive_change allow\n    ),\n);\n</code></pre>"},{"location":"guides/incremental_time/#additive-change-handling","title":"Additive change handling","text":"<p>The <code>on_additive_change</code> configuration setting determines what happens when Vulcan detects an additive change like adding new columns. By default, Vulcan allows these changes since they don't destroy existing data.</p> <p>This example configures a model to raise an error for additive changes (useful for strict schema control):</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column model_time_column,\n        forward_only true,\n        on_additive_change error\n    ),\n);\n</code></pre>"},{"location":"guides/incremental_time/#combining-both-settings","title":"Combining both settings","text":"<p>You can configure both settings together to have fine-grained control over schema evolution:</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column model_time_column,\n        forward_only true,\n        on_destructive_change warn,  -- Warn but allow destructive changes\n        on_additive_change allow     -- Silently allow new columns\n    ),\n);\n</code></pre>"},{"location":"guides/incremental_time/#model-defaults","title":"Model defaults","text":"<p>Default values for both <code>on_destructive_change</code> and <code>on_additive_change</code> can be set for all incremental models in the model defaults configuration.</p>"},{"location":"guides/incremental_time/#common-use-cases","title":"Common use cases","text":"<p>Here are some common patterns for configuring schema change handling:</p> <p>Strict schema control - Prevent any schema changes: </p><pre><code>MODEL (\n    name vulcan_example.strict_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column event_date,\n        forward_only true,\n        on_destructive_change error,  -- Block destructive changes\n        on_additive_change error      -- Block even new columns\n    ),\n);\n</code></pre><p></p> <p>Permissive development model - Allow all schema changes: </p><pre><code>MODEL (\n    name vulcan_example.dev_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column event_date,\n        forward_only true,\n        on_destructive_change allow,  -- Allow dropping columns\n        on_additive_change allow      -- Allow new columns (`allow` is the default value for this setting, so it can be omitted here)\n    ),\n);\n</code></pre><p></p> <p>Production safety - Allow safe changes, warn about risky ones: </p><pre><code>MODEL (\n    name vulcan_example.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column event_date,\n        forward_only true,\n        on_destructive_change warn,   -- Warn about destructive changes\n        on_additive_change allow      -- Allow new columns (`allow` is the default value for this setting, so it can be omitted here)\n    ),\n);\n</code></pre><p></p>"},{"location":"guides/incremental_time/#changes-in-forward-only-plans","title":"Changes in forward-only plans","text":"<p>The Vulcan <code>plan</code> <code>--forward-only</code> option treats all the plan's model changes as forward-only. When this option is specified, Vulcan will check all modified incremental models for both destructive and additive schema changes, not just models configured with <code>forward_only true</code>.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy:</p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models. Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. Learn more about model selectors here.</p>"},{"location":"guides/isolated_systems/","title":"Isolated systems guide","text":""},{"location":"guides/isolated_systems/#isolated-systems-guide","title":"Isolated systems guide","text":"<p>Vulcan is optimized for use in systems where developers have access to production data.</p> <p>Writing code against partial or unrepresentative data can cause problems because you don't become aware of changes in production data until errors have already occurred.</p> <p>Other data products, such as machine learning models, may depend on the distribution of values in the training data - building them on unrepresentative data may lead to different behavior in production than in development.</p> <p>However, some companies store production and non-production data in different data warehouses that can't talk to one another (\"isolated systems\"). This is usually due to information security concerns, as the non-production warehouse may be accessible to more users and/or have looser security restrictions.</p> <p>This guide explains how to use Vulcan with isolated systems and how isolating systems affects Vulcan's behavior.</p>"},{"location":"guides/isolated_systems/#terminology","title":"Terminology","text":"<p>Isolated systems are sometimes referred to as \"isolated environments,\" but we avoid that term because \"environments\" has a specific meaning in Vulcan.</p> <p>Instead, we will refer to them as isolated systems - the \"production system\" and \"non-production system.\"</p> <p>When we refer to \"environments,\" we are always talking about Vulcan environments - the isolated namespaces created and managed by Vulcan.</p>"},{"location":"guides/isolated_systems/#configuring-vulcan","title":"Configuring Vulcan","text":""},{"location":"guides/isolated_systems/#separate-state-data","title":"Separate state data","text":"<p>Vulcan maintains a record of every model version so it can identify changes when models are updated. Those records are called \"state\" data, as in \"the state of the model at that point in time.\"</p> <p>State data can be stored alongside other data in the primary data warehouse or in a separate database. We recommend using a separate transactional database for projects running on cloud SQL engines.</p> <p>Isolated systems must use a separate state database for each system. The state of models and other objects in the non-production system is not accurate for the production system, and sharing state data will prevent the project from running correctly.</p>"},{"location":"guides/isolated_systems/#multiple-gateways","title":"Multiple gateways","text":"<p>Vulcan database connections are configured with gateways that contain connections and other configuration parameters.</p> <p>A gateway must contain a connection to a SQL engine and may optionally contain a different connection to the database where Vulcan should store its state data.</p> <p>Isolated systems should configure two separate gateways: one for the production system and one for the non-production system.</p> <p>For example, this configuration creates gateways named <code>nonproduction</code> and <code>production</code>. You may omit the <code>state_connection</code> keys if state data will be stored in the gateway's primary connection.</p> <pre><code>gateways:\n  nonproduction:\n    connection:\n      ...[your non-production connection parameters]...\n    state_connection:\n      ...[your non-production state connection parameters]...\n  production:\n    connection:\n      ...[your production connection parameters]...\n    state_connection:\n      ...[your production state connection parameters]...\n</code></pre> <p>Vulcan will use the first gateway in the configuration as the default when executing a command. For example, with the configuration above Vulcan would use the <code>nonproduction</code> gateway when executing the command <code>vulcan plan</code>.</p> <p>Commands can override the default gateway with the <code>--gateway</code> option, such as <code>vulcan --gateway production plan</code>.</p>"},{"location":"guides/isolated_systems/#gateway-specific-schemas","title":"Gateway-specific schemas","text":"<p>We recommend using identical schema and model names in both systems, but in some scenarios that is not possible.</p> <p>Schema and model names may be parameterized by gateway using the predefined <code>@gateway</code> macro variable.</p> <p>This example demonstrates conditioning the model schema name on the current gateway with the Vulcan <code>@IF</code> macro operator. If the gateway is named <code>production</code>, <code>my_model</code>'s schema is <code>prod_schema</code>; otherwise, it is <code>dev_schema</code>.</p> <pre><code>MODEL (\n  name @IF(@gateway = 'production', prod_schema, dev_schema).my_model\n)\n</code></pre> <p>To embed the gateway name directly in the schema name, use the curly brace <code>@{gateway}</code> syntax:</p> <pre><code>MODEL (\n  name @{gateway}_schema.my_model\n)\n</code></pre> <p>Learn more about the curly brace <code>@{}</code> syntax here.</p>"},{"location":"guides/isolated_systems/#workflow","title":"Workflow","text":""},{"location":"guides/isolated_systems/#linking-systems","title":"Linking systems","text":"<p>The point of isolating systems is to prevent sharing of data by limiting network communications between the systems. Given this, how can a Vulcan project be shared between them at all?</p> <p>The Vulcan project files provide the link between the systems. The files should be stored in a mutually accessible location, such as a git repository.</p> <p></p>"},{"location":"guides/isolated_systems/#workflow-with-one-system","title":"Workflow with one system","text":"<p>This section describes workflows for updating Vulcan projects with one system.</p> <p>We assume that a version of the Vulcan project is currently running in production and serves as the starting point for code modifications.</p>"},{"location":"guides/isolated_systems/#basic-workflow","title":"Basic workflow","text":"<p>Use this workflow if your data system does not use CI/CD to implement changes:</p> <ul> <li>Make a change to a model</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in a local environment</li> <li>Run <code>vulcan plan</code> to apply the changes to the <code>prod</code> environment</li> </ul>"},{"location":"guides/isolated_systems/#cicd-workflow","title":"CI/CD workflow","text":"<p>Use this workflow with the Vulcan Github CI/CD bot:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Push the branch to the project repo and make a pull request. The bot will create a development environment for you to preview the changes if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>Learn more about synchronized and desynchronized deployments here.</p>"},{"location":"guides/isolated_systems/#reusing-computations","title":"Reusing computations","text":"<p>Local environment previews are computed on the same data used by the <code>prod</code> environment in these workflows, so applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p>"},{"location":"guides/isolated_systems/#workflow-with-isolated-systems","title":"Workflow with isolated systems","text":"<p>This section describes the workflow with isolated systems.</p> <p>This workflow combines the basic and CI/CD workflows above, where the basic workflow is used in the non-production system and the CI/CD workflow is used in the production system:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in the nonproduction system. You may need to include the nonproduction <code>--gateway</code> option, depending on your project configuration.</li> <li>Push the branch to the project repo and make a pull request. The bot will create an environment to preview the changes in the production system if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>The breaking/non-breaking change classifications in the non-production system will not be available to the production system because the systems do not share Vulcan state data. Therefore, the classifications must occur again in the production system.</p>"},{"location":"guides/isolated_systems/#reusing-computations_1","title":"Reusing computations","text":"<p>In isolated systems, Vulcan's virtual data environments operate normally within each system, but not across systems.</p> <p>In the non-production system, computations will be reused across preview environments. However, the system's data are not representative of the production data and will not be reused by the production system.</p> <p>In the production system, the CI/CD bot will execute the necessary computations when a pull request is submitted if it is configured for synchronized deployment. Merging to main and applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p> <p>This approach enables true blue-green deployment. Deploying to production occurs with no system downtime because virtual updates only require swapping views. If issues are identified after changes have been pushed to production, reverting is quick and painless because it just swaps the views back.</p>"},{"location":"guides/linter/","title":"Linter guide","text":""},{"location":"guides/linter/#linter-guide","title":"Linter guide","text":"<p>Linting is a powerful tool for improving code quality and consistency. It enables you to automatically validate model definition, ensuring they adhere to your team's best practices.</p> <p>When a Vulcan plan is created, each model's code is checked for compliance with a set of rules you choose.</p> <p>Vulcan provides built-in rules, and you can define custom rules. This improves code quality and helps detect issues early in the development cycle when they are simpler to debug.</p>"},{"location":"guides/linter/#rules","title":"Rules","text":"<p>Each linting rule is responsible for identifying a pattern in a model's code.</p> <p>Some rules validate that a pattern is not present, such as not allowing <code>SELECT *</code> in a model's outermost query. Other rules validate that a pattern is present, like ensuring that every model's <code>owner</code> field is specified. We refer to both of these below as \"validating a pattern\".</p> <p>Rules are defined in Python. Each rule is an individual Python class that inherits from Vulcan's <code>Rule</code> base class and defines the logic for validating a pattern.</p> <p>We display a portion of the <code>Rule</code> base class's code below (full source code). Its methods and properties illustrate the most important components of the subclassed rules you define.</p> <p>Each rule class you create has four vital components:</p> <ol> <li>Name: the class's name is used as the rule's name.</li> <li>Description: the class should define a docstring that provides a short explanation of the rule's purpose.</li> <li>Pattern validation logic: the class should define a <code>check_model()</code> method containing the core logic that validates the rule's pattern. The method can access any <code>Model</code> attribute.</li> <li>Rule violation logic: if a rule's pattern is not validated, the rule is \"violated\" and the class should return a <code>RuleViolation</code> object. The <code>RuleViolation</code> object should include the contextual information a user needs to understand and fix the problem.</li> </ol> <pre><code># Class name used as rule's name\nclass Rule:\n    # Docstring provides rule's description\n    \"\"\"The base class for a rule.\"\"\"\n\n    # Pattern validation logic goes in `check_model()` method\n    @abc.abstractmethod\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        \"\"\"The evaluation function that checks for a violation of this rule.\"\"\"\n\n    # Rule violation object returned by `violation()` method\n    def violation(self, violation_msg: t.Optional[str] = None) -&gt; RuleViolation:\n        \"\"\"Return a RuleViolation instance if this rule is violated\"\"\"\n        return RuleViolation(rule=self, violation_msg=violation_msg or self.summary)\n</code></pre>"},{"location":"guides/linter/#built-in-rules","title":"Built-in rules","text":"<p>Vulcan includes a set of predefined rules that check for potential SQL errors or enforce code style.</p> <p>An example of the latter is the <code>NoSelectStar</code> rule, which prohibits a model from using <code>SELECT *</code> in its query's outer-most select statement.</p> <p>Here is code for the built-in <code>NoSelectStar</code> rule class, with the different components annotated:</p> <pre><code># Rule's name is the class name `NoSelectStar`\nclass NoSelectStar(Rule):\n    # Docstring explaining rule\n    \"\"\"Query should not contain SELECT * on its outer most projections, even if it can be expanded.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # If this model does not contain a SQL query, there is nothing to validate\n        if not isinstance(model, SqlModel):\n            return None\n\n        # Use the query's `is_star` property to detect the `SELECT *` pattern.\n        # If present, call the `violation()` method to return a `RuleViolation` object.\n        return self.violation() if model.query.is_star else None\n</code></pre> <p>Here are all of Vulcan's built-in linting rules:</p> Name Check type Explanation <code>ambiguousorinvalidcolumn</code> Correctness Vulcan found duplicate columns or was unable to determine whether a column is duplicated or not <code>invalidselectstarexpansion</code> Correctness The query's top-level selection may be <code>SELECT *</code>, but only if Vulcan can expand the <code>SELECT *</code> into individual columns <code>noselectstar</code> Stylistic The query's top-level selection may not be <code>SELECT *</code>, even if Vulcan can expand the <code>SELECT *</code> into individual columns <code>nomissingaudits</code> Governance Vulcan did not find any <code>audits</code> in the model's configuration to test data quality."},{"location":"guides/linter/#user-defined-rules","title":"User-defined rules","text":"<p>You may define custom rules to implement your team's best practices.</p> <p>For instance, you could ensure all models have an <code>owner</code> by defining the following linting rule:</p> linter/user.py<pre><code>import typing as t\n\nfrom vulcan.core.linter.rule import Rule, RuleViolation\nfrom vulcan.core.model import Model\n\nclass NoMissingOwner(Rule):\n    \"\"\"Model owner should always be specified.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # Rule violated if the model's owner field (`model.owner`) is not specified\n        return self.violation() if not model.owner else None\n</code></pre> <p>Place a rule's code in the project's <code>linter/</code> directory. Vulcan will load all subclasses of <code>Rule</code> from that directory.</p> <p>If the rule is specified in the project's configuration file, Vulcan will run it when: - A plan is created during <code>vulcan plan</code> - The command <code>vulcan lint</code> is ran</p> <p>Vulcan will error if a model violates the rule, informing you which model(s) violated the rule. In this example, <code>full_model.sql</code> violated the <code>NoMissingOwner</code> rule, essentially halting execution:</p> <pre><code>$ vulcan plan\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Or through the standalone command, for faster iterations:</p> <pre><code>$ vulcan lint\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Use <code>vulcan lint --help</code> for more information.</p>"},{"location":"guides/linter/#applying-linting-rules","title":"Applying linting rules","text":"<p>Specify which linting rules a project should apply in the project's configuration file.</p> <p>Rules are specified as lists of rule names under the <code>linter</code> key. Globally enable or disable linting with the <code>enabled</code> key, which is <code>false</code> by default.</p> <p>NOTE: you must set the <code>enabled</code> key to <code>true</code> key to apply the project's linting rules.</p>"},{"location":"guides/linter/#specific-linting-rules","title":"Specific linting rules","text":"<p>This example specifies that the <code>\"ambiguousorinvalidcolumn\"</code> and <code>\"invalidselectstarexpansion\"</code> linting rules should be enforced:</p> YAMLPython <pre><code>linter:\n  enabled: true\n  rules: [\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=[\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n    )\n)\n</code></pre>"},{"location":"guides/linter/#all-linting-rules","title":"All linting rules","text":"<p>Apply every built-in and user-defined rule by specifying <code>\"ALL\"</code> instead of a list of rules:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\"\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=\"all\",\n    )\n)\n</code></pre> <p>If you want to apply all rules except for a few, you can specify <code>\"ALL\"</code> and list the rules to ignore in the <code>ignored_rules</code> key:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\" # apply all built-in and user-defined rules and error if violated\n  ignored_rules: [\"noselectstar\"] # but don't run the `noselectstar` rule\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # apply all built-in and user-defined linting rules and error if violated\n        rules=\"all\",\n         # but don't run the `noselectstar` rule\n        ignored_rules=[\"noselectstar\"]\n    )\n)\n</code></pre>"},{"location":"guides/linter/#exclude-a-model-from-linting","title":"Exclude a model from linting","text":"<p>You can specify that a specific model ignore a linting rule by specifying <code>ignored_rules</code> in its <code>MODEL</code> block.</p> <p>This example specifies that the model <code>docs_example.full_model</code> should not run the <code>invalidselectstarexpansion</code> rule:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  ignored_rules [\"invalidselectstarexpansion\"] # or \"ALL\" to turn off linting completely\n);\n</code></pre>"},{"location":"guides/linter/#rule-violation-behavior","title":"Rule violation behavior","text":"<p>Linting rule violations raise an error by default, preventing the project from running until the violation is addressed.</p> <p>You may specify that a rule's violation should not error and only log a warning by specifying it in the <code>warn_rules</code> key instead of the <code>rules</code> key.</p> YAMLPython <pre><code>linter:\n  enabled: True\n  # error if `ambiguousorinvalidcolumn` rule violated\n  rules: [\"ambiguousorinvalidcolumn\"]\n  # but only warn if \"invalidselectstarexpansion\" is violated\n  warn_rules: [\"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # error if `ambiguousorinvalidcolumn` rule violated\n        rules=[\"ambiguousorinvalidcolumn\"],\n        # but only warn if \"invalidselectstarexpansion\" is violated\n        warn_rules=[\"invalidselectstarexpansion\"],\n    )\n)\n</code></pre> <p>Vulcan will raise an error if the same rule is included in more than one of the <code>rules</code>, <code>warn_rules</code>, and <code>ignored_rules</code> keys since they should be mutually exclusive.</p>"},{"location":"guides/migrations/","title":"Migrations guide","text":""},{"location":"guides/migrations/#migrations-guide","title":"Migrations guide","text":"<p>New versions of Vulcan may be incompatible with the project's stored metadata format. Migrations provide a way to upgrade the project metadata format to operate with the new Vulcan version.</p>"},{"location":"guides/migrations/#detecting-incompatibility","title":"Detecting incompatibility","text":"<p>When issuing a Vulcan command, Vulcan will automatically check for incompatibilities between the installed version of Vulcan and the project's metadata format, prompting what action is required. Vulcan commands will not execute until the action is complete.</p>"},{"location":"guides/migrations/#installed-version-is-newer-than-metadata-format","title":"Installed version is newer than metadata format","text":"<p>In this scenario, the project's metadata format needs to be migrated.</p> <pre><code>&gt; vulcan plan my_dev\nError: Vulcan (local) is using version '2' which is ahead of '1' (remote). Please run a migration ('vulcan migrate' command).\n</code></pre>"},{"location":"guides/migrations/#installed-version-is-older-than-metadata-format","title":"Installed version is older than metadata format","text":"<p>Here, the installed version of Vulcan needs to be upgraded.</p> <pre><code>&gt; vulcan plan my_dev\nVulcanError: Vulcan (local) is using version '1' which is behind '2' (remote). Please upgrade Vulcan.\n</code></pre>"},{"location":"guides/migrations/#how-to-migrate","title":"How to migrate","text":""},{"location":"guides/migrations/#built-in-scheduler-migrations","title":"Built-in Scheduler Migrations","text":"<p>The project metadata can be migrated to the latest metadata format using Vulcan's migrate command.</p> <pre><code>&gt; vulcan migrate\n</code></pre> <p>Migration should be issued manually by a single user and the migration will affect all users of the project.  Migrations should ideally run when no one will be running plan/apply.  Migrations should not be run in parallel.  Due to these constraints, it is better for a person responsible for managing Vulcan to manually issue migrations.  Therefore, it is not recommended to issue migrations from CI/CD pipelines.</p>"},{"location":"guides/model_selection/","title":"Model Selection Guide","text":""},{"location":"guides/model_selection/#model-selection-guide","title":"Model Selection Guide","text":"<p>This guide describes how to select specific models to include in a Vulcan plan, which can be useful when modifying a subset of the models in a Vulcan project.</p> <p>Note: the selector syntax described below is also used for the Vulcan <code>plan</code> <code>--allow-destructive-model</code> and <code>--allow-additive-model</code> selectors and for the <code>table_diff</code> command to diff a selection of models.</p>"},{"location":"guides/model_selection/#background","title":"Background","text":"<p>A Vulcan plan automatically detects changes between the local version of a project and the version deployed in an environment. When applied, the plan backfills the directly modified models and their indirectly modified downstream children. This brings all model data into alignment with the local version of the project.</p> <p>In large Vulcan projects, a single model change may impact many downstream models, such that evaluating it and its affected children takes a significant amount of time. In some situations, a user is blocked by the long run time and can accomplish their task without backfilling all changed models and affected children.</p> <p>Vulcan model selection allows you to filter which direct model changes should be included into a plan. This can be useful when you only need to inspect the results of some of the model changes you have made.</p> <p>Model selections only apply to models that have been directly modified. Selected models' indirectly modified children are always included in the plan, unless you additionally specify which models to backfill (more information below).</p>"},{"location":"guides/model_selection/#syntax","title":"Syntax","text":"<p>Model selections are specified in the CLI <code>vulcan plan</code> argument <code>--select-model</code>. Selections may be specified in a number of ways.</p> <p>The simplest selection is a single model name (e.g., <code>example.incremental_model</code>). The <code>--select-model</code> argument may be repeated to specify multiple individual model names:</p> <pre><code>vulcan plan --select-model \"example.incremental_model\" --select-model \"example.full_model\"\n</code></pre> <p>A selection may use the wildcard asterisk character <code>*</code> to select multiple models at once. Any model name matching the non-wildcard characters will match. For example:</p> <ul> <li><code>\"example.seed*\"</code> would match both <code>example.seed_cities</code> and <code>example.seed_states</code></li> <li><code>\"example.*l_model\"</code> would match both <code>example.incremental_model</code> and <code>example.full_model</code></li> </ul> <p>Multiple models can also be selected by using the tag selector syntax <code>tag:tag_name</code>. For example, <code>\"tag:my_tag\"</code> would select all models with the tag <code>my_tag</code>.</p> <p>Assuming all seed models had a \"seed\" tag and all incremental models had an \"incremental\" tag:</p> <ul> <li><code>\"tag:seed\"</code> would match all seed models</li> <li><code>\"tag:incremental\"</code> would match all incremental models</li> </ul> <p>Wildcards also apply to tags. For example, <code>\"tag:reporting*\"</code> would match all models that have a tag starting with \"reporting\".</p>"},{"location":"guides/model_selection/#upstreamdownstream-indicator","title":"Upstream/downstream indicator","text":"<p>By default, only the directly changed models in a selection are included in the plan.</p> <p>All of a model's changed upstream and/or downstream models may be included in a selection with the plus sign <code>+</code>. A plus sign at the beginning of a selection includes changed upstream models, and a plus sign at the end of a selection includes downstream models.</p> <p>For example, consider a three model project with the following structure, where all three models have been changed:</p> <p><code>example.seed_model</code> \u2192 <code>example.incremental_model</code> \u2192 <code>example.full_model</code></p> <p>These selections would include different sets of models in the plan:</p> <ul> <li><code>--select-model \"example.incremental_model\"</code> = <code>incremental_model</code> only</li> <li><code>--select-model \"+example.incremental_model\"</code> = <code>incremental_model</code> and upstream <code>seed_model</code></li> <li><code>--select-model \"example.incremental_model+\"</code> = <code>incremental_model</code> and downstream <code>full_model</code></li> </ul> <p>The upstream/downstream indicator may be combined with the wildcard operator. For example, <code>--select-model \"+example.*l_model\"</code> would include all three models in the project:</p> <ul> <li><code>example.incremental_model</code> matches the wildcard</li> <li><code>example.seed_model</code> is upstream of the incremental model</li> <li><code>example.full_model</code> matches the wildcard</li> </ul> <p>The combination of the upstream/downstream indicator, wildcards, and multiple <code>--select-model</code> arguments enables granular and complex model selections for a plan.</p> <p>Upstream/downstream indicators also apply to tags. For example, <code>--select-model \"+tag:reporting*\"</code> would select all models with tags that start with <code>reporting</code> and their upstream models.</p>"},{"location":"guides/model_selection/#backfill","title":"Backfill","text":"<p>By default, Vulcan backfills all of a plan's directly and indirectly modified models. In large projects, a single model change may impact many downstream models, such that backfilling all the children takes a significant amount of time.</p> <p>You can limit which downstream models are backfilled with <code>plan</code>'s <code>--backfill-model</code> argument, which uses the same selection syntax as <code>--select-model</code>.</p> <p><code>--select-model</code> determines which directly modified models are included in a <code>plan</code>, and <code>--backfill-model</code> determines which models are backfilled by the <code>plan</code>. A model's backfilled data is only current if its parents have also been backfilled, so the parents of each model specified with <code>--backfill-model</code> will also be backfilled.</p> <p>Care is required if both of the <code>--select-model</code> and <code>--backfill-model</code> options are specified because a single model can be affected by both options. For example, consider a model <code>test_model</code>. We have two versions of the model: a new directly modified version (\"test_model modified\") and the existing version already active in an environment (\"test_model existing\"). If <code>test_model</code> is not selected by <code>--select-model</code>, the directly modified version \"test_model modified\" is excluded from the plan. However, if <code>test_model</code> is upstream of a <code>--backfill-model</code> model, the existing version \"test_model existing\" will be backfilled if it has any unprocessed intervals.</p> <p>NOTE: the <code>--backfill-model</code> argument can only be used in development environments (i.e., environments other than <code>prod</code>).</p>"},{"location":"guides/model_selection/#examples","title":"Examples","text":"<p>We now demonstrate the use of <code>--select-model</code> and <code>--backfill-model</code> with the Vulcan <code>sushi</code> example project, available in the <code>examples/sushi</code> directory of the Vulcan Github repository.</p>"},{"location":"guides/model_selection/#sushi","title":"sushi","text":"<p>The sushi project generates and transforms data collected at a sushi restaurant. In this guide, we focus on a set of the project's models related to marketing and customers.</p> <p>The DAG of those models displays the primary set we will use inside the red shape:</p> <p></p> <p>The root of our sub-DAG is <code>items</code> at the bottom. Immediately downstream of it are <code>order_items</code>, <code>waiter_revenue_by_day</code>, <code>customer_revenue_lifetime</code>, and <code>customer_revenue_by_day</code>. Finally, <code>top_waiters</code> is downstream of <code>waiter_revenue_by_day</code>.</p> <p>To prepare for the examples, we have run an initial plan in <code>prod</code> and completed the backfill. We have modified the <code>sushi.items</code> and <code>sushi.order_items</code> models to demonstrate how model selection impacts plans.</p>"},{"location":"guides/model_selection/#selection-examples","title":"Selection examples","text":""},{"location":"guides/model_selection/#no-selection","title":"No selection","text":"<p>If we run a <code>plan</code> without selecting specific models, Vulcan includes the two directly modified models and the four indirectly modified models downstream of <code>sushi.order_items</code>:</p> <pre><code>\u276f vulcan plan dev\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sushi.order_items\n\u2502   \u2514\u2500\u2500 sushi.items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u251c\u2500\u2500 sushi.customer_revenue_by_day\n    \u251c\u2500\u2500 sushi.customer_revenue_lifetime\n    \u2514\u2500\u2500 sushi.top_waiters\n</code></pre>"},{"location":"guides/model_selection/#select-order_items","title":"Select <code>order_items</code>","text":"<p>If we specify the <code>--select-model</code> option to select <code>\"sushi.order_items\"</code>, the directly modified <code>sushi.items</code> model is no longer included in the plan:</p> <pre><code>\u276f vulcan plan dev --select-model \"sushi.order_items\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sushi.order_items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u251c\u2500\u2500 sushi.customer_revenue_lifetime\n    \u251c\u2500\u2500 sushi.customer_revenue_by_day\n    \u2514\u2500\u2500 sushi.top_waiters\n</code></pre>"},{"location":"guides/model_selection/#select-order_items_1","title":"Select <code>+order_items</code>","text":"<p>If we specify the <code>--select-model</code> option with the upstream <code>+</code> to select <code>\"+sushi.order_items\"</code>, the <code>sushi.items</code> model is selected because it is upstream of <code>sushi.order_items</code>:</p> <pre><code>\u276f vulcan plan dev --select-model \"+sushi.order_items\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sushi.items\n\u2502   \u2514\u2500\u2500 sushi.order_items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.top_waiters\n    \u251c\u2500\u2500 sushi.customer_revenue_lifetime\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u2514\u2500\u2500 sushi.customer_revenue_by_day\n</code></pre>"},{"location":"guides/model_selection/#select-items","title":"Select <code>items</code>","text":"<p>If we specify the <code>--select-model</code> option to select <code>\"sushi.items\"</code>, Vulcan does not select <code>sushi.order_items</code> (so it is not classified as directly modified).</p> <p>However, it does classify <code>sushi.order_items</code> as indirectly modified. Its direct modification is excluded by the model selection, but it is indirectly modified by being downstream of the selected <code>sushi.items</code> model:</p> <pre><code>\u276f vulcan plan dev --select-model \"sushi.items\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sushi.items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.order_items\n    \u251c\u2500\u2500 sushi.customer_revenue_by_day\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u251c\u2500\u2500 sushi.customer_revenue_lifetime\n    \u2514\u2500\u2500 sushi.top_waiters\n</code></pre>"},{"location":"guides/model_selection/#select-items_1","title":"Select <code>items+</code>","text":"<p>If we specify the <code>--select-model</code> option with the downstream <code>+</code> to select <code>\"sushi.items+\"</code>, the <code>sushi.order_items</code> model is selected and classified as directly modified because it is downstream of <code>sushi.items</code>:</p> <pre><code>\u276f vulcan plan dev --select-model \"sushi.items+\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sushi.items\n\u2502   \u2514\u2500\u2500 sushi.order_items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u251c\u2500\u2500 sushi.customer_revenue_lifetime\n    \u251c\u2500\u2500 sushi.customer_revenue_by_day\n    \u2514\u2500\u2500 sushi.top_waiters\n</code></pre>"},{"location":"guides/model_selection/#select-items_2","title":"Select <code>*items</code>","text":"<p>If we specify the <code>--select-model</code> option with the wildcard <code>*</code> to select <code>\"sushi.*items\"</code>, both <code>sushi.items</code> and <code>sushi.order_items</code> are selected because they match the wildcard:</p> <pre><code>\u276f vulcan plan dev --select-model \"sushi.*items\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sushi.order_items\n\u2502   \u2514\u2500\u2500 sushi.items\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.waiter_revenue_by_day\n    \u251c\u2500\u2500 sushi.top_waiters\n    \u251c\u2500\u2500 sushi.customer_revenue_by_day\n    \u2514\u2500\u2500 sushi.customer_revenue_lifetime\n</code></pre>"},{"location":"guides/model_selection/#select-with-tags","title":"Select with tags","text":"<p>If we specify the <code>--select-model</code> option with a tag selector like <code>\"tag:reporting\"</code>, all models with the \"reporting\" tag will be selected. Tags are case-insensitive and support wildcards:</p> <pre><code>\u276f vulcan plan dev --select-model \"tag:reporting*\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sushi.daily_revenue\n\u2502   \u2514\u2500\u2500 sushi.monthly_revenue\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sushi.revenue_dashboard\n</code></pre>"},{"location":"guides/model_selection/#select-with-git-changes","title":"Select with git changes","text":"<p>The git-based selector allows you to select models whose files have changed compared to a target branch (default: main). This includes: - Untracked files (new files not in git) - Uncommitted changes in working directory - Committed changes different from the target branch</p> <p>For example:</p> <pre><code>\u276f vulcan plan dev --select-model \"git:feature\"\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sushi.items # Changed in feature branch\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sushi.order_items\n    \u2514\u2500\u2500 sushi.daily_revenue\n</code></pre> <p>You can also combine git selection with upstream/downstream indicators:</p> <pre><code>\u276f vulcan plan dev --select-model \"git:feature+\"\n# Selects changed models and their downstream dependencies\n\n\u276f vulcan plan dev --select-model \"+git:feature\"\n# Selects changed models and their upstream dependencies\n</code></pre>"},{"location":"guides/model_selection/#complex-selections-with-logical-operators","title":"Complex selections with logical operators","text":"<p>The model selector supports combining multiple conditions using logical operators:</p> <ul> <li><code>&amp;</code> (AND): Both conditions must be true</li> <li><code>|</code> (OR): Either condition must be true</li> <li><code>^</code> (NOT): Negates a condition</li> </ul> <p>For example:</p> <pre><code>\u276f vulcan plan dev --select-model \"(tag:finance &amp; ^tag:deprecated)\"\n# Selects models with finance tag that don't have deprecated tag\n\n\u276f vulcan plan dev --select-model \"(+model_a | model_b+)\"\n# Selects model_a and its upstream deps OR model_b and its downstream deps\n\n\u276f vulcan plan dev --select-model \"(tag:finance &amp; git:main)\"\n# Selects changed models that also have the finance tag\n\n\u276f vulcan plan dev --select-model \"^(tag:test) &amp; metrics.*\"\n# Selects models in metrics schema that don't have the test tag\n</code></pre>"},{"location":"guides/model_selection/#backfill-examples","title":"Backfill examples","text":""},{"location":"guides/model_selection/#no-backfill-selection","title":"No backfill selection","text":"<p>Recall that a plan with no selection or backfill options includes all four models, two of which were directly and two of which were indirectly modified.</p> <p>The <code>--backfill-model</code> option does not affect whether a model is included in a plan (i.e., it will still appear in the output shown in the selection examples above). Instead, it determines whether a model is included in the list of models needing backfill (shown at the bottom of the plan's output).</p> <p>With no options specified, the <code>plan</code> will backfill all six models. The backfills occur in the <code>sushi__dev</code> schema because we are creating a plan for the <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n\n&lt; output omitted&gt;\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.items: 2023-12-01 - 2023-12-07\n\u251c\u2500\u2500 sushi__dev.order_items: 2023-12-01 - 2023-12-07\n\u251c\u2500\u2500 sushi__dev.customer_revenue_by_day: 2023-12-01 - 2023-12-07\n\u251c\u2500\u2500 sushi__dev.customer_revenue_lifetime: 2023-12-01 - 2023-12-07\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-01 - 2023-12-07\n\u2514\u2500\u2500 sushi__dev.top_waiters: 2023-12-01 - 2023-12-07\n</code></pre>"},{"location":"guides/model_selection/#backfill-sushiwaiter_revenue_by_day","title":"Backfill <code>sushi.waiter_revenue_by_day</code>","text":"<p>If we specify the <code>--backfill-model</code> option with <code>\"sushi.waiter_revenue_by_day\"</code>, there are fewer models in the backfills list.</p> <p>The <code>sushi__dev.customer_revenue_by_day</code>, <code>sushi__dev.customer_revenue_lifetime</code>, and <code>sushi__dev.top_waiters</code> models are excluded because they are not upstream of <code>sushi.waiter_revenue_by_day</code>.</p> <p>The <code>sushi__dev.items</code> and <code>sushi__dev.order_items</code> models are still included because they are upstream of <code>sushi.waiter_revenue_by_day</code>.</p> <p>Models upstream of those selected in the <code>--backfill-model</code> expression are always included, regardless of whether the expression contains a leading <code>+</code> sign.</p> <pre><code>\u276f vulcan plan dev --backfill-model \"sushi.waiter_revenue_by_day\"\n\n&lt; output omitted&gt;\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.items: 2023-12-04 - 2023-12-10\n\u251c\u2500\u2500 sushi__dev.order_items: 2023-12-04 - 2023-12-10\n\u2514\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-04 - 2023-12-10\n</code></pre>"},{"location":"guides/models/","title":"Models guide","text":""},{"location":"guides/models/#models-guide","title":"Models guide","text":""},{"location":"guides/models/#prerequisites","title":"Prerequisites","text":"<p>Before adding a model, ensure that you have already created your project and that you are working in a dev environment.</p>"},{"location":"guides/models/#adding-a-model","title":"Adding a model","text":"<p>To add a model:</p> <ol> <li>Within your <code>models</code> folder, create a new file. For example, we might add <code>new_model.sql</code> to the quickstart project.</li> <li> <p>Within the file, define a model. For example:</p> <pre><code>MODEL (\n  name vulcan_example.new_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (model_time_column, '%Y-%m-%d'),\n  ),\n);\n\nSELECT *\nFROM vulcan_example.incremental_model\nWHERE model_time_column BETWEEN @start_ds and @end_ds\n</code></pre> <p>Note: The last line in this file is required if your model is incremental. Refer to model kinds for more information about the kinds of models you can create.</p> </li> </ol>"},{"location":"guides/models/#editing-an-existing-model","title":"Editing an existing model","text":"<p>To edit an existing model:</p> <ol> <li>Open the model file you wish to edit in your preferred editor and make a change.</li> <li>To preview an example of what your change looks like without actually creating a table, use the <code>vulcan evaluate</code> command. Refer to evaluating a model below.</li> <li>To materialize this change, use the <code>vulcan plan</code> command. Refer to previewing changes using the <code>plan</code> command below.</li> </ol>"},{"location":"guides/models/#evaluating-a-model","title":"Evaluating a model","text":"<p>The <code>evaluate</code> command will run a query against your database or engine and return a dataframe. It is used to test or iterate on models without database side effects and at minimal cost because Vulcan isn't materializing any data.</p> <p>To evaluate a model:</p> <ol> <li> <p>Run the <code>evaluate</code> command using either the CLI or Notebook. For example, running the <code>evaluate</code> command on <code>incremental_model</code> from the quickstart project:</p> <pre><code>$ vulcan evaluate vulcan_example.incremental_model --start=2020-01-07 --end=2020-01-07\n\nid  item_id          model_time_column\n0   7        1  2020-01-07\n</code></pre> </li> <li> <p>When you run the <code>evaluate</code> command, Vulcan detects the changes made to the model, executes the model as a query using the options passed to <code>evaluate</code>, and shows the output returned by the model query.</p> </li> </ol>"},{"location":"guides/models/#previewing-changes-using-the-plan-command","title":"Previewing changes using the <code>plan</code> command","text":"<p>When Vulcan runs the <code>plan</code> command on your environment, it will show you whether any downstream models are impacted by your changes. If so, Vulcan will prompt you to classify the changes as Breaking or Non-Breaking before applying the changes.</p> <p>To preview changes using <code>plan</code>:</p> <ol> <li>Enter the <code>vulcan plan &lt;environment name&gt;</code> command.</li> <li>Enter <code>1</code> to classify the changes as <code>Breaking</code>, or enter <code>2</code> to classify the changes as <code>Non-Breaking</code>. In this example, the changes are classified as <code>Non-Breaking</code>:</li> </ol> <pre><code>$ vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.full_model\n---\n+++\n@@ -1,6 +1,7 @@\nSELECT\nid,\nitem_id,\n+  1 AS new_column,\nmodel_time_column\nFROM (VALUES\n(1, 1, '2020-01-01'),\nDirectly Modified: vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.full_model\n[1] [Breaking] Backfill vulcan_example.incremental_model and indirectly modified children\n[2] [Non-breaking] Backfill vulcan_example.incremental_model but not indirectly modified children: 2\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 vulcan_example.incremental_model: (2020-01-01, 2023-02-17)\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank for the beginning of history:\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until now:\nApply - Backfill Tables [y/n]: y\n\nvulcan_example__dev.incremental_model \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\nAll model batches have been executed successfully\n\nVirtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n</code></pre> <p>For more information, refer to plans.</p>"},{"location":"guides/models/#reverting-a-change-to-a-model","title":"Reverting a change to a model","text":"<p>Before trying to revert a change, ensure that you have already made a change and that you have run the <code>vulcan plan</code> command.</p> <p>To revert your change:</p> <ol> <li>Open the model file you wish to edit in your preferred editor, and undo a change you made earlier. For this example, we'll remove the column we added in the quickstart example.</li> <li>Run <code>vulcan plan</code> and apply your changes. Enter <code>y</code> to run a Virtual Update.</li> </ol> <pre><code>$ vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `dev` environment:\n\nModels\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.full_model\n---\n+++\n@@ -1,7 +1,6 @@\n\nSELECT\nid,\nitem_id,\n-  1 AS new_column,\nmodel_time_column\nFROM (VALUES\n    (1, 1, '2020-01-01'),\nDirectly Modified: vulcan_example.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.full_model\nApply - Virtual Update [y/n]: y\n\nVirtual Update executed successfully\n</code></pre>"},{"location":"guides/models/#virtual-update","title":"Virtual Update","text":"<p>Reverting to a previous model version is a quick operation since no additional work is being done. For more information, refer to plan application and Virtual Update.</p> <p>Note: The Vulcan janitor runs periodically and automatically to clean up Vulcan artifacts no longer being used, and determines the time-to-live (TTL) for tables (how much time can pass before reverting is no longer possible).</p>"},{"location":"guides/models/#validating-changes-to-a-model","title":"Validating changes to a model","text":""},{"location":"guides/models/#automatic-model-validation","title":"Automatic model validation","text":"<p>Vulcan automatically validates your models in order to ensure the quality and accuracy of your data. This is done via the following:</p> <ul> <li>Running unit tests by default when you execute the <code>plan</code> command. This ensures all changes to applied to any environment are logically validated. Refer to testing for more information.</li> <li>Running audits whenever data is loaded to a table (either for backfill or loading on a cadence). This way you know all data present in any table has passed all defined audits. Refer to auditing for more information.</li> </ul> <p>Vulcan also provides automatic validation via CI/CD by automatically creating a preview environment.</p>"},{"location":"guides/models/#manual-model-validation","title":"Manual model validation","text":"<p>To manually validate your models, you can perform one or more of the following tasks:</p> <ul> <li>Evaluating a model</li> <li>Testing a model using unit tests</li> <li>Auditing a model</li> <li>Previewing changes using the <code>plan</code> command</li> </ul>"},{"location":"guides/models/#deleting-a-model","title":"Deleting a model","text":"<p>Before deleting a model, ensure that you have already run <code>vulcan plan</code>.</p> <p>To delete a model:</p> <ol> <li>Within your <code>models</code> directory, delete the file containing the model and any associated tests in the <code>tests</code> directory. For this example, we'll delete the <code>models/full_model.sql</code> and <code>tests/test_full_model.yaml</code> files from our quickstart project.</li> <li> <p>Run the <code>vulcan plan &lt;environment&gt;</code> command, specifying the environment to which you want to apply the change. In this example, we apply the change to our development environment <code>dev</code>:</p> <pre><code>```bash linenums=\"1\"\n$ vulcan plan dev\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `dev` environment:\n\nModels\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 vulcan_example.full_model\nApply - Virtual Update [y/n]: y\nVirtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n\n\nVirtual Update executed successfully\n```\n</code></pre> <p>Note: If you have other files that reference the model you wish to delete (such as tests), an error message will note the file(s) containing the reference. You must also delete these files to apply the change.</p> </li> <li> <p>Plan and apply your changes to production, and enter <code>y</code> for the Virtual Update. By default, the <code>vulcan plan</code> command targets your production environment:</p> <pre><code>```bash linenums=\"1\"\n$ vulcan plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 vulcan_example.full_model\nApply - Virtual Update [y/n]: y\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n\n\nVirtual Update executed successfully\n```\n</code></pre> </li> <li> <p>Verify that the <code>full_model.sql</code> model was removed from the output.</p> </li> </ol>"},{"location":"guides/models/#viewing-the-dag-of-a-projects-models","title":"Viewing the DAG of a project's models","text":"<p>Before generating a DAG, ensure that you have already installed the graphviz package.</p> <p>To install the package with <code>pip</code>, enter the following command:</p> <pre><code>pip install graphviz\n</code></pre> <p>Alternatively, enter the following command to install graphviz with <code>apt-get</code>:</p> <pre><code>sudo apt-get install graphviz\n</code></pre> <p>To view the DAG, enter the following command:</p> <p><code>vulcan dag FILE</code></p> <p>An html file containing your project's DAG will be placed at the root of your project folder. The DAG can then be viewed by opening this file in your browser.</p>"},{"location":"guides/multi_engine/","title":"Multi-Engine guide","text":""},{"location":"guides/multi_engine/#multi-engine-guide","title":"Multi-Engine guide","text":"<p>Organizations typically connect to a data warehouse through a single engine to ensure data consistency. However, there are cases where the processing capabilities of one engine may be better suited to specific tasks than another.</p> <p>Companies are increasingly decoupling how/where data is stored from the how computations are run on the data, requiring interoperability across platforms and tools. Open table formats like Apache Iceberg, Delta Lake, and Hive provide a common storage format that can be used by multiple SQL engines.</p> <p>Vulcan enables this decoupling by supporting multiple engine adapters within a single project, giving you the flexibility to choose the best engine for each computational task. You can specify the engine each model uses, based on what computations the model performs or other organization-specific considerations.</p>"},{"location":"guides/multi_engine/#configuring-a-project-with-multiple-engines","title":"Configuring a Project with Multiple Engines","text":"<p>Configuring your project to use multiple engines follows a simple process:</p> <ul> <li>Include all required gateway connections in your configuration.</li> <li>Specify the <code>gateway</code> to be used for execution in the <code>MODEL</code> DDL.</li> </ul> <p>If no gateway is explicitly defined for a model, the default_gateway of the project is used.</p> <p>By default, virtual layer views are created in the <code>default_gateway</code>. This approach requires that all engines can read from and write to the same shared catalog, so a view in the <code>default_gateway</code> can access a table in another gateway.</p> <p>Alternatively, each gateway can create the virtual layer views for the models it runs. Use this approach by setting the gateway_managed_virtual_layer flag to <code>true</code> in your project configuration.</p>"},{"location":"guides/multi_engine/#shared-virtual-layer","title":"Shared Virtual Layer","text":"<p>To dive deeper, in Vulcan the physical layer is the concrete data storage layer, where it stores and manages data in database tables and materialized views.</p> <p>While, the virtual layer consists of views, one for each model, each pointing to a snapshot table in the physical layer.</p> <p>In a multi-engine project with a shared data catalog, the model-specific gateway is responsible for the physical layer, while the default gateway is used for managing the virtual layer.</p>"},{"location":"guides/multi_engine/#example-duckdb-postgresql","title":"Example: DuckDB + PostgreSQL","text":"<p>Below is a simple example of setting up a project with connections to both DuckDB and PostgreSQL.</p> <p>In this setup, the PostgreSQL engine is set as the default, so it will be used to manage views in the virtual layer. Meanwhile, DuckDB's attach feature enables read-write access to the PostgreSQL catalog's physical tables.</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        main_db:\n          type: postgres\n          path: 'dbname=main_db user=postgres host=127.0.0.1'\n      extensions:\n        - name: iceberg\n  postgres:\n    connection:\n      type: postgres\n      database: main_db\n      user: user\n      password: password\n      host: 127.0.0.1\n      port: 5432\ndefault_gateway: postgres\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig,\n    PostgresConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"postgres\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"main_db\": DuckDBAttachOptions(\n                        type=\"postgres\",\n                        path=\"dbname=main_db user=postgres host=127.0.0.1\"\n                    ),\n                },\n                extensions=[\"iceberg\"],\n            )\n        ),\n        \"postgres\": GatewayConfig(\n            connection=PostgresConnectionConfig(\n                host=\"127.0.0.1\",\n                port=5432,\n                user=\"postgres\",\n                password=\"password\",\n                database=\"main_db\",\n            )\n        ),\n    },\n    default_gateway=\"postgres\",\n)\n</code></pre> <p>Given this configuration, when a model\u2019s gateway is set to DuckDB, the DuckDB engine will perform the calculations before materializing the physical table in the PostgreSQL <code>main_db</code> catalog.</p> <pre><code>MODEL (\n  name orders.order_ship_date,\n  kind FULL,\n  gateway duckdb,\n);\n\nSELECT\n  l_orderkey,\n  l_shipdate\nFROM\n  iceberg_scan('data/bucket/lineitem_iceberg', allow_moved_paths = true);\n</code></pre> <p>The <code>order_ship_date</code> model specifies the DuckDB engine, which will perform the computations used to create the physical table in the PostgreSQL database.</p> <p>This allows you to efficiently scan data from an Iceberg table, or even query tables directly from S3 when used with the HTTPFS extension.</p> <p> Figure 1: The gateways denote the execution engine, while both the virtual layer\u2019s views and the physical layer's tables reside in Postgres</p> <p>In models where no gateway is specified, such as the <code>customer_orders</code> model, the default PostgreSQL engine will  both create the physical table and the views in the virtual layer.</p>"},{"location":"guides/multi_engine/#gateway-managed-virtual-layer","title":"Gateway-Managed Virtual Layer","text":"<p>By default, all virtual layer views are created in the project's default gateway.</p> <p>If your project's engines don\u2019t have a mutually accessible catalog or your raw data is located in different engines, you may prefer for each model's virtual layer view to exist in the gateway that ran the model. This allows a single Vulcan project to manage isolated sets of models in different gateways, which is sometimes necessary for data governance or security concerns.</p> <p>To enable this, set <code>gateway_managed_virtual_layer</code> to <code>true</code> in your configuration. By default, this flag is set to false.</p>"},{"location":"guides/multi_engine/#example-redshift-athena-snowflake","title":"Example: Redshift + Athena + Snowflake","text":"<p>Consider a scenario where you need to create a project with models in Redshift, Athena and Snowflake, where each engine hosts its models' virtual layer views.</p> <p>First, add the connections to your configuration and set the <code>gateway_managed_virtual_layer</code> flag to <code>true</code>:</p> YAMLPython <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      user: &lt;redshift_user&gt;\n      password: &lt;redshift_password&gt;\n      host: &lt;redshift_host&gt;\n      database: &lt;redshift_database&gt;\n    variables:\n      gw_var: 'redshift'\n  athena:\n    connection:\n      type: athena\n      aws_access_key_id: &lt;athena_aws_access_key_id&gt;\n      aws_secret_access_key: &lt;athena_aws_secret_access_key&gt;\n      s3_warehouse_location: &lt;athena_s3_warehouse_location&gt;\n    variables:\n      gw_var: 'athena'\n  snowflake:\n    connection:\n      type: snowflake\n      account: &lt;snowflake_account&gt;\n      user: &lt;snowflake_user&gt;\n      database: &lt;snowflake_database&gt;\n      warehouse: &lt;snowflake_warehouse&gt;\n    variables:\n      gw_var: 'snowflake'\n\ndefault_gateway: redshift\ngateway_managed_virtual_layer: true\n\nvariables:\n  gw_var: 'global'\n  global_var: 5\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    RedshiftConnectionConfig,\n    AthenaConnectionConfig,\n    SnowflakeConnectionConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"redshift\"),\n    gateways={\n        \"redshift\": GatewayConfig(\n            connection=RedshiftConnectionConfig(\n                user=\"&lt;redshift_user&gt;\",\n                password=\"&lt;redshift_password&gt;\",\n                host=\"&lt;redshift_host&gt;\",\n                database=\"&lt;redshift_database&gt;\",\n            ),\n            variables={\n                \"gw_var\": \"redshift\"\n            },\n        ),\n        \"athena\": GatewayConfig(\n            connection=AthenaConnectionConfig(\n                aws_access_key_id=\"&lt;athena_aws_access_key_id&gt;\",\n                aws_secret_access_key=\"&lt;athena_aws_secret_access_key&gt;\",\n                region_name=\"&lt;athena_region_name&gt;\",\n                s3_warehouse_location=\"&lt;athena_s3_warehouse_location&gt;\",\n            ),\n            variables={\n                \"gw_var\": \"athena\"\n            },\n        ),\n        \"snowflake\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                account=\"&lt;snowflake_account&gt;\",\n                user=\"&lt;snowflake_user&gt;\",\n                database=\"&lt;snowflake_database&gt;\",\n                warehouse=\"&lt;snowflake_warehouse&gt;\",\n            ),\n            variables={\n                \"gw_var\": \"snowflake\"\n            },\n        ),\n    },\n    default_gateway=\"redshift\",\n    gateway_managed_virtual_layer=True,\n    variables={\n        \"gw_var\": \"global\",\n        \"global_var\": 5,\n    },\n)\n</code></pre> <p>Note that gateway-specific variables take precedence over global ones. In the example above, the <code>gw_var</code> used in a model will resolve to the value specified in the model's gateway.</p> <p>For further customization, you can also enable gateway-specific model defaults. This allows you to define custom behaviors, such as specifying a dialect with case-insensitivity normalization.</p> <p>In the example configuration above the default gateway is <code>redshift</code>, so all models without a <code>gateway</code> specification will run on redshift, as in this <code>order_dates</code> model:</p> <pre><code>MODEL (\n  name redshift_schema.order_dates,\n  table_format iceberg,\n);\n\nSELECT\n  order_date,\n  order_id\nFROM\n  bucket.raw_data;\n</code></pre> <p>For the <code>athena_schema.order_status</code> model, we explicitly specify the <code>athena</code> gateway:</p> <pre><code>MODEL (\n  name athena_schema.order_status,\n  table_format iceberg,\n  gateway athena,\n);\n\nSELECT\n  order_id,\n  status\nFROM\n  bucket.raw_data;\n</code></pre> <p>Finally, specifying the <code>snowflake</code> gateway for the <code>customer_orders</code> model ensures it is isolated from the rest and reads from a table within the Snowflake database:</p> <pre><code>MODEL (\n  name snowflake_schema.customer_orders,\n  table_format iceberg,\n  gateway snowflake\n);\n\nSELECT\n  customer_id,\n  orders\nFROM\n  bronze_schema.customer_data;\n</code></pre> <p> Figure 2: The gateways represent the execution engine and indicate where the virtual layer\u2019s views and the physical layer's tables reside</p> <p>When you run the plan, the catalogs for each model will be set automatically based on the gateway\u2019s connection and each corresponding model will be executed by the specified engine:</p> <pre><code>\u276f vulcan plan\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 awsdatacatalog.athena_schema.order_status # each model uses its gateway's catalog and schema\n    \u251c\u2500\u2500 redshift_schema.order_dates\n    \u2514\u2500\u2500 silver.snowflake_schema.customers\nModels needing backfill:\n\u251c\u2500\u2500 awsdatacatalog.athena_schema.order_status: [full refresh]\n\u251c\u2500\u2500 redshift_schema.order_dates: [full refresh]\n\u2514\u2500\u2500 silver.snowflake_schema.customer_orders: [full refresh]\nApply - Backfill Tables [y/n]: y\n</code></pre> <p>The views of the virtual layer will also be created by each corresponding engine.</p> <p>This approach provides isolation between your models, while maintaining centralized control over your project.</p>"},{"location":"guides/multi_repo/","title":"Multi-Repo guide","text":""},{"location":"guides/multi_repo/#multi-repo-guide","title":"Multi-Repo guide","text":"<p>Although mono repos are convenient and easy to use, sometimes your organization may choose to use multiple repos. Vulcan provides native support for multiple repos and makes it easy to maintain data consistency and correctness even with multiple repos. If you are wanting to separate your systems/data and provide isolation, checkout the isolated systems guide.</p>"},{"location":"guides/multi_repo/#bootstrapping-multiple-projects","title":"Bootstrapping multiple projects","text":"<p>Setting up Vulcan with multiple repos is quite simple. Copy the contents of this example multi-repo project.</p> <p>To bootstrap the project, you can point Vulcan at both projects.</p> <pre><code>$ vulcan -p examples/multi/repo_1 -p examples/multi/repo_2/ plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 silver.d\n    \u251c\u2500\u2500 bronze.a\n    \u251c\u2500\u2500 bronze.b\n    \u2514\u2500\u2500 silver.c\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 bronze.a: (2023-04-17, 2023-04-17)\n\u251c\u2500\u2500 bronze.b: (2023-04-17, 2023-04-17)\n\u251c\u2500\u2500 silver.d: (2023-04-17, 2023-04-17)\n\u2514\u2500\u2500 silver.c: (2023-04-17, 2023-04-17)\nApply - Backfill Tables [y/n]: y\nbronze.a \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nsilver.c \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nbronze.b \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nsilver.d \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\nAll model batches have been executed successfully\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n</code></pre> <p>You can see that all 4 models were planned and applied even though bronze is in repo_1 and silver is in repo_2.</p>"},{"location":"guides/multi_repo/#editing-and-planning-one-project","title":"Editing and planning one project","text":"<p>Make a non-breaking change to bronze.a by adding column c.</p> <pre><code>--- a/examples/multi/repo_1/models/a.sql\n+++ b/examples/multi/repo_1/models/a.sql\n@@ -4,4 +4,5 @@ MODEL (\n\n SELECT\n   1 AS col_a,\n-  'b' AS col_b\n+  'b' AS col_b,\n+  'c' AS col_c\n</code></pre> <p>Run a plan with just repo_1.</p> <pre><code>$ vulcan -p examples/multi/repo_1 plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 bronze.a\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 bronze.b\n    \u251c\u2500\u2500 silver.d\n    \u2514\u2500\u2500 silver.c\n---\n\n+++\n\n@@ -1,3 +1,4 @@\n\n SELECT\n   1 AS col_a,\n-  'b' AS col_b\n+  'b' AS col_b,\n+  'c' AS col_c\nDirectly Modified: bronze.a (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u251c\u2500\u2500 silver.c\n    \u251c\u2500\u2500 bronze.b\n    \u2514\u2500\u2500 silver.d\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 bronze.a: (2023-04-17, 2023-04-17)\nApply - Backfill Tables [y/n]: y\nbronze.a \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\nAll model batches have been executed successfully\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n</code></pre> <p>Vulcan detects the entire lineage of the non-breaking change even though you only have one project \"checked out\".</p>"},{"location":"guides/multi_repo/#make-a-breaking-change-and-backfill","title":"Make a breaking change and backfill","text":"<p>Change col_a to 1 + 1.</p> <pre><code>--- a/examples/multi/repo_1/models/a.sql\n+++ b/examples/multi/repo_1/models/a.sql\n@@ -3,5 +3,6 @@ MODEL (\n );\n\n SELECT\n-  1 AS col_a,\n-  'b' AS col_b\n+  1 + 1 AS col_a,\n+  'b' AS col_b,\n+  'c' AS col_c\n</code></pre> <pre><code>$ vulcan -p examples/multi/repo_1 plan\n======================================================================\nSuccessfully Ran 0 tests against duckdb\n----------------------------------------------------------------------\nDifferences from the `prod` environment:\n\nModels\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 bronze.a\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 bronze.b\n    \u251c\u2500\u2500 silver.d\n    \u2514\u2500\u2500 silver.c\n---\n\n+++\n\n@@ -1,4 +1,4 @@\n\n SELECT\n-  1 AS col_a,\n+  1 + 1 AS col_a,\n   'b' AS col_b,\n   'c' AS col_c\nDirectly Modified: bronze.a (Breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u251c\u2500\u2500 silver.d\n    \u251c\u2500\u2500 bronze.b\n    \u2514\u2500\u2500 silver.c\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 bronze.a: (2023-04-17, 2023-04-17)\n\u251c\u2500\u2500 bronze.b: (2023-04-17, 2023-04-17)\n\u251c\u2500\u2500 silver.d: (2023-04-17, 2023-04-17)\n\u2514\u2500\u2500 silver.c: (2023-04-17, 2023-04-17)\nApply - Backfill Tables [y/n]: y\nbronze.a \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nsilver.c \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nbronze.b \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\nsilver.d \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\nAll model batches have been executed successfully\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n</code></pre> <p>Vulcan correctly detects a breaking change and allows you to perform a multi-repo backfill.</p>"},{"location":"guides/multi_repo/#configuring-projects-with-multiple-repositories","title":"Configuring projects with multiple repositories","text":"<p>To add support for multiple repositories, add a <code>project</code> key to the config file in each of the respective repos.</p> <pre><code>project: repo_1\n\ngateways:\n...\n</code></pre> <p>Even if you do not have a need for multiple repos now, consider adding a <code>project</code> key so that you can easily support multiple repos in the future.</p>"},{"location":"guides/multi_repo/#running-migrations-with-multiple-repositories","title":"Running migrations with multiple repositories","text":"<p>When doing a migration, pass in a single repo path using the <code>-p</code> flag. It doesn't matter which repo you choose.</p> <pre><code>$ vulcan -p examples/multi/repo_1 migrate\n</code></pre>"},{"location":"guides/multi_repo/#multi-repo-dbt-projects","title":"Multi-Repo dbt projects","text":"<p>Vulcan also supports multiple repos for dbt projects, allowing it to correctly detect changes and orchestrate backfills even when changes span multiple dbt projects.</p> <p>You can watch a quick demo of this setup or experiment with the multi-repo dbt example yourself.</p>"},{"location":"guides/multi_repo/#multi-repo-mixed-projects","title":"Multi-repo mixed projects","text":"<p>Native Vulcan projects can be used alongside dbt projects in a multi-repo setup.</p> <p>This allows managing and sourcing tables from either project type within the same multi-repo project and facilitates a gradual migration from dbt to Vulcan.</p> <p>Use the same syntax as Vulcan-only multi-repo projects to execute a multi-repo project with either dbt or a combination of dbt and Vulcan projects:</p> <pre><code>$ vulcan -p examples/multi_hybrid/dbt_repo -p examples/multi_hybrid/vulcan_repo plan\n</code></pre> <p>Vulcan will automatically detect dependencies and lineage across both Vulcan and dbt projects, even when models are sourcing from different project types.</p> <p>For an example of this setup, refer to the mixed Vulcan and dbt example.</p>"},{"location":"guides/notifications/","title":"Notifications guide","text":""},{"location":"guides/notifications/#notifications-guide","title":"Notifications guide","text":"<p>Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.</p>"},{"location":"guides/notifications/#notification-targets","title":"Notification targets","text":"<p>Notifications are configured with <code>notification targets</code>. Targets are specified in a project's configuration file (<code>config.yml</code> or <code>config.py</code>), and multiple targets can be specified for a project.</p> <p>A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each event type (e.g., notifications for <code>run</code> will be sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification target, corresponding to the two Slack notification methods and email notification. They are specified in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"guides/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter may be useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"guides/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"guides/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"guides/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n      - apply_failure\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"guides/notifications/#api-configuration","title":"API Configuration","text":"<p>If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n      - apply_end\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"guides/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, new notification targets can be configured to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>). See the definitions of these classes on Github here.</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides/projects/","title":"Project guide","text":""},{"location":"guides/projects/#project-guide","title":"Project guide","text":""},{"location":"guides/projects/#creating-a-project","title":"Creating a project","text":"<p>Before getting started, ensure that you meet the prerequisites for using Vulcan.</p> <p>To create a project from the command line, follow these steps:</p> <ol> <li> <p>Create a directory for your project:</p> <pre><code>mkdir my-project\n</code></pre> </li> <li> <p>Change directories into your new project:</p> <pre><code>cd my-project\n</code></pre> <p>From here, you can create your project structure from scratch, or Vulcan can scaffold one for you. For the purposes of this guide, we'll show you how to scaffold your project so that you can get up and running quickly.</p> </li> <li> <p>To scaffold a project, it is recommended that you use a python virtual environment by running the following commands:</p> <pre><code>python -m venv .venv\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>pip install vulcan\n</code></pre> <p>Note: When using a python virtual environment, you must ensure that it is activated first. You should see <code>(.venv)</code> in your command line; if you don't, run <code>source .venv/bin/activate</code> from your project directory to activate your environment.</p> </li> <li> <p>Once you have activated your environment, run the following command and Vulcan will build out your project:</p> <pre><code>vulcan init [SQL_DIALECT]\n</code></pre> <p>In the command above, you can use any SQL dialect supported by sqlglot, for example \"duckdb\".</p> <p>The following directories and files will be created that you can use to organize your Vulcan project:</p> <ul> <li>config.py (database configuration file)</li> <li>./models (SQL and Python models)</li> <li>./audits (shared audits)</li> <li>./tests (unit tests)</li> <li>./macros</li> </ul> </li> </ol>"},{"location":"guides/projects/#editing-an-existing-project","title":"Editing an existing project","text":"<p>To edit an existing project, open the project file you wish to edit in your preferred editor.</p> <p>If using CLI or Notebook, you can open a file in your project for editing by using the <code>vulcan</code> command with the <code>-p</code> variable, and pointing to your project's path as follows:</p> <pre><code>vulcan -p &lt;your-project-path&gt;\n</code></pre> <p>For more details, refer to CLI</p>"},{"location":"guides/scheduling/","title":"Scheduling guide","text":""},{"location":"guides/scheduling/#scheduling-guide","title":"Scheduling guide","text":"<p>Vulcan offers a built-in scheduler for scheduling model evaluation:</p> <ul> <li>Using Vulcan's built-in scheduler</li> </ul>"},{"location":"guides/scheduling/#built-in-scheduler","title":"Built-in scheduler","text":"<p>Vulcan includes a built-in scheduler that schedules model evaluation without any additional tools or dependencies. It provides all the functionality needed to use Vulcan in production.</p> <p>By default, the scheduler stores your Vulcan project's state (information about models, data, and run history) in the SQL engine used to execute your models. Some engines, such as BigQuery, are not optimized for the transactions the scheduler executes to store state, which may degrade the scheduler's performance.</p> <p>When running the scheduler in production, we recommend evaluating its performance with your SQL engine. If you observe degraded performance, consider providing the scheduler its own transactional database such as PostgreSQL to improve performance. See the connections guide for more information on providing a separate database/engine for the scheduler.</p> <p>To perform model evaluation using the built-in scheduler, run the following command: </p><pre><code>vulcan run\n</code></pre><p></p> <p>The command above will automatically detect missing intervals for all models in the current project and then evaluate them: </p><pre><code>$ vulcan run\n\nAll model batches have been executed successfully\n\nvulcan_example.example_incremental_model \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n       vulcan_example.example_full_model \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n</code></pre><p></p> <p>Note: The <code>vulcan run</code> command performs model evaluation based on the missing data intervals identified at the time of running. It does not run continuously, and will exit once evaluation is complete. You must run this command periodically with a cron job, a CI/CD tool like Jenkins, or in a similar fashion.</p>"},{"location":"guides/signals/","title":"Signals guide","text":""},{"location":"guides/signals/#signals-guide","title":"Signals guide","text":"<p>Vulcan's built-in scheduler controls which models are evaluated when the <code>vulcan run</code> command is executed.</p> <p>It determines whether to evaluate a model based on whether the model's <code>cron</code> has elapsed since the previous evaluation. For example, if a model's <code>cron</code> was <code>@daily</code>, the scheduler would evaluate the model if its last evaluation occurred on any day before today.</p> <p>Unfortunately, the world does not always accommodate our data system's schedules. Data may land in our system after downstream daily models already ran. The scheduler did its job correctly, but today's late data will not be processed until tomorrow's scheduled run.</p> <p>You can use signals to prevent this problem.</p>"},{"location":"guides/signals/#what-is-a-signal","title":"What is a signal?","text":"<p>The scheduler uses two criteria to determine whether a model should be evaluated: whether its <code>cron</code> elapsed since the last evaluation and whether it upstream dependencies' runs have completed.</p> <p>Signals allow you to specify additional criteria that must be met before the scheduler evaluates the model.</p> <p>A signal definition is simply a function that checks whether a criterion is met. Before describing the checking function, we provide some background information about how the scheduler works.</p> <p>The scheduler doesn't actually evaluate \"a model\" - it evaluates a model over a specific time interval. This is clearest for incremental models, where only rows in the time interval are ingested during an evaluation. However, evaluation of non-temporal model kinds like <code>FULL</code> and <code>VIEW</code> are also based on a time interval: the model's <code>cron</code> frequency.</p> <p>The scheduler's decisions are based on these time intervals. For each model, the scheduler examines a set of candidate intervals and identifies the ones that are ready for evaluation.</p> <p>It then divides those into batches (configured with the model's batch_size parameter). For incremental models, it evaluates the model once for each batch. For non-incremental models, it evaluates the model once if any batch contains an interval.</p> <p>Signal checking functions examines a batch of time intervals. The function is always called with a batch of time intervals (DateTimeRanges). It can also optionally be called with key word arguments. It may return <code>True</code> if all intervals are ready for evaluation, <code>False</code> if no intervals are ready, or the time intervals themselves if only some are ready. A checking function is defined with the <code>@signal</code> decorator.</p> <p>One model, multiple signals</p> <p>Multiple signals may be specified for a model. Vulcan categorizes a candidate interval as ready for evaluation if all the signal checking functions determine it is ready.</p>"},{"location":"guides/signals/#defining-a-signal","title":"Defining a signal","text":"<p>To define a signal, create a <code>signals</code> directory in your project folder. Define your signal in a file named <code>__init__.py</code> in that directory (you can have additional python file names as well).</p> <p>A signal is a function that accepts a batch (<code>DateTimeRanges: t.List[t.Tuple[datetime, datetime]]</code>) and returns a batch or a boolean. It needs to use the <code>@signal</code> decorator.</p> <p>We now demonstrate signals of varying complexity.</p>"},{"location":"guides/signals/#simple-example","title":"Simple example","text":"<p>This example defines a <code>RandomSignal</code> method.</p> <p>The method returns <code>True</code> (indicating that all intervals are ready for evaluation) if a random number is greater than a threshold specified in the model definition:</p> <pre><code>import random\nimport typing as t\nfrom vulcan import signal, DatetimeRanges\n\n\n@signal()\ndef random_signal(batch: DatetimeRanges, threshold: float) -&gt; t.Union[bool, DatetimeRanges]:\n    return random.random() &gt; threshold\n</code></pre> <p>Note that the <code>random_signal()</code> takes a mandatory user defined <code>threshold</code> argument.</p> <p>The <code>random_signal()</code> method extracts the threshold metadata and compares a random number to it. The type is inferred based on the same rules as Vulcan Macros.</p> <p>Now that we have a working signal, we need to specify that a model should use the signal by passing metadata to the model DDL's <code>signals</code> key.</p> <p>The <code>signals</code> key accepts an array delimited by brackets <code>[]</code>. Each function in the list should contain the metadata needed for one signal evaluation.</p> <p>This example specifies that the <code>random_signal()</code> should evaluate once with a threshold of 0.5:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind FULL,\n  signals (\n    random_signal(threshold := 0.5), # specify threshold value\n  )\n);\n\nSELECT 1\n</code></pre> <p>The next time this project is <code>vulcan run</code>, our signal will metaphorically flip a coin to determine whether the model should be evaluated.</p>"},{"location":"guides/signals/#advanced-example","title":"Advanced Example","text":"<p>This example demonstrates more advanced use of signals: a signal returning a subset of intervals from a batch (rather than a single <code>True</code>/<code>False</code> value for all intervals in the batch)</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges\nfrom vulcan.utils.date import to_datetime\n\n\n# signal that returns only intervals that are &lt;= 1 week ago\n@signal()\ndef one_week_ago(batch: DatetimeRanges) -&gt; t.Union[bool, DatetimeRanges]:\n    dt = to_datetime(\"1 week ago\")\n\n    return [\n        (start, end)\n        for start, end in batch\n        if start &lt;= dt\n    ]\n</code></pre> <p>Instead of returning a single <code>True</code>/<code>False</code> value for whether a batch of intervals is ready for evaluation, the <code>one_week_ago()</code> function returns specific intervals from the batch.</p> <p>It generates a datetime argument, to which it compares the beginning of each interval in the batch. If the interval start is before that argument, the interval is ready for evaluation and included in the returned list. These signals can be added to a model like so.</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column ds,\n  ),\n  start '2 week ago',\n  signals (\n    one_week_ago(),\n  )\n);\n\n\nSELECT @start_ds AS ds\n</code></pre>"},{"location":"guides/signals/#accessing-execution-context-engine-adapter","title":"Accessing execution context / engine adapter","text":"<p>It is possible to access the execution context in a signal and access the engine adapter (warehouse connection).</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges, ExecutionContext\n\n\n# add the context argument to your function\n@signal()\ndef one_week_ago(batch: DatetimeRanges, context: ExecutionContext) -&gt; t.Union[bool, DatetimeRanges]:\n    return len(context.engine_adapter.fetchdf(\"SELECT 1\")) &gt; 1\n</code></pre>"},{"location":"guides/signals/#testing-signals","title":"Testing Signals","text":"<p>Signals only evaluate on <code>run</code> or with <code>check_intervals</code>.</p> <p>To test signals with the check_intervals command:</p> <ol> <li>Deploy your changes to an environment with <code>vulcan plan my_dev</code>.</li> <li>Run <code>vulcan check_intervals my_dev</code>.</li> </ol> <ul> <li>To check a subset of models use the --select-model flag.</li> <li>To turn off signals and just check missing intervals, use the --no-signals flag.</li> </ul> <ol> <li>To iterate, make changes to the signal, and redeploy with step 1.</li> </ol> <p>Note</p> <p><code>check_intervals</code> only works on remote models in an environment. Local signal changes are never run.</p>"},{"location":"guides/table_migration/","title":"Table migration guide","text":""},{"location":"guides/table_migration/#table-migration-guide","title":"Table migration guide","text":"<p>Vulcan projects can read directly from tables not managed by Vulcan, but in some scenarios it may be useful to migrate an existing table into a Vulcan project.</p> <p>This guide describes two methods for migrating existing tables into a Vulcan project.</p>"},{"location":"guides/table_migration/#do-you-need-to-migrate","title":"Do you need to migrate?","text":"<p>Vulcan does not assume it manages all data sources: SQL models can read from any data source accessible by the SQL engine, treating them as external models that include column-level lineage or as generic sources. This approach is preferred to migrating existing tables into a Vulcan project.</p> <p>You should only migrate a table if both of the following are true:</p> <ol> <li>The table is ingesting from an upstream source that will continue generating new data</li> <li>The table is either too large to be rebuilt or cannot be rebuilt because the necessary historical data is unavailable</li> </ol> <p>If the table's upstream source will not generate more data, there is no ongoing activity for Vulcan to manage. A Vulcan model or any other downstream consumer can select directly from the table under its current name.</p> <p>If the table's upstream source is generating new data, we assume that the table is already being loaded incrementally, as there is no need for migration if the table can be fully rebuilt.</p> <p>We describe two migration methods below. The stage and union method is preferred and should be used if feasible.</p>"},{"location":"guides/table_migration/#migration-methods","title":"Migration methods","text":"<p>This section describes two methods for migrating tables into Vulcan.</p> <p>The method descriptions contain renaming steps that are only necessary if downstream consumers must select from the original table name (e.g., step 2 in the first example). If that is not the case, the original table can retain its name.</p> <p>The table and model names in the examples below are arbitrary - you may name them whatever is appropriate for your project.</p>"},{"location":"guides/table_migration/#stage-and-union","title":"Stage and union","text":"<p>The stage and union method works by treating new and historical data as separate sources.</p> <p>It requires creating an incremental staging model to ingest new records and a <code>VIEW</code> model that unions those records with the existing table's static historical records.</p>"},{"location":"guides/table_migration/#example","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the stage and union method consists of five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_historical</code><ul> <li>Optionally, enable column-level lineage for the table by making it an <code>EXTERNAL</code> model and adding it to the project's <code>external_models.yaml</code> file</li> </ul> </li> <li>Create a new incremental staging model named <code>my_schema.existing_table_staging</code> (see below for code)</li> <li>Create a new <code>VIEW</code> model named <code>my_schema.existing_table</code> (see below for code)</li> <li>Run <code>vulcan plan</code> to create and backfill the models</li> </ol> <p>The staging model would contain code similar to the following for an <code>INCREMENTAL_BY_TIME_RANGE</code> model. An <code>INCREMENTAL_BY_UNIQUE_KEY</code> model would have a different <code>kind</code> specification in the <code>MODEL</code> DDL and might not include the query's <code>WHERE</code> clause.</p> <pre><code>MODEL(\n  name my_schema.existing_table_staging,\n  kind INCREMENTAL_BY_TIME_RANGE ( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column\n  )\n);\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre> <p>The primary model would contain code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind VIEW\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_staging -- New data\nUNION\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_historical; -- Historical data\n</code></pre> <p>Changes to columns in the source data or staging model may require modifying the code selecting from the historical data so the two tables can be safely unioned.</p>"},{"location":"guides/table_migration/#snapshot-replacement","title":"Snapshot replacement","text":"<p>The snapshot replacement method works by renaming an existing table to a name that Vulcan recognizes as an existing Vulcan model.</p>"},{"location":"guides/table_migration/#background","title":"Background","text":"<p>This section briefly describes how Vulcan's virtual data environments, forward-only models, and start times work. This information is not necessary for migrating tables but is necessary for understanding why each step in the migration process is required.</p>"},{"location":"guides/table_migration/#virtual-data-environments","title":"Virtual data environments","text":"<p>Conceptually, Vulcan divides the database into a \"physical layer\" where data is stored and a \"virtual layer\" where data is accessed by end users. The physical layer stores materialized objects like tables, and the virtual layer contains views that point to the physical layer objects.</p> <p>Each time a Vulcan <code>plan</code> adds or modifies a model, Vulcan creates a physical layer \"snapshot\" object to which the virtual layer view points. The snapshot replacement method simply renames the migrating table to the name of the appropriate snapshot table.</p>"},{"location":"guides/table_migration/#forward-only-models","title":"Forward-only models","text":"<p>Sometimes a model's data may be so large that it is not feasible to rebuild either its own or its downstream models' physical tables. In those situations a  \"forward only\" model can be used. The name reflects that the change is only applied \"going forward\" in time.</p> <p>Historical data already in the migrated table should not be overwritten, so we specify that the new model is forward-only in step 3a below.</p>"},{"location":"guides/table_migration/#start-time","title":"Start time","text":"<p>Vulcan incremental by time models track the time periods whose data a model has loaded with the interval approach.</p> <p>The interval approach requires specifying the earliest time interval Vulcan should track - when time \"starts\" for the model. For migrated tables, Vulcan should never load data for the time intervals the table ingested before migration, so interval tracking should start immediately after the time of the last ingested record.</p> <p>In the example below, we set the model's start time in its <code>MODEL</code> DDL (step 3b) and pass it as an option to the <code>vulcan plan</code> command (step 3c). The same value must be used in both the <code>MODEL</code> DDL and the plan command. In this example, the existing table's data ingestion stopped on 2023-12-31, so the model and plan start date is the next day 2024-01-01.</p>"},{"location":"guides/table_migration/#example_1","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the snapshot replacement method involves five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_temp</code></li> <li> <p>Create and initialize an empty incremental model named <code>my_schema.existing_table</code>:</p> <p>a. Make the model forward only by setting the <code>MODEL</code> DDL <code>kind</code>'s <code>forward_only</code> key to <code>true</code></p> <p>b. Specify the start of the first time interval Vulcan should track in the <code>MODEL</code> DDL <code>start</code> key (example uses \"2024-01-01\")</p> <p>c. Create the model in the Vulcan project without backfilling any data by running <code>vulcan plan [environment name] --empty-backfill --start 2024-01-01</code>, replacing \"[environment name]\" with an environment name other than <code>prod</code> and using the same start date from the <code>MODEL</code> DDL in step 3b.</p> </li> <li> <p>Determine the name of the model's snapshot physical table by running <code>vulcan table_name --env [environment name] --prod my_schema.existing_table</code>. For example, it might return <code>vulcan__my_schema.existing_table_123456</code>.</p> </li> <li>Rename the original table <code>my_schema.existing_table_temp</code> to <code>vulcan__my_schema.existing_table_123456</code></li> </ol> <p>The model would have code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind INCREMENTAL_BY_TIME_RANGE( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column,\n    forward_only true -- Forward-only model\n  ),\n  -- Start of first time interval Vulcan should track, immediately\n  --  after the last data point the table ingested. Must match\n  --  the value passed to the `vulcan plan --start` option.\n  start \"2024-01-01\"\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre>"},{"location":"guides/tablediff/","title":"Table Diff Guide","text":""},{"location":"guides/tablediff/#table-diff-guide","title":"Table Diff Guide","text":"<p>Vulcan's table diff tool allows you to compare the schema and data of two data objects. It supports comparison of a Vulcan model across two environments or direct comparison of database tables or views.</p> <p>It provides a method of validating models that can be used along with evaluating a model and testing a model with unit tests.</p> <p>Note: Table diff requires the two objects to already exist in your project's underlying database or engine. If comparing models, this means you should have already planned and applied your changes to an environment.</p>"},{"location":"guides/tablediff/#table-diff-comparisons","title":"Table diff comparisons","text":"<p>Table diff executes two types of comparison on the source and target objects: a schema diff and a row diff.</p> <p>The schema diff identifies whether fields have been added, removed, or changed data types in the target object relative to the source object.</p> <p>The row diff identifies changes in data values across columns with the same name and data type in both tables. It does this by performing an <code>OUTER JOIN</code> of the two tables then, for each column with the same name and data type, comparing data values from one table to those from the other.</p> <p>The table diff tool can be called in two ways: comparison of a Vulcan model across two project environments or direct comparison of tables/views. It executes the comparison using the database or engine specified in the Vulcan project configuration.</p>"},{"location":"guides/tablediff/#diffing-models-across-environments","title":"Diffing models across environments","text":"<p>Compare a Vulcan model across environments with the Vulcan CLI interface by using the command <code>vulcan table_diff [source environment]:[target environment] [model name]</code>.</p> <p>For example, we could make two modifications to the Vulcan quickstart model <code>vulcan_example.incremental_model</code>:</p> <ol> <li>Change the row whose <code>item_id</code> is <code>3</code> to <code>4</code> with a <code>CASE WHEN</code> statement</li> <li>Remove row whose <code>item_id</code> is <code>1</code> by adding a <code>WHERE</code> clause</li> </ol> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  CASE WHEN item_id = 3 THEN 4 ELSE item_id END as item_id, -- Change item_id 3 to 4\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_ds and @end_ds\n  AND id != 1 -- Remove row whose item_id is 1\n</code></pre> <p>After running <code>vulcan plan dev</code> and applying the plan, the updated model will be present in the <code>dev</code> environment but not in <code>prod</code>.</p> <p>Compare the two versions of the model with the table diff tool by running <code>vulcan table_diff prod:dev vulcan_example.incremental_model</code>.</p> <p>The first argument <code>prod:dev</code> specifies that <code>prod</code> is the source environment to which we will compare the target environment <code>dev</code>. The second argument <code>vulcan_example.incremental_model</code> is the name of the model to compare across the <code>prod</code> and <code>dev</code> environments.</p> <p>Because the <code>grain</code> is set to <code>[id, ds]</code> in the <code>MODEL</code> statement, Vulcan knows how to perform the join between the two models. If <code>grain</code> were not set, the command would need to include the <code>-o id -o ds</code> option to specify that the tables should be joined on column <code>id</code> and <code>ds</code>. Specify <code>-o</code> once for each join column.</p> <p>Table diff returns this output:</p> <pre><code>$ vulcan table_diff prod:dev vulcan_example.incremental_model\n\nSchema Diff Between 'PROD' and 'DEV' environments for model 'vulcan_example.incremental_model':\n\u2514\u2500\u2500 Schemas match\n\n\nRow Counts:\n\u251c\u2500\u2500  COMMON: 6 rows\n\u251c\u2500\u2500  PROD ONLY: 1 rows\n\u2514\u2500\u2500  DEV ONLY: 0 rows\n\nCOMMON ROWS column comparison stats:\n         pct_match\nitem_id       83.3\n</code></pre> <p>The \"Schema Diff\" section shows that the <code>PROD</code> and <code>DEV</code> schemas match because no columns have been added, removed, or change data type.</p> <p>The \"Row Counts\" section shows that 6 rows were successfully joined and the 1 row we removed is only present in the <code>PROD</code> model.</p> <p>The <code>COMMON ROWS column comparison stats</code> section shows that the <code>item_id</code> column values had an 83.3% match for the six joined rows (5 of the 6 row values were unchanged by our <code>CASE WHEN</code> statement). All non-join columns with the same data type in both tables are included in the comparison stats.</p> <p>If we include the <code>--show-sample</code> option in the command, the output also includes rows from the different join components.</p> <pre><code>$ vulcan table_diff prod:dev vulcan_example.incremental_model --show-sample\n\nSchema Diff Between 'PROD' and 'DEV' environments for model 'vulcan_example.incremental_model':\n\u2514\u2500\u2500 Schemas match\n\nRow Counts:\n\u251c\u2500\u2500  FULL MATCH: 6 rows (92.31%)\n\u2514\u2500\u2500  PROD ONLY: 1 rows\n\nCOMMON ROWS column comparison stats:\n         pct_match\nitem_id      100.0\n\n\nCOMMON ROWS sample data differences:\n  All joined rows match\n\nPROD ONLY sample rows:\n id event_date  item_id\n  1 2020-01-01        2\n</code></pre> <p>The <code>COMMON ROWS sample data differences</code> section displays the row whose <code>item_id</code> value changed. The <code>PROD__item_id</code> column shows that <code>item_id</code> is 3 in the <code>PROD</code> table, and the <code>DEV__item_id</code> column shows that <code>item_id</code> is 4.0 in the <code>DEV</code> table.</p> <p>The <code>PROD ONLY sample rows</code> section shows the one row that is present in <code>PROD</code> but not in <code>DEV</code>.</p> <p>If we add the <code>--skip-grain-check</code> option, the grain is not validated. By default without this flag, a warning is displayed to the user if rows contain null or duplicate grains.</p> <pre><code>$ vulcan table_diff prod:dev2 vulcan_example.incremental_model\n\nGrain should have unique and not-null audits for accurate results.\n</code></pre> <p>Under the hood, Vulcan stores temporary data in the database to perform the comparison. The default schema for these temporary tables is <code>vulcan_temp</code> but can be changed with the <code>--temp-schema</code> option. The schema can be specified as a <code>CATALOG.SCHEMA</code> or <code>SCHEMA</code>.</p>"},{"location":"guides/tablediff/#diffing-multiple-models-across-environments","title":"Diffing multiple models across environments","text":"<p>Vulcan allows you to compare multiple models across environments at once using model selection expressions. This is useful when you want to validate changes across a set of related models or the entire project.</p> <p>To diff multiple models, use the <code>--select-model</code> (or <code>-m</code> for short) option with the table diff command:</p> <pre><code>vulcan table_diff prod:dev --select-model \"vulcan_example.*\"\n</code></pre> <p>When diffing multiple models, Vulcan will:</p> <ol> <li>Show the models returned by the selector that exist in both environments and have differences</li> <li>Compare these models and display the data diff of each model</li> </ol> <p>Note: Models will only be data diffed if there's a breaking change that impacts them.</p> <p>The <code>--select-model</code> option supports a powerful selection syntax that lets you choose models using patterns, tags, dependencies and git status. For complete details, see the model selection guide.</p> <p>Note: Surround your selection pattern in single or double quotes. Ex: <code>'*'</code>, <code>\"vulcan_example.*\"</code></p> <p>Here are some common examples:</p> <pre><code># Select all models in a schema\nvulcan table_diff prod:dev -m \"vulcan_example.*\"\n\n# Select a model and its dependencies\nvulcan table_diff prod:dev -m \"+model_name\"  # include upstream deps\nvulcan table_diff prod:dev -m \"model_name+\"  # include downstream deps\n\n# Select models by tag\nvulcan table_diff prod:dev -m \"tag:finance\"\n\n# Select models with git changes\nvulcan table_diff prod:dev -m \"git:feature\"\n\n# Use logical operators for complex selections\nvulcan table_diff prod:dev -m \"(metrics.* &amp; ^tag:deprecated)\"  # models in the metrics schema that aren't deprecated\n\n# Combine multiple selectors\nvulcan table_diff prod:dev -m \"tag:finance\" -m \"metrics.*_daily\"\n</code></pre> <p>When multiple selectors are provided, they are combined with OR logic, meaning a model matching any of the selectors will be included.</p> <p>Note</p> <p>All models being compared must have their <code>grain</code> defined that is unique and not null, as this is used to perform the join between the tables in the two environments.</p> <p>If the <code>--warn-grain-check</code> option is used, this requirement is not enforced. Instead of raising an error, a warning is displayed for the models without a defined grain and diffs are computed for the remaining models.</p>"},{"location":"guides/tablediff/#diffing-tables-or-views","title":"Diffing tables or views","text":"<p>Compare specific tables or views with the Vulcan CLI interface by using the command <code>vulcan table_diff [source table]:[target table]</code>.</p> <p>The source and target tables should be fully qualified with catalog or schema names such that a SQL query of the form <code>SELECT ... FROM [source table]</code> would execute correctly.</p> <p>Recall that Vulcan models are accessible via views in the database. In the <code>prod</code> environment, the view has the same name as the model. For example, in the quickstart example project the <code>prod</code> incremental model is represented by the view <code>vulcan_example.incremental_model</code>. In the <code>dev</code> environment, <code>__dev</code> is appended to the schema name so the incremental model is represented by the view <code>vulcan_example__dev.incremental_model</code>.</p> <p>We can replicate the comparison in the previous section by comparing the model views directly. Because we are passing the view names directly, the command needs to manually specify that the join should be on the <code>id</code> and <code>ds</code> columns with the <code>-o id -o ds</code> flags.</p> <pre><code>$ vulcan table_diff vulcan_example.incremental_model:vulcan_example__dev.incremental_model -o id -o event_date --show-sample\n\nSchema Diff Between 'VULCAN_EXAMPLE.INCREMENTAL_MODEL' and 'VULCAN_EXAMPLE__DEV.INCREMENTAL_MODEL':\n\u2514\u2500\u2500 Schemas match\n\n\nRow Counts:\n\u251c\u2500\u2500  FULL MATCH: 6 rows (92.31%)\n\u2514\u2500\u2500  VULCAN_EXAMPLE.INCREMENTAL_MODEL ONLY: 1 rows\n\nCOMMON ROWS column comparison stats:\n         pct_match\nitem_id      100.0\n\n\nCOMMON ROWS sample data differences:\n  All joined rows match\n\nVULCAN_EXAMPLE.INCREMENTAL_MODEL ONLY sample rows:\n id event_date  item_id\n  1 2020-01-01        2\n</code></pre> <p>The output matches, with the exception of the column labels in the <code>COMMON ROWS sample data differences</code>. The underlying table for each column is indicated by <code>s__</code> for \"source\" table (first table in the command's colon operator <code>:</code>) and <code>t__</code> for \"target\" table (second table in the command's colon operator <code>:</code>).</p>"},{"location":"guides/tablediff/#diffing-tables-or-views-across-gateways","title":"Diffing tables or views across gateways","text":"<p>Vulcan executes a project's models with a single database system, specified as a gateway in the project configuration.</p> <p>The within-database table diff tool described above compares tables or environments within such a system. For comparing tables across different database systems, you would need to use external tools or export the data from both systems for comparison.</p>"},{"location":"guides/testing/","title":"Testing guide","text":""},{"location":"guides/testing/#testing-guide","title":"Testing guide","text":""},{"location":"guides/testing/#testing-changes-to-models","title":"Testing changes to models","text":"<p>To run unit tests for your models, run the <code>vulcan test</code> command as follows:</p> <p></p><pre><code>$ vulcan test\n.\n----------------------------------------------------------------------\nRan 1 test in 0.042s\n\nOK\n</code></pre> As the unit tests run, Vulcan will identify any that fail.<p></p> <p>For more information about tests, refer to testing.</p>"},{"location":"guides/testing/#test-changes-to-a-specific-model","title":"Test changes to a specific model","text":"<p>To run a specific model test, pass in the suite file name followed by <code>::</code> and the name of the test; for example: <code>vulcan test tests/test_suite.yaml::test_example_full_model</code>.</p>"},{"location":"guides/testing/#run-a-subset-of-tests","title":"Run a subset of tests","text":"<p>To run a test that matches a pattern or substring, use the following syntax: <code>vulcan test tests/test_example*</code>.</p> <p>Running the above command will run our <code>test_example_full_model</code> test that we ran earlier using <code>vulcan test</code>:</p> <pre><code>$ vulcan test tests/test_example*\n.\n----------------------------------------------------------------------\nRan 1 test in 0.042s\n\nOK\n</code></pre> <p>As another example, running the <code>vulcan test tests/test_order*</code> command would run the following tests:</p> <ul> <li><code>test_orders</code></li> <li><code>test_orders_takeout</code></li> <li><code>test_order_items</code></li> <li><code>test_order_type</code></li> </ul>"},{"location":"guides/testing/#auditing-changes-to-models","title":"Auditing changes to models","text":"<p>To audit your models, run the <code>vulcan audit</code> command as follows:</p> <p></p><pre><code>$ vulcan audit\nFound 1 audit(s).\nassert_positive_order_ids PASS.\n\nFinished with 0 audit error(s).\nDone.\n</code></pre> Note: Ensure that you have already planned and applied your changes before running an audit.<p></p> <p>By default, Vulcan will halt the pipeline when an audit fails in order to prevent potentially invalid data from propagating further downstream. All audits in Vulcan are blocking - when an audit fails, execution halts immediately.</p> <p>For more information about audits, refer to auditing.</p>"},{"location":"guides/ui/","title":"Browser UI guide","text":""},{"location":"guides/ui/#browser-ui-guide","title":"Browser UI guide","text":"<p>Warning</p> <p>Browser UI is deprecated. Please use the VSCode extension instead.</p> <p>Vulcan's free, open-source browser user interface (UI) makes it easy to understand, explore, and modify your Vulcan project.</p> <p>This page describes the UI's components and how they work.</p>"},{"location":"guides/ui/#purpose","title":"Purpose","text":"<p>Modifying Vulcan projects involves changing code, creating environments, and applying plans. The browser UI facilitates this process with a graphical interface for executing Vulcan commands.</p> <p>However, not all Vulcan commands can be executed - for example, <code>vulcan run</code> must be executed via the CLI or Python interfaces.</p>"},{"location":"guides/ui/#editing-code","title":"Editing code","text":"<p>Editing code is the primary activity when modifying Vulcan projects.</p> <p>Many developers edit code in an integrated development environment (IDE), such as VSCode or PyCharm. IDEs include extensive editing functionality like auto-completion, customized key mappings, code validation, and embedded terminals.</p> <p>The Vulcan UI editor is not an IDE; it is a code editor for quickly updating models and writing ad-hoc SQL queries.</p> <p>For development work, we recommend using the Vulcan UI alongside an IDE. The UI's modular design lets you mix and match UI components while you edit code in the IDE. Learn more below.</p>"},{"location":"guides/ui/#setup","title":"Setup","text":"<p>Before beginning, ensure that you meet all the prerequisites for using Vulcan. The Vulcan browser UI requires additional Python libraries not included in the base Vulcan installation.</p> <p>To use the UI, install Vulcan with the <code>web</code> add-on. First, if using a python virtual environment, ensure it's activated by running <code>source .venv/bin/activate</code> command from the folder used during installation.</p> <p>Next, install the UI with <code>pip</code>:</p> <pre><code>pip install \"vulcan[web]\"\n</code></pre>"},{"location":"guides/ui/#start-the-browser-ui","title":"Start the Browser UI","text":"<p>Open the UI by running the <code>vulcan ui</code> command from within the project directory:</p> <pre><code>vulcan ui\n</code></pre> <p>After starting up, the Vulcan web UI is served at <code>http://127.0.0.1:8000</code> by default:</p> <p></p> <p>Navigate to the URL by clicking the link in your terminal (if supported) or copy-pasting it into your web browser:</p> <p></p>"},{"location":"guides/ui/#modules","title":"Modules","text":"<p>The UI consists of modules that contain different functionality, and sets of modules are grouped into \"modes\" (learn more about modes below).</p> <p>The UI modules are:</p> <ul> <li>Code editor</li> <li>Plan builder</li> <li>Data catalog</li> <li>Table and column lineage</li> </ul> <p>The screenshots in most examples below use the default <code>editor</code> mode.</p>"},{"location":"guides/ui/#editor-module","title":"Editor module","text":"<p>The <code>editor</code> module will appear by default if the UI is started without specifying a mode. Its default view contains five panes:</p> <ol> <li>Project directory allows navigation of project directories and files.</li> <li>Editor tabs displays open code editors.</li> <li>Code editor allows viewing and editing code files.</li> <li>Inspector provides settings and information based on recent actions and the currently active pane. (Note: inspector pane is collapsed by default. Expand it by clicking the hamburger button at the top of the collapsed pane - see previous image.)</li> <li>Details displays column-level lineage for models open in the editor and results of queries. (Note: details pane is collapsed by default. It will automatically expand upon opening a model in the editor or running a query.)</li> </ol> <p></p> <p>It also contains nine buttons:</p> <ol> <li>Toggle Editor/Data Catalog/Errors/Plan toggles among the editor module (default), data catalog module, errors view, and plan module. Errors view is only available if an error has occurred.</li> <li>History navigation returns to previous views, similar to the back button in a web browser.</li> <li>Add new tab opens a new code editor window.</li> <li>Plan opens the plan module.</li> <li>Documentation links to the Vulcan documentation website.</li> <li>The crescent moon toggles between page light and dark modes.</li> <li>Run SQL query executes the <code>vulcan fetchdf</code> command.</li> <li>Format SQL query reformats a SQL query using SQLGlot's pretty layout.</li> <li>Change SQL dialect specifies the SQL dialect of the current tab for custom SQL queries. It does not affect the SQL dialect for the project.</li> </ol> <p></p> <p>And it contains four status indicators:</p> <ol> <li>Editor tab language displays the programming language of the current code editor tab (SQL or Python).</li> <li>Current environment displays the currently selected environment</li> <li>Change indicator displays a summary of the changes in the project files relative to the most recently run Vulcan plan in the selected environment.</li> <li>Error indicator displays the count of errors in the project.</li> </ol> <p></p>"},{"location":"guides/ui/#edit-models","title":"Edit models","text":"<p>Open a model in a new tab by clicking its file name in the left-hand project directory pane.</p> <p>The tab will show the model definition, and the details pane at the bottom will display the model in the project's table and column lineage.</p> <p></p> <p>The lineage display will update as model modifications are saved. For example, you might modify the incremental SQL model by adding a new column to the query. Press <code>Cmd + S</code> (<code>Ctrl + S</code> on Windows) to save the modified model file and display the updated lineage:</p> <p></p> <p>The <code>Changes</code> indicator in the top right now shows blue and orange circles that reflect our model update.</p>"},{"location":"guides/ui/#run-sql-queries","title":"Run SQL queries","text":"<p>Run SQL queries by executing them from custom SQL editor tabs.</p> <p>For example, we might add a SQL query <code>select * from vulcan_example.incremental_model</code> to the Custom SQL 1 tab. To run the query, first click the hamburger icon to open the explorer pane:</p> <p></p> <p>Then click the <code>Run Query</code> button in the bottom right to execute the query:</p> <p></p> <p>The results appear in an interactive table in the details pane below the editor.</p>"},{"location":"guides/ui/#plan-module","title":"Plan module","text":"<p>The plan module provides a graphical interface for the <code>vulcan plan</code> command. Its actions change based on the current state of your Vulcan project. For example, it will auto-update impact analysis based on the latest saved changes to a SQL model.</p> <p>Open the plan module by clicking the green circle button on the left-hand pane or the green Plan button on the top bar next to the environment dropdown.</p> <p>The sections below use the Vulcan quickstart project to demonstrate the module.</p>"},{"location":"guides/ui/#new-project","title":"New project","text":"<p>In a brand new project, the only environment is the empty <code>prod</code> environment. The first Vulcan plan must execute every model to populate the production environment.</p> <p>When you open the plan module, it contains multiple pieces of information about the project's first plan:</p> <ul> <li>The <code>Initializing Prod Environment</code> section shows that the plan is initializing the <code>prod</code> environment.</li> <li>The Start and End date sections are grayed out because they are not allowed when running a plan in the <code>prod</code> environment.</li> <li>The <code>Changes</code> section shows that Vulcan detected three models added relative to the current empty environment.</li> <li>The <code>Backfills</code> section shows that backfills will occur for all three of the added models.</li> </ul> <p></p> <p>Vulcan will apply the plan and initiate backfill when you click the blue button labeled <code>Apply Changes And Backfill</code>.</p> <p>The page will update and new output sections will appear. Each section reflects a stage in the plan application and will be green if the step succeeded.</p> <p>The <code>Tests Completed</code> section indicates that the project's unit tests ran successfully.</p> <p>The <code>Snapshot Tables Created</code> indicates that snapshots of the added and modified models were created successfully.</p> <p>The <code>Backfilled</code> section shows progress indicators for the backfill operations. The first progress indicator shows the total number of tasks and completion percentage for the entire backfill operation. The remaining progress bars show completion percentage and run time for each model (very fast in this simple example).</p> <p></p>"},{"location":"guides/ui/#new-environment","title":"New environment","text":"<p>To create a new environment, open the environment menu by clicking the drop-down labeled <code>Environment: prod \\/</code> next to the green <code>Plan</code> button on the top right.</p> <p>To create an environment named \"dev,\" type <code>dev</code> into the Environment field and click the blue <code>Add</code> button.</p> <p></p> <p>The drop-down now shows that the Vulcan UI is working in the <code>dev</code> environment:</p> <p></p> <p>To populate the environment with views of the production environment, click the green <code>Plan</code> button to open the plan module:</p> <p></p> <p>The output section does not list any added/modified models or backfills because <code>dev</code> is being created from the existing <code>prod</code> environment without modification.</p> <p>Clicking the blue <code>Apply Virtual Update</code> button applies the new plan:</p> <p></p>"},{"location":"guides/ui/#existing-environment","title":"Existing environment","text":"<p>If you modify the project files, you will want to apply the changes to an existing environment. In this example, we have changed the project's <code>incremental_model</code> and are applying the changes to the <code>dev</code> environment.</p> <p>The plan module will summarize the changes when you open it:</p> <p></p> <p>The <code>Changes</code> section detects that <code>incremental_model</code> was directly modified and that <code>full_model</code> was indirectly modified because it selects from the incremental model.</p> <p>Click the blue <code>Apply Changes And Backfill</code> button to apply the plan and execute the backfill:</p> <p></p>"},{"location":"guides/ui/#data-catalog-module","title":"Data Catalog module","text":"<p>The data catalog module displays information about all your project's models in one interface.</p> <p>A list of all models is displayed in the left-hand pane. You can filter models by name by typing in the field at the top of the pane.</p> <p>When you choose a model, its query, lineage, and attributes are displayed. This example shows information from the quickstart project incremental model:</p> <p></p> <p>By default, the model definition source code is displayed. If you toggle to <code>Compiled Query</code>, it will display an example of the model query rendered with macro values substituted:</p> <p></p>"},{"location":"guides/ui/#lineage-module","title":"Lineage module","text":"<p>The lineage module displays a graphical representation of the project's table and column lineage.</p> <p>Click a model in the left-hand pane to view its lineage. By default, only the model's upstream parents and downstream children are displayed:</p> <p></p> <p>You may include all a project's models by clicking <code>All</code> in the Show drop-down on the upper right. In this example, two additional models appear:</p> <p></p> <p>Click <code>Connected</code> in the Show drop-down menu to highlight edges between upstream parents and downstream children in blue. This may be helpful when when a project contains many models:</p> <p></p>"},{"location":"guides/ui/#modes","title":"Modes","text":"<p>The Vulcan UI consists of the four modules described above, grouped into three different \"modes.\" Modes allow you to maintain a clean interface by suppressing UI features you don't plan to use during a session.</p> <p>You may specify the UI mode as an option when you start the UI on the command line. For example, to run the UI in plan mode issue the CLI command <code>vulcan ui --mode plan</code> from within the project directory.</p> <p>The UI modes contain these modules:</p> <ul> <li><code>editor</code>: code editor, plan builder, data catalog, table and column lineage</li> <li><code>plan</code>: plan builder, data catalog, table and column lineage</li> <li><code>catalog</code>: data catalog, table and column lineage</li> </ul>"},{"location":"guides/ui/#working-with-an-ide","title":"Working with an IDE","text":"<p>The Vulcan browser UI complements an IDE by rapidly surfacing the implications of changes to project code.</p> <p>This section demonstrates one way to use the UI while editing in VSCode; the UI's <code>plan</code> mode suppresses the code editor module so it's not in the way.</p> <p>VSCode natively supports opening a web browser in a VSCode window. We can open the browser UI in that window and see the effects of our code updates in real time.</p> <p>To use this workflow, first open a terminal in VSCode and navigate to your project directory. Then follow these steps:</p> <p>1. Start the browser UI in <code>plan</code> mode with the command <code>vulcan ui --mode plan</code>:</p> <p></p> <p> 2. In VSCode, type the shortcut <code>cmd+shift+p</code> to open the search menu:</p> <p></p> <p> 3. Type <code>simple browser</code> into the search menu and click the entry <code>Simple browser: Show</code>:</p> <p></p> <p> 4. Copy the web address printed by the command output (<code>http://127.0.0.1:8000</code> by default), paste it into the menu, and click enter:</p> <p></p> <p> 5. The UI will now appear in a VSCode tab:</p> <p></p> <p> 6. Split the VSCode window to open a code editor alongside the UI. As you update models, the UI plan and lineage interfaces will update to reflect the changes in real time:</p> <p></p> <p></p>"},{"location":"guides/vscode/","title":"Visual Studio Code Extension","text":""},{"location":"guides/vscode/#visual-studio-code-extension","title":"Visual Studio Code Extension","text":"<p>Preview</p> <p>The Vulcan Visual Studio Code extension is in preview and undergoing active development. You may encounter bugs or API incompatibilities with the Vulcan version you are running.</p> <p>We encourage you to try the extension and create Github issues for any problems you encounter.</p> <p>In this guide, you'll set up the Vulcan extension in the Visual Studio Code IDE software (which we refer to as \"VSCode\").</p> <p>We'll show you the capabilities of the extension and how to troubleshoot common issues.</p>"},{"location":"guides/vscode/#installation","title":"Installation","text":""},{"location":"guides/vscode/#vscode-extension","title":"VSCode extension","text":"<p>Install the extension through the official Visual Studio marketplace website or by searching for <code>Vulcan</code> in the VSCode \"Extensions\" tab.</p> <p>Learn more about installing VSCode extensions in the official documentation.</p>"},{"location":"guides/vscode/#python-setup","title":"Python setup","text":"<p>While installing the extension is simple, setting up and configuring a Python environment in VSCode is a bit more involved.</p> <p>We recommend using a dedicated Python virtual environment to install Vulcan. Visit the Python documentation for more information about virtual environments.</p> <p>We describe the steps to create and activate a virtual environment below, but additional information is available on the Vulcan installation page.</p> <p>We first install the Vulcan library, which is required by the extension.</p> <p>Open a terminal instance in your Vulcan project's directory and issue this command to create a virtual environment in the <code>.venv</code> directory:</p> <pre><code>python -m venv .venv\n</code></pre> <p>Next, activate the virtual environment:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>Install Vulcan with the <code>lsp</code> extra that enables the VSCode extension (learn more about Vulcan extras here):</p> <pre><code>pip install 'vulcan[lsp]'\n</code></pre>"},{"location":"guides/vscode/#vscode-python-interpreter","title":"VSCode Python interpreter","text":"<p>A Python virtual environment contains its own copy of Python (the \"Python interpreter\").</p> <p>We need to make sure VSCode is using your virtual environment's interpreter rather than a system-wide or other interpreter that does not have access to the Vulcan library we just installed.</p> <p>Confirm that VSCode is using the correct interpreter by going to the command palette and clicking <code>Python: Select Interpreter</code>. Select the Python executable that's in the virtual environment's directory <code>.venv</code>.</p> <p></p> <p>Once that's done, validate that the everything is working correctly by checking the <code>vulcan</code> channel in the output panel. It displays the Python interpreter path and details of your Vulcan installation:</p> <p></p>"},{"location":"guides/vscode/#features","title":"Features","text":"<p>Vulcan's VSCode extension makes it easy to edit and understand your Vulcan project with these features:</p> <ul> <li>Lineage<ul> <li>Interactive view of model lineage</li> </ul> </li> <li>Editor<ul> <li>Auto-completion for model names and Vulcan keywords</li> <li>Model summaries when hovering over model references</li> <li>Links to open model files from model references</li> <li>Inline Vulcan linter diagnostics</li> </ul> </li> <li>VSCode commands<ul> <li>Format Vulcan project files</li> <li>Sign in/out of Tobiko Cloud (Tobiko Cloud users only)</li> </ul> </li> </ul>"},{"location":"guides/vscode/#lineage","title":"Lineage","text":"<p>The extension adds a lineage view to Vulcan models. To view the lineage of a model, go to the <code>Lineage</code> tab in the panel:</p> <p></p>"},{"location":"guides/vscode/#render","title":"Render","text":"<p>The extension allows you to render a model with the macros resolved. You can invoke it either with the command palette <code>Render Vulcan Model</code> or by clicking the preview button in the top right.</p>"},{"location":"guides/vscode/#editor","title":"Editor","text":"<p>The Vulcan VSCode extension includes several features that make editing Vulcan models easier and quicker:</p> <p>Completion</p> <p>See auto-completion suggestions when writing SQL models, keywords, or model names.</p> <p></p> <p>Go to definition and hover information</p> <p>Hovering over a model name shows a tooltip with the model description. </p> <p>In addition to hover information, you can go to a definition of the following objects in a SQL file by either right-clicking and choosing \"Go to definition\" or by <code>Command/Control + Click</code> on the respective reference. This currently works for:</p> <ul> <li>Model references in a SQL file like <code>FROM my_model</code></li> <li>CTE reference in a SQL file like <code>WITH my_cte AS (...) ... FROM my_cte</code> </li> <li>Python macros in a SQL file like <code>SELECT @my_macro(...)</code></li> </ul> <p>Diagnostics</p> <p>If you have the Vulcan linter enabled, issues are reported directly in your editor. This works for both Vulcan's built-in linter rules and custom linter rules.</p> <p></p> <p>Formatting</p> <p>Vulcan's model formatting tool is integrated directly into the editor, so it's easy to format models consistently.</p>"},{"location":"guides/vscode/#commands","title":"Commands","text":"<p>The Vulcan VSCode extension provides the following commands in the VSCode command palette:</p> <ul> <li><code>Format Vulcan project</code></li> <li><code>Sign in to Tobiko Cloud</code> (Tobiko Cloud users only)</li> <li><code>Sign out of Tobiko Cloud</code> (Tobiko Cloud users only)</li> </ul>"},{"location":"guides/vscode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/vscode/#duckdb-concurrent-access","title":"DuckDB concurrent access","text":"<p>If your Vulcan project uses DuckDB to store its state, you will likely encounter problems.</p> <p>Vulcan can create multiple connections to the state database, but DuckDB's local database file does not support concurrent access.</p> <p>Because the VSCode extension establishes a long-running process connected to the database, access conflicts are more likely than with standard Vulcan usage from the CLI. </p> <p>Therefore, we do not recommend using DuckDB as a state store with the VSCode extension.</p>"},{"location":"guides/vscode/#environment-variables","title":"Environment variables","text":"<p>The VSCode extension is based on a language server that runs in the background as a separate process. When the VSCode extension starts the background language server, the server inherits environment variables from the environment where you started VSCode. The server does not inherit environment variables from your terminal instance in VSCode, so it may not have access to variables you use when calling Vulcan from the CLI.</p> <p>If you have environment variables that are needed by the context and the language server, you can use one of these approaches to pass variables to the language server:</p> <ul> <li>Open VSCode from a terminal that has the variables set already. </li> <li>If you have <code>export ENV_VAR=value</code> in your shell configuration file (e.g. <code>.zshrc</code> or <code>.bashrc</code>) when initializing the terminal by default, the variables will be picked up by the language server if opened from that terminal.</li> <li>Use environment variables pulled from somewhere else dynamically in your <code>config.py</code> for example by connecting to a secret store</li> <li>By default, a <code>.env</code> file in your root project directory will automatically be picked up by the language server through the python environment that the extension uses. For exact details on how to set the environment variables in the Python environment that the extension uses, see here</li> </ul> <p>You can verify that the environment variables are being passed to the language server by printing them in your terminal. </p> <ol> <li><code>Cmd +Shift + P</code> (<code>Ctrl + Shift + P</code> in case of Windows) to start the VSCode command bar    </li> <li>Select the option: <code>Vulcan: Print Environment Variables</code></li> <li>You should see the environment variables printed in the terminal    </li> </ol> <p>If you change your setup during development (e.g., add variables to your shell config), you must restart the language server for the changes to take effect. You can do this by running the following command in the terminal:</p> <ol> <li><code>Cmd +Shift + P</code> (<code>Ctrl + Shift + P</code> in case of Windows) to start the VSCode command bar</li> <li>Select the option: <code>Vulcan: Restart Servers</code> </li> </ol> <p>This loaded message will appear in the lower left corner of the VSCode window.</p> <ol> <li>Print the environment variables based on the instructions above to verify the changes have taken effect.</li> </ol>"},{"location":"guides/vscode/#python-environment-issues","title":"Python environment issues","text":"<p>The most common problem is the extension not using the correct Python interpreter.</p> <p>Follow the setup process described above to ensure that the extension is using the correct Python interpreter.</p> <p>If you have checked the VSCode <code>vulcan</code> output channel and the extension is still not using the correct Python interpreter, please raise an issue here.</p>"},{"location":"guides/vscode/#missing-python-dependencies","title":"Missing Python dependencies","text":"<p>When installing Vulcan, some dependencies required by the VSCode extension are not installed unless you specify the <code>lsp</code> \"extra\".</p> <p>Install the <code>lsp</code> extra by running this command in your terminal:</p> <pre><code>pip install 'vulcan[lsp]'\n</code></pre>"},{"location":"guides/vscode/#vulcan-compatibility","title":"Vulcan compatibility","text":"<p>While the Vulcan VSCode extension is in preview and the APIs to the underlying Vulcan version are not stable, we do not guarantee compatibility between the extension and the Vulcan version you are using.</p> <p>If you encounter a problem, please raise an issue here.</p>"},{"location":"integrations/dbt/","title":"dbt","text":""},{"location":"integrations/dbt/#dbt","title":"dbt","text":"<p>Vulcan has native support for running dbt projects with its dbt adapter.</p> <p>Tip</p> <p>If you've never used Vulcan before, learn the basics of how it works in the Vulcan Quickstart!</p>"},{"location":"integrations/dbt/#getting-started","title":"Getting started","text":""},{"location":"integrations/dbt/#installing-vulcan","title":"Installing Vulcan","text":"<p>Vulcan is a Python library you install with the <code>pip</code> command. We recommend running your Vulcan projects in a Python virtual environment, which must be created and activated before running any <code>pip</code> commands.</p> <p>Most people do not use all of Vulcan's functionality. For example, most projects only run on one SQL execution engine.</p> <p>Therefore, Vulcan is packaged with multiple \"extras,\" which you may optionally install based on the functionality your project needs. You may specify all your project's extras in a single <code>pip</code> call.</p> <p>At minimum, using the Vulcan dbt adapter requires installing the dbt extra:</p> <pre><code>&gt; pip install \"vulcan[dbt]\"\n</code></pre> <p>If your project uses any SQL execution engine other than DuckDB, you must install the extra for that engine. For example, if your project runs on the Postgres SQL engine:</p> <pre><code>&gt; pip install \"vulcan[dbt,postgres]\"\n</code></pre> <p>If you would like to use the Vulcan Browser UI to view column-level lineage, include the <code>web</code> extra:</p> <pre><code>&gt; pip install \"vulcan[dbt,web]\"\n</code></pre> <p>Learn more about Vulcan installation and extras here.</p>"},{"location":"integrations/dbt/#reading-a-dbt-project","title":"Reading a dbt project","text":"<p>Prepare an existing dbt project to be run by Vulcan by executing the <code>vulcan init</code> command within the dbt project root directory and with the <code>dbt</code> template option:</p> <pre><code>$ vulcan init -t dbt\n</code></pre> <p>This will create a file called <code>vulcan.yaml</code> containing the default model start date. This configuration file is a minimum starting point for enabling Vulcan to work with your DBT project.</p> <p>As you become more comfortable with running your project under Vulcan, you may specify additional Vulcan configuration as required to unlock more features.</p> <p>profiles.yml</p> <p>Vulcan will use the existing data warehouse connection target from your dbt project's <code>profiles.yml</code> file so the connection configuration does not need to be duplicated in <code>vulcan.yaml</code>. You may change the target at any time in the dbt config and Vulcan will pick up the new target.</p>"},{"location":"integrations/dbt/#setting-model-backfill-start-dates","title":"Setting model backfill start dates","text":"<p>Models require a start date for backfilling data through use of the <code>start</code> configuration parameter. <code>start</code> can be defined individually for each model in its <code>config</code> block or globally in the <code>vulcan.yaml</code> file as follows:</p> vulcan.yamldbt Model <pre><code>model_defaults:\n  start: '2000-01-01'\n</code></pre> <pre><code>{{\n  config(\n    materialized='incremental',\n    start='2000-01-01',\n    ...\n  )\n}}\n</code></pre>"},{"location":"integrations/dbt/#configuration","title":"Configuration","text":"<p>Vulcan derives a project's configuration from its dbt configuration files. This section outlines additional settings specific to Vulcan that can be defined.</p>"},{"location":"integrations/dbt/#selecting-a-different-state-connection","title":"Selecting a different state connection","text":"<p>Certain engines, like Trino, cannot be used to store Vulcan's state.</p> <p>In addition, even if your warehouse is supported for state, you may find that you get better performance by using a traditional database to store state as these are a better fit for the state workload than a warehouse optimized for analytics workloads.</p> <p>In these cases, we recommend specifying a supported production state engine using the <code>state_connection</code> configuration.</p> <p>This involves updating <code>vulcan.yaml</code> to add a gateway configuration for the state connection:</p> <pre><code>gateways:\n  \"\": # \"\" (empty string) is the default gateway\n    state_connection:\n      type: postgres\n      ...\n\nmodel_defaults:\n  start: '2000-01-01'\n</code></pre> <p>Or, for a specific dbt profile defined in <code>profiles.yml</code>, eg <code>dev</code>:</p> <pre><code>gateways:\n  dev: # must match the target dbt profile name\n    state_connection:\n      type: postgres\n      ...\n\nmodel_defaults:\n  start: '2000-01-01'\n</code></pre> <p>Learn more about how to configure state connections here.</p>"},{"location":"integrations/dbt/#runtime-vars","title":"Runtime vars","text":"<p>dbt supports passing variable values at runtime with its CLI <code>vars</code> option.</p> <p>In Vulcan, these variables are passed via configurations. When you initialize a dbt project with <code>vulcan init</code>, a file <code>vulcan.yaml</code> is created in your project directory.</p> <p>You may define global variables in the same way as a native project by adding a <code>variables</code> section to the config.</p> <p>For example, we could specify the runtime variable <code>is_marketing</code> and its value <code>no</code> as:</p> <pre><code>variables:\n  is_marketing: no\n\nmodel_defaults:\n  start: '2000-01-01'\n</code></pre> <p>Variables can also be set at the gateway/profile level which override variables set at the project level. See the variables documentation to learn more about how to specify them at different levels.</p>"},{"location":"integrations/dbt/#combinations","title":"Combinations","text":"<p>Some projects use combinations of runtime variables to control project behavior. Different combinations can be specified in different <code>vulcan_config</code> objects, with the relevant configuration passed to the Vulcan CLI command.</p> <p>Python config</p> <p>Switching between different config objects requires the use of Python config instead of the default YAML config.</p> <p>You will need to create a file called <code>config.py</code> in the root of your project with the following contents:</p> <pre><code>from pathlib import Path\nfrom vulcan.dbt.loader import vulcan_config\n\nconfig = vulcan_config(Path(__file__).parent)\n</code></pre> <p>Note that any config from <code>vulcan.yaml</code> will be overlayed on top of the active Python config so you dont need to remove the <code>vulcan.yaml</code> file</p> <p>For example, consider a project with a special configuration for the <code>marketing</code> department. We could create separate configurations to pass at runtime like this:</p> <pre><code>config = vulcan_config(\n  Path(__file__).parent,\n  variables={\"is_marketing\": \"no\", \"include_pii\": \"no\"}\n)\n\nmarketing_config = vulcan_config(\n  Path(__file__).parent,\n  variables={\"is_marketing\": \"yes\", \"include_pii\": \"yes\"}\n)\n</code></pre> <p>By default, Vulcan will use the configuration object named <code>config</code>. Use a different configuration by passing the object name to Vulcan CLI commands with the <code>--config</code> option. For example, we could run a <code>plan</code> with the marketing configuration like this:</p> <pre><code>vulcan --config marketing_config plan\n</code></pre> <p>Note that the <code>--config</code> option is specified between the word <code>vulcan</code> and the command being executed (e.g., <code>plan</code>, <code>run</code>).</p>"},{"location":"integrations/dbt/#registering-comments","title":"Registering comments","text":"<p>Vulcan automatically registers model descriptions and column comments with the target SQL engine, as described in the Models Overview documentation. Comment registration is on by default for all engines that support it.</p> <p>dbt offers similar comment registration functionality via its <code>persist_docs</code> model configuration parameter, specified by model. Vulcan comment registration is configured at the project level, so it does not use dbt's model-specific <code>persist_docs</code> configuration.</p> <p>Vulcan's project-level comment registration defaults are overridden with the <code>vulcan_config()</code> <code>register_comments</code> argument. For example, this configuration turns comment registration off:</p> <pre><code>config = vulcan_config(\n    Path(__file__).parent,\n    register_comments=False,\n    )\n</code></pre>"},{"location":"integrations/dbt/#running-vulcan","title":"Running Vulcan","text":"<p>Run Vulcan as with a Vulcan project, generating and applying plans, running tests or audits, and executing models with a scheduler if desired.</p> <p>You continue to use your dbt file and project format.</p>"},{"location":"integrations/dbt/#workflow-differences-between-vulcan-and-dbt","title":"Workflow differences between Vulcan and dbt","text":"<p>Consider the following when using a dbt project:</p> <ul> <li>Vulcan will detect and deploy new or modified seeds as part of running the <code>plan</code> command and applying changes - there is no separate seed command. Refer to seed models for more information.</li> <li>The <code>plan</code> command dynamically creates environments, so environments do not need to be hardcoded into your <code>profiles.yml</code> file as targets. To get the most out of Vulcan, point your dbt profile target at the production target and let Vulcan handle the rest for you.</li> <li>The term \"test\" has a different meaning in dbt than in Vulcan:<ul> <li>dbt \"tests\" are audits in Vulcan.</li> <li>Vulcan \"tests\" are unit tests, which test query logic before applying a Vulcan plan.</li> </ul> </li> <li>dbt's' recommended incremental logic is not compatible with Vulcan, so small tweaks to the models are required (don't worry - dbt can still use the models!).</li> </ul>"},{"location":"integrations/dbt/#how-to-use-vulcan-incremental-models-with-dbt-projects","title":"How to use Vulcan incremental models with dbt projects","text":"<p>Incremental loading is a powerful technique when datasets are large and recomputing tables is expensive. Vulcan offers first-class support for incremental models, and its approach differs from dbt's.</p> <p>This section describes how to adapt dbt's incremental models to run on vulcan and maintain backwards compatibility with dbt.</p>"},{"location":"integrations/dbt/#incremental-types","title":"Incremental types","text":"<p>Vulcan supports two approaches to implement idempotent incremental loads:</p> <ul> <li>Using merge (with the vulcan <code>INCREMENTAL_BY_UNIQUE_KEY</code> model kind)</li> <li>Using <code>INCREMENTAL_BY_TIME_RANGE</code> model kind</li> </ul>"},{"location":"integrations/dbt/#incremental-by-unique-key","title":"Incremental by unique key","text":"<p>To enable incremental_by_unique_key incrementality, the model configuration should contain:</p> <ul> <li>The <code>unique_key</code> key with the model's unique key field name or names as the value</li> <li>The <code>materialized</code> key with value <code>'incremental'</code></li> <li>Either:<ul> <li>No <code>incremental_strategy</code> key or</li> <li>The <code>incremental_strategy</code> key with value <code>'merge'</code></li> </ul> </li> </ul>"},{"location":"integrations/dbt/#incremental-by-time-range","title":"Incremental by time range","text":"<p>To enable incremental_by_time_range incrementality, the model configuration must contain:</p> <ul> <li>The <code>materialized</code> key with value <code>'incremental'</code></li> <li>The <code>incremental_strategy</code> key with the value <code>incremental_by_time_range</code></li> <li>The <code>time_column</code> key with the model's time column field name as the value (see <code>time column</code> for details)</li> </ul>"},{"location":"integrations/dbt/#incremental-logic","title":"Incremental logic","text":"<p>Unlike dbt incremental strategies, Vulcan does not require the use of <code>is_incremental</code> jinja blocks to implement incremental logic.  Instead, Vulcan provides predefined time macro variables that can be used in the model's SQL to filter data based on the time column.</p> <p>For example, the SQL <code>WHERE</code> clause with the \"ds\" column goes in a new jinja block gated by <code>{% if vulcan_incremental is defined %}</code> as follows:</p> <pre><code>&gt;   WHERE\n&gt;     ds BETWEEN '{{ start_ds }}' AND '{{ end_ds }}'\n</code></pre> <p><code>{{ start_ds }}</code> and <code>{{ end_ds }}</code> are the jinja equivalents of Vulcan's <code>@start_ds</code> and <code>@end_ds</code> predefined time macro variables. See all predefined time variables available in jinja.</p>"},{"location":"integrations/dbt/#incremental-model-configuration","title":"Incremental model configuration","text":"<p>Vulcan provides configuration parameters that enable control over how incremental computations occur. These parameters are set in the model's <code>config</code> block.</p> <p>See Incremental Model Properties for the full list of incremental model configuration parameters.</p> <p>Note: By default, all incremental dbt models are configured to be forward-only. However, you can change this behavior by setting the <code>forward_only: false</code> setting either in the configuration of an individual model or globally for all models in the <code>dbt_project.yaml</code> file. The forward-only mode aligns more closely with the typical operation of dbt and therefore better meets user's expectations.</p> <p>Similarly, the allow_partials parameter is set to <code>true</code> by default unless the <code>allow_partials</code> parameter is explicitly set to <code>false</code> in the model configuration.</p>"},{"location":"integrations/dbt/#on_schema_change","title":"on_schema_change","text":"<p>Vulcan automatically detects both destructive and additive schema changes to forward-only incremental models and to all incremental models in forward-only plans.</p> <p>A model's <code>on_destructive_change</code> and <code>on_additive_change</code> settings determine whether it errors, warns, silently allows, or ignores the changes. Vulcan provides fine-grained control over both destructive changes (like dropping columns) and additive changes (like adding new columns).</p> <p><code>on_schema_change</code> configuration values are mapped to these Vulcan settings:</p> <code>on_schema_change</code> Vulcan <code>on_destructive_change</code> Vulcan <code>on_additive_change</code> ignore ignore ignore fail error error append_new_columns ignore allow sync_all_columns allow allow"},{"location":"integrations/dbt/#snapshot-support","title":"Snapshot support","text":"<p>Vulcan supports both dbt snapshot strategies of either <code>timestamp</code> or <code>check</code>. Only unsupported snapshot functionality is <code>invalidate_hard_deletes</code> which must be set to <code>True</code>. If set to <code>False</code>, then the snapshot will be skipped and a warning will be logged indicating this happened. Support for this will be added soon.</p>"},{"location":"integrations/dbt/#tests","title":"Tests","text":"<p>Vulcan uses dbt tests to perform Vulcan audits (coming soon).</p> <p>Add Vulcan unit tests to a dbt project by placing them in the \"tests\" directory.</p>"},{"location":"integrations/dbt/#seed-column-types","title":"Seed column types","text":"<p>Vulcan parses seed CSV files using Panda's <code>read_csv</code> utility and its default column type inference.</p> <p>dbt parses seed CSV files using agate's csv reader and customizes agate's default type inference.</p> <p>If Vulcan and dbt infer different column types for a seed CSV file, you may specify a column_types dictionary in your <code>dbt_project.yml</code> file, where the keys define the column names and the values the data types.</p> <pre><code>seeds:\n  &lt;seed name&gt;\n    +column_types:\n      &lt;column name&gt;: &lt;SQL data type&gt;\n</code></pre> <p>Alternatively, you can define this dictionary in the seed seed properties configuration file.</p> <pre><code>seeds:\n  - name: &lt;seed name&gt;\n    config:\n      column_types:\n        &lt;column name&gt;: &lt;SQL data type&gt;\n</code></pre> <p>You may also specify a column's SQL data type in its <code>data_type</code> key, as shown below. The file must list all columns present in the CSV file; Vulcan's default type inference will be used for columns that do not specify the <code>data_type</code> key.</p> <pre><code>seeds:\n  - name: &lt;seed name&gt;\n    columns:\n      - name: &lt;column name&gt;\n        data_type: &lt;SQL data type&gt;\n</code></pre>"},{"location":"integrations/dbt/#package-management","title":"Package Management","text":"<p>Vulcan does not have its own package manager; however, Vulcan's dbt adapter is compatible with dbt's package manager. Continue to use dbt deps and dbt clean to update, add, or remove packages.</p>"},{"location":"integrations/dbt/#documentation","title":"Documentation","text":"<p>Model documentation is available in the Vulcan UI.</p>"},{"location":"integrations/dbt/#supported-dbt-jinja-methods","title":"Supported dbt jinja methods","text":"<p>Vulcan supports running dbt projects using the majority of dbt jinja methods, including:</p> Method Method Method Method adapter env_var project_name target as_bool exceptions ref this as_native from_yaml return to_yaml as_number is_incremental run_query var as_text load_result schema zip api log set builtins modules source config print statement"},{"location":"integrations/dbt/#unsupported-dbt-jinja-methods","title":"Unsupported dbt jinja methods","text":"<p>The dbt jinja methods that are not currently supported are:</p> <ul> <li>debug</li> <li>selected_sources</li> <li>graph.nodes.values</li> <li>graph.metrics.values</li> </ul>"},{"location":"integrations/dbt/#missing-something-you-need","title":"Missing something you need?","text":"<p>Submit an issue, and we'll look into it!</p>"},{"location":"integrations/dlt/","title":"dlt","text":""},{"location":"integrations/dlt/#dlt","title":"dlt","text":"<p>Vulcan enables efforless project generation using data ingested through dlt. This involves creating a baseline project scaffolding, generating incremental models to process the data from the pipeline's tables by inspecting its schema and configuring the gateway connection using the pipeline's credentials.</p>"},{"location":"integrations/dlt/#getting-started","title":"Getting started","text":""},{"location":"integrations/dlt/#reading-from-a-dlt-pipeline","title":"Reading from a dlt pipeline","text":"<p>To load data from a dlt pipeline into Vulcan, ensure the dlt pipeline has been run or restored locally. Then simply execute the vulcan <code>init</code> command within the dlt project root directory using the <code>dlt</code> template option and specifying the pipeline's name with the <code>dlt-pipeline</code> option:</p> <pre><code>$ vulcan init -t dlt --dlt-pipeline &lt;pipeline-name&gt; dialect\n</code></pre> <p>This will create the configuration file and directories, which are found in all Vulcan projects:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. Refer to configuration.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. Refer to models.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. Refer to seeds.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. Refer to auditing.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. Refer to testing.</li> </ul> </li> <li>./macros<ul> <li>Macro files. Refer to macros.</li> </ul> </li> </ul> <p>Vulcan will also automatically generate models to ingest data from the pipeline incrementally. Incremental loading is ideal for large datasets where recomputing entire tables is resource-intensive. In this case utilizing the <code>INCREMENTAL_BY_TIME_RANGE</code> model kind. However, these model definitions can be customized to meet your specific project needs.</p>"},{"location":"integrations/dlt/#specify-the-path-to-the-pipelines-directory","title":"Specify the path to the pipelines directory","text":"<p>The default location for dlt pipelines is <code>~/.dlt/pipelines/&lt;pipeline_name&gt;</code>. If your pipelines are in a different directory, use the <code>--dlt-path</code> argument to specify the path explicitly:</p> <pre><code>$ vulcan init -t dlt --dlt-pipeline &lt;pipeline-name&gt; --dlt-path &lt;pipelines-directory&gt; dialect\n</code></pre>"},{"location":"integrations/dlt/#generating-models-on-demand","title":"Generating models on demand","text":"<p>To update the models in your Vulcan project on demand, use the <code>dlt_refresh</code> command. This allows you to either specify individual tables to generate incremental models from or update all models at once.</p> <ul> <li>Generate all missing tables:</li> </ul> <pre><code>$ vulcan dlt_refresh &lt;pipeline-name&gt;\n</code></pre> <ul> <li>Generate all missing tables and overwrite existing ones (use with <code>--force</code> or <code>-f</code>):</li> </ul> <pre><code>$ vulcan dlt_refresh &lt;pipeline-name&gt; --force\n</code></pre> <ul> <li>Generate specific dlt tables (using <code>--table</code> or <code>-t</code>):</li> </ul> <pre><code>$ vulcan dlt_refresh &lt;pipeline-name&gt; --table &lt;dlt-table&gt;\n</code></pre> <ul> <li>Provide the explicit path to the pipelines directory (using <code>--dlt-path</code>):</li> </ul> <pre><code>$ vulcan dlt_refresh &lt;pipeline-name&gt; --dlt-path &lt;pipelines-directory&gt;\n</code></pre>"},{"location":"integrations/dlt/#configuration","title":"Configuration","text":"<p>Vulcan will retrieve the data warehouse connection credentials from your dlt project to configure the <code>config.yaml</code> file. This configuration can be modified or customized as needed. For more details, refer to the configuration guide.</p>"},{"location":"integrations/dlt/#example","title":"Example","text":"<p>Generating a Vulcan project dlt is quite simple. In this example, we'll use the example <code>sushi_pipeline.py</code> from the sushi-dlt project.</p> <p>First, run the pipeline within the project directory:</p> <pre><code>$ python sushi_pipeline.py\nPipeline sushi load step completed in 2.09 seconds\nLoad package 1728074157.660565 is LOADED and contains no failed jobs\n</code></pre> <p>After the pipeline has run, generate a Vulcan project by executing:</p> <pre><code>$ vulcan init -t dlt --dlt-pipeline sushi duckdb\n</code></pre> <p>Then the Vulcan project is all set up. You can then proceed to run the Vulcan <code>plan</code> command to ingest the dlt pipeline data and populate the Vulcan tables:</p> <pre><code>$ vulcan plan\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 sushi_dataset_vulcan.incremental__dlt_loads\n    \u251c\u2500\u2500 sushi_dataset_vulcan.incremental_sushi_types\n    \u2514\u2500\u2500 sushi_dataset_vulcan.incremental_waiters\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sushi_dataset_vulcan.incremental__dlt_loads: 2024-10-03 - 2024-10-03\n\u251c\u2500\u2500 sushi_dataset_vulcan.incremental_sushi_types: 2024-10-03 - 2024-10-03\n\u2514\u2500\u2500 sushi_dataset_vulcan.incremental_waiters: 2024-10-03 - 2024-10-03\nApply - Backfill Tables [y/n]: y\n[1/1] sushi_dataset_vulcan.incremental__dlt_loads evaluated in 0.01s\n[1/1] sushi_dataset_vulcan.incremental_sushi_types evaluated in 0.00s\n[1/1] sushi_dataset_vulcan.incremental_waiters evaluated in 0.01s\nEvaluating models \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\nAll model batches have been executed successfully\n\nVirtually Updating 'prod' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\n</code></pre> <p>Once the models are planned and applied, you can continue as with any Vulcan project, generating and applying plans, running tests or audits, and executing models with a scheduler if desired.</p>"},{"location":"integrations/github/","title":"GitHub Actions CI/CD Bot","text":""},{"location":"integrations/github/#github-actions-cicd-bot","title":"GitHub Actions CI/CD Bot","text":"<p>The GitHub Actions CI/CD Bot enables teams to automate their Vulcan projects using GitHub Actions. It can be configured to perform the following things:</p> <ul> <li>Automatically run unit tests on PRs</li> <li>Automatically run the linter on PRs</li> <li>Automatically create PR environments that represent the code changes in the PR</li> <li>Automatically categorize and backfill data for models that have changed</li> <li>Automatically deploy changes to production with automatic data gap prevention and merge the PR</li> </ul> <p>All of these features provide summaries and links to the relevant information in the PR so that you can easily see what is happening and why.</p> <p>Due to the variety of ways that this bot can be configured, it is recommended to perform the initial setup and then explore the different configuration options to see which ones fit your use case.</p>"},{"location":"integrations/github/#initial-setup","title":"Initial Setup","text":"<ol> <li>Make sure Vulcan is added to your project's dependencies and it includes the github extra (<code>pip install vulcan[github]</code>).</li> <li>Create a new file in <code>.github/workflows/vulcan.yml</code> with the following contents: <pre><code>name: Vulcan Bot\nrun-name: \ud83d\ude80Vulcan Bot \ud83d\ude80\non:\n  pull_request:\n    types:\n    - synchronize\n    - opened\n# The latest commit is the one that will be used to create the PR environment and deploy to production\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.ref_name }}\n  cancel-in-progress: true\njobs:\n  vulcan:\n    name: Vulcan Actions Workflow\n    runs-on: ubuntu-latest\n    permissions:\n      # Required to access code in PR\n      contents: write\n      # Required to post comments\n      issues: write\n      # Required to update check runs\n      checks: write\n      # Required to merge\n      pull-requests: write\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n      - name: Checkout PR branch\n        uses: actions/checkout@v4\n        with:\n          ref: refs/pull/${{ github.event.issue.pull_request &amp;&amp; github.event.issue.number || github.event.pull_request.number  }}/merge\n      - name: Install Vulcan + Dependencies\n        run: pip install -r requirements.txt\n        shell: bash\n      - name: Run CI/CD Bot\n        run: |\n          vulcan_cicd -p ${{ github.workspace }} github --token ${{ secrets.GITHUB_TOKEN }} run-all\n</code></pre></li> </ol> <p>Next checkout the Core Bot Behavior Configuration Guide to see how to configure the bot's core behavior. Then checkout Bot Configuration to see how to configure the bot's behavior in general. Finally, checkout Custom Workflow Configuration to see the full set of customizations available to the bot.</p>"},{"location":"integrations/github/#core-bot-behavior-configuration-guide","title":"Core Bot Behavior Configuration Guide","text":"<p>There are two fundamental ways the bot can be configured: synchronized or desynchronized production code and data.</p>"},{"location":"integrations/github/#synchronized-vs-desynchronized-deployments","title":"Synchronized vs. Desynchronized Deployments","text":"<p>Typically, CI/CD workflows for data projects follow a flow where code is merged into a main branch and then the production datasets that are represented by the code in main are eventually updated to match the code.  This could either be on merge, and therefore they will be updated after the refresh job completes, or it could be on a schedule where the refresh job is run on a schedule. Either way the data in production is lagging behind the code in main and therefore code and data are desynchronized.   The advantage though of this approach is that users just need to merge a branch in order to see their changes eventually represented in production. The disadvantage is that it can be difficult for users to know the current state of production is and when their changes will be live. In addition, if an error occurs while the data in production is being updated then the data in production may never reflect the state that is represented in the main branch.</p> <p>Vulcan's virtual data environments offer a different approach where the deployment can be synchronized. This means that dev datasets can be quickly deployed to production using Vulcan's virtual update and then immediately after the branch is automatically merged to main. Now the code and data in production are synchronized and users can see their changes immediately.  The disadvantage of this approach is that users can't simply merge a branch to get their changes into production since Vulcan needs to be able to deploy the changes and then perform the merge itself. As a result it requires a \"signal\" from users to indicate that they want a change deployed to production that will then trigger the bot to deploy the changes and then merge the PR.</p> <p>Vulcan's GitHub CI/CD Bot supports either approach and it is up to each team which mode is the best fit given their organization's constraints.</p>"},{"location":"integrations/github/#synchronized-production-code-and-data-configuration","title":"Synchronized Production Code and Data Configuration","text":"<p>Regardless of signal approach being used, the bot needs to be configured to use the merge method you want to use when merging the PR after deploying to production.</p> YAMLPython <pre><code>cicd_bot:\n  type: github\n  merge_method: squash\n</code></pre> <pre><code>from vulcan.integrations.github.cicd.config import GithubCICDBotConfig, MergeMethod\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    cicd_bot=GithubCICDBotConfig(\n        merge_method=MergeMethod.SQUASH\n    )\n)\n</code></pre> <p>In this example we configured the merge method to be <code>squash</code>. See Bot Configuration for more details on the <code>merge_method</code> option.</p>"},{"location":"integrations/github/#required-approval-signal","title":"Required Approval Signal","text":"<p>One way to signal to Vulcan that a PR is ready to go to production is through the use of \"Required Approvers\".  In this approach users configure their Vulcan project to list users that are designated as \"Required Approver\" and then when the bot detects an approval was received from one of these individuals then it determines that it is time to deploy to production. The bot will only do the deploy to prod if the base branch is a production branch (as defined in the bot's configuration but defaults to either <code>main</code> or <code>master</code>). This pattern can be a great fit for teams that already have an approval process like this in place and therefore it actually removes an extra step from either the author or the approver since Vulcan will automate the deployment and merge until of it having to be manually done.</p>"},{"location":"integrations/github/#required-approval-configuration","title":"Required Approval Configuration","text":"<p>In order to configure this pattern, you need to define a user in your Vulcan project that has the \"Required Approver\" role.</p> YAMLPython <pre><code>users:\n- username: &lt;A username to use within Vulcan to represent the user&gt;\n  github_username: &lt;Github username&gt;\n  roles:\n    - required_approver\n</code></pre> <pre><code>from vulcan.core.config import Config\nfrom vulcan.core.user import User, UserRole\n\nconfig = Config(\n    users=[\n        User(\n            username=\"&lt;A username to use within Vulcan to represent the user&gt;\",\n            github_username=\"&lt;Github username&gt;\",\n            roles=[UserRole.REQUIRED_APPROVER],\n        )\n    ]\n)\n</code></pre> <p>The GitHub Actions workflow needs to be updated to trigger the action based on if a pull request review has come in. </p> <pre><code>on:\n  pull_request:\n    types:\n    - synchronize\n    - opened\n  # Required if using required approvers to automate deployments\n  pull_request_review:\n    types:\n    - edited\n    - submitted\n    - dismissed\n</code></pre> <p>Now if the bot detects an approval from this user then it will deploy the changes to production and merge the PR.</p>"},{"location":"integrations/github/#deploy-command-signal","title":"Deploy Command Signal","text":"<p>In this approach users can issue a <code>/deploy</code> command to the bot to signal that they want the changes in the PR to be deployed to production. This pattern is more flexible than the required approval pattern. Deploy command signal can be used alongside the required approval signal or on its own.  The deploy command, if issued, overrides the required approval signal and will deploy the changes to production and merge the PR.</p>"},{"location":"integrations/github/#deploy-command-configuration","title":"Deploy Command Configuration","text":"<p>This command must be enabled in the bot's configuration. </p> YAMLPython <pre><code>cicd_bot:\n  type: github\n  merge_method: squash\n  enable_deploy_command: true\n</code></pre> <pre><code>from vulcan.integrations.github.cicd.config import GithubCICDBotConfig, MergeMethod\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    cicd_bot=GithubCICDBotConfig(\n        enable_deploy_command=True\n        merge_method=MergeMethod.SQUASH\n    )\n)\n</code></pre> <p>Optionally, a <code>command_namespace</code> can be configured to avoid clashing with other bots. See Bot Configuration for more details on the <code>command_namespace</code> option.</p> <p>The GitHub Actions workflow needs to be updated to trigger the action based on if a comment has been created. </p> <pre><code>on:\n  pull_request:\n    types:\n    - synchronize\n    - opened\n  # Required if using comments to issue commands to the bot\n  issue_comment:\n    types:\n    - created\n</code></pre> <p>Note: the <code>issue_comment</code> event will not work until this change is merged into your main branch. Therefore, to enable this you will need to make the change in a branch, merge, and then future branches will support the deploy command.</p>"},{"location":"integrations/github/#desynchronized-production-code-and-data-configuration","title":"Desynchronized Production Code and Data Configuration","text":"<p>In order to support this pattern we need to add an additional step to the workflow that will run the <code>deploy-production</code> command after the merge to main. In addition we need to also update some prior steps with if checks to differentiate the merge vs. non-merge behavior.</p> <pre><code>pull_request:\n    types:\n      - synchronize\n      - opened\n      # Add closed\n      - closed\n...\njobs:\n  vulcan:\n    steps:\n    - name: Checkout PR branch\n      uses: actions/checkout@v4\n      # Add if statement so we don't checkout merged PR but instead main branch\n      if: github.event.pull_request.merged == false\n      with:\n        ref: refs/pull/${{ github.event.issue.pull_request &amp;&amp; github.event.issue.number || github.event.pull_request.number  }}/merge\n    - name: Checkout main branch\n      # Add if statement so we use main when merged\n      if: github.event.pull_request.merged == true\n      uses: actions/checkout@v4\n      with:\n        ref: main\n    ...\n    - name: Run CI/CD Bot\n      # Add if statement so we don't run-all on merged PR\n      if: github.event.pull_request.merged == false\n      run: |\n        vulcan_cicd -p ${{ github.workspace }} github --token ${{ secrets.GITHUB_TOKEN }} run-all\n    # Add deploy step that only runs on merged PR\n    - name: Deploy to Production\n      # `main` should be the name of your branch that represents production\n      if: github.event.pull_request.merged == true &amp;&amp; github.event.pull_request.base.ref == 'main'\n      run: |\n        vulcan_cicd -p ${{ github.workspace }} github --token ${{ secrets.GITHUB_TOKEN }} deploy-production\n</code></pre> <p>Make sure that \"Required Approvers\" are not configured (they are not by default) and \"Deploy Command\" is not enabled (it is not by default).</p>"},{"location":"integrations/github/#bot-configuration","title":"Bot Configuration","text":"<p>The bot's behavior is configured using your project's <code>config.yaml</code> or <code>config.py</code> file. See Vulcan Configuration for more details on how to generally setup and configure Vulcan.</p> <p>Below is an example of how to define the default config for the bot in either YAML or Python.</p> YAMLPython <pre><code>cicd_bot:\n  type: github\n</code></pre> <pre><code>from vulcan.integrations.github.cicd.config import GithubCICDBotConfig\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    cicd_bot=GithubCICDBotConfig()\n)\n</code></pre>"},{"location":"integrations/github/#configuration-properties","title":"Configuration Properties","text":"Option Description Type Required <code>invalidate_environment_after_deploy</code> Indicates if the PR environment created should be automatically invalidated after changes are deployed. Invalidated environments are cleaned up automatically by the Janitor. Default: <code>True</code> bool N <code>merge_method</code> The merge method to use when automatically merging a PR after deploying to prod. Defaults to <code>None</code> meaning automatic merge is not done. Options: <code>merge</code>, <code>squash</code>, <code>rebase</code> string N <code>enable_deploy_command</code> Indicates if the <code>/deploy</code> command should be enabled in order to allowed synchronized deploys to production. Default: <code>False</code> bool N <code>command_namespace</code> The namespace to use for Vulcan commands. For example if you provide <code>#Vulcan</code> as a value then commands will be expected in the format of <code>#Vulcan/&lt;command&gt;</code>. Default: <code>None</code> meaning no namespace is used. string N <code>auto_categorize_changes</code> Auto categorization behavior to use for the bot. If not provided then the project-wide categorization behavior is used. See Auto-categorize model changes for details. dict N <code>default_pr_start</code> Default start when creating PR environment plans. If running in a mode where the bot automatically backfills models (based on <code>auto_categorize_changes</code> behavior) then this can be used to limit the amount of data backfilled. Defaults to <code>None</code> meaning the start date is set to the earliest model's start or to 1 day ago if data previews need to be computed. str N <code>pr_min_intervals</code> Intended for use when <code>default_pr_start</code> is set to a relative time, eg <code>1 week ago</code>. This ensures that at least this many intervals across every model are included for backfill in the PR environment. Without this, models with an interval unit wider than <code>default_pr_start</code> (such as <code>@monthly</code> models if <code>default_pr_start</code> was set to <code>1 week ago</code>) will be excluded from backfill entirely. int N <code>skip_pr_backfill</code> Indicates if the bot should skip backfilling models in the PR environment. Default: <code>True</code> bool N <code>pr_include_unmodified</code> Indicates whether to include unmodified models in the PR environment. Default to the project's config value (which defaults to <code>False</code>) bool N <code>run_on_deploy_to_prod</code> Indicates whether to run latest intervals when deploying to prod. If set to false, the deployment will backfill only the changed models up to the existing latest interval in production, ignoring any missing intervals beyond this point. Default: <code>False</code> bool N <code>pr_environment_name</code> The name of the PR environment to create for which a PR number will be appended to. Defaults to the repo name if not provided. Note: The name will be normalized to alphanumeric + underscore and lowercase. str N <code>prod_branch_name</code> The name of the git branch associated with production. Ex: <code>prod</code>. Default: <code>main</code> or <code>master</code> is considered prod str N <code>forward_only_branch_suffix</code> If the git branch has this suffix, trigger a forward-only plan instead of a normal plan. Default: <code>-forward-only</code> str N <p>Example with all properties defined:</p> YAMLPython <pre><code>cicd_bot:\n  type: github\n  invalidate_environment_after_deploy: false\n  enable_deploy_command: true\n  merge_method: squash\n  command_namespace: \"#Vulcan\"\n  auto_categorize_changes:\n    external: full\n    python: full\n    sql: full\n    seed: full\n  default_pr_start: \"1 week ago\"\n  skip_pr_backfill: false\n  run_on_deploy_to_prod: false\n  prod_branch_name: production\n</code></pre> <pre><code>from vulcan.integrations.github.cicd.config import GithubCICDBotConfig, MergeMethod\nfrom vulcan.core.config import AutoCategorizationMode, CategorizerConfig, Config\n\nconfig = Config(\n    cicd_bot=GithubCICDBotConfig(\n        invalidate_environment_after_deploy=False,\n        enable_deploy_command=True,\n        merge_method=MergeMethod.SQUASH,\n        command_namespace=\"#Vulcan\",\n        auto_categorize_changes=CategorizerConfig(\n            external=AutoCategorizationMode.FULL,\n            python=AutoCategorizationMode.FULL,\n            sql=AutoCategorizationMode.FULL,\n            seed=AutoCategorizationMode.FULL,\n        ),\n        default_pr_start=\"1 week ago\",\n        skip_pr_backfill=False,\n        run_on_deploy_to_prod=False,\n        prod_branch_name=\"production\",\n    )\n)\n</code></pre>"},{"location":"integrations/github/#bot-output","title":"Bot Output","text":"<p>Step outputs are created by the bot that capture the status of each of the checks that reach a conclusion in a run.  These can be used to potentially trigger follow up steps in the workflow. These are the possible outputs (based on how the bot is configured) that are created by the bot:</p> <ul> <li><code>run_unit_tests</code></li> <li><code>linter</code></li> <li><code>has_required_approval</code></li> <li><code>pr_environment_synced</code></li> <li><code>prod_plan_preview</code></li> <li><code>prod_environment_synced</code></li> </ul> <p>There are many possible conclusions so the best use case for this is likely to check for <code>success</code> conclusion in order to potentially run follow up steps.  Note that in error cases conclusions may not be set and therefore you will get an empty string.</p> <p>Example of running a step after pr environment has been synced: </p><pre><code>  steps:\n    - id: run-bot\n      ...\n    - id: do-after-pr-env-sync\n      if: steps.run-bot.outputs.pr_environment_synced == 'success'\n      run: ...\n</code></pre><p></p> <p>In addition, there are custom outputs listed below:</p> <ul> <li><code>created_pr_environment</code> - set to <code>\"true\"</code> (a string with a value of <code>true</code>) if a PR environment was created for the first time. It is absent, or considered empty string if you check for it, if it is not created for the first time</li> <li><code>pr_environment_name</code> - the name of the PR environment. It is output whenever PR environment synced check reaches a conclusion. Therefore make sure to check the status of <code>created_pr_environment</code> or <code>pr_environment_synced</code> before acting on this output </li> </ul> <p>Note: The <code>linter</code> step will run only if it's enabled in the project's configuration (<code>config.yaml</code> / <code>config.py</code>). The step will fail if the linter finds errors, otherwise it'll output only the warnings.</p>"},{"location":"integrations/github/#custom-workflow-configuration","title":"Custom Workflow Configuration","text":"<p>You can configure each individual action to run as a separate step. This can allow for more complex workflows or integrating specific steps with other actions you want to trigger. Run <code>vulcan_cicd github</code> to see a list of commands that can be supplied and their potential options. </p><pre><code>  Github Action CI/CD Bot\n\nOptions:\n  -t, --token TEXT  The Github Token to be used. Pass in `${{\n                    secrets.GITHUB_TOKEN }}` if you want to use the one\n                    created by Github actions\n  --help            Show this message and exit.\n\nCommands:\n  check-required-approvers  Checks if a required approver has provided...\n  deploy-production         Deploys the production environment\n  run-all                   Runs all the commands in the correct order.\n  run-tests                 Runs the unit tests\n  update-pr-environment     Creates or updates the PR environments\n</code></pre><p></p>"},{"location":"integrations/github/#example-synchronized-full-workflow","title":"Example Synchronized Full Workflow","text":"<p>This workflow involves configuring a Vulcan connection to Databricks.</p> <pre><code>name: Vulcan Bot\nrun-name: \ud83d\ude80Vulcan Bot \ud83d\ude80\non:\n  pull_request:\n    types:\n    - synchronize\n    - opened\n  # Required if using required approvers to automate deployments\n  pull_request_review:\n    types:\n    - edited\n    - submitted\n    - dismissed\n  # Required if using comments to issue commands to the bot\n  issue_comment:\n    types:\n    - created\n# The latest commit is the one that will be used to create the PR environment and deploy to production\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.ref_name }}\n  cancel-in-progress: true\njobs:\n  vulcan:\n    name: Vulcan Actions Workflow\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n      # Required to post comments\n      issues: write\n      # Required to update check runs\n      checks: write\n      # Required to merge\n      pull-requests: write\n    env:\n      VULCAN__GATEWAYS__DATABRICKS__CONNECTION__TYPE: \"databricks\"\n      VULCAN__GATEWAYS__DATABRICKS__CONNECTION__SERVER_HOSTNAME: \"XXXXXXXXXXXXXXX\"\n      VULCAN__GATEWAYS__DATABRICKS__CONNECTION__HTTP_PATH: \"XXXXXXXXXXXX\"\n      VULCAN__GATEWAYS__DATABRICKS__CONNECTION__ACCESS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n      VULCAN__DEFAULT_GATEWAY: \"databricks\"\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      - name: Checkout PR branch\n        uses: actions/checkout@v4\n        with:\n          ref: refs/pull/${{ github.event.issue.pull_request &amp;&amp; github.event.issue.number || github.event.pull_request.number  }}/merge\n      - name: Install Dependencies\n        run: pip install -r requirements.txt\n        shell: bash\n      - name: Run CI/CD Bot\n        run: |\n          vulcan_cicd -p ${{ github.workspace }} github --token ${{ secrets.GITHUB_TOKEN }} run-all\n</code></pre>"},{"location":"integrations/github/#example-screenshots","title":"Example Screenshots","text":""},{"location":"integrations/github/#automated-unit-tests-with-error-summary","title":"Automated Unit Tests with Error Summary","text":""},{"location":"integrations/github/#automated-linting-with-error-summary","title":"Automated Linting with Error Summary","text":""},{"location":"integrations/github/#automated-linting-with-warning-summary","title":"Automated Linting with Warning Summary","text":""},{"location":"integrations/github/#automatically-create-pr-environments-that-represent-the-code-changes-in-the-pr","title":"Automatically create PR Environments that represent the code changes in the PR","text":""},{"location":"integrations/github/#enforce-that-certain-reviewers-have-approved-of-the-pr-before-it-can-be-merged","title":"Enforce that certain reviewers have approved of the PR before it can be merged","text":""},{"location":"integrations/github/#preview-prod-plans-before-deploying","title":"Preview Prod Plans before Deploying","text":""},{"location":"integrations/github/#automatic-deployments-to-production-and-merge","title":"Automatic deployments to production and merge","text":""},{"location":"integrations/overview/","title":"Overview","text":""},{"location":"integrations/overview/#overview","title":"Overview","text":""},{"location":"integrations/overview/#tools","title":"Tools","text":"<p>Vulcan supports integrations with the following tools:</p> <ul> <li>dbt</li> <li>dlt</li> <li>GitHub Actions</li> <li>Kestra</li> </ul>"},{"location":"integrations/overview/#execution-engines","title":"Execution engines","text":"<p>Vulcan supports the following execution engines for running Vulcan projects (engine <code>type</code> in parentheses - example usage: <code>pip install \"vulcan[databricks]\"</code>):</p> <ul> <li>Athena (athena)</li> <li>Azure SQL (azuresql)</li> <li>BigQuery (bigquery)</li> <li>ClickHouse (clickhouse)</li> <li>Databricks (databricks)</li> <li>DuckDB (duckdb)</li> <li>Fabric (fabric)</li> <li>MotherDuck (motherduck)</li> <li>MSSQL (mssql)</li> <li>MySQL (mysql)</li> <li>Postgres (postgres)</li> <li>GCP Postgres (gcppostgres)</li> <li>Redshift (redshift)</li> <li>Snowflake (snowflake)</li> <li>Spark (spark)</li> <li>Trino (trino)</li> </ul>"},{"location":"integrations/engines/athena/","title":"Athena","text":""},{"location":"integrations/engines/athena/#athena","title":"Athena","text":""},{"location":"integrations/engines/athena/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[athena]\"\n</code></pre>"},{"location":"integrations/engines/athena/#connection-options","title":"Connection options","text":""},{"location":"integrations/engines/athena/#pyathena-connection-options","title":"PyAthena connection options","text":"<p>Vulcan leverages the PyAthena DBAPI driver to connect to Athena. Therefore, the connection options relate to the PyAthena connection options. Note that PyAthena uses boto3 under the hood so you can also use boto3 environment variables for configuration.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>athena</code> string Y <code>aws_access_key_id</code> The access key for your AWS user string N <code>aws_secret_access_key</code> The secret key for your AWS user string N <code>role_arn</code> The ARN of a role to assume once authenticated string N <code>role_session_name</code> The session name to use when assuming <code>role_arn</code> string N <code>region_name</code> The AWS region to use string N <code>work_group</code> The Athena workgroup to send queries to string N <code>s3_staging_dir</code> The S3 location for Athena to write query results. Only required if not using <code>work_group</code> OR the configured <code>work_group</code> doesnt have a results location set string N <code>schema_name</code> The default schema to place objects in if a schema isnt specified. Defaults to <code>default</code> string N <code>catalog_name</code> The default catalog to place schemas in. Defaults to <code>AwsDataCatalog</code> string N"},{"location":"integrations/engines/athena/#vulcan-connection-options","title":"Vulcan connection options","text":"<p>These options are specific to Vulcan itself and are not passed to PyAthena</p> Option Description Type Required <code>s3_warehouse_location</code> Set the base path in S3 where Vulcan will instruct Athena to place table data. Only required if you arent specifying the location in the model itself. See S3 Locations below. string N"},{"location":"integrations/engines/athena/#model-properties","title":"Model properties","text":"<p>The Athena adapter utilises the following model top-level properties:</p> Name Description Type Required <code>table_format</code> Sets the table_type Athena uses when creating the table. Valid values are <code>hive</code> or <code>iceberg</code>. string N <code>storage_format</code> Configures the file format to be used by the <code>table_format</code>. For Hive tables, this sets the STORED AS option. For Iceberg tables, this sets format property. string N <p>The Athena adapter recognises the following model physical_properties:</p> Name Description Type Default <code>s3_base_location</code> <code>s3://</code> base URI of where the snapshot tables for this model should be written. Overrides <code>s3_warehouse_location</code> if one is configured. string"},{"location":"integrations/engines/athena/#s3-locations","title":"S3 Locations","text":"<p>When creating tables, Athena needs to know where in S3 the table data is located. You cannot issue a <code>CREATE TABLE</code> statement without specifying a <code>LOCATION</code> for the table data.</p> <p>In addition, unlike other engines such as Trino, Athena will not infer a table location if you set a schema location via <code>CREATE SCHEMA &lt;schema&gt; LOCATION 's3://schema/location'</code>.</p> <p>Therefore, in order for Vulcan to issue correct <code>CREATE TABLE</code> statements to Athena, you need to configure where the tables should be stored. There are two options for this:</p> <ul> <li>Project-wide: set <code>s3_warehouse_location</code> in the connection config. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_warehouse_location&gt;/&lt;schema_name&gt;/&lt;snapshot_table_name&gt;</code> when it creates a snapshot of your model.</li> <li>Per-model: set <code>s3_base_location</code> in the model <code>physical_properties</code>. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_base_location&gt;/&lt;snapshot_table_name&gt;</code> every time it creates a snapshot of your model. This takes precedence over any <code>s3_warehouse_location</code> set in the connection config.</li> </ul>"},{"location":"integrations/engines/athena/#limitations","title":"Limitations","text":"<p>Athena was initially designed to read data stored in S3 and to do so without changing that data. This means that it does not have good support for mutating tables. In particular, it will not delete data from Hive tables.</p> <p>Consequently, forward only changes that mutate the schemas of existing tables have a high chance of failure because Athena supports very limited schema modifications on Hive tables.</p> <p>However, Athena does support Apache Iceberg tables which allow a full range of operations. These can be used for more complex model types such as <code>INCREMENTAL_BY_UNIQUE_KEY</code> and <code>SCD_TYPE_2</code>.</p> <p>To use an Iceberg table for a model, set <code>table_format iceberg</code> in the model properties.</p> <p>In general, Iceberg tables offer the most flexibility and you'll run into the least Vulcan limitations when using them. However, we create Hive tables by default because Athena creates Hive tables by default, so Iceberg tables are opt-in rather than opt-out.</p>"},{"location":"integrations/engines/azuresql/","title":"Azure SQL","text":""},{"location":"integrations/engines/azuresql/#azure-sql","title":"Azure SQL","text":"<p>Azure SQL is \"a family of managed, secure, and intelligent products that use the SQL Server database engine in the Azure cloud.\"</p>"},{"location":"integrations/engines/azuresql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>azuresql</code></p>"},{"location":"integrations/engines/azuresql/#installation","title":"Installation","text":""},{"location":"integrations/engines/azuresql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[azuresql]\"\n</code></pre>"},{"location":"integrations/engines/azuresql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[azuresql-odbc]\"\n</code></pre>"},{"location":"integrations/engines/azuresql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>azuresql</code> string Y <code>host</code> The hostname of the Azure SQL server string Y <code>user</code> The username / client ID to use for authentication with the Azure SQL server string N <code>password</code> The password / client secret to use for authentication with the Azure SQL server string N <code>port</code> The port number of the Azure SQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"integrations/engines/bigquery/","title":"BigQuery","text":""},{"location":"integrations/engines/bigquery/#bigquery","title":"BigQuery","text":""},{"location":"integrations/engines/bigquery/#introduction","title":"Introduction","text":"<p>This guide provides step-by-step instructions on how to connect Vulcan to the BigQuery SQL engine.</p> <p>It will walk you through the steps of installing Vulcan and BigQuery connection libraries locally, configuring the connection in Vulcan, and running the quickstart project.</p>"},{"location":"integrations/engines/bigquery/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes the following about the BigQuery project being used with Vulcan:</p> <ul> <li>The project already exists</li> <li>Project CLI/API access is enabled</li> <li>Project billing is configured (i.e. it's not a sandbox project)</li> <li>Vulcan can authenticate using an account with permissions to execute commands against the project</li> </ul>"},{"location":"integrations/engines/bigquery/#installation","title":"Installation","text":"<p>Follow the quickstart installation guide up to the step that installs Vulcan, where we deviate to also install the necessary BigQuery libraries.</p> <p>Instead of installing just Vulcan core, we will also include the BigQuery engine libraries:</p> <pre><code>&gt; pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"integrations/engines/bigquery/#install-google-cloud-sdk","title":"Install Google Cloud SDK","text":"<p>Vulcan connects to BigQuery via the Python <code>google-cloud-bigquery</code> library, which uses the Google Cloud SDK <code>gcloud</code> tool for authenticating with BigQuery.</p> <p>Follow these steps to install and configure the Google Cloud SDK on your computer:</p> <ul> <li>Download the appropriate installer for your system from the Google Cloud installation guide</li> <li> <p>Unpack the downloaded file with the <code>tar</code> command:</p> <pre><code>&gt; tar -xzvf google-cloud-cli-{SYSTEM_SPECIFIC_INFO}.tar.gz\n</code></pre> </li> <li> <p>Run the installation script:</p> <pre><code>&gt; ./google-cloud-sdk/install.sh\n</code></pre> </li> <li> <p>Reload your shell profile (e.g., for zsh):</p> <pre><code>&gt; source $HOME/.zshrc\n</code></pre> </li> <li> <p>Run <code>gcloud init</code> to setup authentication</p> </li> </ul>"},{"location":"integrations/engines/bigquery/#configuration","title":"Configuration","text":""},{"location":"integrations/engines/bigquery/#configure-vulcan-for-bigquery","title":"Configure Vulcan for BigQuery","text":"<p>Add the following gateway specification to your Vulcan project's <code>config.yaml</code> file:</p> <pre><code>bigquery:\n  connection:\n    type: bigquery\n    project: &lt;your_project_id&gt;\n\ndefault_gateway: bigquery\n</code></pre> <p>This creates a gateway named <code>bigquery</code> and makes it your project's default gateway.</p> <p>It uses the <code>oauth</code> authentication method, which does not specify a username or other information directly in the connection configuration. Other authentication methods are described below.</p> <p>In BigQuery, navigate to the dashboard and select the BigQuery project your Vulcan project will use. From the Google Cloud dashboard, use the arrow to open the pop-up menu:</p> <p></p> <p>Now we can identify the project ID needed in the <code>config.yaml</code> gateway specification above. Select the project that you want to work with, the project ID that you need to add to your yaml file is the ID label from the pop-up menu.</p> <p></p> <p>For this guide, the Docs-Demo is the one we will use, thus the project ID for this example is <code>healthy-life-440919-s0</code>.</p>"},{"location":"integrations/engines/bigquery/#usage","title":"Usage","text":""},{"location":"integrations/engines/bigquery/#test-the-connection","title":"Test the connection","text":"<p>Run the following command to verify that Vulcan can connect to BigQuery:</p> <pre><code>&gt; vulcan info\n</code></pre> <p>The output will look something like this:</p> <p></p> <ul> <li> <p>Set quota project (optional)</p> <p>You may see warnings like this when you run <code>vulcan info</code>:</p> <p></p> <p>You can avoid these warnings about quota projects by running:</p> <pre><code>&gt; gcloud auth application-default set-quota-project &lt;your_project_id&gt;\n&gt; gcloud config set project &lt;your_project_id&gt;\n</code></pre> </li> </ul>"},{"location":"integrations/engines/bigquery/#create-and-run-a-plan","title":"Create and run a plan","text":"<p>We've verified our connection, so we're ready to create and execute a plan in BigQuery:</p> <pre><code>&gt; vulcan plan\n</code></pre>"},{"location":"integrations/engines/bigquery/#view-results-in-bigquery-console","title":"View results in BigQuery Console","text":"<p>Let's confirm that our project models are as expected.</p> <p>First, navigate to the BigQuery Studio Console:</p> <p></p> <p>Then use the left sidebar to find your project and the newly created models:</p> <p></p> <p>We have confirmed that our Vulcan project is running properly in BigQuery!</p>"},{"location":"integrations/engines/bigquery/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>bigquery</code></p>"},{"location":"integrations/engines/bigquery/#installation_1","title":"Installation","text":"<pre><code>pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"integrations/engines/bigquery/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>bigquery</code> string Y <code>method</code> Connection methods - see allowed values below. Default: <code>oauth</code>. string N <code>project</code> The ID of the GCP project string N <code>location</code> The location of for the datasets (can be regional or multi-regional) string N <code>execution_project</code> The name of the GCP project to bill for the execution of the models. If not set, the project associated with the model will be used. string N <code>quota_project</code> The name of the GCP project used for the quota. If not set, the <code>quota_project_id</code> set within the credentials of the account is used to authenticate to BigQuery. string N <code>keyfile</code> Path to the keyfile to be used with service-account method string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>token</code> OAuth 2.0 access token string N <code>refresh_token</code> OAuth 2.0 refresh token string N <code>client_id</code> OAuth 2.0 client ID string N <code>client_secret</code> OAuth 2.0 client secret string N <code>token_uri</code> OAuth 2.0 authorization server's token endpoint URI string N <code>scopes</code> The scopes used to obtain authorization list N <code>impersonated_service_account</code> If set, Vulcan will attempt to impersonate this service account string N <code>job_creation_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to be created. int N <code>job_execution_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to complete. int N <code>job_retries</code> The number of times to retry the underlying job if it fails. (Default: <code>1</code>) int N <code>priority</code> The priority of the underlying job. (Default: <code>INTERACTIVE</code>) string N <code>maximum_bytes_billed</code> The maximum number of bytes to be billed for the underlying job. int N"},{"location":"integrations/engines/bigquery/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>oauth (default)<ul> <li>Related Credential Configuration:<ul> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>oauth-secrets<ul> <li>Related Credential Configuration:<ul> <li><code>token</code> (Optional): Can be None if refresh information is provided.</li> <li><code>refresh_token</code> (Optional): If specified, credentials can be refreshed.</li> <li><code>client_id</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>client_secret</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>token_uri</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>scopes</code> (Optional): OAuth 2.0 credentials can not request additional scopes after authorization. The scopes must be derivable from the refresh token if refresh information is provided (e.g. The refresh token scopes are a superset of this or contain a wild card scope like 'https://www.googleapis.com/auth/any-api')</li> </ul> </li> </ul> </li> <li>service-account<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>service-account-json<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile_json</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> </ul> <p>If the <code>impersonated_service_account</code> argument is set, Vulcan will:</p> <ol> <li>Authenticate user account credentials with one of the methods above</li> <li>Attempt to impersonate the service account with those credentials</li> </ol> <p>The user account must have sufficient permissions to impersonate the service account.</p>"},{"location":"integrations/engines/bigquery/#permissions-required","title":"Permissions Required","text":"<p>With any of the above connection methods, ensure these BigQuery permissions are enabled to allow Vulcan to work correctly.</p> <ul> <li><code>BigQuery Data Owner</code></li> <li><code>BigQuery User</code></li> </ul>"},{"location":"integrations/engines/clickhouse/","title":"ClickHouse","text":""},{"location":"integrations/engines/clickhouse/#clickhouse","title":"ClickHouse","text":"<p>This page describes Vulcan support for the ClickHouse engine, including configuration options specific to ClickHouse.</p> <p>Note</p> <p>ClickHouse may not be used for the Vulcan state connection.</p>"},{"location":"integrations/engines/clickhouse/#background","title":"Background","text":"<p>ClickHouse is a distributed, column-oriented SQL engine designed to rapidly execute analytical workloads.</p> <p>It provides users fine-grained control of its behavior, but that control comes at the cost of complex configuration.</p> <p>This section provides background information about ClickHouse, providing context for how to use Vulcan with the ClickHouse engine.</p>"},{"location":"integrations/engines/clickhouse/#object-naming","title":"Object naming","text":"<p>Most SQL engines use a three-level hierarchical naming scheme: tables/views are nested within schemas, and schemas are nested within catalogs. For example, the full name of a table might be <code>my_catalog.my_schema.my_table</code>.</p> <p>ClickHouse instead uses a two-level hierarchical naming scheme that has no counterpart to catalog. In addition, it calls the second level in the hierarchy \"databases.\" Vulcan and its documentation refer to this second level as \"schemas.\"</p> <p>Vulcan fully supports ClickHouse's two-level naming scheme without user action.</p>"},{"location":"integrations/engines/clickhouse/#table-engines","title":"Table engines","text":"<p>Every ClickHouse table is created with a \"table engine\" that controls how the table's data is stored and queried. ClickHouse's (and Vulcan's) default table engine is <code>MergeTree</code>.</p> <p>The <code>MergeTree</code> engine family requires that every table be created with an <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY</code> clause into every <code>MergeTree</code> family table's <code>CREATE</code> statement, or you can specify the columns/expressions by which the table should be ordered.</p>"},{"location":"integrations/engines/clickhouse/#clickhouse-modes-of-operation","title":"ClickHouse modes of operation","text":"<p>Conceptually, it may be helpful to view ClickHouse as having three modes of operation: single server, cluster, and ClickHouse Cloud. Vulcan supports all three modes.</p>"},{"location":"integrations/engines/clickhouse/#single-server-mode","title":"Single server mode","text":"<p>Single server mode is similar to other SQL engines: aside from choosing each table's engine, you do not need to worry about how computations are executed. You issue standard SQL commands/queries, and ClickHouse executes them.</p>"},{"location":"integrations/engines/clickhouse/#cluster-mode","title":"Cluster mode","text":"<p>Cluster mode allows you to scale your ClickHouse engine to any number of networked servers. This enables massive workloads, but requires that you specify how computations are executed by the networked servers.</p> <p>ClickHouse coordinates the computations on the networked servers with ClickHouse Keeper (it also supports Apache ZooKeeper).</p> <p>You specify named virtual clusters of servers in the Keeper configuration, and those clusters provide namespaces for data objects and computations. For example, you might include all networked servers in the cluster you name <code>MyCluster</code>.</p> <p>In general, you must be connected to a ClickHouse server to execute commands. By default, each command you execute runs in single-server mode on the server you are connected to.</p> <p>To associate an object with a cluster, DDL commands that create or modify it must include the text <code>ON CLUSTER [your cluster name]</code>.</p> <p>If you provide a cluster name in your Vulcan connection configuration, Vulcan will automatically inject the <code>ON CLUSTER</code> statement into the DDL commands for all objects created while executing the project. We provide more information about clusters in Vulcan below.</p>"},{"location":"integrations/engines/clickhouse/#clickhouse-cloud-mode","title":"ClickHouse Cloud mode","text":"<p>ClickHouse Cloud is a managed ClickHouse platform. It allows you to scale ClickHouse without administering a cluster yourself or modifying your SQL commands to run on the cluster.</p> <p>ClickHouse Cloud automates ClickHouse's cluster controls, which sometimes constrains ClickHouse's flexibility or how you execute SQL commands. For example, creating a table with a <code>SELECT</code> command must occur in two steps on ClickHouse Cloud. Vulcan handles this limitation for you.</p> <p>Aside from those constraints, ClickHouse Cloud mode is similar to single server mode - you run standard SQL commands/queries, and ClickHouse Cloud executes them.</p>"},{"location":"integrations/engines/clickhouse/#permissions","title":"Permissions","text":"<p>In the default Vulcan configuration, users must have sufficient permissions to create new ClickHouse databases.</p> <p>Alternatively, you can configure specific databases where Vulcan should create table and view objects.</p>"},{"location":"integrations/engines/clickhouse/#environment-views","title":"Environment views","text":"<p>Use the <code>environment_suffix_target</code> key in your project configuration to specify that environment views should be created within the model's database instead of in a new database:</p> <pre><code>environment_suffix_target: table\n</code></pre>"},{"location":"integrations/engines/clickhouse/#physical-tables","title":"Physical tables","text":"<p>Use the <code>physical_schema_mapping</code> key in your project configuration to specify the databases where physical tables should be created.</p> <p>The key accepts a dictionary of regular expressions that map model database names to the corresponding databases where physical tables should be created.</p> <p>Vulcan will compare a model's database name to each regular expression and use the first match to determine which database a physical table should be created in.</p> <p>For example, this configuration places every model's physical table in the <code>model_physical_tables</code> database because the regular expression <code>.*</code> matches any database name:</p> <pre><code>physical_schema_mapping:\n  '.*': model_physical_tables\n</code></pre>"},{"location":"integrations/engines/clickhouse/#cluster-specification","title":"Cluster specification","text":"<p>A ClickHouse cluster allows multiple networked ClickHouse servers to operate on the same data object. Every cluster must be named in the ClickHouse configuration files, and that name is passed to a table's DDL statements in the <code>ON CLUSTER</code> clause.</p> <p>For example, we could create a table <code>my_schema.my_table</code> on cluster <code>TheCluster</code> like this: <code>CREATE TABLE my_schema.my_table ON CLUSTER TheCluster (col1 Int8)</code>.</p> <p>To create Vulcan objects on a cluster, provide the cluster name to the <code>cluster</code> key in the Vulcan connection definition (see all connection parameters below).</p> <p>Vulcan will automatically inject the <code>ON CLUSTER</code> clause and cluster name you provide into all project DDL statements.</p>"},{"location":"integrations/engines/clickhouse/#model-definition","title":"Model definition","text":"<p>This section describes how you control a table's engine and other ClickHouse-specific functionality in Vulcan models.</p>"},{"location":"integrations/engines/clickhouse/#table-engine","title":"Table engine","text":"<p>Vulcan uses the <code>MergeTree</code> table engine with an empty <code>ORDER BY</code> clause by default.</p> <p>Specify a different table engine by passing the table engine definition to the model DDL's <code>storage_format</code> parameter. For example, you could specify the <code>Log</code> table engine like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    storage_format Log,\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>You may also specify more complex table engine definitions. For example:</p> <pre><code>MODEL (\n    name my_schema.my_rep_table,\n    kind full,\n    storage_format ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name', '{replica}', ver),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre>"},{"location":"integrations/engines/clickhouse/#order-by","title":"ORDER BY","text":"<p><code>MergeTree</code> family engines require that a table's <code>CREATE</code> statement include the <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY ()</code> when creating a table with an engine in the <code>MergeTree</code> family. This creates the table without any ordering.</p> <p>You may specify columns/expressions to <code>ORDER BY</code> by passing them to the model <code>physical_properties</code> dictionary's <code>order_by</code> key.</p> <p>For example, you could order by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2)\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>order_by</code> key name and value <code>(col1, col2)</code>.</p> <p>Complex <code>ORDER BY</code> expressions may need to be passed in single quotes, with interior single quotes escaped by the <code>\\</code> character.</p>"},{"location":"integrations/engines/clickhouse/#primary-key","title":"PRIMARY KEY","text":"<p>Table engines may also accept a <code>PRIMARY KEY</code> specification. Similar to <code>ORDER BY</code>, specify a primary key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>primary_key</code> key name and value <code>col1</code>.</p>"},{"location":"integrations/engines/clickhouse/#ttl","title":"TTL","text":"<p>ClickHouse tables accept a TTL expression that triggers actions like deleting rows after a certain amount of time has passed.</p> <p>Similar to <code>ORDER_BY</code> and <code>PRIMARY_KEY</code>, specify a TTL key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1,\n        ttl = timestamp + INTERVAL 1 WEEK\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>ttl</code> key name and value <code>timestamp + INTERVAL 1 WEEK</code>.</p>"},{"location":"integrations/engines/clickhouse/#partitioning","title":"Partitioning","text":"<p>Some ClickHouse table engines support partitioning. Specify the partitioning columns/expressions in the model DDL's <code>partitioned_by</code> key.</p> <p>For example, you could partition by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    partitioned_by (col1, col2),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Learn more below about how Vulcan uses partitioned tables to improve performance.</p>"},{"location":"integrations/engines/clickhouse/#settings","title":"Settings","text":"<p>ClickHouse supports an immense number of settings, many of which can be altered in multiple places: ClickHouse configuration files, Python client connection arguments, DDL statements, SQL queries, and others.</p> <p>This section discusses how to control ClickHouse settings in Vulcan.</p>"},{"location":"integrations/engines/clickhouse/#connection-settings","title":"Connection settings","text":"<p>Vulcan connects to Python with the <code>clickhouse-connect</code> library. Its connection method accepts a dictionary of arbitrary settings that are passed to ClickHouse.</p> <p>Specify these settings in the <code>connection_settings</code> key. This example demonstrates how to set the <code>distributed_ddl_task_timeout</code> setting to <code>300</code>:</p> <pre><code>clickhouse_gateway:\n  connection:\n    type: clickhouse\n    host: localhost\n    port: 8123\n    username: user\n    password: pw\n    connection_settings:\n      distributed_ddl_task_timeout: 300\n  state_connection:\n    type: duckdb\n</code></pre>"},{"location":"integrations/engines/clickhouse/#ddl-settings","title":"DDL settings","text":"<p>ClickHouse settings may also be specified in DDL commands like <code>CREATE</code>.</p> <p>Specify these settings in a model DDL's <code>physical_properties</code> key (where the <code>order_by</code> and <code>primary_key</code> values are specified, if present).</p> <p>This example demonstrates how to set the <code>index_granularity</code> setting to <code>128</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        index_granularity = 128\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>index_granularity</code> key name and value <code>128</code>.</p>"},{"location":"integrations/engines/clickhouse/#query-settings","title":"Query settings","text":"<p>ClickHouse settings may be specified directly in a model's query with the <code>SETTINGS</code> keyword.</p> <p>This example demonstrates setting the <code>join_use_nulls</code> setting to <code>1</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n);\n\nselect\n    *\nfrom other_schema.other_table\nSETTINGS join_use_nulls = 1;\n</code></pre> <p>Multiple settings may be specified in a query with repeated use of the <code>SETTINGS</code> keyword: <code>SELECT * FROM other_table SETTINGS first_setting = 1 SETTINGS second_setting = 2;</code>.</p>"},{"location":"integrations/engines/clickhouse/#usage-by-vulcan","title":"Usage by Vulcan","text":"<p>The ClickHouse setting <code>join_use_nulls</code> affects the behavior of Vulcan SCD models and table diffs. This section describes how Vulcan uses query settings to control that behavior.</p> <p>Background</p> <p>In general, table <code>JOIN</code>s can return empty cells for rows not present in both tables.</p> <p>For example, consider <code>LEFT JOIN</code>ing two tables <code>left</code> and <code>right</code>, where the column <code>right_column</code> is only present in the <code>right</code> table. Any rows only present in the <code>left</code> table will have no value for <code>right_column</code> in the joined table.</p> <p>In other SQL engines, those empty cells are filled with <code>NULL</code>s.</p> <p>In contrast, ClickHouse fills the empty cells with data type-specific default values (e.g., 0 for integer column types). It will instead fill the cells with <code>NULL</code>s if you set the <code>join_use_nulls</code> setting to <code>1</code>.</p> <p>Vulcan</p> <p>Vulcan automatically generates SQL queries for both SCD Type 2 models and table diff comparisons. These queries include table <code>JOIN</code>s and calculations based on the presence of <code>NULL</code> values.</p> <p>Because those queries expect <code>NULL</code> values in empty cells, Vulcan automatically adds <code>SETTINGS join_use_nulls = 1</code> to the generated SCD and table diff SQL code.</p> <p>The SCD model definition query is embedded as a CTE in the full Vulcan-generated query. If run alone, the model definition query would use the ClickHouse server's current <code>join_use_nulls</code> value.</p> <p>If that value is not <code>1</code>, the Vulcan setting on the outer query would override the server value and produce incorrect results.</p> <p>Therefore, Vulcan uses the following procedure to ensure the model definition query runs with the correct <code>join_use_nulls</code> value:</p> <ul> <li>If the model query sets <code>join_use_nulls</code> itself, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>1</code>, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>0</code>, add <code>SETTINGS join_use_nulls = 0</code> to the CTE model query<ul> <li>All other CTEs and the outer query will still execute with a <code>join_use_nulls</code> value of <code>1</code></li> </ul> </li> </ul>"},{"location":"integrations/engines/clickhouse/#performance-considerations","title":"Performance considerations","text":"<p>ClickHouse is optimized for writing/reading records, so deleting/replacing records can be extremely slow.</p> <p>This section describes why Vulcan needs to delete/replace records and how the ClickHouse engine adapter works around the limitations.</p>"},{"location":"integrations/engines/clickhouse/#why-delete-or-replace","title":"Why delete or replace?","text":"<p>Vulcan \"materializes\" model kinds in a number of ways, such as:</p> <ul> <li>Replacing an entire table (<code>FULL</code> models)</li> <li>Replacing records in a specific time range (<code>INCREMENTAL_BY_TIME_RANGE</code> models)</li> <li>Replacing records with specific key values (<code>INCREMENTAL_BY_UNIQUE_KEY</code> models)</li> <li>Replacing records in specific partitions (<code>INCREMENTAL_BY_PARTITION</code> models)</li> </ul> <p>Different SQL engines provide different methods for performing record replacement.</p> <p>Some engines natively support updating or inserting (\"upserting\") records. For example, in some engines you can <code>merge</code> a new table into an existing table based on a key. Records in the new table whose keys are already in the existing table will update/replace the existing records. Records in the new table without keys in the existing table will be inserted into the existing table.</p> <p>Other engines do not natively support upserts, so Vulcan replaces records in two steps: delete the records to update/replace from the existing table, then insert the new records.</p> <p>ClickHouse does not support upserts, and it performs the two step delete/insert operation so slowly as to be unusable. Therefore, Vulcan uses a different method for replacing records.</p>"},{"location":"integrations/engines/clickhouse/#temp-table-swap","title":"Temp table swap","text":"<p>Vulcan uses what we call the \"temp table swap\" method of replacing records in ClickHouse.</p> <p>Because ClickHouse is optimized for writing and reading records, it is often faster to copy most of a table than to delete a small portion of its records. That is the approach used by the temp table swap method (with optional performance improvements for partitioned tables).</p> <p>The temp table swap has four steps:</p> <ol> <li>Make an empty temp copy of the existing table that has the same structure (columns, data types, table engine, etc.)</li> <li>Insert new records into the temp table</li> <li>Insert the existing records that should be kept into the temp table</li> <li>Swap the table names, such that the temp table now has the existing table's name</li> </ol> <p>Figure 1 illustrates these four steps: </p> <p> Figure 1: steps to execute a temp table swap </p> <p>The weakness of this method is that it requires copying all existing rows to keep (step three), which can be problematic for large tables.</p> <p>To address this weakness, Vulcan instead uses partition swapping if a table is partitioned.</p>"},{"location":"integrations/engines/clickhouse/#partition-swap","title":"Partition swap","text":"<p>ClickHouse supports partitioned tables, which store groups of records in separate files, or \"partitions.\"</p> <p>A table is partitioned based on a table column or SQL expression - the \"partitioning key.\" All records with the same value for the partitioning key are stored together in a partition.</p> <p>For example, consider a table containing each record's creation date in a datetime column. If we partition the table by month, all the records whose timestamp was in January will be stored in one partition, records from February in another partition, and so on.</p> <p>Table partitioning provides a major benefit for improving swap performance: records can be inserted, updated, or deleted in individual partitions.</p> <p>Vulcan leverages this to avoid copying large numbers of existing records into a temp table. Instead, it only copies the records that are in partitions affected by a load's newly ingested records.</p> <p>Vulcan automatically uses partition swapping for any incremental model that specifies the <code>partitioned_by</code> key.</p>"},{"location":"integrations/engines/clickhouse/#choosing-a-partitioning-key","title":"Choosing a partitioning key","text":"<p>The first step of partitioning a table is choosing its partitioning key (columns or expression). The primary consideration for a key is the total number of partitions it will generate, which affects table performance.</p> <p>Too many partitions can drastically decrease performance because the overhead of handling partition files swamps the benefits of copying fewer records. Too few partitions decreases swap performance because many existing records must still be copied in each incremental load.</p> <p>How many partitions is too many?</p> <p>ClickHouse's documentation specifically warns against tables having too many partitions, suggesting a maximum of 1000.</p> <p>The total number of partitions in a table is determined by the actual data in the table, not by the partition column/expression alone.</p> <p>For example, consider a table partitioned by date. If we insert records created on <code>2024-10-23</code>, the table will have one partition. If we then insert records from <code>2024-10-24</code>, the table will have two partitions. One partition is created for each unique value of the key.</p> <p>For each partitioned table in your project, carefully consider the number of partitions created by the combination of your partitioning expression and the characteristics of your data.</p>"},{"location":"integrations/engines/clickhouse/#incremental-by-time-models","title":"Incremental by time models","text":"<p><code>INCREMENTAL_BY_TIME_RANGE</code> kind models must be partitioned by time. If the model's <code>time_column</code> is not present in any <code>partitioned_by</code> expression, Vulcan will automatically add it as the first partitioning expression.</p> <p>By default, <code>INCREMENTAL_BY_TIME_RANGE</code> models partition by week, so the maximum recommended 1000 partitions corresponds to about 19 years of data. Vulcan projects have widely varying time ranges and data sizes, so you should choose a model's partitioning key based on the data your system will process.</p> <p>If a model has many records in each partition, you may see additional performance benefits by including the time column in the model's <code>ORDER_BY</code> expression.</p> <p>Partitioning by time</p> <p><code>INCREMENTAL_BY_TIME_RANGE</code> models must be partitioned by time.</p> <p>Vulcan will automatically partition them by week unless the <code>partitioned_by</code> configuration key includes the time column or an expression based on it.</p> <p>Choose a model's time partitioning granularity based on the characteristics of the data it will process, making sure the total number of partitions is 1000 or fewer.</p>"},{"location":"integrations/engines/clickhouse/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>clickhouse</code></p> Option Description Type Required <code>type</code> Engine type name - must be <code>clickhouse</code> string Y <code>host</code> ClickHouse server hostname or IP address string Y <code>username</code> ClickHouse user name string Y <code>password</code> ClickHouse user password string N <code>port</code> The ClickHouse HTTP or HTTPS port (Default: <code>8123</code>) int N <code>cluster</code> ClickHouse cluster name string N <code>connect_timeout</code> Connection timeout in seconds (Default: <code>10</code>) int N <code>send_receive_timeout</code> Send/receive timeout in seconds (Default: <code>300</code>) int N <code>query_limit</code> Query result limit (Default: <code>0</code> - no limit) int N <code>use_compression</code> Whether to use compression (Default: <code>True</code>) bool N <code>compression_method</code> Compression method to use string N <code>http_proxy</code> HTTP proxy address (equivalent to setting the HTTP_PROXY environment variable) string N <code>verify</code> Verify server TLS/SSL certificate (Default: <code>True</code>) bool N <code>ca_cert</code> Ignored if verify is <code>False</code>. If verify is <code>True</code>, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Not necessary if the ClickHouse server certificate is a globally trusted root as verified by the operating system. string N <code>client_cert</code> File path to a TLS Client certificate in .pem format (for mutual TLS authentication). The file should contain a full certificate chain, including any intermediate certificates. string N <code>client_cert_key</code> File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file. string N <code>https_proxy</code> HTTPS proxy address (equivalent to setting the HTTPS_PROXY environment variable) string N <code>server_host_name</code> The ClickHouse server hostname as identified by the CN or SNI of its TLS certificate. Set this to avoid SSL errors when connecting through a proxy or tunnel with a different hostname. string N <code>tls_mode</code> Controls advanced TLS behavior. proxy and strict do not invoke ClickHouse mutual TLS connection, but do send client cert and key. mutual assumes ClickHouse mutual TLS auth with a client certificate. string N <code>connection_settings</code> Additional connection settings dict N <code>connection_pool_options</code> Additional options                                                                                                                                         for the HTTP connection pool dict N"},{"location":"integrations/engines/databricks/","title":"Databricks","text":""},{"location":"integrations/engines/databricks/#databricks","title":"Databricks","text":"<p>This page provides information about how to use Vulcan with the Databricks SQL engine. It begins with a description of the three methods for connecting Vulcan to Databricks.</p> <p>After that is a Connection Quickstart that demonstrates how to connect to Databricks, or you can skip directly to information about using Databricks with the built-in.</p>"},{"location":"integrations/engines/databricks/#databricks-connection-methods","title":"Databricks connection methods","text":"<p>Databricks provides multiple computing options and connection methods. This section describes the three methods for connecting with Vulcan.</p>"},{"location":"integrations/engines/databricks/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Vulcan connects to Databricks with the Databricks SQL Connector library by default.</p> <p>The SQL Connector is bundled with Vulcan and automatically installed when you include the <code>databricks</code> extra in the command <code>pip install \"vulcan[databricks]\"</code>.</p> <p>The SQL Connector has all the functionality needed for Vulcan to execute SQL models on Databricks and Python models that do not return PySpark DataFrames.</p> <p>If you have Python models returning PySpark DataFrames, check out the Databricks Connect section.</p>"},{"location":"integrations/engines/databricks/#databricks-connect","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan must use the Databricks Connect library to connect to Databricks (instead of the Databricks SQL Connector library).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>Find more configuration details below.</p>"},{"location":"integrations/engines/databricks/#databricks-notebook-interface","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly in a Databricks Cluster interface (like in a Databricks Notebook using the notebook magic commands), the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>Find more configuration details below.</p>"},{"location":"integrations/engines/databricks/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Databricks.</p> <p>It demonstrates connecting to a Databricks All-Purpose Compute instance with the <code>databricks-sql-connector</code> Python library bundled with Vulcan.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"integrations/engines/databricks/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Databricks account with access to an appropriate Databricks Workspace<ul> <li>The Workspace must support authenticating with personal access tokens (Databricks Community Edition workspaces do not)</li> <li>Your account must have Workspace Access and Create Compute permissions (these permissions are enabled by default)</li> </ul> </li> <li>Your Databricks compute resources have Unity Catalog activated</li> <li>Your computer has Vulcan installed with the Databricks extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[databricks]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init duckdb</code></li> </ul> </li> </ol> <p>Unity Catalog required</p> <p>Databricks compute resources used by Vulcan must have Unity Catalog activated.</p>"},{"location":"integrations/engines/databricks/#get-connection-info","title":"Get connection info","text":"<p>The first step to configuring a Databricks connection is gathering the necessary information from your Databricks compute instance.</p>"},{"location":"integrations/engines/databricks/#create-compute","title":"Create Compute","text":"<p>We must have something to connect to, so we first create and activate a Databricks compute instance. If you already have one running, skip to the next section.</p> <p>We begin in the default view for our Databricks Workspace. Access the Compute view by clicking the <code>Compute</code> entry in the left-hand menu:</p> <p></p> <p>In the Compute view, click the <code>Create compute</code> button:</p> <p></p> <p>Modify compute cluster options if desired and click the <code>Create compute</code> button:</p> <p></p>"},{"location":"integrations/engines/databricks/#get-jdbcodbc-info","title":"Get JDBC/ODBC info","text":"<p>Scroll to the bottom of the view and click the open the <code>Advanced Options</code> view:</p> <p></p> <p>Click the <code>JDBC/ODBC</code> tab:</p> <p></p> <p>Open your project's <code>config.yaml</code> configuration file in a text editor and add a new gateway named <code>databricks</code> below the existing <code>local</code> gateway:</p> <p></p> <p>Copy the <code>server_hostname</code> and <code>http_path</code> connection values from the Databricks JDBC/ODBC tab to the <code>config.yaml</code> file:</p> <p></p>"},{"location":"integrations/engines/databricks/#get-personal-access-token","title":"Get personal access token","text":"<p>The final piece of information we need for the <code>config.yaml</code> file is your personal access token.</p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre> <p> To create a personal access token, click on your profile logo and go to your profile's <code>Settings</code> page:</p> <p></p> <p>Go to the <code>Developer</code> view in the User menu. Depending on your account's role, your page may not display the Workspace Admin section of the page.</p> <p></p> <p>Click the <code>Manage</code> button in the Access Tokens section:</p> <p></p> <p>Click the <code>Generate new token</code> button:</p> <p></p> <p>Name your token in the <code>Comment</code> field, and click the <code>Generate</code> button:</p> <p></p> <p>Click the copy button and paste the token into the <code>access_token</code> key:</p> <p></p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre>"},{"location":"integrations/engines/databricks/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>databricks</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Databricks. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan --gateway databricks info</code>.</p> <p>We manually specify the <code>databricks</code> gateway because it is not our project's default gateway:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Databricks SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Databricks is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"integrations/engines/databricks/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>databricks</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>databricks_state.db</code> file:</p> <p></p> <p>Now we no longer see the warning when running <code>vulcan --gateway databricks info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"integrations/engines/databricks/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>For convenience, we can omit the <code>--gateway</code> option from our CLI commands by specifying <code>databricks</code> as our project's <code>default_gateway</code>:</p> <p></p> <p>And run a <code>vulcan plan</code> in Databricks:</p> <p></p> <p>And confirm that our schemas and objects exist in the Databricks catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Databricks!</p> <p>Tip</p> <p>Vulcan connects to your Databricks Cluster's default catalog by default. Connect to a different catalog by specifying its name in the connection configuration's <code>catalog</code> parameter.</p>"},{"location":"integrations/engines/databricks/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>databricks</code></p>"},{"location":"integrations/engines/databricks/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[databricks]\"\n</code></pre>"},{"location":"integrations/engines/databricks/#connection-method-details","title":"Connection method details","text":"<p>Databricks provides multiple computing options and connection methods. The section above explains how to use them with Vulcan, and this section provides additional configuration details.</p>"},{"location":"integrations/engines/databricks/#databricks-sql-connector_1","title":"Databricks SQL Connector","text":"<p>Vulcan uses the Databricks SQL Connector to connect to Databricks by default. Learn more above.</p>"},{"location":"integrations/engines/databricks/#databricks-connect_1","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan needs to use the Databricks Connect to connect to Databricks (instead of the Databricks SQL Connector).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>If Vulcan detects that you have Databricks Connect installed, then it will automatically configure the connection and use it for all Python models that return a Pandas or PySpark DataFrame.</p> <p>To have databricks-connect installed but ignored by Vulcan, set <code>disable_databricks_connect</code> to <code>true</code> in the connection configuration.</p> <p>Databricks Connect can execute SQL and DataFrame operations on different clusters by setting the Vulcan <code>databricks_connect_*</code> connection options. For example, these options could configure Vulcan to run SQL on a Databricks SQL Warehouse while still routing DataFrame operations to a normal Databricks Cluster.</p> <p>Note</p> <p>If using Databricks Connect, make sure to learn about the Databricks requirements and limitations.</p>"},{"location":"integrations/engines/databricks/#databricks-notebook-interface_1","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly on a Databricks Cluster (like in a Databricks Notebook using the notebook magic commands), the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>The only relevant Vulcan configuration parameter is the optional <code>catalog</code> parameter.</p>"},{"location":"integrations/engines/databricks/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>databricks</code> string Y <code>server_hostname</code> Databricks instance host name string N <code>http_path</code> HTTP path, either to a DBSQL endpoint (such as <code>/sql/1.0/endpoints/1234567890abcdef</code>) or to an All-Purpose cluster (such as <code>/sql/protocolv1/o/1234567890123456/1234-123456-slid123</code>) string N <code>access_token</code> HTTP Bearer access token, such as Databricks Personal Access Token string N <code>catalog</code> The name of the catalog to use for the connection. Defaults to use Databricks cluster default. string N <code>auth_type</code> SQL Connector Only: Set to 'databricks-oauth' or 'azure-oauth' to trigger OAuth (or dont set at all to use <code>access_token</code>) string N <code>oauth_client_id</code> SQL Connector Only: Optional M2M OAuth Client ID to use when <code>auth_type</code> is set string N <code>oauth_client_secret</code> SQL Connector Only: Optional M2M OAuth Client Secret to use when <code>auth_type</code> is set string N <code>http_headers</code> SQL Connector Only: An optional dictionary of HTTP headers that will be set on every request dict N <code>session_configuration</code> SQL Connector Only: An optional dictionary of Spark session parameters. Execute the SQL command <code>SET -v</code> to get a full list of available commands. dict N <code>databricks_connect_server_hostname</code> Databricks Connect Only: Databricks Connect server hostname. Uses <code>server_hostname</code> if not set. string N <code>databricks_connect_access_token</code> Databricks Connect Only: Databricks Connect access token. Uses <code>access_token</code> if not set. string N <code>databricks_connect_cluster_id</code> Databricks Connect Only: Databricks Connect cluster ID. Uses <code>http_path</code> if not set. Cannot be a Databricks SQL Warehouse. string N <code>databricks_connect_use_serverless</code> Databricks Connect Only: Use a serverless cluster for Databricks Connect instead of <code>databricks_connect_cluster_id</code>. bool N <code>force_databricks_connect</code> When running locally, force the use of Databricks Connect for all model operations (so don't use SQL Connector for SQL models) bool N <code>disable_databricks_connect</code> When running locally, disable the use of Databricks Connect for all model operations (so use SQL Connector for all models) bool N <code>disable_spark_session</code> Do not use SparkSession if it is available (like when running in a notebook). bool N"},{"location":"integrations/engines/databricks/#model-table-properties-to-support-altering-tables","title":"Model table properties to support altering tables","text":"<p>If you are making a change to the structure of a table that is forward only, then you may need to add the following to your model's <code>physical_properties</code>:</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    ...\n    physical_properties (\n        'delta.columnMapping.mode' = 'name'\n    ),\n)\n</code></pre> <p>If you attempt to alter without having this property set, you will get an error similar to <code>databricks.sql.exc.ServerOperationError: [DELTA_UNSUPPORTED_DROP_COLUMN] DROP COLUMN is not supported for your Delta table.</code>. Databricks Documentation for more details.</p>"},{"location":"integrations/engines/duckdb/","title":"DuckDB","text":""},{"location":"integrations/engines/duckdb/#duckdb","title":"DuckDB","text":"<p>DuckDB state connection limitations</p> <p>DuckDB is a single user database. Using it for a state connection in your Vulcan project limits you to a single workstation. This means your project cannot be shared amongst your team members or your CI/CD infrastructure. This is usually fine for proof of concept or test projects but it will not scale to production usage.</p> <p>For production projects, use Tobiko Cloud or a more robust state database such as Postgres.</p>"},{"location":"integrations/engines/duckdb/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>duckdb</code></p>"},{"location":"integrations/engines/duckdb/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>duckdb</code> string Y <code>database</code> The optional database name. If not specified, the in-memory database is used. Cannot be defined if using <code>catalogs</code>. string N <code>catalogs</code> Mapping to define multiple catalogs. Can attach DuckDB catalogs or catalogs for other connections. First entry is the default catalog. Cannot be defined if using <code>database</code>. dict N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g., S3) using DuckDB secrets. Can be a list of secret configurations or a dictionary with custom secret names. list/dict N <code>filesystems</code> Configuration for registering <code>fsspec</code> filesystems to the DuckDB connection. dict N"},{"location":"integrations/engines/duckdb/#duckdb-catalogs-example","title":"DuckDB Catalogs Example","text":"<p>This example specifies two catalogs. The first catalog is named \"persistent\" and maps to the DuckDB file database <code>local.duckdb</code>. The second catalog is named \"ephemeral\" and maps to the DuckDB in-memory database.</p> <p><code>persistent</code> is the default catalog since it is the first entry in the dictionary. Vulcan will place models without an explicit catalog, such as <code>my_schema.my_model</code>, into the <code>persistent</code> catalog <code>local.duckdb</code> DuckDB file database.</p> <p>Vulcan will place models with the explicit catalog \"ephemeral\", such as <code>ephemeral.other_schema.other_model</code>, into the <code>ephemeral</code> catalog DuckDB in-memory database.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        persistent: 'local.duckdb'\n        ephemeral: ':memory:'\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"persistent\": \"local.duckdb\"\n                    \"ephemeral\": \":memory:\"\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"integrations/engines/duckdb/#ducklake-catalog-example","title":"DuckLake Catalog Example","text":"YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: 'catalog.ducklake'\n          data_path: data/ducklake\n          encrypted: True\n          data_inlining_row_limit: 10\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"ducklake\": DuckDBAttachOptions(\n                        type=\"ducklake\",\n                        path=\"catalog.ducklake\",\n                        data_path=\"data/ducklake\",\n                        encrypted=True,\n                        data_inlining_row_limit=10,\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"integrations/engines/duckdb/#other-connection-catalogs-example","title":"Other Connection Catalogs Example","text":"<p>Catalogs can also be defined to connect to anything that DuckDB can be attached to.</p> <p>Below are examples of connecting to a SQLite database and a PostgreSQL database. The SQLite database is read-write, while the PostgreSQL database is read-only.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        memory: ':memory:'\n        sqlite:\n          type: sqlite\n          path: 'test.db'\n        postgres:\n          type: postgres\n          path: 'dbname=postgres user=postgres host=127.0.0.1'\n          read_only: true\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"memory\": \":memory:\",\n                    \"sqlite\": DuckDBAttachOptions(\n                        type=\"sqlite\",\n                        path=\"test.db\"\n                    ),\n                    \"postgres\": DuckDBAttachOptions(\n                        type=\"postgres\",\n                        path=\"dbname=postgres user=postgres host=127.0.0.1\",\n                        read_only=True\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"integrations/engines/duckdb/#catalogs-for-postgresql","title":"Catalogs for PostgreSQL","text":"<p>In PostgreSQL, the catalog name must match the actual catalog name it is associated with, as shown in the example above, where the database name (<code>dbname</code> in the path) is the same as the catalog name.</p>"},{"location":"integrations/engines/duckdb/#connectors-without-schemas","title":"Connectors without schemas","text":"<p>Some connections, like SQLite, do not support schema names and therefore objects will be attached under the default schema name of <code>main</code>.</p> <p>Example: mounting a SQLite database with the name <code>sqlite</code> that has a table <code>example_table</code> will be accessible as <code>sqlite.main.example_table</code>.</p>"},{"location":"integrations/engines/duckdb/#sensitive-fields-in-paths","title":"Sensitive fields in paths","text":"<p>If a connector, like Postgres, requires sensitive information in the path, it might support defining environment variables instead. See DuckDB Documentation for more information.</p>"},{"location":"integrations/engines/duckdb/#cloud-service-authentication","title":"Cloud service authentication","text":"<p>DuckDB can read data directly from cloud services via extensions (e.g., httpfs, azure).</p> <p>The <code>secrets</code> option allows you to configure DuckDB's Secrets Manager to authenticate with external services like S3. This is the recommended approach for cloud storage authentication in DuckDB v0.10.0 and newer, replacing the legacy authentication method via variables.</p>"},{"location":"integrations/engines/duckdb/#secrets-configuration","title":"Secrets Configuration","text":"<p>The <code>secrets</code> option supports two formats:</p> <ol> <li>List format (default secrets): A list of secret configurations where each secret uses DuckDB's default naming</li> <li>Dictionary format (named secrets): A dictionary where keys are custom secret names and values are the secret configurations</li> </ol> <p>This flexibility allows you to organize multiple secrets of the same type or reference specific secrets by name in your SQL queries.</p>"},{"location":"integrations/engines/duckdb/#list-format-example-default-secrets","title":"List Format Example (Default Secrets)","text":"<p>Using a list creates secrets with DuckDB's default naming:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        - type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets=[\n                    {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    }\n                ]\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"integrations/engines/duckdb/#dictionary-format-example-named-secrets","title":"Dictionary Format Example (Named Secrets)","text":"<p>Using a dictionary allows you to assign custom names to your secrets for better organization and reference:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        my_s3_secret:\n          type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n        my_azure_secret:\n          type: azure\n          account_name: \"YOUR_AZURE_ACCOUNT\"\n          account_key: \"YOUR_AZURE_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets={\n                    \"my_s3_secret\": {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    },\n                    \"my_azure_secret\": {\n                        \"type\": \"azure\",\n                        \"account_name\": \"YOUR_AZURE_ACCOUNT\",\n                        \"account_key\": \"YOUR_AZURE_KEY\"\n                    }\n                }\n            )\n        ),\n    }\n)\n</code></pre> <p>After configuring the secrets, you can directly reference S3 paths in your catalogs or in SQL queries without additional authentication steps.</p> <p>Refer to the official DuckDB documentation for the full list of supported S3 secret parameters and for more information on the Secrets Manager configuration.</p> <p>Note: Loading credentials at runtime using <code>load_aws_credentials()</code> or similar deprecated functions may fail when using Vulcan.</p>"},{"location":"integrations/engines/duckdb/#file-system-configuration-example-for-microsoft-onelake","title":"File system configuration example for Microsoft Onelake","text":"<p>The <code>filesystems</code> accepts a list of file systems to register in the DuckDB connection. This is especially useful for Azure Storage Accounts, as it adds write support for DuckDB which is not natively supported by DuckDB (yet).</p> YAML <pre><code>gateways:\n  ducklake:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: myducklakecatalog.duckdb\n          data_path: abfs://MyFabricWorkspace/MyFabricLakehouse.Lakehouse/Files/DuckLake.Files\n    extensions:\n      - ducklake\n    filesystems:\n      - fs: abfs\n        account_name: onelake\n        account_host: onelake.blob.fabric.microsoft.com\n        client_id: {{ env_var('AZURE_CLIENT_ID') }}\n        client_secret: {{ env_var('AZURE_CLIENT_SECRET') }}\n        tenant_id: {{ env_var('AZURE_TENANT_ID') }}\n        # anon: False # To use azure.identity.DefaultAzureCredential authentication \n</code></pre> <p>Refer to the documentation for <code>fsspec</code> fsspec.filesystem and <code>adlfs</code> adlfs.AzureBlobFileSystem for a full list of storage options. </p>"},{"location":"integrations/engines/fabric/","title":"Fabric","text":""},{"location":"integrations/engines/fabric/#fabric","title":"Fabric","text":"<p>Info</p> <p>The Fabric engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"integrations/engines/fabric/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>fabric</code></p> <p>NOTE: Fabric Warehouse is not recommended to be used for the Vulcan state connection.</p>"},{"location":"integrations/engines/fabric/#installation","title":"Installation","text":""},{"location":"integrations/engines/fabric/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[fabric]\"\n</code></pre>"},{"location":"integrations/engines/fabric/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>fabric</code> string Y <code>host</code> The hostname of the Fabric Warehouse server string Y <code>user</code> The client id to use for authentication with the Fabric Warehouse server string N <code>password</code> The client secret to use for authentication with the Fabric Warehouse server string N <code>port</code> The port number of the Fabric Warehouse server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pyodbc string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>tenant_id</code> The Azure / Entra tenant UUID string Y <code>workspace_id</code> The Fabric workspace UUID. The preferred way to retrieve it is by running <code>notebookutils.runtime.context.get(\"currentWorkspaceId\")</code> in a python notebook. string Y <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"integrations/engines/gcp-postgres/","title":"GCP Postgres","text":""},{"location":"integrations/engines/gcp-postgres/#gcp-postgres","title":"GCP Postgres","text":""},{"location":"integrations/engines/gcp-postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>gcp_postgres</code></p>"},{"location":"integrations/engines/gcp-postgres/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[gcppostgres]\"\n</code></pre>"},{"location":"integrations/engines/gcp-postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>gcp_postgres</code> string Y <code>instance_connection_string</code> Connection name for the postgres instance string Y <code>user</code> The username (postgres or IAM) to use for authentication string Y <code>password</code> The password to use for authentication. Required when connecting as a Postgres user string N <code>enable_iam_auth</code> Enables IAM authentication. Required when connecting as an IAM user boolean N <code>keyfile</code> Path to the keyfile to be used with enable_iam_auth instead of ADC string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>db</code> The name of the database instance to connect to string Y <code>ip_type</code> The IP type to use for the connection. Must be one of <code>public</code>, <code>private</code>, or <code>psc</code>. Default: <code>public</code> string N <code>timeout</code> The connection timeout in seconds. Default: <code>30</code> integer N <code>scopes</code> The scopes to use for the connection. Default: <code>(https://www.googleapis.com/auth/sqlservice.admin,)</code> tuple[str] N <code>driver</code> The driver to use for the connection. Default: <code>pg8000</code>. Note: only <code>pg8000</code> is tested string N"},{"location":"integrations/engines/motherduck/","title":"MotherDuck","text":""},{"location":"integrations/engines/motherduck/#motherduck","title":"MotherDuck","text":"<p>This page provides information about how to use Vulcan with MotherDuck.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to MotherDuck, or you can skip directly to information about using MotherDuck with the built-in scheduler.</p>"},{"location":"integrations/engines/motherduck/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with MotherDuck.</p> <p>It demonstrates connecting to MotherDuck with the <code>duckdb</code> library bundled with Vulcan.</p> <p>MotherDuck provides a single way to authorize a connection. This quickstart demonstrates authenticating with a token.</p> <p>Tip</p> <p>This quick start assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you\u2019re not familiar, work through the Vulcan Quickstart before continuing.</p>"},{"location":"integrations/engines/motherduck/#prerequisites","title":"Prerequisites","text":"<p>Before working through this quickstart guide, ensure that:</p> <ol> <li>You have a motherduck account and an access token.</li> <li>Your computer has Vulcan installed with the DuckDB extra available.</li> <li>Install from command line with the command <code>pip install \u201cvulcan[duckdb]\u201d</code></li> <li>You have initialized a Vulcan example project on your computer</li> <li>Open a command line interface and navigate to the directory where the project files should go.</li> <li>Initialize the project with the command <code>vulcan init duckdb</code>, since <code>duckdb</code> is the dialect.</li> </ol>"},{"location":"integrations/engines/motherduck/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access your MotherDuck databases. Since permission is granted to specific databases for a specific user, you should create a service account for Vulcan that will contain the credentials for writing to MotherDuck.</p>"},{"location":"integrations/engines/motherduck/#configure-the-connection","title":"Configure the connection","text":"<p>We now have what is required to configure Vulcan\u2019s connection to MotherDuck.</p> <p>We start the configuration by adding a gateway named <code>motherduck</code> to our example project\u2019s config.yaml file and making it our <code>default gateway</code>, as well as adding our token, persistent, and ephemeral catalogs.</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n        catalogs:\n          persistent: \"md:\"\n          ephemeral: \":memory:\"\n      token: &lt;your_token&gt;\n\ndefault_gateway: motherduck\n</code></pre> <p>Catalogs can be defined to connect to anything that DuckDB can be attached to.</p> <p>Warning</p> <p>Best practice for storing secrets like tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>MOTHERDUCK_TOKEN</code> for the configuration's <code>token</code> parameter:</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n      token: {{ env_var('MOTHERDUCK_TOKEN') }}\n</code></pre>"},{"location":"integrations/engines/motherduck/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>motherduck</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to MotherDuck. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p>"},{"location":"integrations/engines/motherduck/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in MotherDuck:</p> <p></p> <p>And confirm that our schemas and objects exist in the MotherDuck catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on MotherDuck!</p>"},{"location":"integrations/engines/motherduck/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>motherduck</code></p>"},{"location":"integrations/engines/motherduck/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>motherduck</code> string Y <code>database</code> The database name. string Y <code>token</code> The optional MotherDuck token. If not specified, the user will be prompted to login with their web browser. string N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g. S3) using DuckDB secrets. dict N"},{"location":"integrations/engines/mssql/","title":"MSSQL","text":""},{"location":"integrations/engines/mssql/#mssql","title":"MSSQL","text":""},{"location":"integrations/engines/mssql/#installation","title":"Installation","text":""},{"location":"integrations/engines/mssql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[mssql]\"\n</code></pre>"},{"location":"integrations/engines/mssql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[mssql-odbc]\"\n</code></pre>"},{"location":"integrations/engines/mssql/#incremental-by-unique-key-merge","title":"Incremental by unique key <code>MERGE</code>","text":"<p>Vulcan executes a <code>MERGE</code> statement to insert rows for incremental by unique key model kinds.</p> <p>By default, the <code>MERGE</code> statement updates all non-key columns of an existing row when a new row with the same key values is inserted. If all column values match between the two rows, those updates are unnecessary.</p> <p>Vulcan provides an optional performance optimization that skips unnecessary updates by comparing column values with the <code>EXISTS</code> and <code>EXCEPT</code> operators.</p> <p>Enable the optimization by setting the <code>mssql_merge_exists</code> key to <code>true</code> in the <code>physical_properties</code> section of the <code>MODEL</code> statement.</p> <p>For example:</p> <pre><code>MODEL (\n    name vulcan_example.unique_key,\n    kind INCREMENTAL_BY_UNIQUE_KEY (\n        unique_key id\n    ),\n    cron '@daily',\n    physical_properties (\n        mssql_merge_exists = true\n    )\n);\n</code></pre> <p>Not all column types supported</p> <p>The <code>mssql_merge_exists</code> optimization is not supported for all column types, including <code>GEOMETRY</code>, <code>XML</code>, <code>TEXT</code>, <code>NTEXT</code>, <code>IMAGE</code>, and most user-defined types.</p> <p>Learn more in the MSSQL <code>EXCEPT</code> statement documentation.</p>"},{"location":"integrations/engines/mssql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mssql</code></p>"},{"location":"integrations/engines/mssql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mssql</code> string Y <code>host</code> The hostname of the MSSQL server string Y <code>user</code> The username / client id to use for authentication with the MSSQL server string N <code>password</code> The password / client secret to use for authentication with the MSSQL server string N <code>port</code> The port number of the MSSQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection (e.g., ODBC Driver 18 for SQL Server). string N <code>odbc_properties</code> ODBC connection properties (e.g., authentication: ActiveDirectoryServicePrincipal). See more here. dict N"},{"location":"integrations/engines/mysql/","title":"MySQL","text":""},{"location":"integrations/engines/mysql/#mysql","title":"MySQL","text":""},{"location":"integrations/engines/mysql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mysql</code></p>"},{"location":"integrations/engines/mysql/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[mysql]\"\n</code></pre>"},{"location":"integrations/engines/mysql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mysql</code> string Y <code>host</code> The hostname of the MysQL server string Y <code>user</code> The username to use for authentication with the MySQL server string Y <code>password</code> The password to use for authentication with the MySQL server string Y <code>port</code> The port number of the MySQL server int N <code>charset</code> The character set used for the connection string N <code>ssl_disabled</code> Is SSL disabled bool N"},{"location":"integrations/engines/postgres/","title":"Postgres","text":""},{"location":"integrations/engines/postgres/#postgres","title":"Postgres","text":""},{"location":"integrations/engines/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"integrations/engines/postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"integrations/engines/redshift/","title":"Redshift","text":""},{"location":"integrations/engines/redshift/#redshift","title":"Redshift","text":""},{"location":"integrations/engines/redshift/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>redshift</code></p>"},{"location":"integrations/engines/redshift/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[redshift]\"\n</code></pre>"},{"location":"integrations/engines/redshift/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>redshift</code> string Y <code>user</code> The username to use for authentication with the Amazon Redshift cluster string N <code>password</code> The password to use for authentication with the Amazon Redshift cluster string N <code>database</code> The name of the database instance to connect to string N <code>host</code> The hostname of the Amazon Redshift cluster string N <code>port</code> The port number of the Amazon Redshift cluster int N <code>ssl</code> Is SSL enabled. SSL must be enabled when authenticating using IAM (Default: <code>True</code>) bool N <code>sslmode</code> The security of the connection to the Amazon Redshift cluster. <code>verify-ca</code> and <code>verify-full</code> are supported. string N <code>timeout</code> The number of seconds before the connection to the server will timeout. int N <code>tcp_keepalive</code> Is TCP keepalive used. (Default: <code>True</code>) bool N <code>application_name</code> The name of the application string N <code>preferred_role</code> The IAM role preferred for the current connection string N <code>principal_arn</code> The ARN of the IAM entity (user or role) for which you are generating a policy string N <code>credentials_provider</code> The class name of the IdP that will be used for authenticating with the Amazon Redshift cluster string N <code>region</code> The AWS region of the Amazon Redshift cluster string N <code>cluster_identifier</code> The cluster identifier of the Amazon Redshift cluster string N <code>iam</code> If IAM authentication is enabled. IAM must be True when authenticating using an IdP dict N <code>is_serverless</code> If the Amazon Redshift cluster is serverless (Default: <code>False</code>) bool N <code>serverless_acct_id</code> The account ID of the serverless cluster string N <code>serverless_work_group</code> The name of work group for serverless end point string N <code>enable_merge</code> Whether the incremental_by_unique_key model kind will use the native Redshift MERGE operation or Vulcan's logical merge. (Default: <code>False</code>) bool N"},{"location":"integrations/engines/redshift/#performance-considerations","title":"Performance Considerations","text":""},{"location":"integrations/engines/redshift/#timestamp-macro-variables-and-sort-keys","title":"Timestamp Macro Variables and Sort Keys","text":"<p>When working with Redshift tables that have a <code>TIMESTAMP</code> sort key, using the standard <code>@start_dt</code> and <code>@end_dt</code> macro variables may lead to performance issues. These macros render as <code>TIMESTAMP WITH TIME ZONE</code> values in SQL queries, which prevents Redshift from performing efficient pruning when filtering against <code>TIMESTAMP</code> (without timezone) sort keys.</p> <p>This can result in full table scans instead, causing significant performance degradation.</p> <p>Solution: Use the <code>_dtntz</code> (datetime no timezone) variants of macro variables:</p> <ul> <li><code>@start_dtntz</code> instead of <code>@start_dt</code></li> <li><code>@end_dtntz</code> instead of <code>@end_dt</code></li> </ul> <p>These variants render as <code>TIMESTAMP WITHOUT TIME ZONE</code>, allowing Redshift to properly utilize sort key optimizations.</p> <p>Example:</p> <pre><code>-- Inefficient: May cause full table scan\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dt\n  AND timestamp_column &lt; @end_dt\n\n-- Efficient: Uses sort key optimization\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dtntz\n  AND timestamp_column &lt; @end_dtntz\n\n-- Alternative: Cast to timestamp\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_ts::timestamp\n  AND timestamp_column &lt; @end_ts::timestamp\n</code></pre>"},{"location":"integrations/engines/risingwave/","title":"RisingWave","text":""},{"location":"integrations/engines/risingwave/#risingwave","title":"RisingWave","text":"<p>This page provides information about how to use Vulcan with the RisingWave streaming database engine.</p> <p>Info</p> <p>The RisingWave engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"integrations/engines/risingwave/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>risingwave</code></p>"},{"location":"integrations/engines/risingwave/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[risingwave]\"\n</code></pre>"},{"location":"integrations/engines/risingwave/#connection-options","title":"Connection options","text":"<p>RisingWave is based on Postgres and uses the same <code>psycopg2</code> connection library. Therefore, the connection parameters are very similar to Postgres.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>risingwave</code> string Y <code>host</code> The hostname of the RisingWave server string Y <code>user</code> The username to use for authentication with the RisingWave server string Y <code>password</code> The password to use for authentication with the RisingWave server string N <code>port</code> The port number of the RisingWave engine server int Y <code>database</code> The name of the database instance to connect to string Y <code>role</code> The role to use for authentication with the RisingWave server string N <code>sslmode</code> The security of the connection to the RisingWave server string N"},{"location":"integrations/engines/risingwave/#extra-features","title":"Extra Features","text":"<p>As a streaming database engine, RisingWave contains some extra features tailored specifically to streaming usecases.</p> <p>Primarily, these are:  - Sources which are used to stream records into RisingWave from streaming sources like Kafka  - Sinks which are used to write the results of data processed by RisingWave to an external target, such as an Apache Iceberg table in object storage.</p> <p>RisingWave exposes these features via normal SQL statements, namely <code>CREATE SOURCE</code> and <code>CREATE SINK</code>. To utilize these in Vulcan, you can use them in pre / post statements.</p> <p>Here is an example of creating a Sink from a Vulcan model using a post statement:</p> <pre><code>MODEL (\n    name vulcan_example.view_model,\n    kind VIEW (\n      materialized true\n    )\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id;\n\nCREATE\n  SINK IF NOT EXISTS kafka_sink\nFROM\n  @this_model\nWITH (\n  connector='kafka',\n  \"properties.bootstrap.server\"='localhost:9092',\n  topic='test1',\n)\nFORMAT PLAIN\nENCODE JSON (force_append_only=true);\n</code></pre> <p>@this_model</p> <p>The <code>@this_model</code> macro resolves to the physical table for the current version of the model. See here for more information.</p>"},{"location":"integrations/engines/snowflake/","title":"Snowflake","text":""},{"location":"integrations/engines/snowflake/#snowflake","title":"Snowflake","text":"<p>This page provides information about how to use Vulcan with the Snowflake SQL engine.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the built-in.</p>"},{"location":"integrations/engines/snowflake/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.</p> <p>It demonstrates connecting to Snowflake with the <code>snowflake-connector-python</code> library bundled with Vulcan.</p> <p>Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are described below.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"integrations/engines/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Snowflake account and know your username and password</li> <li>Your Snowflake account has at least one warehouse available for running computations</li> <li>Your computer has Vulcan installed with the Snowflake extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[snowflake]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init snowflake</code></li> </ul> </li> </ol>"},{"location":"integrations/engines/snowflake/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access different types of database objects.</p> <p>Vulcan's core functionality requires relatively broad permissions, including:</p> <ol> <li>Ability to create and delete schemas in a database</li> <li>Ability to create, modify, delete, and query tables and views in the schemas it creates</li> </ol> <p>If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.</p> <p>We now describe how to grant Vulcan appropriate permissions.</p>"},{"location":"integrations/engines/snowflake/#snowflake-roles","title":"Snowflake roles","text":"<p>Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a \"role\" that you then grant to the user.</p> <p>Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.</p> <p>The role must be granted <code>USAGE</code> on a warehouse so it can execute computations. We describe other permissions below.</p>"},{"location":"integrations/engines/snowflake/#database-permissions","title":"Database permissions","text":"<p>The top-level object container in Snowflake is a \"database\" (often called a \"catalog\" in other engines). Vulcan does not need permission to create databases; it may use an existing one.</p> <p>The simplest way to grant Vulcan sufficient permissions for a database is to give it <code>OWNERSHIP</code> of the database, which includes all the necessary permissions.</p> <p>Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.</p>"},{"location":"integrations/engines/snowflake/#granting-the-permissions","title":"Granting the permissions","text":"<p>This section provides example code for creating a <code>vulcan</code> role, granting it sufficient permissions, and granting it to a user.</p> <p>The code must be executed by a user with <code>USERADMIN</code> level permissions or higher. We provide two versions of the code, one that grants database <code>OWNERSHIP</code> to the role and another that does not.</p> <p>Both examples create a role named <code>vulcan</code>, grant it usage of the warehouse <code>compute_wh</code>, create a database named <code>demo_db</code>, and assign the role to the user <code>demo_user</code>. The step that creates the database can be omitted if the database already exists.</p> With database ownershipWithout database ownership <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\n\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\nGRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre> <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\n\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\nGRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database\n\nGRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates\nGRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas\nGRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas\nGRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas\nGRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre>"},{"location":"integrations/engines/snowflake/#get-connection-info","title":"Get connection info","text":"<p>Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.</p>"},{"location":"integrations/engines/snowflake/#account-name","title":"Account name","text":"<p>Snowflake connection configurations require the <code>account</code> parameter that identifies the Snowflake account Vulcan should connect to.</p> <p>Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a <code>/</code>.</p> <p>This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:</p> <p></p> <p>In this example, our organization name is <code>idapznw</code>, and our account name is <code>wq29399</code>.</p> <p>We concatenate the two components, separated by a <code>-</code>, for the Vulcan <code>account</code> parameter: <code>idapznw-wq29399</code>.</p>"},{"location":"integrations/engines/snowflake/#warehouse-name","title":"Warehouse name","text":"<p>Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.</p> <p>Some Snowflake user accounts may have a default warehouse they automatically use when connecting.</p> <p>The connection configuration's <code>warehouse</code> parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.</p>"},{"location":"integrations/engines/snowflake/#database-name","title":"Database name","text":"<p>Snowflake user accounts may have a \"Default Namespace\" that includes a default database they automatically use when connecting.</p> <p>The connection configuration's <code>database</code> parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.</p>"},{"location":"integrations/engines/snowflake/#configure-the-connection","title":"Configure the connection","text":"<p>We now have the information we need to configure Vulcan's connection to Snowflake.</p> <p>We start the configuration by adding a gateway named <code>snowflake</code> to our example project's config.yaml file and making it our <code>default_gateway</code>:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>And we specify the <code>account</code>, <code>user</code>, <code>password</code>, <code>database</code>, and <code>warehouse</code> connection parameters using the information from above:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Warning</p> <p>Best practice for storing secrets like passwords is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>SNOWFLAKE_PASSWORD</code> for the configuration's <code>password</code> parameter:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      password: {{ env_var('SNOWFLAKE_PASSWORD') }}\n</code></pre>"},{"location":"integrations/engines/snowflake/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>snowflake</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Snowflake SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"integrations/engines/snowflake/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>snowflake</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>snowflake_state.db</code> file:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; your password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n    state_connection:\n      type: duckdb\n      database: snowflake_state.db\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Now we no longer see the warning when running <code>vulcan info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"integrations/engines/snowflake/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in Snowflake:</p> <p></p> <p>And confirm that our schemas and objects exist in the Snowflake catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Snowflake!</p>"},{"location":"integrations/engines/snowflake/#where-are-the-row-counts","title":"Where are the row counts?","text":"<p>Vulcan reports the number of rows processed by each model in its <code>plan</code> and <code>run</code> terminal output.</p> <p>However, due to limitations in the Snowflake Python connector, row counts cannot be determined for <code>CREATE TABLE AS</code> statements. Therefore, Vulcan does not report row counts for certain model kinds, such as <code>FULL</code> models.</p> <p>Learn more about the connector limitation on Github.</p>"},{"location":"integrations/engines/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"integrations/engines/snowflake/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[snowflake]\"\n</code></pre>"},{"location":"integrations/engines/snowflake/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account name string Y <code>user</code> The Snowflake username string N <code>password</code> The Snowflake password string N <code>authenticator</code> The Snowflake authenticator method string N <code>warehouse</code> The Snowflake warehouse name string N <code>database</code> The Snowflake database name string N <code>role</code> The Snowflake role name string N <code>token</code> The Snowflake OAuth 2.0 access token string N <code>private_key</code> The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). string N <code>private_key_path</code> The optional path to the private key to use for authentication. This would be used instead of <code>private_key</code>. string N <code>private_key_passphrase</code> The optional passphrase to use to decrypt <code>private_key</code> (if in PEM format) or <code>private_key_path</code>. Keys can be created without encryption so only provide this if needed. string N <code>session_parameters</code> The optional session parameters to set for the connection. dict N"},{"location":"integrations/engines/snowflake/#lowercase-object-names","title":"Lowercase object names","text":"<p>Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command <code>CREATE SCHEMA vulcan</code> will generate a schema named <code>VULCAN</code> in Snowflake.</p> <p>If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.</p> <p>For example, a connection to the database <code>\"my_db\"</code> would include:</p> <pre><code>connection:\n  type: snowflake\n  &lt;other connection options&gt;\n  database: '\"my_db\"' # outer single and inner double quotes\n</code></pre>"},{"location":"integrations/engines/snowflake/#snowflake-authorization-methods","title":"Snowflake authorization methods","text":"<p>The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.</p> <p>This section describes how to configure other authorization methods.</p>"},{"location":"integrations/engines/snowflake/#snowflake-sso-authorization","title":"Snowflake SSO Authorization","text":"<p>Vulcan supports Snowflake SSO authorization connections using the <code>externalbrowser</code> authenticator method. For example:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: ************\n      user: ************\n      authenticator: externalbrowser\n      warehouse: ************\n      database: ************\n      role: ************\n</code></pre>"},{"location":"integrations/engines/snowflake/#snowflake-oauth-authorization","title":"Snowflake OAuth Authorization","text":"<p>Vulcan supports Snowflake OAuth authorization connections using the <code>oauth</code> authenticator method. For example:</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      authenticator: oauth\n      token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                authenticator=\"oauth\",\n                token=\"eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"integrations/engines/snowflake/#snowflake-private-key-authorization","title":"Snowflake Private Key Authorization","text":"<p>Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).</p> <p>The <code>account</code> and <code>user</code> parameters are required for each of these methods.</p> <p>Private Key Path</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key_path: '/path/to/key.key'\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key_path=\"/path/to/key.key\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key PEM</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: |\n        -----BEGIN PRIVATE KEY-----\n        ...\n        -----END PRIVATE KEY-----\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"\"\"\n                -----BEGIN PRIVATE KEY-----\n                ...\n                -----END PRIVATE KEY-----\"\"\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Base64</p> <p>Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Bytes</p> YAMLPython <p>Base64 encode the bytes and follow Private Key Base64 instructions.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    GatewayConfig,\n    ModelDefaultsConfig,\n    SnowflakeConnectionConfig,\n)\n\nfrom cryptography.hazmat.primitives import serialization\n\nkey = \"\"\"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\"\"\".encode()\n\np_key= serialization.load_pem_private_key(key, password=None)\n\npkb = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=pkb,\n            ),\n        ),\n    }\n)\n</code></pre> <p>The authenticator method is assumed to be <code>snowflake_jwt</code> when <code>private_key</code> is provided, but it can also be explicitly provided in the connection configuration.</p>"},{"location":"integrations/engines/snowflake/#configuring-virtual-warehouses","title":"Configuring Virtual Warehouses","text":"<p>The Snowflake Virtual Warehouse a model should use can be specified in the <code>session_properties</code> attribute of the model definition:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  session_properties (\n    'warehouse' = TEST_WAREHOUSE,\n  ),\n);\n</code></pre>"},{"location":"integrations/engines/snowflake/#custom-view-and-table-types","title":"Custom View and Table types","text":"<p>Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the <code>physical_properties</code> and <code>virtual_properties</code> attributes respectively. For example:</p>"},{"location":"integrations/engines/snowflake/#secure-views","title":"Secure Views","text":"<p>A table can be exposed through a <code>SECURE</code> view in the virtual layer by specifying the <code>creatable_type</code> property and setting it to <code>SECURE</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  virtual_properties (\n      creatable_type = SECURE\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"integrations/engines/snowflake/#transient-tables","title":"Transient Tables","text":"<p>A model can use a <code>TRANSIENT</code> table in the physical layer by specifying the <code>creatable_type</code> property and setting it to <code>TRANSIENT</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  physical_properties (\n      creatable_type = TRANSIENT\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"integrations/engines/snowflake/#iceberg-tables","title":"Iceberg Tables","text":"<p>In order for Snowflake to be able to create an Iceberg table, there must be an External Volume configured to store the Iceberg table data on.</p> <p>Once that is configured, you can create a model backed by an Iceberg table by using <code>table_format iceberg</code> like so:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = '&lt;external volume name&gt;'\n  )\n);\n</code></pre> <p>To prevent having to specify <code>catalog = 'snowflake'</code> and <code>external_volume = '&lt;external volume name&gt;'</code> on every model, see the Snowflake documentation for:</p> <ul> <li>Configuring a default Catalog</li> <li>Configuring a default External Volume</li> </ul> <p>Alternatively you can also use model defaults to set defaults at the Vulcan level instead.</p> <p>To utilize the wide variety of optional properties that Snowflake makes available for Iceberg tables, simply specify them as <code>physical_properties</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = 'my_external_volume',\n    base_location = 'my/product_reviews/'\n  )\n);\n</code></pre> <p>External catalogs</p> <p>Setting <code>catalog = 'snowflake'</code> to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake does not support writing to Iceberg tables configured under external catalogs.</p> <p>You can however still reference a table from an external catalog in your model as a normal external table.</p>"},{"location":"integrations/engines/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/engines/snowflake/#frequent-authentication-prompts","title":"Frequent Authentication Prompts","text":"<p>When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.</p> <p>To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:</p> <ul> <li>For general authentication, see Connection Caching Documentation</li> <li>For MFA specifically, see MFA Token Caching Documentation.</li> </ul>"},{"location":"integrations/engines/spark/","title":"Spark","text":""},{"location":"integrations/engines/spark/#spark","title":"Spark","text":""},{"location":"integrations/engines/spark/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>spark</code></p> <p>NOTE: Spark may not be used for the Vulcan state connection.</p>"},{"location":"integrations/engines/spark/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>spark</code> string Y <code>config_dir</code> Value to set for <code>SPARK_CONFIG_DIR</code> string N <code>catalog</code> The catalog to use when issuing commands. See Catalog Support for details string N <code>config</code> Key/value pairs to set for the Spark Configuration. dict N"},{"location":"integrations/engines/spark/#catalog-support","title":"Catalog Support","text":"<p>Vulcan's Spark integration is only designed/tested with a single catalog usage in mind.  Therefore all Vulcan models must be defined with a single catalog.</p> <p>If <code>catalog</code> is not set, then the behavior changes based on spark release:</p> <ul> <li>If &gt;=3.4, then the default catalog is determined at runtime</li> <li>If &lt;3.4, then the default catalog is <code>spark_catalog</code> </li> </ul>"},{"location":"integrations/engines/trino/","title":"Trino","text":""},{"location":"integrations/engines/trino/#trino","title":"Trino","text":""},{"location":"integrations/engines/trino/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>trino</code></p> <p>NOTE: Trino may not be used for the Vulcan state connection.</p>"},{"location":"integrations/engines/trino/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[trino]\"\n</code></pre> <p>If you are using Oauth for Authentication, it is recommended to install keyring cache: </p><pre><code>pip install \"trino[external-authentication-token-cache]\"\n</code></pre><p></p>"},{"location":"integrations/engines/trino/#trino-connector-support","title":"Trino Connector Support","text":"<p>The trino engine adapter has been tested against the Hive Connector, Iceberg Connector, and Delta Lake Connector.</p> <p>Please let us know on Slack if you are wanting to use another connector or have tried another connector.</p>"},{"location":"integrations/engines/trino/#hive-connector-configuration","title":"Hive Connector Configuration","text":"<p>Recommended hive catalog properties configuration (<code>&lt;catalog_name&gt;.properties</code>):</p> <pre><code>hive.metastore-cache-ttl=0s\nhive.metastore-refresh-interval=5s\nhive.metastore-timeout=10s\nhive.allow-drop-table=true\nhive.allow-add-column=true\nhive.allow-drop-column=true\nhive.allow-rename-column=true\nhive.allow-rename-table=true\n</code></pre>"},{"location":"integrations/engines/trino/#iceberg-connector-configuration","title":"Iceberg Connector Configuration","text":"<p>If you're using a hive metastore for the Iceberg catalog, the properties are mostly the same as the Hive connector.</p> <pre><code>iceberg.catalog.type=hive_metastore\n# metastore properties as per the Hive Connector Configuration above\n</code></pre> <p>Note: The Trino Iceberg Connector must be configured with an <code>iceberg.catalog.type</code> that supports views. At the time of this writing, this is <code>hive_metastore</code>, <code>glue</code>, and <code>rest</code>.</p> <p>The <code>jdbc</code> and <code>nessie</code> iceberg catalog types do not support views and are thus incompatible with Vulcan.</p> <p>Nessie</p> <p>Nessie is supported when used as an Iceberg REST Catalog (<code>iceberg.catalog.type=rest</code>). For more information on how to configure the Trino Iceberg connector for this, see the Nessie documentation.</p>"},{"location":"integrations/engines/trino/#delta-lake-connector-configuration","title":"Delta Lake Connector Configuration","text":"<p>The Trino adapter Delta Lake connector has only been tested with the Hive metastore catalog type.</p> <p>The properties file must include the Hive metastore URI and catalog name in addition to any other general properties.</p> <pre><code>hive.metastore.uri=thrift://example.net:9083\ndelta.hive-catalog-name=datalake_delta # example catalog name, can be any valid string\n</code></pre>"},{"location":"integrations/engines/trino/#aws-glue","title":"AWS Glue","text":"<p>AWS Glue provides an implementation of the Hive metastore catalog.</p> <p>Your Trino project's physical data objects are stored in a specific location, such as an AWS S3 bucket. Hive provides a default location, which you can override in its configuration file.</p> <p>Set the default location for your project's tables in the Hive catalog configuration's <code>hive.metastore.glue.default-warehouse-dir</code> parameter.</p> <p>For example:</p> <pre><code>hive.metastore=glue\nhive.metastore.glue.default-warehouse-dir=s3://my-bucket/\n</code></pre>"},{"location":"integrations/engines/trino/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>trino</code> string Y <code>user</code> The username (of the account) to log in to your cluster. When connecting to Starburst Galaxy clusters, you must include the role of the user as a suffix to the username. string Y <code>host</code> The hostname of your cluster. Don't include the <code>http://</code> or <code>https://</code> prefix. string Y <code>catalog</code> The name of a catalog in your cluster. string Y <code>http_scheme</code> The HTTP scheme to use when connecting to your cluster. By default, it's <code>https</code> and can only be <code>http</code> for no-auth or basic auth. string N <code>port</code> The port to connect to your cluster. By default, it's <code>443</code> for <code>https</code> scheme and <code>80</code> for <code>http</code> int N <code>roles</code> Mapping of catalog name to a role dict N <code>http_headers</code> Additional HTTP headers to send with each request. dict N <code>session_properties</code> Trino session properties. Run <code>SHOW SESSION</code> to see all options. dict N <code>retries</code> Number of retries to attempt when a request fails. Default: <code>3</code> int N <code>timezone</code> Timezone to use for the connection. Default: client-side local timezone string N <code>schema_location_mapping</code> A mapping of regex patterns to S3 locations to use for the <code>LOCATION</code> property when creating schemas. See Table and Schema locations for more details. dict N <code>catalog_type_overrides</code> A mapping of catalog names to their connector type. This is used to enable/disable connector specific behavior. See Catalog Type Overrides for more details. dict N"},{"location":"integrations/engines/trino/#table-and-schema-locations","title":"Table and Schema locations","text":"<p>When using connectors that are decoupled from their storage (such as the Iceberg, Hive or Delta connectors), when creating new tables Trino needs to know the location in the physical storage it should write the table data to.</p> <p>This location gets stored against the table in the metastore so that any engine trying to read the data knows where to look.</p>"},{"location":"integrations/engines/trino/#default-behaviour","title":"Default behaviour","text":"<p>Trino allows you to optionally configure a <code>default-warehouse-dir</code> property at the Metastore level. When creating objects, Trino will infer schema locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;</code> and table locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;/&lt;table name&gt;</code>.</p> <p>However, if you dont set this property, Trino can still infer table locations if a schema location is explicitly set.</p> <p>For example, if you specify the <code>LOCATION</code> property when creating a schema like so:</p> <pre><code>CREATE SCHEMA staging_data\nWITH (LOCATION = 's3://warehouse/production/staging_data')\n</code></pre> <p>Then any tables created under that schema will have their location inferred as <code>&lt;schema location&gt;/&lt;table name&gt;</code>.</p> <p>If you specify neither a <code>default-warehouse-dir</code> in the metastore config nor a schema location when creating the schema, you must specify an explicit table location when creating the table or Trino will produce an error.</p> <p>Creating a table in a specific location is very similar to creating a schema in a specific location:</p> <pre><code>CREATE TABLE staging_data.customers (customer_id INT)\nWITH (LOCATION = 's3://warehouse/production/staging_data/customers')\n</code></pre>"},{"location":"integrations/engines/trino/#configuring-in-vulcan","title":"Configuring in Vulcan","text":"<p>Within Vulcan, you can configure the value to use for the <code>LOCATION</code> property when Vulcan creates tables and schemas. This overrides what Trino would have inferred based on the cluster configuration.</p>"},{"location":"integrations/engines/trino/#schemas","title":"Schemas","text":"<p>To configure the <code>LOCATION</code> property that Vulcan will specify when issuing <code>CREATE SCHEMA</code> statements, you can use the <code>schema_location_mapping</code> connection property. This applies to all schemas that Vulcan creates, including its internal ones.</p> <p>The simplest example is to emulate a <code>default-warehouse-dir</code>:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '.*': 's3://warehouse/production/@{schema_name}'\n</code></pre> <p>This will cause all schemas to get created with their location set to <code>s3://warehouse/production/&lt;schema name&gt;</code>. The table locations will be inferred by Trino as <code>s3://warehouse/production/&lt;schema name&gt;/&lt;table name&gt;</code> so all objects will effectively be created under <code>s3://warehouse/production/</code>.</p> <p>It's worth mentioning that if your models are using fully qualified three part names, eg <code>&lt;catalog&gt;.&lt;schema&gt;.&lt;name&gt;</code> then string being matched against the <code>schema_location_mapping</code> regex will be <code>&lt;catalog&gt;.&lt;schema&gt;</code> and not just the <code>&lt;schema&gt;</code> itself. This allows you to set different locations for the same schema name if that schema name is used across multiple catalogs.</p> <p>If your models are using two part names, eg <code>&lt;schema&gt;.&lt;table&gt;</code> then only the <code>&lt;schema&gt;</code> part will be matched against the regex.</p> <p>Here's an example:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '^utils$': 's3://utils-bucket/@{schema_name}'\n        '^landing\\..*$': 's3://raw-data/@{catalog_name}/@{schema_name}'\n        '^staging.*$': 's3://bucket/@{schema_name}_dev'\n        '^vulcan.*$': 's3://vulcan-internal/dev/@{schema_name}'\n</code></pre> <p>This would perform the following mappings:</p> <ul> <li>a schema called <code>sales</code> would not be mapped to a location at all because it doesnt match any of the patterns. It would be created without a <code>LOCATION</code> property</li> <li>a schema called <code>utils</code> would be mapped to the location <code>s3://utils-bucket/utils</code> because it directly matches the <code>^utils$</code> pattern</li> <li>a schema called <code>transactions</code> in a catalog called <code>landing</code> would be mapped to the location <code>s3://raw-data/landing/transactions</code> because the string <code>landing.transactions</code> matches the <code>^landing\\..*$</code> pattern</li> <li>schemas called <code>staging_customers</code> and <code>staging_accounts</code> would be mapped to the locations <code>s3://bucket/staging_customers_dev</code> and <code>s3://bucket/staging_accounts_dev</code> respectively because they match the <code>^staging.*$</code> pattern</li> <li>a schema called <code>accounts</code> in a catalog called <code>staging</code> would be mapped to the location <code>s3://bucket/accounts_dev</code> because the string <code>staging.accounts</code> matches the <code>^staging.*$</code> pattern</li> <li>schemas called <code>vulcan__staging_customers</code> and <code>vulcan__staging_utils</code> would be mapped to the locations <code>s3://vulcan-internal/dev/vulcan__staging_customers</code> and <code>s3://vulcan-internal/dev/vulcan__staging_utils</code> respectively because they match the <code>^vulcan.*$</code> pattern</li> </ul> <p>Placeholders</p> <p>You may use the <code>@{catalog_name}</code> and <code>@{schema_name}</code> placeholders in the mapping value.</p> <p>If there is a match on one of the patterns then the catalog / schema that Vulcan is about to use in the <code>CREATE SCHEMA</code> statement will be substituted into these placeholders.</p> <p>Note the use of curly brace syntax <code>@{}</code> when referencing these placeholders - learn more here.</p>"},{"location":"integrations/engines/trino/#tables","title":"Tables","text":"<p>Often, you don't need to configure an explicit table location because if you have configured explicit schema locations, table locations are automatically inferred by Trino to be a subdirectory under the schema location.</p> <p>However, if you need to, you can configure an explicit table location by adding a <code>location</code> property to the model <code>physical_properties</code>.</p> <p>Note that you need to use the @resolve_template macro to generate a unique table location for each model version. Otherwise, all model versions will be written to the same location and clobber each other.</p> <pre><code>MODEL (\n  name staging.customers,\n  kind FULL,\n  physical_properties (\n    location = @resolve_template('s3://warehouse/@{catalog_name}/@{schema_name}/@{table_name}')\n  )\n);\n\nSELECT ...\n</code></pre> <p>This will cause Vulcan to set the specified <code>LOCATION</code> when issuing a <code>CREATE TABLE</code> statement.</p>"},{"location":"integrations/engines/trino/#catalog-type-overrides","title":"Catalog Type Overrides","text":"<p>Vulcan attempts to determine the connector type of a catalog by querying the <code>system.metadata.catalogs</code> table and checking the <code>connector_name</code> column. It checks if the connector name is <code>hive</code> for Hive connector behavior or contains <code>iceberg</code> or <code>delta_lake</code> for Iceberg or Delta Lake connector behavior respectively. However, the connector name may not always be a reliable way to determine the connector type, for example when using a custom connector or a fork of an existing connector. To handle such cases, you can use the <code>catalog_type_overrides</code> connection property to explicitly specify the connector type for specific catalogs. For example, to specify that the <code>datalake</code> catalog is using the Iceberg connector and the <code>analytics</code> catalog is using the Hive connector, you can configure the connection as follows:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      catalog_type_overrides:\n        datalake: iceberg\n        analytics: hive\n</code></pre>"},{"location":"integrations/engines/trino/#authentication","title":"Authentication","text":"No AuthBasic AuthLDAPKerberosJWTCertificateOauth Option Description Type Required <code>method</code> <code>no-auth</code> (Default) string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    user: [user]\n    host: [host]\n    catalog: [catalog]\n    # Most likely you will want http for scheme when not using auth\n    http_scheme: http\n</code></pre> Option Description Type Required <code>method</code> <code>basic</code> string Y <code>password</code> The password to use when authenticating. string Y <code>verify</code> Boolean flag for whether SSL verification should occur. Default: trinodb Python client default (<code>true</code> as of this writing) bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: basic\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on Basic Authentication</li> <li>Python Client Basic Authentication</li> </ul> Option Description Type Required <code>method</code> <code>ldap</code> string Y <code>password</code> The password to use when authenticating. string Y <code>impersonation_user</code> Override the provided username. This lets you impersonate another user. string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: ldap\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on LDAP Authentication</li> <li>Python Client LDAP Authentication</li> </ul> Option Description Type Required <code>method</code> <code>kerberos</code> string Y <code>keytab</code> Path to keytab. Ex: <code>/tmp/trino.keytab</code> string Y <code>krb5_config</code> Path to config. Ex: <code>/tmp/krb5.conf</code> string Y <code>principal</code> Principal.  Ex: <code>user@company.com</code> string Y <code>service_name</code> Service name (default is <code>trino</code>) string N <code>hostname_override</code> Kerberos hostname for a host whose DNS name doesn't match string N <code>mutual_authentication</code> Boolean flag for mutual authentication. Default: <code>false</code> bool N <code>force_preemptive</code> Boolean flag to preemptively initiate the Kerberos GSS exchange. Default: <code>false</code> bool N <code>sanitize_mutual_error_response</code> Boolean flag to strip content and headers from error responses. Default: <code>true</code> bool N <code>delegate</code> Boolean flag for credential delegation (<code>GSS_C_DELEG_FLAG</code>). Default: <code>false</code> bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: kerberos\n    user: user\n    keytab: /tmp/trino.keytab\n    krb5_config: /tmp/krb5.conf\n    principal: trino@company.com\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Kerberos Authentication</li> <li>Python Client Kerberos Authentication</li> </ul> Option Description Type Required <code>method</code> <code>jwt</code> string Y <code>jwt_token</code> The JWT string. string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: jwt\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on JWT Authentication</li> <li>Python Client JWT Authentication</li> </ul> Option Description Type Required <code>method</code> <code>certificate</code> string Y <code>cert</code> The full path to a certificate file string Y <code>client_certificate</code> Path to client certificate. Ex: <code>/tmp/client.crt</code> string Y <code>client_private_key</code> Path to client private key. Ex: <code>/tmp/client.key</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: certificate\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n    cert: [path/to/cert_file]\n    client_certificate: [path/to/client/cert]\n    client_private_key: [path/to/client/key]\n</code></pre> Option Description Type Required <code>method</code> <code>oauth</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: oauth\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Oauth Authentication</li> <li>Python Client Oauth Authentication</li> </ul>"},{"location":"quickstart/cli/","title":"CLI","text":""},{"location":"quickstart/cli/#cli","title":"CLI","text":"<p>In this quickstart, you'll use the Vulcan command line interface (CLI) to get up and running with Vulcan's scaffold generator.</p> <p>It will create an example project that runs locally on your computer using DuckDB as an embedded SQL engine.</p> <p>Before beginning, ensure that you meet all the prerequisites for using Vulcan.</p> Learn more about the quickstart project structure <p>This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.</p> <p>The project contains three models with a CSV file as the only data source:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502seed_data.csv\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n             \u2502\n            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502seed_model.sql\u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                          \u2502\n                         \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502incremental_model.sql\u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                                              \u2502\n                                             \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                             \u2502full_model.sql\u2502\n                                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.</p>"},{"location":"quickstart/cli/#1-create-the-vulcan-project","title":"1. Create the Vulcan project","text":"<p>First, create a project directory and navigate to it:</p> <p></p><pre><code>mkdir vulcan-example\n</code></pre> <pre><code>cd vulcan-example\n</code></pre><p></p> <p>If using a Python virtual environment, ensure it's activated first by running the <code>source .venv/bin/activate</code> command from the folder used during installation.</p>"},{"location":"quickstart/cli/#11-initialize-the-project","title":"1.1 Initialize the project","text":"<p>Vulcan includes a scaffold generator to initialize a new Vulcan project.</p> <p>The scaffold generator will ask you some questions and create a Vulcan configuration file based on your responses.</p> <p>Depending on your answers, it will also create multiple files for the Vulcan example project used in this quickstart.</p> <p>Start the scaffold generator by executing the <code>vulcan init</code> command:</p> <pre><code>vulcan init\n</code></pre> Skip the questions <p>If you don't want to use the interactive scaffold generator, you can initialize your project with arguments to the <code>vulcan init</code> command.</p> <p>The only required argument is <code>engine</code>, which specifies the SQL engine your project will use. Specify one of the engine <code>type</code>s in the list of supported engines.</p> <p>In this example, we specify the <code>duckdb</code> engine:</p> <pre><code>vulcan init duckdb\n</code></pre> <p>The scaffold will include a Vulcan configuration file and example project directories and files. You're now ready to continue the quickstart below.</p>"},{"location":"quickstart/cli/#project-type","title":"Project type","text":"<p>The first question asks about the type of project you want to create. Enter the number corresponding to the type of project you want to create and press <code>Enter</code>.</p> <pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nWelcome to Vulcan!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nWhat type of project do you want to set up?\n\n    [1] DEFAULT - Create Vulcan example project models and files\n    [2] dbt     - You have an existing dbt project and want to run it with Vulcan\n    [3] EMPTY   - Create a Vulcan configuration file and project directories only\n\nEnter a number: 1\n</code></pre> <p>For this quickstart, choose the <code>DEFAULT</code> option <code>1</code> so the example project files are included in the project directories.</p>"},{"location":"quickstart/cli/#sql-engine","title":"SQL engine","text":"<p>The second question asks which SQL engine your project will use. Vulcan will include that engine's connection settings in the configuration file, which you will fill in later to connect your project to the engine.</p> <p>For this quickstart, choose the <code>DuckDB</code> option <code>1</code> so we can run the example project with the built-in DuckDB engine that doesn't need additional configuration.</p> <pre><code>Choose your SQL engine:\n\n    [1]  DuckDB\n    [2]  Snowflake\n    [3]  Databricks\n    [4]  BigQuery\n    [5]  MotherDuck\n    [6]  ClickHouse\n    [7]  Redshift\n    [8]  Spark\n    [9]  Trino\n    [10] Azure SQL\n    [11] MSSQL\n    [12] Postgres\n    [13] GCP Postgres\n    [14] MySQL\n    [15] Athena\n    [16] RisingWave\n\nEnter a number: 1\n</code></pre>"},{"location":"quickstart/cli/#cli-mode","title":"CLI mode","text":"<p>Vulcan's core commands have multiple options that alter their behavior. Some of those options streamline the Vulcan <code>plan</code> workflow and CLI output.</p> <p>If you prefer a streamlined workflow (no prompts, no file diff previews, auto-apply changes), choose the <code>FLOW</code> CLI mode to automatically include those options in your project configuration file.</p> <p>If you prefer to see all the output Vulcan provides, choose <code>DEFAULT</code> mode, which we will use in this quickstart:</p> <pre><code>Choose your Vulcan CLI experience:\n\n    [1] DEFAULT - See and control every detail\n    [2] FLOW    - Automatically run changes and show summary output\n\nEnter a number: 1\n</code></pre>"},{"location":"quickstart/cli/#ready-to-go","title":"Ready to go","text":"<p>Your project is now ready to go, and Vulcan displays a message with some good next steps.</p> <p>If you chose the DuckDB engine, you're ready to move forward and run the example project with DuckDB.</p> <p>If you chose a different engine, add your engine's connection information to the <code>config.yaml</code> file before you run any additional Vulcan commands.</p> <pre><code>Your Vulcan project is ready!\n\nNext steps:\n- Update your gateway connection settings (e.g., username/password) in the project configuration file:\n    /vulcan-example/config.yaml\n- Run command in CLI: vulcan plan\n- (Optional) Explain a plan: vulcan plan --explain\n\nQuickstart guide:\nhttps://vulcan.readthedocs.io/en/stable/quickstart/cli/\n\nNeed help?\n- Docs:   https://vulcan.readthedocs.io\n- Slack:  https://www.tobikodata.com/slack\n- GitHub: https://github.com/TobikoData/vulcan/issues\n</code></pre> Learn more about the project's configuration: <code>config.yaml</code> <p>Vulcan project-level configuration parameters are specified in the <code>config.yaml</code> file in the project directory.</p> <p>This example project uses the embedded DuckDB SQL engine, so its configuration specifies <code>duckdb</code> as the gateway's connection type. All available configuration settings are included in the file, with optional settings set to their default value and commented out.</p> <p>Vulcan requires a default model SQL dialect. Vulcan automatically specifies the SQL dialect for your project's SQL engine, which it places in the config <code>model_defaults</code> <code>dialect</code> key. In this example, we specified the DuckDB engine, so <code>duckdb</code> is the default SQL dialect:</p> <pre><code># --- Gateway Connection ---\ngateways:\n  duckdb:\n    connection:\n      # For more information on configuring the connection to your execution engine, visit:\n      # https://vulcan.readthedocs.io/en/stable/reference/configuration/#connection\n      # https://vulcan.readthedocs.io/en/stable/integrations/engines/duckdb/#connection-options\n      #\n      type: duckdb               # &lt;-- DuckDB engine\n      database: db.db\n      # concurrent_tasks: 1\n      # register_comments: True  # &lt;-- Optional setting `register_comments` has a default value of True\n      # pre_ping: False\n      # pretty_sql: False\n      # catalogs:                # &lt;-- Optional setting `catalogs` has no default value\n      # extensions:\n      # connector_config:\n      # secrets:\n      # token:\n\ndefault_gateway: duckdb\n\n# --- Model Defaults ---\n# https://vulcan.readthedocs.io/en/stable/reference/model_configuration/#model-defaults\n\nmodel_defaults:\n  dialect: duckdb                # &lt;-- Models written in DuckDB SQL dialect by default\n  start: 2025-06-12 # Start date for backfill history\n  cron: '@daily'    # Run models daily at 12am UTC (can override per model)\n\n# --- Linting Rules ---\n# Enforce standards for your team\n# https://vulcan.readthedocs.io/en/stable/guides/linter/\n\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre> <p>Learn more about Vulcan project configuration here.</p> <p>The scaffold generator creates multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).</p> Learn more about the project directories and files <p>Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.</p> <p>The scaffold generator will create the following configuration file and directories:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. More info about configuration here.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. More info about models here.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. More info about seeds here.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. More info about audits here.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. More info about tests here.</li> </ul> </li> <li>./macros<ul> <li>Macro files. More info about macros here.</li> </ul> </li> </ul> <p>It will also create the files needed for this quickstart example:</p> <ul> <li>./models<ul> <li>full_model.sql</li> <li>incremental_model.sql</li> <li>seed_model.sql</li> </ul> </li> <li>./seeds<ul> <li>seed_data.csv</li> </ul> </li> <li>./audits<ul> <li>assert_positive_order_ids.sql</li> </ul> </li> <li>./tests<ul> <li>test_full_model.yaml</li> </ul> </li> </ul> <p>Finally, the scaffold generator creates data for the example project to use.</p> Learn more about the project's data <p>The data used in this example project is contained in the <code>seed_data.csv</code> file in the <code>/seeds</code> project directory. The data reflects sales of 3 items over 7 days in January 2020.</p> <p>The file contains three columns, <code>id</code>, <code>item_id</code>, and <code>event_date</code>, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.</p> <p>This is the complete dataset:</p> id item_id event_date 1 2 2020-01-01 2 1 2020-01-01 3 3 2020-01-03 4 1 2020-01-04 5 1 2020-01-05 6 1 2020-01-06 7 1 2020-01-07"},{"location":"quickstart/cli/#2-create-a-prod-environment","title":"2. Create a prod environment","text":"<p>Vulcan's key actions are creating and applying plans to environments. At this point, the only environment is the empty <code>prod</code> environment.</p> Learn more about Vulcan plans and environments <p>Vulcan's key actions are creating and applying plans to environments.</p> <p>A Vulcan environment is an isolated namespace containing models and the data they generated.</p> <p>The most important environment is <code>prod</code> (\"production\"), which consists of the databases behind the applications your business uses to operate each day. Environments other than <code>prod</code> provide a place where you can test and preview changes to model code before they go live and affect business operations.</p> <p>A Vulcan plan contains a comparison of one environment to another and the set of changes needed to bring them into alignment.</p> <p>For example, if a new SQL model was added, tested, and run in the <code>dev</code> environment, it would need to be added and run in the <code>prod</code> environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.</p> <p>Breaking changes are those that invalidate data already existing in an environment. For example, if a <code>WHERE</code> clause was added to a model in the <code>dev</code> environment, existing data created by that model in the <code>prod</code> environment are now invalid because they may contain rows that would be filtered out by the new <code>WHERE</code> clause.</p> <p>Other changes, like adding a new column to a model in <code>dev</code>, are non-breaking because all the existing data in <code>prod</code> are still valid to use - only new data must be added to align the environments.</p> <p>After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to \"backfill\" data to apply the plan. (In this context, backfill is a generic term for updating or adding to a table's data, including an initial load or full refresh.)</p> Learn more about a plan's actions: <code>vulcan plan --explain</code> <p>Before applying a plan, you can view a detailed description of the actions it will take by passing the explain flag in your <code>vulcan plan</code> command:</p> <pre><code>vulcan plan --explain\n</code></pre> <p>Passing the explain flag for the quickstart example project above adds the following information to the output:</p> <pre><code>Explained plan\n\u251c\u2500\u2500 Validate SQL and create physical layer tables and views if they do not exist\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502       \u2514\u2500\u2500 Create table if it doesn't exist\n\u251c\u2500\u2500 Backfill models by running their queries and run standalone audits\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u2514\u2500\u2500 Fully refresh table\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u2514\u2500\u2500 Fully refresh table\n\u2514\u2500\u2500 Update the virtual layer for environment 'prod'\n    \u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n        \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n        \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n        \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n</code></pre> <p>The explanation has three top-level sections, corresponding to the three types of actions a plan takes:</p> <ul> <li>Validate SQL and create physical layer tables and views if they do not exist</li> <li>Backfill models by running their queries and run standalone audits</li> <li>Update the virtual layer for environment 'prod'</li> </ul> <p>Each section lists the affected models and provides more information about what will occur. For example, the first model in the first section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n</code></pre> <p>The first line shows the model name <code>vulcan_example.seed_model</code> and the physical layer table Vulcan will create to store its data: <code>db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172</code>. The second and third lines tell us that in this step Vulcan will dry-run the model query and create the physical layer table if it doesn't exist.</p> <p>The second section describes what will occur during the backfill step. The second model in this section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n</code></pre> <p>The first line shows the model name <code>vulcan_example.full_model</code> and the physical layer table Vulcan will insert the model's data into: <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>. The second and third lines tell us that the backfill action will fully refresh the model's physical table and run the <code>assert_positive_order_ids</code> audit.</p> <p>The final section describes Vulcan's action during the virtual layer update step. The first model in this section is:</p> <pre><code>\u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n    \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n</code></pre> <p>The virtual layer step will update the <code>vulcan_example.full_model</code> virtual layer view to <code>SELECT * FROM</code> the physical table <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>.</p> <p>The first Vulcan plan must execute every model to populate the production environment. Running <code>vulcan plan</code> will generate the plan and the following output:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb in 0.1 seconds.\n----------------------------------------------------------------------\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u2514\u2500\u2500 vulcan_example.seed_model\nModels needing backfill:\n\u251c\u2500\u2500 vulcan_example.full_model: [full refresh]\n\u251c\u2500\u2500 vulcan_example.incremental_model: [2020-01-01 - 2025-06-22]\n\u2514\u2500\u2500 vulcan_example.seed_model: [full refresh]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 3 of the output notes that <code>vulcan plan</code> successfully executed the project's test <code>tests/test_full_model.yaml</code> with duckdb.</p> <p>Line 6 describes what environments the plan will affect when applied - a new <code>prod</code> environment in this case.</p> <p>Lines 8-12 of the output show that Vulcan detected three new models relative to the current empty environment.</p> <p>Lines 13-16 list each model that will be executed by the plan, along with the date intervals or refresh types. For both <code>full_model</code> and <code>seed_model</code>, it shows <code>[full refresh]</code>, while for <code>incremental_model</code> it shows a specific date range <code>[2020-01-01 - 2025-06-22]</code>. The incremental model date range begins from 2020-01-01 because its definition specifies a model start date of <code>2020-01-01</code>.</p> Learn more about the project's models <p>A plan's actions are determined by the kinds of models the project uses. This example project uses three model kinds:</p> <ol> <li><code>SEED</code> models read data from CSV files stored in the Vulcan project directory.</li> <li><code>FULL</code> models fully refresh (rewrite) the data associated with the model every time the model is run.</li> <li><code>INCREMENTAL_BY_TIME_RANGE</code> models use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.</li> </ol> <p>We now briefly review each model in the project.</p> <p>The first model is a <code>SEED</code> model that imports <code>seed_data.csv</code>. This model consists of only a <code>MODEL</code> statement because <code>SEED</code> models do not query a database.</p> <p>In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the <code>grain</code> of the model to the columns that collectively form the model's unique identifier, <code>id</code> and <code>event_date</code>.</p> <pre><code>MODEL (\n  name vulcan_example.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INTEGER,\n    item_id INTEGER,\n    event_date DATE\n  ),\n  grain (id, event_date)\n);\n</code></pre> <p>The second model is an <code>INCREMENTAL_BY_TIME_RANGE</code> model that includes both a <code>MODEL</code> statement and a SQL query selecting from the first seed model.</p> <p>The <code>MODEL</code> statement's <code>kind</code> property includes the required specification of the data column containing each record's timestamp. It also includes the optional <code>start</code> property specifying the earliest date/time for which the model should process data and the <code>cron</code> property specifying that the model should run daily. It sets the model's grain to columns <code>id</code> and <code>event_date</code>.</p> <p>The SQL query includes a <code>WHERE</code> clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre> <p>The final model in the project is a <code>FULL</code> model. In addition to properties used in the other models, its <code>MODEL</code> statement includes the <code>audits</code> property. The project includes a custom <code>assert_positive_order_ids</code> audit in the project <code>audits</code> directory; it verifies that all <code>item_id</code> values are positive numbers. It will be run every time the model is executed.</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>Line 18 asks you whether to proceed with executing the model backfills described in lines 13-16. Enter <code>y</code> and press <code>Enter</code>, and Vulcan will execute the models and return this output:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example.seed_model          [insert seed file]                 0.01s\n[1/1] vulcan_example.incremental_model   [insert 2020-01-01 - 2025-06-22]   0.01s\n[1/1] vulcan_example.full_model          [full refresh, audits \u27141]          0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Vulcan performs three actions when applying the plan:</p> <ul> <li>Creating and storing new versions of the models</li> <li>Evaluating/running the models</li> <li>Virtually updating the plan's target environment</li> </ul> <p>Lines 2-4 show the progress and completion of the first step - updating the physical layer (creating new model versions).</p> <p>Lines 6-11 show the execution of each model with their specific operations and timing. Line 6 shows the seed model being inserted, line 8 shows the incremental model being inserted for the specified date range, and line 10 shows the full model being processed with its audit check passing.</p> <p>Lines 12-14 show the progress and completion of the second step - executing model batches.</p> <p>Lines 16-18 show the progress and completion of the final step - virtually updating the plan's target environment, which makes the data available for querying.</p> <p>Let's take a quick look at the project's DuckDB database file to see the objects Vulcan created. First, we open the built-in DuckDB CLI tool with the <code>duckdb db.db</code> command, then run our two queries.</p> <p>Our first query shows the three physical tables Vulcan created in the <code>vulcan__vulcan_example</code> schema (one table for each model):</p> <p></p> <p>Our second query shows that in the <code>vulcan</code> schema Vulcan created three virtual layer views that read from the three physical tables:</p> <p></p> <p>You've now created a new production environment with all of history backfilled!</p>"},{"location":"quickstart/cli/#3-update-a-model","title":"3. Update a model","text":"<p>Now that we have populated the <code>prod</code> environment, let's modify one of the SQL models.</p> <p>We modify the incremental SQL model by adding a new column to the query. Open the <code>models/incremental_model.sql</code> file and add <code>'z' AS new_column</code> below <code>item_id</code> as follows:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  'z' AS new_column, -- Added column\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre>"},{"location":"quickstart/cli/#4-work-with-a-development-environment","title":"4. Work with a development environment","text":""},{"location":"quickstart/cli/#41-create-a-dev-environment","title":"4.1 Create a dev environment","text":"<p>Now that you've modified a model, it's time to create a development environment so that you can validate the model change without affecting production.</p> <p>Run <code>vulcan plan dev</code> to create a development environment called <code>dev</code>:</p> <pre><code>$ vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nNew environment `dev` will be created from `prod`\n\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example__dev.incremental_model\n(Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.incremental_model: [2020-01-01 - 2025-04-17]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 6 of the output states that a new environment <code>dev</code> will be created from the existing <code>prod</code> environment.</p> <p>Lines 10-15 summarize the differences between the modified model and the <code>prod</code> environment, detecting that we directly modified <code>incremental_model</code> and that <code>full_model</code> was indirectly modified because it selects from the incremental model. Note that the model schemas are <code>vulcan_example__dev</code>, indicating that they are being created in the <code>dev</code> environment.</p> <p>On line 31, we see that Vulcan automatically classified the change as <code>Non-breaking</code> because it understood that the change was additive (added a column not used by <code>full_model</code>) and did not invalidate any data already in <code>prod</code>.</p> <p>Enter <code>y</code> at the prompt and press <code>Enter</code> to apply the plan and execute the backfill:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example__dev.incremental_model  [insert 2020-01-01 - 2025-04-17] 0.03s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Lines 3-5 show the progress and completion of updating the physical layer.</p> <p>Line 7 shows that Vulcan applied the change and evaluated <code>vulcan_example__dev.incremental_model</code> for the date range from 2020-01-01 to 2025-04-17.</p> <p>Lines 9-11 show the progress and completion of executing model batches.</p> <p>Lines 13-15 show the progress and completion of updating the virtual layer.</p> <p>Vulcan did not need to backfill anything for the <code>full_model</code> since the change was <code>Non-breaking</code>.</p>"},{"location":"quickstart/cli/#42-validate-updates-in-dev","title":"4.2 Validate updates in dev","text":"<p>You can now view this change by querying data from <code>incremental_model</code> with <code>vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"</code>.</p> <p>Note that the environment name <code>__dev</code> is appended to the schema namespace <code>vulcan_example</code> in the query:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre> <p>You can see that <code>new_column</code> was added to the dataset. The production table was not modified; you can validate this by querying the production table using <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>.</p> <p>Note that nothing has been appended to the schema namespace <code>vulcan_example</code> in this query because <code>prod</code> is the default environment.</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id   event_date\n0   1        2   2020-01-01\n1   2        1   2020-01-01\n2   3        3   2020-01-03\n3   4        1   2020-01-04\n4   5        1   2020-01-05\n5   6        1   2020-01-06\n6   7        1   2020-01-07\n</code></pre> <p>The production table does not have <code>new_column</code> because the changes to <code>dev</code> have not yet been applied to <code>prod</code>.</p>"},{"location":"quickstart/cli/#5-update-the-prod-environment","title":"5. Update the prod environment","text":""},{"location":"quickstart/cli/#51-apply-updates-to-prod","title":"5.1 Apply updates to prod","text":"<p>Now that we've tested the changes in dev, it's time to move them to production. Run <code>vulcan plan</code> to plan and apply your changes to the <code>prod</code> environment.</p> <p>Enter <code>y</code> and press <code>Enter</code> at the <code>Apply - Virtual Update [y/n]:</code> prompt to apply the plan and execute the backfill:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.full_model (Indirect Non-breaking)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Note that a backfill was not necessary and only a Virtual Update occurred, as indicated by the \"SKIP: No physical layer updates to perform\" and \"SKIP: No model batches to execute\" messages. This is because the changes were already calculated and executed in the <code>dev</code> environment, and Vulcan is smart enough to recognize that it only needs to update the virtual references to the existing tables rather than recomputing everything.</p>"},{"location":"quickstart/cli/#52-validate-updates-in-prod","title":"5.2 Validate updates in prod","text":"<p>Double-check that the data updated in <code>prod</code> by running <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre>"},{"location":"quickstart/cli/#6-next-steps","title":"6. Next steps","text":"<p>Congratulations, you've now conquered the basics of using Vulcan!</p> <p>From here, you can:</p> <ul> <li>Learn more about Vulcan CLI commands</li> <li>Set up a connection to a database or SQL engine</li> <li>Learn more about Vulcan concepts</li> <li>Join our Slack community</li> </ul>"},{"location":"quickstart/notebook/","title":"Notebook","text":""},{"location":"quickstart/notebook/#notebook","title":"Notebook","text":"<p>In this quickstart, you'll use the Vulcan notebook interface to get up and running with Vulcan's scaffold generator. This example project will run locally on your computer using DuckDB as an embedded SQL engine.</p> <p>Before beginning, ensure that you meet all the prerequisites for using Vulcan.</p> <p>The notebook interface works with both Jupyter and Databricks notebooks. Learn more about configuring a Databricks connection at the Execution Engines page.</p> Learn more about the quickstart project structure <p>This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.</p> <p>The project contains three models with a CSV file as the only data source:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502seed_data.csv\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n             \u2502\n            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502seed_model.sql\u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                          \u2502\n                         \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502incremental_model.sql\u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                                              \u2502\n                                             \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                             \u2502full_model.sql\u2502\n                                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.</p>"},{"location":"quickstart/notebook/#1-create-the-vulcan-project","title":"1. Create the Vulcan project","text":"<p>First, create a Vulcan project directory with your operating system's graphical or command-line tools. Next, create a Jupyter or Databricks notebook file - it does not need to be in the Vulcan project directory.</p> <p>If using a python virtual environment, ensure it's activated first by running the <code>source .venv/bin/activate</code> command from the folder used during installation.</p> <p>Import the Vulcan library to load the notebook magic commands:</p> <p></p> <p>Next, create a Vulcan scaffold with the <code>%init</code> notebook magic, specifying a default SQL dialect for your models. The dialect should correspond to the dialect most of your models are written in; it can be overridden for specific models in the model's <code>MODEL</code> specification. All SQL dialects supported by the SQLGlot library are allowed.</p> <p>In this example, we specify the <code>duckdb</code> dialect:</p> <p></p> <p>If the scaffold is successfully created, it will return <code>Vulcan project scaffold created</code>.</p> <p>The scaffold will include a Vulcan configuration file for the example project.</p> Learn more about the project's configuration <p>Vulcan project-level configuration parameters are specified in the <code>config.yaml</code> file in the project directory.</p> <p>This example project uses the embedded DuckDB SQL engine, so its configuration specifies <code>duckdb</code> as the local gateway's connection and the <code>local</code> gateway as the default.</p> <p>The command to run the scaffold generator requires a default SQL dialect for your models, which it places in the config <code>model_defaults</code> <code>dialect</code> key. In this example, we specified the <code>duckdb</code> SQL dialect as the default:</p> <pre><code>gateways:\n  local:\n    connection:\n      type: duckdb\n      database: ./db.db\n\ndefault_gateway: local\n\nmodel_defaults:\n  dialect: duckdb\n</code></pre> <p>Learn more about Vulcan project configuration here.</p> <p>The scaffold will also include multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).</p> Learn more about the project directories and files <p>Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.</p> <p>The scaffold generator will create the following configuration file and directories:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. Refer to configuration.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. Refer to models.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. Refer to seeds.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. Refer to auditing.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. Refer to testing.</li> </ul> </li> <li>./macros<ul> <li>Macro files. Refer to macros.</li> </ul> </li> </ul> <p>It will also create the files needed for this quickstart example:</p> <ul> <li>./models<ul> <li>full_model.sql</li> <li>incremental_model.sql</li> <li>seed_model.sql</li> </ul> </li> <li>./seeds<ul> <li>seed_data.csv</li> </ul> </li> <li>./audits<ul> <li>assert_positive_order_ids.sql</li> </ul> </li> <li>./tests<ul> <li>test_full_model.yaml</li> </ul> </li> </ul> <p>Finally, the scaffold will include data for the example project to use.</p> Learn more about the project's data <p>The data used in this example project is contained in the <code>seed_data.csv</code> file in the <code>/seeds</code> project directory. The data reflects sales of 3 items over 7 days in January 2020.</p> <p>The file contains three columns, <code>id</code>, <code>item_id</code>, and <code>event_date</code>, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.</p> <p>This is the complete dataset:</p> id item_id event_date 1 2 2020-01-01 2 1 2020-01-01 3 3 2020-01-03 4 1 2020-01-04 5 1 2020-01-05 6 1 2020-01-06 7 1 2020-01-07 <p>Inform Vulcan of the project location by setting a context with the <code>%context</code> notebook magic. If the context is set successfully, it will return a message including the repository or list of repositories:</p> <p></p> <p>You can specify multiple directories in one call to <code>%context</code> if your Vulcan project has multiple repositories.</p>"},{"location":"quickstart/notebook/#2-create-a-prod-environment","title":"2. Create a prod environment","text":"<p>Vulcan's key actions are creating and applying plans to environments. At this point, the only environment is the empty <code>prod</code> environment.</p> Learn more about Vulcan plans and environments <p>Vulcan's key actions are creating and applying plans to environments.</p> <p>A Vulcan environment is an isolated namespace containing models and the data they generated. The most important environment is <code>prod</code> (\"production\"), which consists of the databases behind the applications your business uses to operate each day. Environments other than <code>prod</code> provide a place where you can test and preview changes to model code before they go live and affect business operations.</p> <p>A Vulcan plan contains a comparison of one environment to another and the set of changes needed to bring them into alignment. For example, if a new SQL model was added, tested, and run in the <code>dev</code> environment, it would need to be added and run in the <code>prod</code> environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.</p> <p>Breaking changes are those that invalidate data already existing in an environment. For example, if a <code>WHERE</code> clause was added to a model in the <code>dev</code> environment, existing data created by that model in the <code>prod</code> environment are now invalid because they may contain rows that would be filtered out by the new <code>WHERE</code> clause. Other changes, like adding a new column to a model in <code>dev</code>, are non-breaking because all the existing data in <code>prod</code> are still valid to use - only new data must be added to align the environments.</p> <p>After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to \"backfill\" data to apply the plan - in this context, backfill is a generic term for updating or adding to a table's data (including an initial load or full refresh).</p> <p>The first Vulcan plan must execute every model to populate the production environment. Running the notebook magic <code>%plan</code> will generate the plan and the following output:</p> <p></p> <p>The first block of output notes that <code>%plan</code> successfully executed the project's test <code>tests/test_full_model.yaml</code> with duckdb.</p> <p>The <code>New environment</code> line describes what environments the plan will affect when applied - a new <code>prod</code> environment in this case.</p> <p>The <code>Summary of differences</code> section shows that Vulcan detected three new models relative to the current empty environment.</p> <p>The <code>Models needing backfill</code> section lists each model that will be executed by the plan, along with the date intervals that will be run. Both <code>full_model</code> and <code>incremental_model</code> show <code>2020-01-01</code> as their start date because:</p> <ol> <li>The incremental model specifies that date in the <code>start</code> property of its <code>MODEL</code> statement and</li> <li>The full model depends on the incremental model.</li> </ol> <p>The <code>seed_model</code> date range begins on the same day the plan was made because <code>SEED</code> models have no temporality associated with them other than whether they have been modified since the previous Vulcan plan.</p> Learn more about the project's models <p>A plan's actions are determined by the kinds of models the project uses. This example project uses three model kinds:</p> <ol> <li><code>SEED</code> models read data from CSV files stored in the Vulcan project directory.</li> <li><code>FULL</code> models fully refresh (rewrite) the data associated with the model every time the model is run.</li> <li><code>INCREMENTAL_BY_TIME_RANGE</code> models use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.</li> </ol> <p>We now briefly review each model in the project.</p> <p>The first model is a <code>SEED</code> model that imports <code>seed_data.csv</code>. This model consists of only a <code>MODEL</code> statement because <code>SEED</code> models do not query a database.</p> <p>In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the <code>grain</code> of the model to the columns that collectively form the model's unique identifier, <code>id</code> and <code>event_date</code>.</p> <pre><code>MODEL (\n  name vulcan_example.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INTEGER,\n    item_id INTEGER,\n    event_date DATE\n  ),\n  grain (id, event_date)\n);\n</code></pre> <p>The second model is an <code>INCREMENTAL_BY_TIME_RANGE</code> model that includes both a <code>MODEL</code> statement and a SQL query selecting from the first seed model.</p> <p>The <code>MODEL</code> statement's <code>kind</code> property includes the required specification of the data column containing each record's timestamp. It also includes the optional <code>start</code> property specifying the earliest date/time for which the model should process data and the <code>cron</code> property specifying that the model should run daily. It sets the model's grain to columns <code>id</code> and <code>event_date</code>.</p> <p>The SQL query includes a <code>WHERE</code> clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre> <p>The final model in the project is a <code>FULL</code> model. In addition to properties used in the other models, its <code>MODEL</code> statement includes the <code>audits</code> property. The project includes a custom <code>assert_positive_order_ids</code> audit in the project <code>audits</code> directory; it verifies that all <code>item_id</code> values are positive numbers. It will be run every time the model is executed.</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>Click the green button labeled <code>Apply - Backfill Tables</code> to apply the plan and initiate backfill. The following output will be displayed:</p> <p></p> <p>The first output block shows the completion percentage and run time for each model (very fast in this simple example). The following line shows that the <code>prod</code> environment now points to the tables created during model execution.</p> <p>You've now created a new production environment with all of history backfilled.</p>"},{"location":"quickstart/notebook/#3-update-a-model","title":"3. Update a model","text":"<p>Now that we have have populated the <code>prod</code> environment, let's modify one of the SQL models.</p> <p>We can modify the incremental SQL model using the <code>%model</code> line notebook magic (note the single <code>%</code>) and the model name:</p> <p></p> <p>After we execute the cell, the contents will be replaced by the <code>%%model</code> cell notebook magic (note the double <code>%%</code>) and the model contents, along with a rendered version of the model SQL query. Vulcan has automatically added explicit column aliases to the query (e.g., <code>id AS id</code>):</p> <p></p> <p>We modify the incremental SQL model by adding a new column to the query. When we execute the cell it will write the updated model contents to the file and update the rendered version of the query:</p> <p></p>"},{"location":"quickstart/notebook/#4-work-with-a-development-environment","title":"4. Work with a development environment","text":""},{"location":"quickstart/notebook/#41-create-a-dev-environment","title":"4.1 Create a dev environment","text":"<p>Now that you've modified a model, it's time to create a development environment so that you can validate the model change without affecting production.</p> <p>Run <code>%plan dev</code> to create a development environment called <code>dev</code>. The following output will be displayed:</p> <p></p> <p>The first block of output notes that <code>%plan</code> successfully executed the project's test <code>tests/test_full_model.yaml</code> with duckdb.</p> <p>The <code>New environment</code> line describes what environments the plan will affect when applied - a new <code>dev</code> environment will be created from the existing <code>prod</code> environment.</p> <p>The <code>Summary of differences</code> section summarizes the differences between the modified model and the new <code>dev</code> environment (right now just a copy of <code>prod</code>), detecting that we directly modified <code>incremental_model</code> and that <code>full_model</code> was indirectly modified because it selects from the incremental model. It shows a diff between the existing and updated model.</p> <p>Vulcan automatically classified the change as <code>Non-breaking</code> because understood that the change was additive (added a column not used by <code>full_model</code>) and did not invalidate any data already in <code>prod</code>.</p> <p>The <code>Models needing backfill</code> section shows that only the directly modified <code>incremental_model</code> needs backfill and provides a date picker to specify the start and end dates for the backfill.</p> <p>Click the green button to perform the backfill:</p> <p></p> <p>The output shows that Vulcan created a new model version in <code>dev</code>. The last line of the output shows that Vulcan applied the change to <code>vulcan_example__dev.incremental_model</code>. In the model schema, the suffix \"<code>__dev</code>\" indicates that it is in the <code>dev</code> environment.</p> <p>Vulcan did not need to backfill anything for the <code>full_model</code> since the change was <code>Non-breaking</code>.</p>"},{"location":"quickstart/notebook/#42-validate-updates-in-dev","title":"4.2 Validate updates in dev","text":"<p>You can now view this change by querying data from <code>incremental_model</code> with the <code>%%fetchdf</code> cell magic (note the two <code>%</code> symbols) and the SQL query <code>select * from vulcan_example__dev.incremental_model</code>.</p> <p>Note that the environment name <code>__dev</code> is appended to the schema namespace <code>vulcan_example</code> in the query:</p> <p></p> <p>You can see that <code>new_column</code> was added to the dataset.</p> <p>The production table was not modified; you can validate this by querying the production table using <code>%%fetchdf</code> and the query <code>select * from vulcan_example.incremental_model</code>.</p> <p>Note that nothing has been appended to the schema namespace <code>vulcan_example</code> because <code>prod</code> is the default environment:</p> <p></p> <p>The production table does not have <code>new_column</code> because the changes to <code>dev</code> have not yet been applied to <code>prod</code>.</p>"},{"location":"quickstart/notebook/#5-update-the-prod-environment","title":"5. Update the prod environment","text":"<p>Now that we've tested the changes in dev, it's time to move them to production. Run <code>%plan</code> to plan and apply your changes to the <code>prod</code> environment:</p> <p></p> <p>Click the green <code>Apply - Virtual Update</code> button to apply the plan and execute the backfill:</p> <p></p> <p>Note that a backfill was not necessary and only a Virtual Update occurred.</p>"},{"location":"quickstart/notebook/#52-validate-updates-in-prod","title":"5.2 Validate updates in prod","text":"<p>Double-check that the data updated in <code>prod</code> by running <code>%%fetchdf</code> with the SQL query <code>select * from vulcan_example.incremental_model</code>:</p> <p></p> <p><code>new_column</code> is now present in the <code>prod</code> incremental model.</p>"},{"location":"quickstart/notebook/#6-next-steps","title":"6. Next steps","text":"<p>Congratulations, you've now conquered the basics of using Vulcan!</p> <p>From here, you can:</p> <ul> <li>Learn more about Vulcan notebook commands</li> <li>Set up a connection to a database or SQL engine</li> <li>Learn more about Vulcan concepts</li> <li>Join our Slack community</li> </ul>"},{"location":"quickstart/ui/","title":"Browser UI","text":""},{"location":"quickstart/ui/#browser-ui","title":"Browser UI","text":"<p>Warning</p> <p>Browser UI is deprecated. Please use the VSCode extension instead.</p> <p>In this quickstart, you'll use the Vulcan browser user interface to get up and running with Vulcan's scaffold generator. This example project will run locally on your computer using DuckDB as an embedded SQL engine.</p> Learn more about the quickstart project structure <p>This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.</p> <p>The project contains three models with a CSV file as the only data source:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502seed_data.csv\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n             \u2502\n            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502seed_model.sql\u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                          \u2502\n                        \u250c\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502incremental_model.sql\u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                                             \u2502\n                                            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                            \u2502full_model.sql\u2502\n                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.</p>"},{"location":"quickstart/ui/#setup","title":"Setup","text":"<p>Before beginning, ensure that you meet all the prerequisites for using Vulcan. The Vulcan browser UI requires additional Python libraries not included in the base Vulcan installation.</p> <p>To use the UI, install Vulcan with the <code>web</code> add-on. First, if using a python virtual environment, ensure it's activated by running <code>source .venv/bin/activate</code> command from the folder used during installation.</p> <p>Next, install the UI with <code>pip</code>:</p> <pre><code>pip install \"vulcan[web]\"\n</code></pre>"},{"location":"quickstart/ui/#1-create-the-vulcan-project","title":"1. Create the Vulcan project","text":"<p>Before working in the Vulcan browser UI, create a project directory with your operating system's graphical interface or from the command line:</p> <pre><code>mkdir vulcan-example\n</code></pre> <p>Navigate to the directory on the command line:</p> <pre><code>cd vulcan-example\n</code></pre> <p>If using a python virtual environment, ensure it's activated by running <code>source .venv/bin/activate</code> from the folder used during installation.</p> <p>Create a Vulcan scaffold with the following command, specifying a default SQL dialect for your models. The dialect should correspond to the dialect most of your models are written in; it can be overridden for specific models in the model's <code>MODEL</code> specification. All SQL dialects supported by the SQLGlot library are allowed.</p> <p>In this example, we specify the <code>duckdb</code> dialect:</p> <pre><code>vulcan init duckdb\n</code></pre> <p>The scaffold will include a Vulcan configuration file for the example project.</p> Learn more about the project's configuration <p>Vulcan project-level configuration parameters are specified in the <code>config.yaml</code> file in the project directory.</p> <p>This example project uses the embedded DuckDB SQL engine, so its configuration specifies <code>duckdb</code> as the local gateway's connection and the <code>local</code> gateway as the default.</p> <p>The command to run the scaffold generator requires a default SQL dialect for your models, which it places in the config <code>model_defaults</code> <code>dialect</code> key. In this example, we specified the <code>duckdb</code> SQL dialect as the default:</p> <pre><code>gateways:\n  local:\n    connection:\n      type: duckdb\n      database: ./db.db\n\ndefault_gateway: local\n\nmodel_defaults:\n  dialect: duckdb\n</code></pre> <p>Learn more about Vulcan project configuration here.</p> <p>The scaffold will also include multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).</p> Learn more about the project directories and files <p>Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.</p> <p>The scaffold generator will create the following configuration file and directories:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. Refer to configuration.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. Refer to models.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. Refer to seeds.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. Refer to auditing.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. Refer to testing.</li> </ul> </li> <li>./macros<ul> <li>Macro files. Refer to macros.</li> </ul> </li> </ul> <p>It will also create the files needed for this quickstart example:</p> <ul> <li>./models<ul> <li>full_model.sql</li> <li>incremental_model.sql</li> <li>seed_model.sql</li> </ul> </li> <li>./seeds<ul> <li>seed_data.csv</li> </ul> </li> <li>./audits<ul> <li>assert_positive_order_ids.sql</li> </ul> </li> <li>./tests<ul> <li>test_full_model.yaml</li> </ul> </li> </ul> <p>Finally, the scaffold will include data for the example project to use.</p> Learn more about the project's data <p>The data used in this example project is contained in the <code>seed_data.csv</code> file in the <code>/seeds</code> project directory. The data reflects sales of 3 items over 7 days in January 2020.</p> <p>The file contains three columns, <code>id</code>, <code>item_id</code>, and <code>event_date</code>, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.</p> <p>This is the complete dataset:</p> id item_id event_date 1 2 2020-01-01 2 1 2020-01-01 3 3 2020-01-03 4 1 2020-01-04 5 1 2020-01-05 6 1 2020-01-06 7 1 2020-01-07"},{"location":"quickstart/ui/#2-open-the-vulcan-web-ui","title":"2. Open the Vulcan web UI","text":"<p>Open the UI by running the <code>vulcan ui</code> command from within the project directory:</p> <pre><code>vulcan ui\n</code></pre> <p>After starting up, the Vulcan web UI is served at <code>http://127.0.0.1:8000</code> by default:</p> <p></p> <p>Navigate to the URL by clicking the link in your terminal (if supported) or copy-pasting it into your web browser:</p> <p></p> <p>The Vulcan UI default view contains five panes:</p> <ol> <li>Project directory allows navigation of project directories and files.</li> <li>Editor tabs displays open code editors.</li> <li>Code editor allows viewing and editing code files.</li> <li>Inspector provides settings and information based on recent actions and the currently active pane. (Note: inspector pane is collapsed by default. Expand it by clicking the hamburger button at the top of the collapsed pane - see previous image.)</li> <li>Details displays column-level lineage for models open in the editor and results of queries. (Note: details pane is collapsed by default. It will automatically expand upon opening a model in the editor or running a query.)</li> </ol> <p></p> <p>It also contains nine buttons:</p> <ol> <li>Toggle Editor/Data Catalog/Errors toggles among the Code Editor (default), Data Catalog, and Errors views. Errors view is only available if an error has occurred.</li> <li>History navigation returns to previous views, similar to the back button in a web browser.</li> <li>Add new tab opens a new code editor window.</li> <li>Run plan command executes the <code>vulcan plan</code> command.</li> <li>Documentation links to the Vulcan documentation website.</li> <li>The crescent moon toggles between page light and dark modes.</li> <li>Run SQL query executes the <code>vulcan fetchdf</code> command.</li> <li>Format SQL query reformats a SQL query using SQLGlot's pretty layout.</li> <li>Change SQL dialect specifies the SQL dialect of the current tab for custom SQL queries. It does not affect the SQL dialect for the project.</li> </ol> <p></p> <p>The default view contains four status indicators:</p> <ol> <li>Editor tab language displays the programming language of the current code editor tab (SQL or Python).</li> <li>Current environment displays the currently selected environment</li> <li>Change indicator displays a summary of the changes in the project files relative to the most recently run Vulcan plan in the selected environment.</li> <li>Error indicator displays the count of errors in the project.</li> </ol> <p></p>"},{"location":"quickstart/ui/#3-plan-and-apply-environments","title":"3. Plan and apply environments","text":""},{"location":"quickstart/ui/#31-create-a-prod-environment","title":"3.1 Create a prod environment","text":"<p>Vulcan's key actions are creating and applying plans to environments. At this point, the only environment is the empty <code>prod</code> environment.</p> Learn more about Vulcan plans and environments <p>Vulcan's key actions are creating and applying plans to environments.</p> <p>A Vulcan environment is an isolated namespace containing models and the data they generated. The most important environment is <code>prod</code> (\"production\"), which consists of the databases behind the applications your business uses to operate each day. Environments other than <code>prod</code> provide a place where you can test and preview changes to model code before they go live and affect business operations.</p> <p>A Vulcan plan contains a comparison of one environment to another and the set of changes needed to bring them into alignment. For example, if a new SQL model was added, tested, and run in the <code>dev</code> environment, it would need to be added and run in the <code>prod</code> environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.</p> <p>Breaking changes are those that invalidate data already existing in an environment. For example, if a <code>WHERE</code> clause was added to a model in the <code>dev</code> environment, existing data created by that model in the <code>prod</code> environment are now invalid because they may contain rows that would be filtered out by the new <code>WHERE</code> clause. Other changes, like adding a new column to a model in <code>dev</code>, are non-breaking because all the existing data in <code>prod</code> are still valid to use - only new data must be added to align the environments.</p> <p>After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to \"backfill\" data to apply the plan - in this context, backfill is a generic term for updating or adding to a table's data (including an initial load or full refresh).</p> <p>The first Vulcan plan must execute every model to populate the production environment. Click the green <code>Plan</code> button in the top right, and a new pane will open.</p> <p>The pane contains multiple pieces of information about the plan:</p> <ul> <li>The <code>Initializing Prod Environment</code> section shows that the plan is initializing the <code>prod</code> environment.</li> <li>The Start and End date sections are grayed out because they are not allowed when running a plan in the <code>prod</code> environment.</li> <li>The <code>Changes</code> section shows that Vulcan detected three models added relative to the current empty environment.</li> <li>The <code>Backfills</code> section shows that backfills will occur for all three of the added models.</li> </ul> Learn more about the project's models <p>A plan's actions are determined by the kinds of models the project uses. This example project uses three model kinds:</p> <ol> <li><code>SEED</code> models read data from CSV files stored in the Vulcan project directory.</li> <li><code>FULL</code> models fully refresh (rewrite) the data associated with the model every time the model is run.</li> <li><code>INCREMENTAL_BY_TIME_RANGE</code> models use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.</li> </ol> <p>We now briefly review each model in the project.</p> <p>The first model is a <code>SEED</code> model that imports <code>seed_data.csv</code>. This model consists of only a <code>MODEL</code> statement because <code>SEED</code> models do not query a database.</p> <p>In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the <code>grain</code> of the model to the columns that collectively form the model's unique identifier, <code>id</code> and <code>ds</code>.</p> <pre><code>MODEL (\n  name vulcan_example.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INTEGER,\n    item_id INTEGER,\n    event_date DATE\n  ),\n  grain (id, event_date)\n);\n</code></pre> <p>The second model is an <code>INCREMENTAL_BY_TIME_RANGE</code> model that includes both a <code>MODEL</code> statement and a SQL query selecting from the first seed model.</p> <p>The <code>MODEL</code> statement's <code>kind</code> property includes the required specification of the data column containing each record's timestamp. It also includes the optional <code>start</code> property specifying the earliest date/time for which the model should process data and the <code>cron</code> property specifying that the model should run daily. It sets the model's grain to columns <code>id</code> and <code>event_date</code>.</p> <p>The SQL query includes a <code>WHERE</code> clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre> <p>The final model in the project is a <code>FULL</code> model. In addition to properties used in the other models, its <code>MODEL</code> statement includes the <code>audits</code> property. The project includes a custom <code>assert_positive_order_ids</code> audit in the project <code>audits</code> directory; it verifies that all <code>item_id</code> values are positive numbers. It will be run every time the model is executed.</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p></p> <p>Click the blue button labeled <code>Apply Changes And Backfill</code> to apply the plan and initiate backfill.</p> <p>The page will update and new output sections will appear. Each section reflects a stage in the plan application and will be green if the step succeeded.</p> <p>The <code>Tests Completed</code> section indicates that the project's unit tests ran successfully.</p> <p>The <code>Snapshot Tables Created</code> indicates that snapshots of the added and modified models were created successfully.</p> <p>The <code>Backfilled</code> section shows progress indicators for the backfill operations. The first progress indicator shows the total number of tasks and completion percentage for the entire backfill operation. The remaining progress bars show completion percentage and run time for each model (very fast in this simple example).</p> <p></p> <p>Click the <code>Go Back</code> button to close the pane.</p> <p>You've now created a new production environment with all of history backfilled.</p>"},{"location":"quickstart/ui/#22-create-a-dev-environment","title":"2.2 Create a dev environment","text":"<p>Now that you've created the production environment, it's time to create a development environment so you can modify models without affecting production.</p> <p>Open the environment menu by clicking the button labeled <code>prod \\/</code> next to the green <code>Plan</code> button on the top right. Type <code>dev</code> into the Environment field and click the blue <code>Add</code> button.</p> <p></p> <p>The button now shows that the Vulcan UI is working in the <code>dev</code> environment:</p> <p></p> <p>Click the green <code>Plan</code> button, and a new pane will open:</p> <p></p> <p>The output section does not list any added/modified models or backfills because <code>dev</code> is being created from the existing <code>prod</code> environment without modification. Because the project has not been modified, no new computations need to run and a virtual update occurs.</p> <p>Click the blue <code>Apply Virtual Update</code> button to apply the new plan:</p> <p></p> <p>The output confirms that the tests, virtual update, snapshot table creation, and environment promotion steps have completed. Click the <code>Go Back</code> button to close the pane.</p>"},{"location":"quickstart/ui/#3-make-your-first-update","title":"3. Make your first update","text":"<p>Now that we have populated both <code>prod</code> and <code>dev</code> environments, let's modify one of the SQL models, validate it in <code>dev</code>, and push it to <code>prod</code>.</p>"},{"location":"quickstart/ui/#31-edit-the-model-query","title":"3.1 Edit the model query","text":"<p>To modify the incremental SQL model, open it in the editor by clicking on it in the project directory pane on the left side of the window.</p> <p>The <code>Details</code> pane at the bottom displays the project's table and column lineage.</p> <p></p> <p>Modify the incremental SQL model by adding a new column to the query. Press <code>Cmd + S</code> (<code>Ctrl + S</code> on Windows) to save the modified model file and display the updated lineage:</p> <p></p>"},{"location":"quickstart/ui/#4-plan-and-apply-updates","title":"4. Plan and apply updates","text":"<p>Preview the impact of the change by clicking the green <code>Plan</code> button in the top right.</p> <p></p> <p>The <code>Changes</code> section detects that we directly modified <code>incremental_model</code> and that <code>full_model</code> was indirectly modified because it selects from the incremental model. Vulcan understood that the change was additive (added a column not used by <code>full_model</code>) and was automatically classified as a non-breaking change.</p> <p>The <code>Backfill</code> section shows that only <code>incremental_model</code> requires backfill. Click the blue <code>Apply Changes And Backfill</code> button to apply the plan and execute the backfill:</p> <p></p> <p>Vulcan applies the change to <code>vulcan_example.incremental_model</code> and backfills the model. The <code>Backfilled</code> section shows that the backfill completed successfully.</p>"},{"location":"quickstart/ui/#41-validate-updates-in-dev","title":"4.1 Validate updates in dev","text":"<p>You can now view this change by querying data from <code>incremental_model</code>. Add the SQL query <code>select * from vulcan_example__dev.incremental_model</code> to the Custom SQL 1 tab in the editor:</p> <p></p> <p>Note that the environment name <code>__dev</code> is appended to the schema namespace <code>vulcan_example</code> in the query: <code>select * from vulcan_example__dev.incremental_model</code>.</p> <p>Click the <code>Run Query</code> button in the bottom right to execute the query:</p> <p></p> <p>You can see that <code>new_column</code> was added to the dataset. The production table was not modified; you can validate this by modifying the query so it selects from the production table with <code>select * from vulcan_example.incremental_model</code>.</p> <p>Note that nothing has been appended to the schema namespace <code>vulcan_example</code> because <code>prod</code> is the default environment.</p> <p></p> <p>The production table does not have <code>new_column</code> because the changes to <code>dev</code> have not yet been applied to <code>prod</code>.</p>"},{"location":"quickstart/ui/#42-apply-updates-to-prod","title":"4.2 Apply updates to prod","text":"<p>Now that we've tested the changes in dev, it's time to move them to prod. Open the environment menu in top right and select the <code>prod</code> environment:</p> <p></p> <p>Click the green <code>Plan</code> button to open the run plan interface:</p> <p></p> <p>Click the blue <code>Apply Virtual Update</code> button, and a warning screen will appear:</p> <p></p> <p>Click the <code>Yes, Run prod</code> button to proceed with applying the plan:</p> <p></p> <p>Note that a backfill was not necessary and only a Virtual Update occurred - the computations have already occurred when backfilling the model in <code>dev</code>. Click the <code>Go Back</code> button to close the pane.</p>"},{"location":"quickstart/ui/#43-validate-updates-in-prod","title":"4.3. Validate updates in prod","text":"<p>Double-check that the data updated in <code>prod</code> by re-running the SQL query from the editor. Click the <code>Run Query</code> button to execute the query:</p> <p></p> <p><code>new_column</code> is now present in the <code>prod</code> incremental model.</p>"},{"location":"quickstart/ui/#5-next-steps","title":"5. Next steps","text":"<p>Congratulations, you've now conquered the basics of using Vulcan!</p> <p>From here, you can:</p> <ul> <li>Set up a connection to a database or SQL engine</li> <li>Learn more about Vulcan concepts</li> <li>Join our Slack community</li> </ul>"},{"location":"reference/cli/","title":"CLI","text":""},{"location":"reference/cli/#cli","title":"CLI","text":"<pre><code>Usage: vulcan [OPTIONS] COMMAND [ARGS]...\n\n  Vulcan command line tool.\n\nOptions:\n  --version            Show the version and exit.\n  -p, --paths TEXT     Path(s) to the Vulcan config/project.\n  --config TEXT        Name of the config object. Only applicable to\n                       configuration defined using Python script.\n  --gateway TEXT       The name of the gateway.\n  --ignore-warnings    Ignore warnings.\n  --debug              Enable debug mode.\n  --log-to-stdout      Display logs in stdout.\n  --log-file-dir TEXT  The directory to write log files to.\n  --help               Show this message and exit.\n\nCommands:\n  audit                   Run audits for the target model(s).\n  clean                   Clears the Vulcan cache and any build artifacts.\n  create_external_models  Create a schema file containing external model...\n  create_test             Generate a unit test fixture for a given model.\n  dag                     Render the DAG as an html file.\n  destroy                 The destroy command removes all project resources.\n  diff                    Show the diff between the local state and the...\n  dlt_refresh             Attaches to a DLT pipeline with the option to...\n  environments            Prints the list of Vulcan environments with...\n  evaluate                Evaluate a model and return a dataframe with a...\n  fetchdf                 Run a SQL query and display the results.\n  format                  Format all SQL models and audits.\n  info                    Print information about a Vulcan project.\n  init                    Create a new Vulcan repository.\n  invalidate              Invalidate the target environment, forcing its...\n  janitor                 Run the janitor process on-demand.\n  migrate                 Migrate Vulcan to the current running version.\n  plan                    Apply local changes to the target environment.\n  prompt                  Uses LLM to generate a SQL query from a prompt.\n  render                  Render a model's query, optionally expanding...\n  rewrite                 Rewrite a SQL expression with semantic...\n  rollback                Rollback Vulcan to the previous migration.\n  run                     Evaluate missing intervals for the target...\n  state                   Commands for interacting with state\n  table_diff              Show the diff between two tables.\n  table_name              Prints the name of the physical table for the...\n  test                    Run model unit tests.\n  ui                      Start a browser-based Vulcan UI.\n  lint                    Run the linter for the target model(s).\n</code></pre>"},{"location":"reference/cli/#audit","title":"audit","text":"<pre><code>Usage: vulcan audit [OPTIONS]\n\n  Run audits for the target model(s).\n\nOptions:\n  --model TEXT           A model to audit. Multiple models can be audited.\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#check_intervals","title":"check_intervals","text":"<pre><code>Usage: vulcan check_intervals [OPTIONS] [ENVIRONMENT]\n\n  Show missing intervals in an environment, respecting signals.\n\nOptions:\n  --no-signals         Disable signal checks and only show missing intervals.\n  --select-model TEXT  Select specific models to show missing intervals for.\n  -s, --start TEXT     The start datetime of the interval for which this\n                       command will be applied.\n  -e, --end TEXT       The end datetime of the interval for which this command\n                       will be applied.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#clean","title":"clean","text":"<pre><code>Usage: vulcan clean [OPTIONS]\n\n  Clears the Vulcan cache and any build artifacts.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#create_external_models","title":"create_external_models","text":"<pre><code>Usage: vulcan create_external_models [OPTIONS]\n\n  Create a schema file containing external model schemas.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#create_test","title":"create_test","text":"<pre><code>Usage: vulcan create_test [OPTIONS] MODEL\n\n  Generate a unit test fixture for a given model.\n\nOptions:\n  -q, --query &lt;TEXT TEXT&gt;...  Queries that will be used to generate data for\n                              the model's dependencies.\n  -o, --overwrite             When true, the fixture file will be overwritten\n                              in case it already exists.\n  -v, --var &lt;TEXT TEXT&gt;...    Key-value pairs that will define variables\n                              needed by the model.\n  -p, --path TEXT             The file path corresponding to the fixture,\n                              relative to the test directory. By default, the\n                              fixture will be created under the test directory\n                              and the file name will be inferred based on the\n                              test's name.\n  -n, --name TEXT             The name of the test that will be created. By\n                              default, it's inferred based on the model's\n                              name.\n  --include-ctes              When true, CTE fixtures will also be generated.\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#dag","title":"dag","text":"<pre><code>Usage: vulcan dag [OPTIONS] FILE\n\n  Render the DAG as an html file.\n\nOptions:\n  --select-model TEXT  Select specific models to include in the dag.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#destroy","title":"destroy","text":"<pre><code>Usage: vulcan destroy\n\n  Removes all state tables, the Vulcan cache and all project resources, including warehouse objects. This includes all tables, views and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.\n\nOptions:\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#dlt_refresh","title":"dlt_refresh","text":"<pre><code>Usage: dlt_refresh PIPELINE [OPTIONS]\n\n  Attaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.\n\nOptions:\n  -t, --table TEXT  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.\n  -f, --force       If set it will overwrite existing models with the new generated models from the DLT tables.\n  --help            Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#diff","title":"diff","text":"<pre><code>Usage: vulcan diff [OPTIONS] ENVIRONMENT\n\n  Show the diff between the local state and the target environment.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#environments","title":"environments","text":"<pre><code>Usage: vulcan environments [OPTIONS]\n\n  Prints the list of Vulcan environments with its expiry datetime.\n\nOptions:\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#evaluate","title":"evaluate","text":"<pre><code>Usage: vulcan evaluate [OPTIONS] MODEL\n\n  Evaluate a model and return a dataframe with a default limit of 1000.\n\nOptions:\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --limit INTEGER        The number of rows which the query should be limited\n                         to.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#fetchdf","title":"fetchdf","text":"<pre><code>Usage: vulcan fetchdf [OPTIONS] SQL\n\n  Run a SQL query and display the results.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#format","title":"format","text":"<pre><code>Usage: vulcan format [OPTIONS]\n\n  Format all SQL models and audits.\n\nOptions:\n  -t, --transpile TEXT        Transpile project models to the specified\n                              dialect.\n  --append-newline            Include a newline at the end of each file.\n  --no-rewrite-casts          Preserve the existing casts, without rewriting\n                              them to use the :: syntax.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --check                     Whether or not to check formatting (but not\n                              actually format anything).\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#info","title":"info","text":"<pre><code>Usage: vulcan info [OPTIONS]\n\n  Print information about a Vulcan project.\n\n  Includes counts of project models and macros and connection tests for the\n  data warehouse.\n\nOptions:\n  --skip-connection  Skip the connection test.\n  -v, --verbose      Verbose output.\n  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#init","title":"init","text":"<pre><code>Usage: vulcan init [OPTIONS] [ENGINE]\n\n  Create a new Vulcan repository.\n\nOptions:\n  -t, --template TEXT  Project template. Supported values: dbt, dlt, default,\n                       empty.\n  --dlt-pipeline TEXT  DLT pipeline for which to generate a Vulcan project.\n                       Use alongside template: dlt\n  --dlt-path TEXT      The directory where the DLT pipeline resides. Use\n                       alongside template: dlt\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#invalidate","title":"invalidate","text":"<pre><code>Usage: vulcan invalidate [OPTIONS] ENVIRONMENT\n\n  Invalidate the target environment, forcing its removal during the next run\n  of the janitor process.\n\nOptions:\n  -s, --sync  Wait for the environment to be deleted before returning. If not\n              specified, the environment will be deleted asynchronously by the\n              janitor process. This option requires a connection to the data\n              warehouse.\n  --help      Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#janitor","title":"janitor","text":"<pre><code>Usage: vulcan janitor [OPTIONS]\n\n  Run the janitor process on-demand.\n\n  The janitor cleans up old environments and expired snapshots.\n\nOptions:\n  --ignore-ttl  Cleanup snapshots that are not referenced in any environment,\n                regardless of when they're set to expire\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#migrate","title":"migrate","text":"<pre><code>Usage: vulcan migrate [OPTIONS]\n\n  Migrate Vulcan to the current running version.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>migrate</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"reference/cli/#plan","title":"plan","text":"<pre><code>Usage: vulcan plan [OPTIONS] [ENVIRONMENT]\n\n  Apply local changes to the target environment.\n\nOptions:\n  -s, --start TEXT                The start datetime of the interval for which\n                                  this command will be applied.\n  -e, --end TEXT                  The end datetime of the interval for which\n                                  this command will be applied.\n  --execution-time TEXT           The execution time (defaults to now).\n  --create-from TEXT              The environment to create the target\n                                  environment from if it doesn't exist.\n                                  Default: prod.\n  --skip-tests                    Skip tests prior to generating the plan if\n                                  they are defined.\n  --skip-linter                   Skip linting prior to generating the plan if\n                                  the linter is enabled.\n  -r, --restate-model TEXT        Restate data for specified models and models\n                                  downstream from the one specified. For\n                                  production environment, all related model\n                                  versions will have their intervals wiped,\n                                  but only the current versions will be\n                                  backfilled. For development environment,\n                                  only the current model versions will be\n                                  affected.\n  --no-gaps                       Ensure that new snapshots have no data gaps\n                                  when comparing to existing snapshots for\n                                  matching models in the target environment.\n  --skip-backfill, --dry-run      Skip the backfill step and only create a\n                                  virtual update for the plan.\n  --empty-backfill                Produce empty backfill. Like --skip-backfill\n                                  no models will be backfilled, unlike --skip-\n                                  backfill missing intervals will be recorded\n                                  as if they were backfilled.\n  --forward-only                  Create a plan for forward-only changes.\n  --allow-destructive-model TEXT  Allow destructive forward-only changes to\n                                  models whose names match the expression.\n  --allow-additive-model TEXT     Allow additive forward-only changes to\n                                  models whose names match the expression.\n  --effective-from TEXT           The effective date from which to apply\n                                  forward-only changes on production.\n  --no-prompts                    Disable interactive prompts for the backfill\n                                  time range. Please note that if this flag is\n                                  set and there are uncategorized changes,\n                                  plan creation will fail.\n  --auto-apply                    Automatically apply the new plan after\n                                  creation.\n  --no-auto-categorization        Disable automatic change categorization.\n  --include-unmodified            Include unmodified models in the target\n                                  environment.\n  --select-model TEXT             Select specific model changes that should be\n                                  included in the plan.\n  --backfill-model TEXT           Backfill only the models whose names match\n                                  the expression.\n  --no-diff                       Hide text differences for changed models.\n  --run                           Run latest intervals as part of the plan\n                                  application (prod environment only).\n  --enable-preview                Enable preview for forward-only models when\n                                  targeting a development environment.\n  --diff-rendered                 Output text differences for the rendered\n                                  versions of the models and standalone\n                                  audits.\n  --explain                       Explain the plan instead of applying it.\n  -v, --verbose                   Verbose output. Use -vv for very verbose\n                                  output.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#prompt","title":"prompt","text":"<pre><code>Usage: vulcan prompt [OPTIONS] PROMPT\n\n  Uses LLM to generate a SQL query from a prompt.\n\nOptions:\n  -e, --evaluate           Evaluate the generated SQL query and display the\n                           results.\n  -t, --temperature FLOAT  Sampling temperature. 0.0 - precise and\n                           predictable, 0.5 - balanced, 1.0 - creative.\n                           Default: 0.7\n  -v, --verbose            Verbose output.\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#render","title":"render","text":"<pre><code>Usage: vulcan render [OPTIONS] MODEL\n\n  Render a model's query, optionally expanding referenced models.\n\nOptions:\n  -s, --start TEXT            The start datetime of the interval for which\n                              this command will be applied.\n  -e, --end TEXT              The end datetime of the interval for which this\n                              command will be applied.\n  --execution-time TEXT       The execution time (defaults to now).\n  --expand TEXT               Whether or not to expand materialized models\n                              (defaults to False). If True, all referenced\n                              models are expanded as raw queries. Multiple\n                              model names can also be specified, in which case\n                              only they will be expanded as raw queries.\n  --dialect TEXT              The SQL dialect to render the query as.\n  --no-format                 Disable fancy formatting of the query.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#rewrite","title":"rewrite","text":"<pre><code>Usage: vulcan rewrite [OPTIONS] SQL\n\n  Rewrite a SQL expression with semantic references into an executable query.\n\n  https://vulcan.readthedocs.io/en/latest/concepts/metrics/overview/\n\nOptions:\n  --read TEXT   The input dialect of the sql string.\n  --write TEXT  The output dialect of the sql string.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#rollback","title":"rollback","text":"<pre><code>Usage: vulcan rollback [OPTIONS]\n\n  Rollback Vulcan to the previous migration.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>rollback</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"reference/cli/#run","title":"run","text":"<pre><code>Usage: vulcan run [OPTIONS] [ENVIRONMENT]\n\n  Evaluate missing intervals for the target environment.\n\nOptions:\n  -s, --start TEXT              The start datetime of the interval for which\n                                this command will be applied.\n  -e, --end TEXT                The end datetime of the interval for which\n                                this command will be applied.\n  --skip-janitor                Skip the janitor task.\n  --ignore-cron                 Run for all missing intervals, ignoring\n                                individual cron schedules.\n  --select-model TEXT           Select specific models to run. Note: this\n                                always includes upstream dependencies.\n  --exit-on-env-update INTEGER  If set, the command will exit with the\n                                specified code if the run is interrupted by an\n                                update to the target environment.\n  --no-auto-upstream            Do not automatically include upstream models.\n                                Only applicable when --select-model is used.\n                                Note: this may result in missing / invalid\n                                data for the selected models.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#state","title":"state","text":"<pre><code>Usage: vulcan state [OPTIONS] COMMAND [ARGS]...\n\n  Commands for interacting with state\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  export  Export the state database to a file\n  import  Import a state export file back into the state database\n</code></pre>"},{"location":"reference/cli/#export","title":"export","text":"<pre><code>Usage: vulcan state export [OPTIONS]\n\n  Export the state database to a file\n\nOptions:\n  -o, --output-file FILE  Path to write the state export to  [required]\n  --environment TEXT      Name of environment to export. Specify multiple\n                          --environment arguments to export multiple\n                          environments\n  --local                 Export local state only. Note that the resulting\n                          file will not be importable\n  --no-confirm            Do not prompt for confirmation before exporting\n                          existing state\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#import","title":"import","text":"<pre><code>Usage: vulcan state import [OPTIONS]\n\n  Import a state export file back into the state database\n\nOptions:\n  -i, --input-file FILE  Path to the state file  [required]\n  --replace              Clear the remote state before loading the file. If\n                         omitted, a merge is performed instead\n  --no-confirm           Do not prompt for confirmation before updating\n                         existing state\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#table_diff","title":"table_diff","text":"<pre><code>Usage: vulcan table_diff [OPTIONS] SOURCE:TARGET [MODEL]\n\n  Show the diff between two tables or multiple models across two environments.\n\nOptions:\n  -o, --on TEXT            The column to join on. Can be specified multiple\n                           times. The model grain will be used if not\n                           specified.\n  -s, --skip-columns TEXT  The column(s) to skip when comparing the source and\n                           target table.\n  --where TEXT             An optional where statement to filter results.\n  --limit INTEGER          The limit of the sample dataframe.\n  --show-sample            Show a sample of the rows that differ. With many\n                           columns, the output can be very wide.\n  -d, --decimals INTEGER   The number of decimal places to keep when comparing\n                           floating point columns. Default: 3\n  --skip-grain-check       Disable the check for a primary key (grain) that is\n                           missing or is not unique.\n  --warn-grain-check       Warn if any selected model is missing a grain,\n                           and compute diffs for the remaining models.\n  --temp-schema TEXT       Schema used for temporary tables. It can be\n                           `CATALOG.SCHEMA` or `SCHEMA`. Default:\n                           `vulcan_temp`\n  -m, --select-model TEXT  Select specific models to table diff.\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#table_name","title":"table_name","text":"<pre><code>Usage: vulcan table_name [OPTIONS] MODEL_NAME\n\n  Prints the name of the physical table for the given model.\n\nOptions:\n  --environment, --env TEXT  The environment to source the model version from.\n  --prod                     If set, return the name of the physical table\n                             that will be used in production for the model\n                             version promoted in the target environment.\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#test","title":"test","text":"<pre><code>Usage: vulcan test [OPTIONS] [TESTS]...\n\n  Run model unit tests.\n\nOptions:\n  -k TEXT              Only run tests that match the pattern of substring.\n  -v, --verbose        Verbose output.\n  --preserve-fixtures  Preserve the fixture tables in the testing database,\n                       useful for debugging.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#ui","title":"ui","text":"<pre><code>Usage: vulcan ui [OPTIONS]\n\n  Start a browser-based Vulcan UI.\n\nOptions:\n  --host TEXT                     Bind socket to this host. Default: 127.0.0.1\n  --port INTEGER                  Bind socket to this port. Default: 8000\n  --mode [ide|catalog|docs|plan]  Mode to start the UI in. Default: ide\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#lint","title":"lint","text":"<pre><code>Usage: vulcan lint [OPTIONS]\n  Run linter for the target model(s).\n\nOptions:\n  --model TEXT           A model to lint. Multiple models can be linted.  If no models are specified, every model will be linted.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/configuration/","title":"Vulcan configuration","text":""},{"location":"reference/configuration/#vulcan-configuration","title":"Vulcan configuration","text":"<p>This page lists Vulcan configuration options and their parameters. Learn more about Vulcan configuration in the configuration guide.</p> <p>Configuration options for model definitions are listed in the model configuration reference page.</p>"},{"location":"reference/configuration/#root-configurations","title":"Root configurations","text":"<p>A Vulcan project configuration consists of root level parameters within which other parameters are defined.</p> <p>Two important root level parameters are <code>gateways</code> and gateway/connection defaults, which have their own sections below.</p> <p>This section describes the other root level configuration parameters.</p>"},{"location":"reference/configuration/#projects","title":"Projects","text":"<p>Configuration options for Vulcan project directories.</p> Option Description Type Required <code>ignore_patterns</code> Files that match glob patterns specified in this list are ignored when scanning the project folder (Default: <code>[]</code>) list[string] N <code>project</code> The project name of this config. Used for multi-repo setups. string N <code>cache_dir</code> The directory to store the Vulcan cache. Can be an absolute path or relative to the project directory. (Default: <code>.cache</code>) string N <code>log_limit</code> The default number of historical log files to keep (Default: <code>20</code>) int N"},{"location":"reference/configuration/#database-physical-layer","title":"Database (Physical Layer)","text":"<p>Configuration options for how Vulcan manages database objects in the physical layer.</p> Option Description Type Required <code>snapshot_ttl</code> The period of time that a model snapshot not a part of any environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>physical_schema_override</code> (Deprecated) Use <code>physical_schema_mapping</code> instead. A mapping from model schema names to names of schemas in which physical tables for the corresponding models will be placed. dict[string, string] N <code>physical_schema_mapping</code> A mapping from regular expressions to names of schemas in which physical tables for the corresponding models will be placed. (Default physical schema name: <code>vulcan__[model schema]</code>) dict[string, string] N <code>physical_table_naming_convention</code> Sets which parts of the model name are included in the physical table names. Options are <code>schema_and_table</code>, <code>table_only</code> or <code>hash_md5</code> - additional details. (Default: <code>schema_and_table</code>) string N"},{"location":"reference/configuration/#environments-virtual-layer","title":"Environments (Virtual Layer)","text":"<p>Configuration options for how Vulcan manages environment creation and promotion in the virtual layer.</p> Option Description Type Required <code>environment_ttl</code> The period of time that a development environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>pinned_environments</code> The list of development environments that are exempt from deletion due to expiration list[string] N <code>default_target_environment</code> The name of the environment that will be the default target for the <code>vulcan plan</code> and <code>vulcan run</code> commands. (Default: <code>prod</code>) string N <code>environment_suffix_target</code> Whether Vulcan views should append their environment name to the <code>schema</code>, <code>table</code> or <code>catalog</code> - additional details. (Default: <code>schema</code>) string N <code>gateway_managed_virtual_layer</code> Whether Vulcan views of the virtual layer will be created by the default gateway or model specified gateways - additional details. (Default: False) boolean N <code>environment_catalog_mapping</code> A mapping from regular expressions to catalog names. The catalog name is used to determine the target catalog for a given environment. dict[string, string] N <code>virtual_environment_mode</code> Determines the Virtual Data Environment (VDE) mode. If set to <code>full</code>, VDE is used in both production and development environments. The <code>dev_only</code> option enables VDE only in development environments, while in production, no virtual layer is used and models are materialized directly using their original names (i.e., no versioned physical tables). (Default: <code>full</code>) string N"},{"location":"reference/configuration/#models","title":"Models","text":"Option Description Type Required <code>time_column_format</code> The default format to use for all model time columns. This time format uses python format codes (Default: <code>%Y-%m-%d</code>) string N <code>infer_python_dependencies</code> Whether Vulcan will statically analyze Python code to automatically infer Python package requirements. (Default: True) boolean N <code>model_defaults</code> Default properties to set on each model. At a minimum, <code>dialect</code> must be set. dict[string, any] Y <p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key.</p> <p>See all the keys allowed in <code>model_defaults</code> at the model configuration reference page.</p>"},{"location":"reference/configuration/#variables","title":"Variables","text":"<p>The <code>variables</code> key can be used to provide values for user-defined variables, accessed using the <code>@VAR</code> macro function in SQL model definitions, <code>context.var</code> method in Python model definitions, and <code>evaluator.var</code> method in Python macro functions.</p> <p>The <code>variables</code> key consists of a mapping of variable names to their values - see an example on the Vulcan macros concepts page. Note that keys are case insensitive.</p> <p>Global variable values may be any of the data types in the table below or lists or dictionaries containing those types.</p> Option Description Type Required <code>variables</code> Mapping of variable names to values dict[string, int | float | bool | string | list | dict] N"},{"location":"reference/configuration/#before_all-after_all","title":"Before_all / after_all","text":"<p>The <code>before_all</code> and <code>after_all</code> keys can be used to specify lists of SQL statements and/or Vulcan macros that are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands. For more information and examples, see the configuration guide.</p> Option Description Type Required <code>before_all</code> List of SQL statements to be executed at the start of the <code>plan</code> and <code>run</code> commands. list[string] N <code>after_all</code> List of SQL statements to be executed at the end of the <code>plan</code> and <code>run</code> commands. list[string] N"},{"location":"reference/configuration/#plan","title":"Plan","text":"<p>Configuration for the <code>vulcan plan</code> command.</p> Option Description Type Required <code>auto_categorize_changes</code> Indicates whether Vulcan should attempt to automatically categorize model changes during plan creation per each model source type (additional details) dict[string, string] N <code>include_unmodified</code> Indicates whether to create views for all models in the target development environment or only for modified ones (Default: False) boolean N <code>auto_apply</code> Indicates whether to automatically apply a new plan after creation (Default: False) boolean N <code>forward_only</code> Indicates whether the plan should be forward-only (Default: False) boolean N <code>enable_preview</code> Indicates whether to enable data preview for forward-only models when targeting a development environment (Default: True, except for dbt projects where the target engine does not support cloning) Boolean N <code>no_diff</code> Don't show diffs for changed models (Default: False) boolean N <code>no_prompts</code> Disables interactive prompts in CLI (Default: True) boolean N <code>always_recreate_environment</code> Always recreates the target environment from the environment specified in <code>create_from</code> (by default <code>prod</code>) (Default: False) boolean N"},{"location":"reference/configuration/#run","title":"Run","text":"<p>Configuration for the <code>vulcan run</code> command. Please note that this is only applicable when configured with the builtin scheduler.</p> Option Description Type Required <code>environment_check_interval</code> The number of seconds to wait between attempts to check the target environment for readiness (Default: 30 seconds) int N <code>environment_check_max_wait</code> The maximum number of seconds to wait for the target environment to be ready (Default: 6 hours) int N"},{"location":"reference/configuration/#format","title":"Format","text":"<p>Formatting settings for the <code>vulcan format</code> command and UI.</p> Option Description Type Required <code>normalize</code> Whether to normalize SQL (Default: False) boolean N <code>pad</code> The number of spaces to use for padding (Default: 2) int N <code>indent</code> The number of spaces to use for indentation (Default: 2) int N <code>normalize_functions</code> Whether to normalize function names. Supported values are: 'upper' and 'lower' (Default: None) string N <code>leading_comma</code> Whether to use leading commas (Default: False) boolean N <code>max_text_width</code> The maximum text width in a segment before creating new lines (Default: 80) int N <code>append_newline</code> Whether to append a newline to the end of the file (Default: False) boolean N <code>no_rewrite_casts</code> Preserve the existing casts, without rewriting them to use the :: syntax. (Default: False) boolean N"},{"location":"reference/configuration/#janitor","title":"Janitor","text":"<p>Configuration for the <code>vulcan janitor</code> command.</p> Option Description Type Required <code>warn_on_delete_failure</code> Whether to warn instead of erroring if the janitor fails to delete the expired environment schema / views (Default: False) boolean N <code>expired_snapshots_batch_size</code> Maximum number of expired snapshots to clean in a single batch (Default: 200) int N"},{"location":"reference/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> dictionary defines how Vulcan should connect to the data warehouse, state backend, test backend, and scheduler.</p> <p>It takes one or more named <code>gateway</code> configuration keys, each of which can define its own connections. Gateway names are case-insensitive - Vulcan normalizes all gateway names to lowercase during configuration validation, allowing you to use any case when referencing gateways. A named gateway does not need to specify all four components and will use defaults if any are omitted - more information is provided about gateway defaults below.</p> <p>For example, a project might configure the <code>gate1</code> and <code>gate2</code> gateways:</p> <pre><code>gateways:\n  gate1:\n    connection:\n      ...\n    state_connection: # defaults to `connection` if omitted\n      ...\n    test_connection: # defaults to `connection` if omitted\n      ...\n    scheduler: # defaults to `builtin` if omitted\n      ...\n  gate2:\n    connection:\n      ...\n</code></pre> <p>Find additional information about gateways in the configuration guide gateways section.</p>"},{"location":"reference/configuration/#gateway","title":"Gateway","text":"<p>Configuration for each named gateway.</p>"},{"location":"reference/configuration/#connections","title":"Connections","text":"<p>A named gateway key may define any or all of a data warehouse connection, state backend connection, state schema name, test backend connection, and scheduler.</p> <p>Some connections use default values if not specified:</p> <ul> <li>The <code>connection</code> key may be omitted if a <code>default_connection</code> is specified.</li> <li>The state connection defaults to <code>connection</code> if omitted.</li> <li>The test connection defaults to <code>connection</code> if omitted.</li> </ul> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p> Option Description Type Required <code>connection</code> The data warehouse connection for core Vulcan functions. connection configuration N (if <code>default_connection</code> specified) <code>state_connection</code> The data warehouse connection where Vulcan will store internal information about the project. (Default: <code>connection</code> if using builtin scheduler, otherwise scheduler database) connection configuration N <code>state_schema</code> The name of the schema where state information should be stored. (Default: <code>vulcan</code>) string N <code>test_connection</code> The data warehouse connection Vulcan will use to execute tests. (Default: <code>connection</code>) connection configuration N <code>scheduler</code> The scheduler Vulcan will use to execute tests. (Default: <code>builtin</code>) scheduler configuration N <code>variables</code> The gateway-specific variables which override the root-level variables by key. dict[string, int | float | bool | string | list | dict] N"},{"location":"reference/configuration/#connection","title":"Connection","text":"<p>Configuration for a data warehouse connection.</p> <p>Most parameters are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</p>"},{"location":"reference/configuration/#general","title":"General","text":"Option Description Type Required <code>type</code> The engine type name, listed in engine-specific configuration pages below. str Y <code>concurrent_tasks</code> The maximum number of concurrent tasks that will be run by Vulcan. (Default: 4 for engines that support concurrent tasks.) int N <code>register_comments</code> Whether Vulcan should register model comments with the SQL engine (if the engine supports it). (Default: <code>true</code>.) bool N <code>pre_ping</code> Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive. This can only be enabled for engines with transaction support. bool N <code>pretty_sql</code> If SQL should be formatted before being executed, not recommended in a production setting. (Default: <code>false</code>.) bool N"},{"location":"reference/configuration/#engine-specific","title":"Engine-specific","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Athena</li> <li>BigQuery</li> <li>ClickHouse</li> <li>Databricks</li> <li>DuckDB</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"reference/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans.</p> <p>By default, the scheduler type is set to <code>builtin</code> and uses the gateway's connection to store metadata.</p> <p>Below is the list of configuration options specific to each corresponding scheduler type. Find additional details in the configuration overview scheduler section.</p>"},{"location":"reference/configuration/#builtin","title":"Builtin","text":"<p>Type: <code>builtin</code></p> <p>No configuration options are supported by this scheduler type.</p>"},{"location":"reference/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. Find additional details in the configuration overview page gateway/connection defaults section.</p>"},{"location":"reference/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> Option Description Type Required <code>default_gateway</code> The name of a gateway to use if one is not provided explicitly (Default: the gateway defined first in the <code>gateways</code> option). Gateway names are case-insensitive. string N"},{"location":"reference/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> Option Description Type Required <code>default_connection</code> The default connection to use if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_test_connection</code> The default connection to use when running tests if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_scheduler</code> The default scheduler configuration to use if one is not specified in a gateway (Default: built-in scheduler) scheduler N"},{"location":"reference/configuration/#debug-mode","title":"Debug mode","text":"<p>Enable debug mode in one of two ways:</p> <ul> <li>Pass the <code>--debug</code> flag between the CLI command and the subcommand. For example, <code>vulcan --debug plan</code>.</li> <li>Set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</li> </ul> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ vulcan --debug plan\n</code></pre> <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; vulcan --debug plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; vulcan --debug plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"reference/configuration/#parallel-loading","title":"Parallel loading","text":"<p>Vulcan by default uses all of your cores when loading models and snapshots. It takes advantage of <code>fork</code> which is not available on Windows. The default is to use the same number of workers as cores on your machine if fork is available.</p> <p>You can override this setting by setting the environment variable <code>MAX_FORK_WORKERS</code>. A value of 1 will disable forking and load things sequentially.</p>"},{"location":"reference/model_configuration/","title":"Model configuration","text":""},{"location":"reference/model_configuration/#model-configuration","title":"Model configuration","text":"<p>This page lists Vulcan model configuration options and their parameters.</p> <p>Learn more about specifying Vulcan model properties in the model concepts overview page.</p>"},{"location":"reference/model_configuration/#general-model-properties","title":"General model properties","text":"<p>Configuration options for Vulcan model properties. Supported by all model kinds other than <code>SEED</code> models.</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. str N <code>project</code> The name of the project the model belongs to - used in multi-repo deployments str N <code>kind</code> The model kind (Additional Details). (Default: <code>VIEW</code>) str | dict N <code>audits</code> Vulcan audits that should run against the model's output array[str] N <code>dialect</code> The SQL dialect in which the model's query is written. All SQL dialects supported by the SQLGlot library are allowed. str N <code>owner</code> The owner of a model; may be used for notification purposes str N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name str N <code>tags</code> Arbitrary strings used to organize or classify a model array[str] N <code>cron</code> The cron expression specifying how often the model should be refreshed. (Default: <code>@daily</code>) str N <code>interval_unit</code> The temporal granularity of the model's data intervals. Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code>. (Default: inferred from <code>cron</code>) str N <code>start</code> The date/time that determines the earliest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. (Default: <code>yesterday</code>) str | int N <code>end</code> The date/time that determines the latest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. str | int N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). str N <code>column_descriptions</code> A key-value mapping of column names to column comments that will be registered in the SQL engine's table COMMENT field (if supported by the engine). Specified as key-value pairs (<code>column_name = 'column comment'</code>). If present, inline column comments will not be registered in the SQL engine. dict N <code>grains</code> The column(s) whose combination uniquely identifies each row in the model str | array[str] N <code>references</code> The model column(s) used to join to other models' grains str | array[str] N <code>depends_on</code> Models on which this model depends, in addition to the ones inferred from the model's query. (Default: dependencies inferred from model code) array[str] N <code>table_format</code> The table format that should be used to manage the physical files (eg <code>iceberg</code>, <code>hive</code>, <code>delta</code>); only applicable to engines such as Spark and Athena str N <code>storage_format</code> The storage format that should be used to store physical files (eg <code>parquet</code>, <code>orc</code>); only applicable to engines such as Spark and Athena str N <code>partitioned_by</code> The column(s) and/or column expressions used define a model's partitioning key. Required for the <code>INCREMENTAL_BY_PARTITION</code> model kind. Optional for all other model kinds; used to partition the model's physical table in engines that support partitioning. str | array[str] N <code>clustered_by</code> The column(s) and/or column expressions used to cluster the model's physical table; only applicable to engines that support clustering str N <code>columns</code> The column names and data types returned by the model. Disables automatic inference of column names and types from the SQL query. array[str] N <code>physical_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model table / view in the physical layer. Specified as key-value pairs (<code>key = value</code>). The view/table type (e.g. <code>TEMPORARY</code>, <code>TRANSIENT</code>) can be added with the <code>creatable_type</code> key. dict N <code>virtual_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model view in the virtual layer. Specified as key-value pairs (<code>key = value</code>). The view type (e.g. <code>SECURE</code>) can be added with the <code>creatable_type</code> key. dict N <code>session_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the engine session. Specified as key-value pairs (<code>key = value</code>). dict N <code>allow_partials</code> Whether this model can process partial (incomplete) data intervals bool N <code>enabled</code> Whether the model is enabled. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to ignore this model when loading the project. bool N <code>gateway</code> Specifies the gateway to use for the execution of this model. When not specified, the default gateway is used. str N <code>optimize_query</code> Whether the model's query should be optimized. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to disable query canonicalization &amp; simplification. This should be turned off only if the optimized query leads to errors such as surpassing text limit. bool N <code>ignored_rules</code> A list of linter rule names (or \"ALL\") to be ignored/excluded for this model str | array[str] N <code>formatting</code> Whether the model will be formatted. All models are formatted by default. Setting this to <code>false</code> causes Vulcan to ignore this model during <code>vulcan format</code>. bool N"},{"location":"reference/model_configuration/#model-defaults","title":"Model defaults","text":"<p>The Vulcan project-level configuration must contain the <code>model_defaults</code> key and must specify a value for its <code>dialect</code> key. Other values are set automatically unless explicitly overridden in the model definition. Learn more about project-level configuration in the configuration guide.</p> <p>In <code>physical_properties</code>, <code>virtual_properties</code>, and <code>session_properties</code>, when both project-level and model-specific properties are defined, they are merged, with model-level properties taking precedence. To unset a project-wide property for a specific model, set it to <code>None</code> in the <code>MODEL</code>'s DDL properties or within the <code>@model</code> decorator for Python models.</p> <p>For example, with the following <code>model_defaults</code> configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    partition_expiration_days: 7\n    require_partition_filter: True\n    project_level_property: \"value\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"partition_expiration_days\": 7,\n      \"require_partition_filter\": True,\n      \"project_level_property\": \"value\"\n    },\n  ),\n)\n</code></pre> <p>To override <code>partition_expiration_days</code>, add a new <code>creatable_type</code> property and unset <code>project_level_property</code>, you can define the model as follows:</p> SQLPython <pre><code>MODEL (\n  ...,\n  physical_properties (\n    partition_expiration_days = 14,\n    creatable_type = TRANSIENT,\n    project_level_property = None,\n  )\n);\n</code></pre> <pre><code>@model(\n  ...,\n  physical_properties={\n    \"partition_expiration_days\": 14,\n    \"creatable_type\": \"TRANSIENT\",\n    \"project_level_property\": None\n  },\n)\n</code></pre> <p>You can also use the <code>@model_kind_name</code> variable to fine-tune control over <code>physical_properties</code> in <code>model_defaults</code>. This holds the current model's kind name and is useful for conditionally assigning a property. For example, to disable <code>creatable_type</code> for your project's <code>VIEW</code> kind models:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    creatable_type: \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"creatable_type\": \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\",\n    },\n  ),\n)\n</code></pre> <p>You can aso define <code>pre_statements</code>, <code>post_statements</code> and <code>on_virtual_update</code> statements at the project level that will be applied to all models. These default statements are merged with any model-specific statements, with default statements executing first, followed by model-specific statements.</p> YAMLPython <pre><code>model_defaults:\n  dialect: duckdb\n  pre_statements:\n    - \"SET timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"duckdb\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre> <p>The Vulcan project-level <code>model_defaults</code> key supports the following options, described in the general model properties table above:</p> <ul> <li>kind</li> <li>dialect</li> <li>cron</li> <li>owner</li> <li>start</li> <li>table_format</li> <li>storage_format</li> <li>physical_properties</li> <li>virtual_properties</li> <li>session_properties (on per key basis)</li> <li>on_destructive_change (described below)</li> <li>on_additive_change (described below)</li> <li>audits (described here)</li> <li>optimize_query</li> <li>allow_partials</li> <li>enabled</li> <li>interval_unit</li> <li>pre_statements (described here)</li> <li>post_statements (described here)</li> <li>on_virtual_update (described here)</li> </ul>"},{"location":"reference/model_configuration/#model-naming","title":"Model Naming","text":"<p>Configuration option for name inference. Learn more in the model naming guide.</p> Option Description Type Required <code>infer_names</code> Whether to automatically infer model names based on the directory structure (Default: <code>False</code>) bool N"},{"location":"reference/model_configuration/#model-kind-properties","title":"Model kind properties","text":"<p>Configuration options for kind-specific Vulcan model properties, in addition to the general model properties listed above.</p> <p>Learn more about model kinds at the model kind concepts page. Learn more about specifying model kind in Python models at the Python models concepts page.</p>"},{"location":"reference/model_configuration/#view-models","title":"<code>VIEW</code> models","text":"<p>Configuration options for models of the <code>VIEW</code> kind (in addition to general model properties).</p> Option Description Type Required <code>materialized</code> Whether views should be materialized (for engines supporting materialized views). (Default: <code>False</code>) bool N <p>Python model kind <code>name</code> enum value: ModelKindName.VIEW</p>"},{"location":"reference/model_configuration/#full-models","title":"<code>FULL</code> models","text":"<p>The <code>FULL</code> model kind does not support any configuration options other than the general model properties listed above.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.FULL</p>"},{"location":"reference/model_configuration/#incremental-models","title":"Incremental models","text":"<p>Configuration options for all incremental models (in addition to general model properties).</p> Option Description Type Required <code>forward_only</code> Whether the model's changes should always be classified as forward-only. (Default: <code>False</code>) bool N <code>on_destructive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes a destructive modification to the model schema. Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>error</code>) str N <code>on_additive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes an additive modification to the model schema (like adding new columns). Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>allow</code>) str N <code>disable_restatement</code> Whether restatements should be disabled for the model. (Default: <code>False</code>) bool N"},{"location":"reference/model_configuration/#incremental-by-time-range","title":"Incremental by time range","text":"<p>Configuration options for <code>INCREMENTAL_BY_TIME_RANGE</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>time_column</code> The model column containing each row's timestamp. Should be UTC time zone. str Y <code>format</code> Argument to <code>time_column</code>. Format of the time column's data. (Default: <code>%Y-%m-%d</code>) str N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) int N <code>batch_concurrency</code> The maximum number of batches that can run concurrently for this model. (Default: the number of concurrent tasks set in the connection settings) int N <code>lookback</code> The number of <code>interval_unit</code>s prior to the current interval that should be processed - learn more. (Default: <code>0</code>) int N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_TIME_RANGE</p>"},{"location":"reference/model_configuration/#incremental-by-unique-key","title":"Incremental by unique key","text":"<p>Configuration options for <code>INCREMENTAL_BY_UNIQUE_KEY</code> models (in addition to general model properties and incremental model properties). Batch concurrency cannot be set for incremental by unique key models because they cannot safely be run in parallel.</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key str | array[str] Y <code>when_matched</code> SQL logic used to update columns when a match occurs - only available on engines that support <code>MERGE</code>. (Default: update all columns) str N <code>merge_filter</code> A single or a conjunction of predicates used to filter data in the ON clause of a MERGE operation - only available on engines that support <code>MERGE</code> str N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) int N <code>lookback</code> The number of time unit intervals prior to the current interval that should be processed. (Default: <code>0</code>) int N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</p>"},{"location":"reference/model_configuration/#incremental-by-partition","title":"Incremental by partition","text":"<p>The <code>INCREMENTAL_BY_PARTITION</code> models kind does not support any configuration options other than the general model properties and incremental model properties.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_PARTITION</p>"},{"location":"reference/model_configuration/#scd-type-2-models","title":"SCD Type 2 models","text":"<p>Configuration options for <code>SCD_TYPE_2</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key array[str] Y <code>valid_from_name</code> The model column containing each row's valid from date. (Default: <code>valid_from</code>) str N <code>valid_to_name</code> The model column containing each row's valid to date. (Default: <code>valid_to</code>) str N <code>invalidate_hard_deletes</code> If set to true, when a record is missing from the source table it will be marked as invalid - see here for more information. (Default: <code>True</code>) bool N"},{"location":"reference/model_configuration/#scd-type-2-by-time","title":"SCD Type 2 By Time","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_TIME</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>updated_at_name</code> The model column containing each row's updated at date. (Default: <code>updated_at</code>) str N <code>updated_at_as_valid_from</code> By default, for new rows the <code>valid_from</code> column is set to 1970-01-01 00:00:00. This sets <code>valid_from</code> to the value of <code>updated_at</code> when the row is inserted. (Default: <code>False</code>) bool N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_TIME</p>"},{"location":"reference/model_configuration/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_COLUMN</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>columns</code> Columns whose changed data values indicate a data update (instead of an <code>updated_at</code> column). <code>*</code> to represent that all columns should be checked. str | array[str] Y <code>execution_time_as_valid_from</code> By default, for new rows <code>valid_from</code> is set to 1970-01-01 00:00:00. This changes the behavior to set it to the execution_time of when the pipeline ran. (Default: <code>False</code>) bool N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_COLUMN</p>"},{"location":"reference/model_configuration/#seed-models","title":"<code>SEED</code> models","text":"<p>Configuration options for <code>SEED</code> models. <code>SEED</code> models do not support all the general properties supported by other models; they only support the properties listed in this table.</p> <p>Top-level options inside the MODEL DDL:</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. str N <code>kind</code> The model kind. Must be <code>SEED</code>. str Y <code>columns</code> The column names and data types in the CSV file. Disables automatic inference of column names and types by the pandas CSV reader. NOTE: order of columns overrides the order specified in the CSV header row (if present). array[str] N <code>audits</code> Vulcan audits that should run against the model's output array[str] N <code>owner</code> The owner of a model; may be used for notification purposes str N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name str N <code>tags</code> Arbitrary strings used to organize or classify a model array[str] N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). str N <p>Options specified within the top-level <code>kind</code> property:</p> Option Description Type Required <code>path</code> Path to seed CSV file. str Y <code>batch_size</code> The maximum number of CSV rows ingested in each batch. All rows ingested in one batch if not specified. int N <code>csv_settings</code> Pandas CSV reader settings (overrides default values). Specified as key-value pairs (<code>key = value</code>). dict N <p> Options specified within the <code>kind</code> property's <code>csv_settings</code> property (overrides default Pandas CSV reader settings):</p> Option Description Type Required <code>delimiter</code> Character or regex pattern to treat as the delimiter. More information at the Pandas documentation. str N <code>quotechar</code> Character used to denote the start and end of a quoted item. More information at the Pandas documentation. str N <code>doublequote</code> When quotechar is specified, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. More information at the Pandas documentation. bool N <code>escapechar</code> Character used to escape other characters. More information at the Pandas documentation. str N <code>skipinitialspace</code> Skip spaces after delimiter. More information at the Pandas documentation. bool N <code>lineterminator</code> Character used to denote a line break. More information at the Pandas documentation. str N <code>encoding</code> Encoding to use for UTF when reading/writing (ex. 'utf-8'). More information at the Pandas documentation. str N <code>na_values</code> An array of values that should be recognized as NA/NaN. In order to specify such an array per column, a mapping in the form of <code>(col1 = (v1, v2, ...), col2 = ...)</code> can be passed instead. These values can be integers, strings, booleans or NULL, and they are converted to their corresponding Python values. More information at the Pandas documentation. array[value] | array[array[key = value]] N <code>keep_default_na</code> Whether or not to include the default NaN values when parsing the data. More information at the Pandas documentation. bool N <p>Python model kind <code>name</code> enum value: ModelKindName.SEED</p>"},{"location":"reference/notebook/","title":"Notebook","text":""},{"location":"reference/notebook/#notebook","title":"Notebook","text":"<p>Vulcan supports Jupyter and Databricks Notebooks. Magics are loaded automatically when <code>vulcan</code> or one of its modules is imported.</p> <p>NOTE: VSCode notebooks may not properly render widgets with <code>ipywidgets</code> library version &gt;=8 (VSCode Github issue). You can work around this by downgrading the ipywidgets version with <code>pip install \"ipywidgets&lt;8\"</code>.</p>"},{"location":"reference/notebook/#vulcan-project-setup","title":"Vulcan project setup","text":"<p>Notebooks locate a Vulcan project by setting a <code>context</code> with either the Python API or a notebook magic.</p> <p>Set the context with the Python <code>Context</code> function as follows:</p> <pre><code>from vulcan import Context\n\ncontext = Context(paths=\"path_to_vulcan_project\")\n</code></pre> <p>Alternatively, set the context with a notebook magic:</p> <pre><code>import vulcan\n\n%context path_to_vulcan_project\n</code></pre>"},{"location":"reference/notebook/#quickstart-project","title":"Quickstart project","text":"<p>If desired, you can create the quickstart example project with the Python <code>init_example_project</code> function. The function requires a default SQL dialect for the project's models; this example uses <code>snowflake</code>:</p> <pre><code>from vulcan.cli.project_init import init_example_project\n\ninit_example_project(\"path_to_project_directory\", engine_type=\"snowflake\")\n</code></pre> <p>Alternatively, create the project with a notebook magic:</p> <pre><code>%init path_to_project_directory snowflake\n</code></pre>"},{"location":"reference/notebook/#databricks-notebooks","title":"Databricks notebooks","text":"<p>To use your Databricks cluster, update the <code>default_connection</code> and <code>test_connection</code> in the <code>config.yaml</code> file.</p> <p>See the Execution Engines page for information on configuring a Databricks connection.</p>"},{"location":"reference/notebook/#magic-commands","title":"Magic Commands","text":"<p>All available magic commands can be listed with <code>%lsmagic</code> and then the docstring for any given magic can be displayed with <code>%magic_name?</code> for line magics and <code>%%magic_name?</code> for call magics.</p>"},{"location":"reference/notebook/#context","title":"context","text":"<pre><code>%context [--config CONFIG] [--gateway GATEWAY] [--ignore-warnings]\n               [--debug]\n               paths [paths ...]\n\nSets the context in the user namespace.\n\npositional arguments:\n  paths              The path(s) to the Vulcan project(s).\n\noptions:\n  --config CONFIG    Name of the config object. Only applicable to\n                     configuration defined using Python script.\n  --gateway GATEWAY  The name of the gateway.\n  --ignore-warnings  Ignore warnings.\n  --debug            Enable debug mode.\n</code></pre>"},{"location":"reference/notebook/#init","title":"init","text":"<pre><code>%init [--template TEMPLATE] [--dlt-pipeline PIPELINE] path sql_dialect\n\nCreates a Vulcan project scaffold with a default SQL dialect.\n\npositional arguments:\n  path                  The path where the new Vulcan project should be\n                        created.\n  sql_dialect           Default model SQL dialect. Supported values: '',\n                        'bigquery', 'clickhouse', 'databricks', 'doris',\n                        'drill', 'duckdb', 'hive', 'mysql', 'oracle',\n                        'postgres', 'presto', 'redshift', 'snowflake',\n                        'spark', 'spark2', 'sqlite', 'starrocks', 'tableau',\n                        'teradata', 'trino', 'tsql'.\n\noptions:\n  --template TEMPLATE, -t TEMPLATE\n                        Project template. Supported values: dbt,\n                        dlt, default, empty.\n  --dlt-pipeline PIPELINE\n                        DLT pipeline for which to generate a Vulcan project.\n                        This option is supported if the template is dlt.\n</code></pre>"},{"location":"reference/notebook/#plan","title":"plan","text":"<pre><code>%plan [--start START] [--end END] [--execution-time EXECUTION_TIME]\n            [--create-from CREATE_FROM] [--skip-tests]\n            [--restate-model [RESTATE_MODEL ...]] [--no-gaps]\n            [--skip-backfill, --dry-run] [--forward-only]\n            [--effective-from EFFECTIVE_FROM] [--no-prompts] [--auto-apply]\n            [--no-auto-categorization] [--include-unmodified]\n            [--select-model [SELECT_MODEL ...]]\n            [--backfill-model [BACKFILL_MODEL ...]] [--no-diff] [--run]\n            [environment] [--diff-rendered]\n\nGoes through a set of prompts to both establish a plan and apply it\n\npositional arguments:\n  environment           The environment to run the plan against\n\noptions:\n  --start START, -s START\n                        Start date to backfill.\n  --end END, -e END     End date to backfill.\n  --execution-time EXECUTION_TIME\n                        Execution time.\n  --create-from CREATE_FROM\n                        The environment to create the target environment from\n                        if it doesn't exist. Default: prod.\n  --skip-tests, -t      Skip the unit tests defined for the model.\n  --restate-model &lt;[RESTATE_MODEL ...]&gt;, -r &lt;[RESTATE_MODEL ...]&gt;\n                        Restate data for specified models (and models\n                        downstream from the one specified). For production\n                        environment, all related model versions will have\n                        their intervals wiped, but only the current versions\n                        will be backfilled. For development environment, only\n                        the current model versions will be affected.\n  --no-gaps, -g         Ensure that new snapshots have no data gaps when\n                        comparing to existing snapshots for matching models in\n                        the target environment.\n  --skip-backfill, --dry-run\n                        Skip the backfill step and only create a virtual update for the plan.\n  --forward-only        Create a plan for forward-only changes.\n  --effective-from EFFECTIVE_FROM\n                        The effective date from which to apply forward-only\n                        changes on production.\n  --no-prompts          Disables interactive prompts for the backfill time\n                        range. Please note that if this flag is set and there\n                        are uncategorized changes, plan creation will fail.\n  --auto-apply          Automatically applies the new plan after creation.\n  --no-auto-categorization\n                        Disable automatic change categorization.\n  --include-unmodified  Include unmodified models in the target environment.\n  --select-model &lt;[SELECT_MODEL ...]&gt;\n                        Select specific model changes that should be included\n                        in the plan.\n  --backfill-model &lt;[BACKFILL_MODEL ...]&gt;\n                        Backfill only the models whose names match the\n                        expression. This is supported only when targeting a\n                        development environment.\n  --no-diff             Hide text differences for changed models.\n  --run                 Run latest intervals as part of the plan application\n                        (prod environment only).\n  --diff-rendered       Output text differences for the rendered versions of models and standalone audits\n</code></pre>"},{"location":"reference/notebook/#run_dag","title":"run_dag","text":"<pre><code>%run_dag [--start START] [--end END] [--skip-janitor] [--ignore-cron]\n               [environment]\n\nEvaluate the DAG of models using the built-in scheduler.\n\npositional arguments:\n  environment           The environment to run against\n\noptions:\n  --start START, -s START\n                        Start date to evaluate.\n  --end END, -e END     End date to evaluate.\n  --skip-janitor        Skip the janitor task.\n  --ignore-cron         Run for all missing intervals, ignoring individual\n                        cron schedules.\n</code></pre>"},{"location":"reference/notebook/#evaluate","title":"evaluate","text":"<pre><code>%evaluate [--start START] [--end END] [--execution-time EXECUTION_TIME]\n                [--limit LIMIT]\n                model\n\nEvaluate a model query and fetches a dataframe.\n\npositional arguments:\n  model                 The model.\n\noptions:\n  --start START, -s START\n                        Start date to render.\n  --end END, -e END     End date to render.\n  --execution-time EXECUTION_TIME\n                        Execution time.\n  --limit LIMIT         The number of rows which the query should be limited\n                        to.\n</code></pre>"},{"location":"reference/notebook/#render","title":"render","text":"<pre><code>%render [--start START] [--end END] [--execution-time EXECUTION_TIME]\n              [--expand EXPAND] [--dialect DIALECT] [--no-format]\n              [--normalize] [--pad PAD] [--indent INDENT]\n              [--normalize-functions NORMALIZE_FUNCTIONS] [--leading-comma]\n              [--max-text-width MAX_TEXT_WIDTH]\n              model\n\nRenders a model's query, optionally expanding referenced models.\n\npositional arguments:\n  model                 The model.\n\noptions:\n  --start START, -s START\n                        Start date to render.\n  --end END, -e END     End date to render.\n  --execution-time EXECUTION_TIME\n                        Execution time.\n  --expand EXPAND       Whether or not to use expand materialized models,\n                        defaults to False. If True, all referenced models are\n                        expanded as raw queries. If a list, only referenced\n                        models are expanded as raw queries.\n  --dialect DIALECT     SQL dialect to render.\n  --no-format           Disable fancy formatting of the query.\n  --normalize           Whether or not to normalize identifiers to lowercase.\n  --pad PAD             Determines the pad size in a formatted string.\n  --indent INDENT       Determines the indentation size in a formatted string.\n  --normalize-functions NORMALIZE_FUNCTIONS\n                        Whether or not to normalize all function names.\n                        Possible values are: 'upper', 'lower'\n  --leading-comma       Determines whether or not the comma is leading or\n                        trailing in select expressions. Default is trailing.\n  --max-text-width MAX_TEXT_WIDTH\n                        The max number of characters in a segment before\n                        creating new lines in pretty mode.\n</code></pre>"},{"location":"reference/notebook/#dag","title":"dag","text":"<pre><code>%dag [--file FILE]\n\nDisplays the HTML DAG.\n\noptions:\n  --file FILE, -f FILE  An optional file path to write the HTML output to.\n</code></pre>"},{"location":"reference/notebook/#destroy","title":"destroy","text":"<pre><code>%destroy\n\nRemoves all state tables, the Vulcan cache, and other project resources, including warehouse objects. This includes all tables, views, and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.\n</code></pre>"},{"location":"reference/notebook/#dlt_refresh","title":"dlt_refresh","text":"<pre><code>%dlt_refresh PIPELINE [--table] TABLE [--force]\n\nAttaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.\n\noptions:\n  --table TABLE, -t TABLE  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.\n  --force, -f              If set it will overwrite existing models with the new generated models from the DLT tables.\n</code></pre>"},{"location":"reference/notebook/#environments","title":"environments","text":"<pre><code>%environments\n\nPrints the list of Vulcan environments with its expiry datetime.\n</code></pre>"},{"location":"reference/notebook/#fetchdf","title":"fetchdf","text":"<pre><code>%%fetchdf [df_var]\n\nFetches a dataframe from sql, optionally storing it in a variable.\n\npositional arguments:\n  df_var  An optional variable name to store the resulting dataframe.\n</code></pre>"},{"location":"reference/notebook/#test","title":"test","text":"<pre><code>%test [--ls] model [test_name]\n\nAllow the user to list tests for a model, output a specific test, and then write their changes back\n\npositional arguments:\n  model      The model.\n  test_name  The test name to display\n\noptions:\n  --ls       List tests associated with a model\n</code></pre>"},{"location":"reference/notebook/#migrate","title":"migrate","text":"<pre><code>%migrate\n\nMigrate Vulcan to the current running version\n</code></pre>"},{"location":"reference/notebook/#create_external_models","title":"create_external_models","text":"<pre><code>%create_external_models\n\nCreate a schema file containing external model schemas.\n</code></pre>"},{"location":"reference/notebook/#table_diff","title":"table_diff","text":"<pre><code>%table_diff [--on [ON ...]] [--skip-columns [SKIP_COLUMNS ...]]\n                [--model MODEL] [--where WHERE] [--limit LIMIT]\n                [--show-sample] [--decimals DECIMALS] [--skip-grain-check]\n                [--warn-grain-check] [--temp-schema SCHEMA]\n                [--select-model [SELECT_MODEL ...]] SOURCE:TARGET\n\nShow the diff between two tables.\n\nCan either be two tables or two environments and a model.\n\npositional arguments:\n  SOURCE:TARGET         Source and target in `SOURCE:TARGET` format\n\noptions:\n  --on &lt;[ON ...]&gt;       The column to join on. Can be specified multiple\n                        times. The model grain will be used if not specified.\n  --skip-columns &lt;[SKIP_COLUMNS ...]&gt;\n                        The column(s) to skip when comparing the source and\n                        target table.\n  --model MODEL         The model to diff against when source and target are\n                        environments and not tables.\n  --where WHERE         An optional where statement to filter results.\n  --limit LIMIT         The limit of the sample dataframe.\n  --show-sample         Show a sample of the rows that differ. With many\n                        columns, the output can be very wide.\n  --decimals DECIMALS   The number of decimal places to keep when comparing\n                        floating point columns. Default: 3\n  --skip-grain-check    Disable the check for a primary key (grain) that is\n                        missing or is not unique.\n  --warn-grain-check    Warn if any selected model is missing a grain,\n                        and compute diffs for the remaining models.\n  --temp-schema SCHEMA  The schema to use for temporary tables.\n  --select-model &lt;[SELECT_MODEL ...]&gt;\n                        Select specific models to diff using a pattern.\n</code></pre>"},{"location":"reference/notebook/#model","title":"model","text":"<pre><code>%model [--start START] [--end END] [--execution-time EXECUTION_TIME]\n             [--dialect DIALECT]\n             model\n\nRenders the model and automatically fills in an editable cell with the model definition.\n\npositional arguments:\n  model                 The model.\n\noptions:\n  --start START, -s START\n                        Start date to render.\n  --end END, -e END     End date to render.\n  --execution-time EXECUTION_TIME\n                        Execution time.\n  --dialect DIALECT, -d DIALECT\n                        The rendered dialect.\n</code></pre>"},{"location":"reference/notebook/#diff","title":"diff","text":"<pre><code>%diff environment\n\nShow the diff between the local state and the target environment.\n\npositional arguments:\n  environment  The environment to diff local state against.\n</code></pre>"},{"location":"reference/notebook/#invalidate","title":"invalidate","text":"<pre><code>%invalidate environment\n\nInvalidate the target environment, forcing its removal during the next run of the janitor process.\n\npositional arguments:\n  environment  The environment to invalidate.\n</code></pre>"},{"location":"reference/notebook/#janitor","title":"janitor","text":"<pre><code>%janitor\n\nRun the janitor process to clean up old environments and expired snapshots.\n\noptions:\n  --ignore-ttl Cleanup snapshots that are not referenced in any environment, regardless of when they're set to expire\n</code></pre>"},{"location":"reference/notebook/#create_test","title":"create_test","text":"<pre><code>%create_test [--query QUERY [QUERY ...]] [--overwrite]\n                   [--var VAR [VAR ...]] [--path PATH] [--name NAME]\n                   [--include-ctes]\n                   model\n\nGenerate a unit test fixture for a given model.\n\npositional arguments:\n  model\n\noptions:\n  --query &lt;QUERY [QUERY ...]&gt;, -q &lt;QUERY [QUERY ...]&gt;\n                        Queries that will be used to generate data for the\n                        model's dependencies.\n  --overwrite, -o       When true, the fixture file will be overwritten in\n                        case it already exists.\n  --var &lt;VAR [VAR ...]&gt;, -v &lt;VAR [VAR ...]&gt;\n                        Key-value pairs that will define variables needed by\n                        the model.\n  --path PATH, -p PATH  The file path corresponding to the fixture, relative\n                        to the test directory. By default, the fixture will be\n                        created under the test directory and the file name\n                        will be inferred based on the test's name.\n  --name NAME, -n NAME  The name of the test that will be created. By default,\n                        it's inferred based on the model's name.\n  --include-ctes        When true, CTE fixtures will also be generated.\n</code></pre>"},{"location":"reference/notebook/#run_test","title":"run_test","text":"<pre><code>%run_test [--pattern [PATTERN ...]] [--verbose] [--preserve-fixtures] [tests ...]\n\nRun unit test(s).\n\npositional arguments:\n  tests\n\noptions:\n  --pattern &lt;[PATTERN ...]&gt;, -k &lt;[PATTERN ...]&gt;\n                        Only run tests that match the pattern of substring.\n  --verbose, -v         Verbose output.\n  --preserve-fixtures   Preserve the fixture tables in the testing database,\n                        useful for debugging.\n</code></pre>"},{"location":"reference/notebook/#audit","title":"audit","text":"<pre><code>%audit [--start START] [--end END] [--execution-time EXECUTION_TIME]\n             [models ...]\n\nRun audit(s)\n\npositional arguments:\n  models                A model to audit. Multiple models can be audited.\n\noptions:\n  --start START, -s START\n                        Start date to audit.\n  --end END, -e END     End date to audit.\n  --execution-time EXECUTION_TIME\n                        Execution time.\n</code></pre>"},{"location":"reference/notebook/#check_intervals","title":"check_intervals","text":"<pre><code>%check_intervals [--no-signals] [--select-model [SELECT_MODEL ...]]\n                   [--start START] [--end END]\n                   [environment]\n\nShow missing intervals in an environment, respecting signals.\n\npositional arguments:\n  environment           The environment to check intervals for.\n\noptions:\n  --no-signals          Disable signal checks and only show missing intervals.\n  --select-model &lt;[SELECT_MODEL ...]&gt;\n                        Select specific model changes that should be included\n                        in the plan.\n  --start START, -s START\n                        Start date of intervals to check for.\n  --end END, -e END     End date of intervals to check for.\n</code></pre>"},{"location":"reference/notebook/#rollback","title":"rollback","text":"<pre><code>%rollback\n\nRollback Vulcan to the previous migration.\n</code></pre>"},{"location":"reference/notebook/#clean","title":"clean","text":"<pre><code>%clean\n\nClears the Vulcan cache and any build artifacts.\n</code></pre>"},{"location":"reference/notebook/#rewrite","title":"rewrite","text":"<pre><code>%rewrite [--read READ] [--write WRITE]\n\nRewrite a sql expression with semantic references into an executable query.\n\nhttps://vulcan.readthedocs.io/en/latest/concepts/metrics/overview/\n\noptions:\n  --read READ    The input dialect of the sql string.\n  --write WRITE  The output dialect of the sql string.\n</code></pre>"},{"location":"reference/notebook/#format","title":"format","text":"<pre><code>%format [--transpile TRANSPILE] [--append-newline] [--no-rewrite-casts]\n              [--normalize] [--pad PAD] [--indent INDENT]\n              [--normalize-functions NORMALIZE_FUNCTIONS] [--leading-comma]\n              [--max-text-width MAX_TEXT_WIDTH] [--check]\n\nFormat all SQL models and audits.\n\noptions:\n  --transpile TRANSPILE, -t TRANSPILE\n                        Transpile project models to the specified dialect.\n  --append-newline      Whether or not to append a newline to the end of the\n                        file.\n  --no-rewrite-casts    Preserve the existing casts, without rewriting them\n                        to use the :: syntax.\n  --normalize           Whether or not to normalize identifiers to lowercase.\n  --pad PAD             Determines the pad size in a formatted string.\n  --indent INDENT       Determines the indentation size in a formatted string.\n  --normalize-functions NORMALIZE_FUNCTIONS\n                        Whether or not to normalize all function names.\n                        Possible values are: 'upper', 'lower'\n  --leading-comma       Determines whether or not the comma is leading or\n                        trailing in select expressions. Default is trailing.\n  --max-text-width MAX_TEXT_WIDTH\n                        The max number of characters in a segment before\n                        creating new lines in pretty mode.\n  --check               Whether or not to check formatting (but not actually\n                        format anything).\n</code></pre>"},{"location":"reference/notebook/#lint","title":"lint","text":"<pre><code>%lint [--models ...]\n\nRun the linter on the target models(s)\n\npositional arguments:\n  --models                A model to lint. Multiple models can be linted. If no models are specified, every model will be linted.\n</code></pre>"},{"location":"reference/overview/","title":"Overview","text":""},{"location":"reference/overview/#overview","title":"Overview","text":"<p>Vulcan can be used with a CLI, Notebook, or directly through Python. Each interface aims to have parity in both functionality and arguments.</p>"},{"location":"reference/python/","title":"Python API","text":""},{"location":"reference/python/#python-api","title":"Python API","text":"<p>Vulcan is built in Python, and its complete Python API reference is located here.</p> <p>The Python API reference is comprehensive and includes the internal components of Vulcan. Those components are likely only of interest if you want to modify Vulcan itself.</p> <p>If you want to use Vulcan via its Python API, the best approach is to study how the Vulcan CLI calls it behind the scenes. The CLI implementation code shows exactly which Python methods are called for each CLI command and can be viewed on Github. For example, the Python code executed by the <code>plan</code> command is located here.</p> <p>Almost all the relevant Python methods are in the Vulcan <code>Context</code> class.</p>"}]}