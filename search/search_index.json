{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/_]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About","text":""},{"location":"#about","title":"About","text":"<p>Vulcan is a complete stack for building data products.</p> <p>Vulcan is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can efficiently run and deploy data transformations written in SQL or Python with visibility and control at any size.</p> <p>Note: You may need to run <code>python3</code> or <code>pip3</code> instead of <code>python</code> or <code>pip</code>, depending on your python installation.</p> <p>Follow the quickstart guide to learn how to use Vulcan. You already have a head start!</p>"},{"location":"comparisons/","title":"Comparisons","text":""},{"location":"comparisons/#comparisons","title":"Comparisons","text":"<p>This documentation is a work in progress.</p> <p>There are many tools and frameworks in the data ecosystem. This page tries to make sense of it all.</p> <p>If you are not familiar with Vulcan, it will be helpful to first read about Vulcan to better understand the comparisons. </p>"},{"location":"comparisons/#dbt","title":"dbt","text":"<p>dbt is a tool for data transformations. It is a pioneer in this space and has shown how valuable transformation frameworks can be. Although dbt is a fantastic tool, it has trouble scaling with data and organizational size.</p> <p>dbt built their product focused on simple data transformations. By default, it fully refreshes data warehouses by executing templated SQL in the correct order.</p> <p>Over time dbt has seen that data transformations are not enough to operate a scalable and robust data product. As a result, advanced features are patched in, such as state management (defer) and incremental loads, to try to address these needs while pushing the burden of correctness onto users with increased complexity. These \"advanced\" features make up some of the fundamental building blocks of a DataOps framework.</p> <p>In other words, the challenge of implementing these features in dbt falls primarily on you: more jinja macro blocks, more manual configuration, more custom tooling, and more opportunities for error. We needed an easier, more reliable way, so we designed Vulcan from the ground up to be a robust DataOps framework.</p> <p>Vulcan aims to be dbt format-compatible. Importing existing dbt projects with minor changes is in development.</p>"},{"location":"comparisons/#feature-comparisons","title":"Feature comparisons","text":"Feature dbt Vulcan Modeling <code>SQL models</code> \u2705 \u2705 <code>Python models</code> \u2705 \u2705\u2705 <code>Jinja support</code> \u2705 \u2705 <code>Jinja macros</code> \u2705 \u2705 <code>Python macros</code> \u274c \u2705 Validation <code>SQL semantic validation</code> \u274c \u2705 <code>Unit tests</code> \u274c \u2705 <code>Table diff</code> \u274c \u2705 <code>Data audits</code> \u2705 \u2705 <code>Schema contracts</code> \u2705 \u2705 <code>Data contracts</code> \u274c \u2705 Deployment <code>Virtual Data Environments</code> \u274c \u2705 <code>Open-source CI/CD bot</code> \u274c \u2705 <code>Data consistency enforcement</code> \u274c \u2705 Interfaces <code>CLI</code> \u2705 \u2705 <code>Paid UI</code> \u2705 \u274c <code>Open-source UI</code> \u274c \u2705 <code>Native Notebook Support</code> \u274c \u2705 Visualization <code>Documentation generation</code> \u2705 \u2705 <code>Column-level lineage</code> \u274c \u2705 Miscellaneous <code>Package manager</code> \u2705 \u274c <code>Multi-repository support</code> \u274c \u2705 <code>SQL transpilation</code> \u274c \u2705"},{"location":"comparisons/#environments","title":"Environments","text":"<p>Development and staging environments in dbt are costly to make and not fully representative of what will go into production.</p> <p>The standard approach to creating a new environment in dbt is to rerun your entire warehouse in a new environment. This may work at small scales, but even then it wastes time and money. Here's why:</p> <p>The first part of running a data transformation system is repeatedly iterating through three steps: create or modify model code, execute the models, evaluate the outputs. Practitioners may repeat these steps many times in a day's work.</p> <p>These steps incur costs to organizations: compute costs to run the models and staff time spent waiting on them to run. Inefficiencies compound rapidly because the steps are repeated so frequently. dbt's default full refresh approach leads to the most costly version of this loop: recomputing every model every time.</p> <p>Vulcan takes another approach. It examines the code modifications and the dependency structure among the models to determine which models are affected -- and executes only those models. This results in the least costly version of the loop: computing only what is required every time through.</p> <p>This enables Vulcan to provide efficient isolated Virtual Environments. Environments in dbt cost compute and storage, but creating a development environment in Vulcan is free -- you can quickly access a full replica of any other environment with a single command.</p> <p>Additionally, Vulcan ensures that promotion of staging environments to production is predictable and consistent. There is no concept of promotion in dbt, so queries are all rerun when it's time to deploy something. In Vulcan, promotions are simple pointer swaps so there is no wasted compute.</p>"},{"location":"comparisons/#incremental-models","title":"Incremental models","text":"<p>Implementing incremental models is difficult and error-prone in dbt because it does not keep track of state.</p>"},{"location":"comparisons/#complexity","title":"Complexity","text":"<p>Since there is no state of which incremental intervals have already run in dbt, users must write and maintain subqueries to find missing date boundaries themselves:</p> <pre><code>-- dbt incremental\nSELECT *\nFROM {{ ref(raw.events) }} e\nJOIN {{ ref(raw.event_dims) }} d\n  ON e.id = d.id\n-- must specify the is_incremental flag because this predicate will fail if the model has never run before\n{% if is_incremental() %}\n    -- this filter dynamically scans the current model to find the date boundary\n    AND d.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n{% if is_incremental() %}\n  WHERE e.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Manually specifying macros to find date boundaries is repetitive and error-prone.</p> <p>The example above shows how incremental models behave differently in dbt depending on whether they have been run before. As models become more complex, the cognitive burden of having two run times, \"first time full refresh\" vs. \"subsequent incremental\", increases.</p> <p>Vulcan keeps track of which date ranges exist, producing a simplified and efficient query as follows:</p> <pre><code>-- Vulcan incremental\nSELECT *\nFROM raw.events e\nJOIN raw.event_dims d\n  -- date ranges are handled automatically by Vulcan\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n</code></pre>"},{"location":"comparisons/#data-leakage","title":"Data leakage","text":"<p>dbt does not check whether the data inserted into an incremental table should be there or not. This can lead to problems and consistency issues, such as late-arriving data overriding past partitions. These problems are called \"data leakage.\"</p> <p>Vulcan wraps all queries in a subquery with a time filter under the hood to enforce that the data inserted for a particular batch is as expected and reproducible every time.</p> <p>In addition, dbt only supports the 'insert/overwrite' incremental load pattern for systems that natively support it. Vulcan enables 'insert/overwrite' on any system, because it is the most robust approach to incremental loading, while 'Append' pipelines risk data inaccuracy in the variety of scenarios where your pipelines may run more than once for a given date.</p> <p>This example shows the time filtering subquery Vulcan applies to all queries as a guard against data leakage: </p><pre><code>-- original query\nSELECT *\nFROM raw.events\nJOIN raw.event_dims d\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n\n-- with automated data leakage guard\nSELECT *\nFROM (\n  SELECT *\n  FROM raw.events\n  JOIN raw.event_dims d\n    ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\n  WHERE d.ds BETWEEN @start_ds AND @end_ds\n)\nWHERE ds BETWEEN @start_ds AND @end_ds\n</code></pre><p></p>"},{"location":"comparisons/#data-gaps","title":"Data gaps","text":"<p>The main pattern used to implement incremental models in dbt is checking for the most recent data with MAX(date). This pattern does not catch missing data from the past, or \"data gaps.\"</p> <p>Vulcan stores each date interval a model has been run with, so it knows exactly what dates are missing: </p><pre><code>Expected dates: 2022-01-01, 2022-01-02, 2022-01-03\nMissing past data: ?, 2022-01-02, 2022-01-03\nData gap: 2022-01-01, ?, 2022-01-03\n</code></pre><p></p> <p>Vulcan will automatically fill these data gaps on the next run.</p>"},{"location":"comparisons/#performance","title":"Performance","text":"<p>Subqueries that look for MAX(date) could have a performance impact on the primary query. Vulcan is able to avoid these extra subqueries.</p> <p>Additionally, dbt expects an incremental model to be able to fully refresh the first time it runs. For some large data sets, this is cost-prohibitive or infeasible.</p> <p>Vulcan is able to batch up backfills into more manageable chunks.</p>"},{"location":"comparisons/#sql-understanding","title":"SQL understanding","text":"<p>dbt heavily relies on Jinja. It has no understanding of SQL and treats all queries as raw strings without context. This means that simple syntax errors like trailing commas are difficult to debug and require a full run to detect.</p> <p>Vulcan supports Jinja, but it does not rely on it - instead, it parses/understands SQL through SQLGlot. Simple errors can be detected at compile time, so you no longer have to wait minutes or even longer to see that you've referenced a column incorrectly or missed a comma.</p> <p>Additionally, having a first-class understanding of SQL supercharges Vulcan with features such as transpilation, column-level lineage, and automatic change categorization.</p>"},{"location":"comparisons/#testing","title":"Testing","text":"<p>Data quality checks such as detecting NULL values and duplicated rows are extremely valuable for detecting upstream data issues and large scale problems. However, they are not meant for testing edge cases or business logic, and they are not sufficient for creating robust data pipelines.</p> <p>Unit and integration tests are the tools to use to validate business logic. Vulcan encourages users to add unit tests to all of their models to ensure changes don't unexpectedly break assumptions. Unit tests are designed to be fast and self contained so that they can run in continuous integration (CI) frameworks.</p>"},{"location":"comparisons/#python-models","title":"Python models","text":"<p>dbt's Python models only run remotely on adapters of data platforms that have a full Python runtime, limiting the number of users that can take advantage of them and making the models difficult to debug.</p> <p>Vulcan's Python models run locally and can be used with any data warehouse. Breakpoints can be added to debug the model.</p>"},{"location":"comparisons/#data-contracts","title":"Data contracts","text":"<p>dbt offers manually configured schema contracts that will check the model's schema against the yaml schema at runtime. Models can be versioned to allow downstream teams time to migrate to the latest version, at the risk of a fragmented source of truth during the migration period.</p> <p>Vulcan provides automatic schema contracts and data contracts via <code>vulcan plan</code>, which checks the model's schema and query logic for changes that affect downstream users. <code>vulcan plan</code> will show which models have breaking changes and which downstream models are affected.</p> <p>While breaking changes can be rolled out as separate models to allow for a migration period, Vulcan's Virtual Preview empowers teams to collaborate on migrations before the changes are deployed to prod, maintaining a single source of truth across the business.</p>"},{"location":"cli-command/cli/","title":"CLI Commands","text":""},{"location":"cli-command/cli/#cli-commands","title":"CLI Commands","text":"<pre><code>Usage: vulcan [OPTIONS] COMMAND [ARGS]...\n\n  Vulcan command line tool.\n\nOptions:\n  --version            Show the version and exit.\n  -p, --paths TEXT     Path(s) to the Vulcan config/project.\n  --config TEXT        Name of the config object. Only applicable to\n                       configuration defined using Python script.\n  --gateway TEXT       The name of the gateway.\n  --ignore-warnings    Ignore warnings.\n  --debug              Enable debug mode.\n  --log-to-stdout      Display logs in stdout.\n  --log-file-dir TEXT  The directory to write log files to.\n  --dotenv PATH        Path to a custom .env file to load environment\n                       variables.\n  --help               Show this message and exit.\n\nCommands:\n  api                     Start the Vulcan API server (models, metrics,...\n  audit                   Run audits for the target model(s).\n  check_intervals         Show missing intervals in an environment,...\n  clean                   Clears the Vulcan cache and any build artifacts.\n  create_external_models  Create a schema file containing external model...\n  create_test             Generate a unit test fixture for a given model.\n  dag                     Render the DAG as an html file.\n  destroy                 The destroy command removes all project resources.\n  diff                    Show the diff between the local state and the...\n  dlt_refresh             Attaches to a DLT pipeline with the option to...\n  environments            Prints the list of Vulcan environments with its...\n  evaluate                Evaluate a model and return a dataframe with a...\n  fetchdf                 Run a SQL query and display the results.\n  format                  Format all SQL models and audits.\n  graphql                 Manage the GraphQL service (subcommands: up,...\n  info                    Print information about a Vulcan project.\n  invalidate              Invalidate the target environment, forcing its...\n  janitor                 Run the janitor process on-demand.\n  lint                    Run the linter for the target model(s).\n  migrate                 Migrate Vulcan to the current running version.\n  plan                    Apply local changes to the target environment.\n  render                  Render a model's query, optionally expanding...\n  rollback                Rollback Vulcan to the previous migration.\n  run                     Evaluate missing intervals for the target...\n  semantic                Semantic layer operations.\n  state                   Commands for interacting with state\n  table_diff              Show the diff between two tables or a selection...\n  table_name              Prints the name of the physical table for the...\n  test                    Run model unit tests.\n  transpile               Transpile a semantic SQL or REST-style semantic...\n  transpiler              Manage the Transpiler service (subcommands: up,...\n</code></pre>"},{"location":"cli-command/cli/#audit","title":"audit","text":"<pre><code>Usage: vulcan audit [OPTIONS]\n\n  Run audits for the target model(s).\n\nOptions:\n  --model TEXT           A model to audit. Multiple models can be audited.\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan audit\n  Found 11 audit(s).\n  unique_values on model sales.daily_sales \u2705 PASS.\n  not_null on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  unique_values on model raw.raw_products \u2705 PASS.\n  not_null on model raw.raw_products \u2705 PASS.\n  unique_values on model raw.raw_customers \u2705 PASS.\n  not_null on model raw.raw_customers \u2705 PASS.\n  unique_values on model raw.raw_orders \u2705 PASS.\n  not_null on model raw.raw_orders \u2705 PASS.\n  positive_values on model raw.raw_orders \u2705 PASS.\n\n  Finished with 0 audit errors and 0 audits skipped.\n  Done.\n</code></pre>"},{"location":"cli-command/cli/#check_intervals","title":"check_intervals","text":"<pre><code>Usage: vulcan check_intervals [OPTIONS] [ENVIRONMENT]\n\n  Show missing intervals in an environment, respecting signals.\n\nOptions:\n  --no-signals         Disable signal checks and only show missing intervals.\n  --select-model TEXT  Select specific models to show missing intervals for.\n  -s, --start TEXT     The start datetime of the interval for which this\n                       command will be applied.\n  -e, --end TEXT       The end datetime of the interval for which this command\n                       will be applied.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#clean","title":"clean","text":"<pre><code>Usage: vulcan clean [OPTIONS]\n\n  Clears the Vulcan cache and any build artifacts.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#create_external_models","title":"create_external_models","text":"<pre><code>Usage: vulcan create_external_models [OPTIONS]\n\n  Create a schema file containing external model schemas.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_external_models\n</code></pre>"},{"location":"cli-command/cli/#create_test","title":"create_test","text":"<pre><code>Usage: vulcan create_test [OPTIONS] MODEL\n\n  Generate a unit test fixture for a given model.\n\nOptions:\n  -q, --query &lt;TEXT TEXT&gt;...  Queries that will be used to generate data for\n                              the model's dependencies.\n  -o, --overwrite             When true, the fixture file will be overwritten\n                              in case it already exists.\n  -v, --var &lt;TEXT TEXT&gt;...    Key-value pairs that will define variables\n                              needed by the model.\n  -p, --path TEXT             The file path corresponding to the fixture,\n                              relative to the test directory. By default, the\n                              fixture will be created under the test directory\n                              and the file name will be inferred based on the\n                              test's name.\n  -n, --name TEXT             The name of the test that will be created. By\n                              default, it's inferred based on the model's\n                              name.\n  --include-ctes              When true, CTE fixtures will also be generated.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_test sales.daily_sales --query raw.raw_orders \"SELECT * FROM raw.raw_orders\"\n</code></pre>"},{"location":"cli-command/cli/#dag","title":"dag","text":"<pre><code>Usage: vulcan dag [OPTIONS] FILE\n\n  Render the DAG as an html file.\n\nOptions:\n  --select-model TEXT  Select specific models to include in the dag.\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan dag ./dag.html\n</code></pre>"},{"location":"cli-command/cli/#destroy","title":"destroy","text":"<pre><code>Usage: vulcan destroy\n\n  Removes all state tables, the Vulcan cache and all project resources, including warehouse objects. This includes all tables, views and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.\n\nOptions:\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan destroy\n[WARNING] This will permanently delete all engine-managed objects, state tables and Vulcan cache.\nThe operation may disrupt any currently running or scheduled plans.\n\nSchemas to be deleted:\n  \u2022 warehouse.raw\n  \u2022 warehouse.sales\n\nSnapshot tables to be deleted:\n  \u2022 warehouse.vulcan__raw.raw__raw_customers__1474975870\n  \u2022 warehouse.vulcan__raw.raw__raw_orders__1032938324\n  \u2022 warehouse.vulcan__raw.raw__raw_products__3337559381\n  \u2022 warehouse.vulcan__sales.sales__daily_sales__2671854529\n\nThis action will DELETE ALL the above resources managed by Vulcan AND\npotentially external resources created by other tools in these schemas.\n\nAre you ABSOLUTELY SURE you want to proceed with deletion? [y/n]: y\nEnvironment 'prod' invalidated.\n\nDeleted object warehouse.raw\nDeleted object warehouse.sales\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381__dev\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324__dev\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324\nState tables removed.\nDestroy completed successfully.\n</code></pre>"},{"location":"cli-command/cli/#dlt_refresh","title":"dlt_refresh","text":"<pre><code>Usage: dlt_refresh PIPELINE [OPTIONS]\n\n  Attaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.\n\nOptions:\n  -t, --table TEXT  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.\n  -f, --force       If set it will overwrite existing models with the new generated models from the DLT tables.\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#diff","title":"diff","text":"<pre><code>Usage: vulcan diff [OPTIONS] ENVIRONMENT\n\n  Show the diff between the local state and the target environment.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan diff prod\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n        --- .../daily_sales.sql\n\n        +++ .../daily_sales.sql\n\n        @@ -20,10 +20,11 @@\n\n          grains (order_date)\n        )\n        SELECT\n          CAST(order_date AS TIMESTAMP) AS order_date,\n          CAST(COUNT(order_id) AS INT) AS total_orders,\n          CAST(SUM(total_amount) AS DOUBLE PRECISION) AS total_revenue,\n        -  CAST(MAX(order_id) AS VARCHAR) AS last_order_id\n        +  CAST(MAX(order_id) AS VARCHAR) AS last_order_id,\n        +  COUNT(DISTINCT product_id) AS total_products\n        FROM raw.raw_orders\n        GROUP BY\n          order_date\nSemantics:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 semantic-model:sales.daily_sales\n    \u251c\u2500\u2500 semantic-metric:order_volume\n    \u2514\u2500\u2500 semantic-metric:revenue_trends\nQuality Checks:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:accuracy\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:timeliness\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:completeness\n    \u2514\u2500\u2500 check-suite:sales.daily_sales:validity\n</code></pre>"},{"location":"cli-command/cli/#environments","title":"environments","text":"<pre><code>Usage: vulcan environments [OPTIONS]\n\n  Prints the list of Vulcan environments with its expiry datetime.\n\nOptions:\n  --help             Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan environments\nNumber of Vulcan environments are: 2\nprod - No Expiry\ndev - 2025-12-23 00:00:00\n</code></pre>"},{"location":"cli-command/cli/#evaluate","title":"evaluate","text":"<pre><code>Usage: vulcan evaluate [OPTIONS] MODEL\n\n  Evaluate a model and return a dataframe with a default limit of 1000.\n\nOptions:\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --limit INTEGER        The number of rows which the query should be limited\n                         to.\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan evaluate sales.daily_sales\n   order_date  total_orders  total_revenue last_order_id  total_products\n0  2024-01-05             1          70.77          O001               1\n1  2024-01-10             1          44.22          O002               1\n2  2024-01-15             1          65.52          O003               1\n3  2024-01-20             1          79.42          O004               1\n4  2024-02-01             1          91.35          O005               1\n....\n19 2024-05-15             1          38.38          O020               1\n</code></pre>"},{"location":"cli-command/cli/#fetchdf","title":"fetchdf","text":"<pre><code>Usage: vulcan fetchdf [OPTIONS] SQL\n\n  Run a SQL query and display the results.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan fetchdf \"select count(*) from sales.daily_sales\"\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 count \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 20    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli-command/cli/#format","title":"format","text":"<pre><code>Usage: vulcan format [OPTIONS]\n\n  Format all SQL models and audits.\n\nOptions:\n  -t, --transpile TEXT        Transpile project models to the specified\n                              dialect.\n  --append-newline            Include a newline at the end of each file.\n  --no-rewrite-casts          Preserve the existing casts, without rewriting\n                              them to use the :: syntax.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --check                     Whether or not to check formatting (but not\n                              actually format anything).\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#info","title":"info","text":"<pre><code>Usage: vulcan info [OPTIONS]\n\n  Print information about a Vulcan project.\n\n  Includes counts of project models and macros and connection tests for the\n  data warehouse.\n\nOptions:\n  --skip-connection  Skip the connection test.\n  -v, --verbose      Verbose output.\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan info\nModels: 4\nMacros: 0\nData warehouse connection succeeded\nState backend connection succeeded\n</code></pre>"},{"location":"cli-command/cli/#init","title":"init","text":"<pre><code>Usage: vulcan init [OPTIONS] [ENGINE]\n\n  Create a new Vulcan repository.\n\nOptions:\n  -t, --template TEXT  Project template. Supported values: dbt, dlt, default,\n                       empty.\n  --dlt-pipeline TEXT  DLT pipeline for which to generate a Vulcan project.\n                       Use alongside template: dlt\n  --dlt-path TEXT      The directory where the DLT pipeline resides. Use\n                       alongside template: dlt\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan init postgres\n</code></pre>"},{"location":"cli-command/cli/#invalidate","title":"invalidate","text":"<pre><code>Usage: vulcan invalidate [OPTIONS] ENVIRONMENT\n\n  Invalidate the target environment, forcing its removal during the next run\n  of the janitor process.\n\nOptions:\n  -s, --sync  Wait for the environment to be deleted before returning. If not\n              specified, the environment will be deleted asynchronously by the\n              janitor process. This option requires a connection to the data\n              warehouse.\n  --help      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan invalidate dev\nEnvironment 'dev' invalidated.\n</code></pre>"},{"location":"cli-command/cli/#janitor","title":"janitor","text":"<pre><code>Usage: vulcan janitor [OPTIONS]\n\n  Run the janitor process on-demand.\n\n  The janitor cleans up old environments and expired snapshots.\n\nOptions:\n  --ignore-ttl  Cleanup snapshots that are not referenced in any environment,\n                regardless of when they're set to expire\n  --help        Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan janitor\nDeleted object warehouse.sales__dev\nDeleted object warehouse.raw__dev\nCleanup complete.\n</code></pre>"},{"location":"cli-command/cli/#migrate","title":"migrate","text":"<pre><code>Usage: vulcan migrate [OPTIONS]\n\n  Migrate Vulcan to the current running version.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>migrate</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-command/cli/#plan","title":"plan","text":"<pre><code>Usage: vulcan plan [OPTIONS] [ENVIRONMENT]\n\n  Apply local changes to the target environment.\n\nOptions:\n  -s, --start TEXT                The start datetime of the interval for which\n                                  this command will be applied.\n  -e, --end TEXT                  The end datetime of the interval for which\n                                  this command will be applied.\n  --execution-time TEXT           The execution time (defaults to now).\n  --create-from TEXT              The environment to create the target\n                                  environment from if it doesn't exist.\n                                  Default: prod.\n  --skip-tests                    Skip tests prior to generating the plan if\n                                  they are defined.\n  --skip-linter                   Skip linting prior to generating the plan if\n                                  the linter is enabled.\n  -r, --restate-model TEXT        Restate data for specified models and models\n                                  downstream from the one specified. For\n                                  production environment, all related model\n                                  versions will have their intervals wiped,\n                                  but only the current versions will be\n                                  backfilled. For development environment,\n                                  only the current model versions will be\n                                  affected.\n  --no-gaps                       Ensure that new snapshots have no data gaps\n                                  when comparing to existing snapshots for\n                                  matching models in the target environment.\n  --skip-backfill, --dry-run      Skip the backfill step and only create a\n                                  virtual update for the plan.\n  --empty-backfill                Produce empty backfill. Like --skip-backfill\n                                  no models will be backfilled, unlike --skip-\n                                  backfill missing intervals will be recorded\n                                  as if they were backfilled.\n  --forward-only                  Create a plan for forward-only changes.\n  --allow-destructive-model TEXT  Allow destructive forward-only changes to\n                                  models whose names match the expression.\n  --allow-additive-model TEXT     Allow additive forward-only changes to\n                                  models whose names match the expression.\n  --effective-from TEXT           The effective date from which to apply\n                                  forward-only changes on production.\n  --no-prompts                    Disable interactive prompts for the backfill\n                                  time range. Please note that if this flag is\n                                  set and there are uncategorized changes,\n                                  plan creation will fail.\n  --auto-apply                    Automatically apply the new plan after\n                                  creation.\n  --no-auto-categorization        Disable automatic change categorization.\n  --include-unmodified            Include unmodified models in the target\n                                  environment.\n  --select-model TEXT             Select specific model changes that should be\n                                  included in the plan.\n  --backfill-model TEXT           Backfill only the models whose names match\n                                  the expression.\n  --no-diff                       Hide text differences for changed models.\n  --run                           Run latest intervals as part of the plan\n                                  application (prod environment only).\n  --enable-preview                Enable preview for forward-only models when\n                                  targeting a development environment.\n  --diff-rendered                 Output text differences for the rendered\n                                  versions of the models and standalone\n                                  audits.\n  --explain                       Explain the plan instead of applying it.\n  --ignore-cron                   Run all missing intervals, ignoring\n                                  individual cron schedules. Only applies if\n                                  --run is set.\n  --min-intervals INTEGER         For every model, ensure at least this many\n                                  intervals are covered by a missing intervals\n                                  check regardless of the plan start date\n  -v, --verbose                   Verbose output. Use -vv for very verbose\n                                  output.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#api","title":"api","text":"<pre><code>Usage: vulcan api [OPTIONS]\n\n  Start the Vulcan API server (models, metrics, lineage, telemetry).\n\nOptions:\n  --host TEXT        Bind socket to this host. Default: 0.0.0.0\n  --port INTEGER     Bind socket to this port. Default: 8000\n  --reload           Enable auto-reload on file changes. Default: False\n  --workers INTEGER  Number of worker processes. Default: 1\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#render","title":"render","text":"<pre><code>Usage: vulcan render [OPTIONS] MODEL\n\n  Render a model's query, optionally expanding referenced models.\n\nOptions:\n  -s, --start TEXT            The start datetime of the interval for which\n                              this command will be applied.\n  -e, --end TEXT              The end datetime of the interval for which this\n                              command will be applied.\n  --execution-time TEXT       The execution time (defaults to now).\n  --expand TEXT               Whether or not to expand materialized models\n                              (defaults to False). If True, all referenced\n                              models are expanded as raw queries. Multiple\n                              model names can also be specified, in which case\n                              only they will be expanded as raw queries.\n  --dialect TEXT              The SQL dialect to render the query as.\n  --no-format                 Disable fancy formatting of the query.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan render sales.daily_sales\n\nSELECT\n  CAST(\"raw_orders\".\"order_date\" AS TIMESTAMP) AS \"order_date\",\n  CAST(COUNT(\"raw_orders\".\"order_id\") AS INT) AS \"total_orders\",\n  CAST(SUM(\"raw_orders\".\"total_amount\") AS DOUBLE PRECISION) AS \"total_revenue\",\n  CAST(MAX(\"raw_orders\".\"order_id\") AS VARCHAR) AS \"last_order_id\",\n  COUNT(DISTINCT \"raw_orders\".\"product_id\") AS \"total_products\"\nFROM \"warehouse\".\"vulcan__raw\".\"raw__raw_orders__1032938324\" AS \"raw_orders\" /* warehouse.raw.raw_orders */\nGROUP BY\n  \"raw_orders\".\"order_date\"\nORDER BY\n  \"order_date\"\n</code></pre>"},{"location":"cli-command/cli/#rollback","title":"rollback","text":"<pre><code>Usage: vulcan rollback [OPTIONS]\n\n  Rollback Vulcan to the previous migration.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>rollback</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-command/cli/#run","title":"run","text":"<pre><code>Usage: vulcan run [OPTIONS] [ENVIRONMENT]\n\n  Evaluate missing intervals for the target environment.\n\nOptions:\n  -s, --start TEXT              The start datetime of the interval for which\n                                this command will be applied.\n  -e, --end TEXT                The end datetime of the interval for which\n                                this command will be applied.\n  --skip-janitor                Skip the janitor task.\n  --ignore-cron                 Run for all missing intervals, ignoring\n                                individual cron schedules.\n  --select-model TEXT           Select specific models to run. Note: this\n                                always includes upstream dependencies.\n  --exit-on-env-update INTEGER  If set, the command will exit with the\n                                specified code if the run is interrupted by an\n                                update to the target environment.\n  --no-auto-upstream            Do not automatically include upstream models.\n                                Only applicable when --select-model is used.\n                                Note: this may result in missing / invalid\n                                data for the selected models.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#state","title":"state","text":"<pre><code>Usage: vulcan state [OPTIONS] COMMAND [ARGS]...\n\n  Commands for interacting with state\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  export  Export the state database to a file\n  import  Import a state export file back into the state database\n</code></pre>"},{"location":"cli-command/cli/#export","title":"export","text":"<pre><code>Usage: vulcan state export [OPTIONS]\n\n  Export the state database to a file\n\nOptions:\n  -o, --output-file FILE  Path to write the state export to  [required]\n  --environment TEXT      Name of environment to export. Specify multiple\n                          --environment arguments to export multiple\n                          environments\n  --local                 Export local state only. Note that the resulting\n                          file will not be importable\n  --no-confirm            Do not prompt for confirmation before exporting\n                          existing state\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#import","title":"import","text":"<pre><code>Usage: vulcan state import [OPTIONS]\n\n  Import a state export file back into the state database\n\nOptions:\n  -i, --input-file FILE  Path to the state file  [required]\n  --replace              Clear the remote state before loading the file. If\n                         omitted, a merge is performed instead\n  --no-confirm           Do not prompt for confirmation before updating\n                         existing state\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#table_diff","title":"table_diff","text":"<pre><code>Usage: vulcan table_diff [OPTIONS] SOURCE:TARGET [MODEL]\n\n  Show the diff between two tables or a selection of models when they are\n  specified.\n\nOptions:\n  -o, --on TEXT            The column to join on. Can be specified multiple\n                           times. The model grain will be used if not\n                           specified.\n  -s, --skip-columns TEXT  The column(s) to skip when comparing the source and\n                           target table.\n  --where TEXT             An optional where statement to filter results.\n  --limit INTEGER          The limit of the sample dataframe.\n  --show-sample            Show a sample of the rows that differ. With many\n                           columns, the output can be very wide.\n  -d, --decimals INTEGER   The number of decimal places to keep when comparing\n                           floating point columns. Default: 3\n  --skip-grain-check       Disable the check for a primary key (grain) that is\n                           missing or is not unique.\n  --warn-grain-check       Warn if any selected model is missing a grain,\n                           and compute diffs for the remaining models.\n  --temp-schema TEXT       Schema used for temporary tables. It can be\n                           `CATALOG.SCHEMA` or `SCHEMA`. Default:\n                           `vulcan_temp`\n  -m, --select-model TEXT  Specify one or more models to data diff. Use\n                           wildcards to diff multiple models. Ex: '*' (all\n                           models with applied plan diffs), 'demo.model+'\n                           (this and downstream models),\n                           'git:feature_branch' (models with direct\n                           modifications in this branch only)\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#table_name","title":"table_name","text":"<pre><code>Usage: vulcan table_name [OPTIONS] MODEL_NAME\n\n  Prints the name of the physical table for the given model.\n\nOptions:\n  --environment, --env TEXT  The environment to source the model version from.\n  --prod                     If set, return the name of the physical table\n                             that will be used in production for the model\n                             version promoted in the target environment.\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#test","title":"test","text":"<pre><code>Usage: vulcan test [OPTIONS] [TESTS]...\n\n  Run model unit tests.\n\nOptions:\n  -k TEXT              Only run tests that match the pattern of substring.\n  -v, --verbose        Verbose output.\n  --preserve-fixtures  Preserve the fixture tables in the testing database,\n                       useful for debugging.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#semantic","title":"semantic","text":"<pre><code>Usage: vulcan semantic [OPTIONS] {export} [ENVIRONMENT]\n\n  Semantic layer operations.\n\n  This command provides semantic layer export functionality, allowing users to\n  convert semantic models and metrics into CubeJS-compatible YAML schemas.\n\nOptions:\n  -o, --output PATH   Output file path for the CubeJS schema.  [required]\n  --strict            Strict mode: export only explicitly defined semantic\n                      models.\n  --no-auto-measures  Disable automatic generation of measures (e.g., _count)\n                      for models with grains.\n  --no-confirm        Do not prompt for confirmation before overwriting\n                      existing output file.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#transpile","title":"transpile","text":"<pre><code>Usage: vulcan transpile [OPTIONS] [QUERY]\n\n  Transpile a semantic SQL or REST-style semantic query to executable SQL.\n\nOptions:\n  --format [sql|rest]        Input type: semantic SQL ('sql') or REST-style\n                             semantic payload ('rest').  [required]\n  --file TEXT                Read query or REST payload from file. Use '-' to\n                             read from stdin.\n  --user TEXT                User id to propagate in the X-User header\n                             (defaults to 'cli').\n  --disable-post-processing  Disable post-processing in the Transpiler.\n  --style [pretty|compact]   SQL output style: 'pretty' (formatted with\n                             indentation), 'compact' (unformatted but\n                             processed),\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#transpiler","title":"transpiler","text":"<pre><code>Usage: vulcan transpiler [OPTIONS] {up|down}\n\n  Manage the Transpiler service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#graphql","title":"graphql","text":"<pre><code>Usage: vulcan graphql [OPTIONS] {up|down}\n\n  Manage the GraphQL service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#lint","title":"lint","text":"<pre><code>Usage: vulcan lint [OPTIONS]\n  Run linter for the target model(s).\n\nOptions:\n  --model TEXT           A model to lint. Multiple models can be linted.  If no models are specified, every model will be linted.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/","title":"Custom materializations","text":""},{"location":"components/advanced-features/custom_materializations/#custom-materializations","title":"Custom materializations","text":"<p>Vulcan supports a variety of model kinds that reflect the most common approaches to evaluating and materializing data transformations.</p> <p>Sometimes, however, a specific use case cannot be addressed with an existing model kind. For scenarios like this, Vulcan allows users to create their own materialization implementation using Python.</p> <p>Advanced Feature</p> <p>This is an advanced feature and should only be considered if all other approaches have been exhausted. If you're at this decision point, we recommend you reach out to our team in the community slack before investing time building a custom materialization. If an existing model kind can solve your problem, we want to clarify the Vulcan documentation; if an existing kind can almost solve your problem, we want to consider modifying the kind so all Vulcan users can benefit.</p>"},{"location":"components/advanced-features/custom_materializations/#background","title":"Background","text":"<p>A Vulcan model kind consists of methods for executing and managing the outputs of data transformations - collectively, these are the kind's \"materialization.\"</p> <p>Some materializations are relatively simple. For example, the SQL FULL model kind completely replaces existing data each time it is run, so its materialization boils down to executing <code>CREATE OR REPLACE [table name] AS [your model query]</code>.</p> <p>The materializations for other kinds, such as INCREMENTAL BY TIME RANGE, require additional logic to process the correct time intervals and replace/insert their results into an existing table.</p> <p>A model kind's materialization may differ based on the SQL engine executing the model. For example, PostgreSQL does not support <code>CREATE OR REPLACE TABLE</code>, so <code>FULL</code> model kinds instead <code>DROP</code> the existing table then <code>CREATE</code> a new table. Vulcan already contains the logic needed to materialize existing model kinds on all supported engines.</p>"},{"location":"components/advanced-features/custom_materializations/#overview","title":"Overview","text":"<p>Custom materializations are analogous to new model kinds. Users specify them by name in a model definition's <code>MODEL</code> block, and they may accept user-specified arguments.</p> <p>A custom materialization must:</p> <ul> <li>Be written in Python code</li> <li>Be a Python class that inherits the Vulcan <code>CustomMaterialization</code> base class</li> <li>Use or override the <code>insert</code> method from the Vulcan <code>MaterializableStrategy</code> class/subclasses</li> <li>Be loaded or imported by Vulcan at runtime</li> </ul> <p>A custom materialization may:</p> <ul> <li>Use or override methods from the Vulcan <code>MaterializableStrategy</code> class/subclasses</li> <li>Use or override methods from the Vulcan <code>EngineAdapter</code> class/subclasses</li> <li>Execute arbitrary SQL code and fetch results with the engine adapter <code>execute</code> and related methods</li> </ul> <p>A custom materialization may perform arbitrary Python processing with Pandas or other libraries, but in most cases that logic should reside in a Python model instead of the materialization.</p> <p>A Vulcan project will automatically load any custom materializations present in its <code>materializations/</code> directory. Alternatively, the materialization may be bundled into a Python package and installed with standard methods.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-materialization","title":"Creating a custom materialization","text":"<p>Create a new custom materialization by adding a <code>.py</code> file containing the implementation to the <code>materializations/</code> folder in the project directory. Vulcan will automatically import all Python modules in this folder at project load time and register the custom materializations.</p> <p>A custom materialization must be a class that inherits the <code>CustomMaterialization</code> base class and provides an implementation for the <code>insert</code> method.</p>"},{"location":"components/advanced-features/custom_materializations/#simple-example","title":"Simple example","text":"<p>Here's a complete example of a custom materialization that demonstrates custom insert logic:</p> <pre><code>import typing as t\nfrom sqlalchemy import text\nfrom vulcan import CustomMaterialization\nfrom vulcan import Model\n\nclass SimpleCustomMaterialization(CustomMaterialization):\n    \"\"\"Simple custom materialization - demonstrates custom insert logic\"\"\"\n\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Union[str, t.Any],\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        \"\"\"Custom insert logic for tables\"\"\"\n\n        print(f\"Custom materialization: Processing table {table_name}\")\n        print(f\"Model: {model.name}\")\n        print(f\"Is first insert: {is_first_insert}\")\n\n        if is_first_insert:\n            print(\"Creating table for the first time\")\n            # Create the table normally using the adapter\n            self.adapter.create_table(\n                table_name,\n                columns=model.columns_to_types,\n                target_columns_to_types=model.columns_to_types,\n                partitioned_by=model.partitioned_by,\n            )\n\n        # Insert data with custom logic\n        if isinstance(query_or_df, str):\n            print(\"Executing SQL query\")\n            # Execute the query - Vulcan provides the INSERT INTO ... SELECT query\n            self.adapter.execute(text(query_or_df))\n        else:\n            print(\"Inserting DataFrame\")\n            # Insert DataFrame normally - useful for Python models that return DataFrames\n            self.adapter.insert_append(table_name, query_or_df)\n\n        print(f\"Custom materialization completed for {table_name}\")\n</code></pre> <p>Let's break down this materialization:</p> Component Description <code>NAME</code> The name used to reference this materialization in model definitions (<code>simple_custom</code>) <code>table_name</code> The target table name where data will be inserted <code>query_or_df</code> Either a SQL query string or a DataFrame (Pandas, PySpark, Snowpark) <code>model</code> The model definition object with access to all model properties <code>is_first_insert</code> <code>True</code> if this is the first insert for the current model version <code>render_kwargs</code> Dictionary of arguments used to render the model query <code>self.adapter</code> The engine adapter for executing SQL and interacting with the database"},{"location":"components/advanced-features/custom_materializations/#minimal-example","title":"Minimal example","text":"<p>For a simpler full-refresh materialization:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#controlling-table-creation-and-deletion","title":"Controlling table creation and deletion","text":"<p>You can control how data objects (tables, views, etc.) are created and deleted by overriding the <code>create</code> and <code>delete</code> methods:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(self, table_name: str, query_or_df: t.Any, model: Model, \n               is_first_insert: bool, render_kwargs: t.Dict[str, t.Any], **kwargs: t.Any) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n\n    def create(\n        self,\n        table_name: str,\n        model: Model,\n        is_table_deployable: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Custom table/view creation logic\n        # Uses self.adapter methods like create_table, create_view, or ctas\n        self.adapter.create_table(\n            table_name,\n            columns=model.columns_to_types,\n            target_columns_to_types=model.columns_to_types,\n        )\n\n    def delete(self, name: str, **kwargs: t.Any) -&gt; None:\n        # Custom table/view deletion logic\n        self.adapter.drop_table(name)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#using-a-custom-materialization","title":"Using a custom materialization","text":"<p>Specify the model kind <code>CUSTOM</code> in a model definition to use a custom materialization. Set the <code>materialization</code> attribute to the <code>NAME</code> from your custom materialization:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom'\n  ),\n  grain (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers c\nLEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name\nORDER BY total_spent DESC\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.custom_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.CUSTOM,\n        materialization=\"simple_custom\",\n    ),\n    grain=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Python model using custom materialization with dynamic dependencies\"\"\"\n\n    # Simple customer summary\n    query = \"\"\"\n    SELECT \n        c.customer_id,\n        c.name as customer_name,\n        COUNT(DISTINCT o.order_id) as total_orders,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_spent\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id, c.name\n    ORDER BY total_spent DESC\n    \"\"\"\n\n    # Execute query and return results\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#passing-properties-to-the-materialization","title":"Passing properties to the materialization","text":"<p>A custom materialization can accept configuration through <code>materialization_properties</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom',\n    materialization_properties (\n      'config_key' = 'config_value',\n      'batch_size' = 1000\n    )\n  )\n);\n</code></pre> <p>Access these properties in your materialization via <code>model.custom_materialization_properties</code>:</p> <pre><code>class SimpleCustomMaterialization(CustomMaterialization):\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Access custom properties\n        config_value = model.custom_materialization_properties.get(\"config_key\")\n        batch_size = model.custom_materialization_properties.get(\"batch_size\", 500)\n\n        print(f\"Config value: {config_value}, Batch size: {batch_size}\")\n\n        # Proceed with insert logic\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#extending-customkind","title":"Extending <code>CustomKind</code>","text":"<p>Warning</p> <p>This is even lower level usage that contains extra complexity and relies on knowledge of Vulcan internals. If you don't need this level of complexity, stick with the method described above.</p> <p>In many cases, the above usage of a custom materialization will suffice. However, you may want tighter integration with Vulcan's internals:</p> <ul> <li>Validate custom properties before any database connections are made</li> <li>Leverage existing functionality that relies on specific properties being present</li> </ul> <p>In this case, you can provide a subclass of <code>CustomKind</code> for Vulcan to use instead of <code>CustomKind</code> itself. During project load, Vulcan will instantiate your subclass instead of <code>CustomKind</code>.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-kind","title":"Creating a custom kind","text":"<pre><code>import typing as t\nfrom typing_extensions import Self\nfrom pydantic import model_validator\nfrom sqlglot import exp\nfrom vulcan import CustomKind\nfrom vulcan.utils.pydantic import list_of_fields_validator\nfrom vulcan.utils.errors import ConfigError\n\nclass MyCustomKind(CustomKind):\n\n    _primary_key: t.List[exp.Expression]\n\n    @model_validator(mode=\"after\")\n    def _validate_model(self) -&gt; Self:\n        self._primary_key = list_of_fields_validator(\n            self.materialization_properties.get(\"primary_key\"),\n            {\"dialect\": self.dialect}\n        )\n        if not self.primary_key:\n            raise ConfigError(\"primary_key must be specified\")\n        return self\n\n    @property\n    def primary_key(self) -&gt; t.List[exp.Expression]:\n        return self._primary_key\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#using-the-custom-kind-in-a-model","title":"Using the custom kind in a model","text":"<pre><code>MODEL (\n  name vulcan_demo.my_model,\n  kind CUSTOM (\n    materialization 'my_custom_full',\n    materialization_properties (\n      primary_key = (col1, col2)\n    )\n  )\n);\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#linking-to-your-materialization","title":"Linking to your materialization","text":"<p>Specify the custom kind as a generic type parameter on your materialization class:</p> <pre><code>class CustomFullMaterialization(CustomMaterialization[MyCustomKind]):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        assert isinstance(model.kind, MyCustomKind)\n\n        self.adapter.merge(\n            ...,\n            unique_key=model.kind.primary_key\n        )\n</code></pre> <p>When Vulcan loads your custom materialization, it will inspect the Python type signature for generic parameters that are subclasses of <code>CustomKind</code>. If found, it will instantiate your subclass when building <code>model.kind</code> instead of using the default <code>CustomKind</code> class.</p> <p>Benefits of this approach:</p> <ul> <li>Early validation: Validation for <code>primary_key</code> happens at load time instead of evaluation time, so issues are caught before applying a plan</li> <li>Type safety: <code>model.kind</code> resolves to your custom kind object, giving access to extra properties without additional validation</li> </ul>"},{"location":"components/advanced-features/custom_materializations/#sharing-custom-materializations","title":"Sharing custom materializations","text":""},{"location":"components/advanced-features/custom_materializations/#copying-files","title":"Copying files","text":"<p>The simplest (but least robust) way to use a custom materialization in multiple Vulcan projects is for each project to place a copy of the materialization's Python code in its <code>materializations/</code> directory.</p> <p>If you use this approach, we strongly recommend storing the materialization code in a version-controlled repository and creating a reliable method of notifying users when it is updated.</p>"},{"location":"components/advanced-features/custom_materializations/#python-packaging","title":"Python packaging","text":"<p>A more robust way to share custom materializations is to create and publish a Python package containing the implementation.</p> <p>This is required when a Vulcan project uses Airflow or other external schedulers where the scheduler cluster doesn't have the <code>materializations/</code> folder available.</p> <p>Package and expose custom materializations with the setuptools entrypoints mechanism:</p> pyproject.tomlsetup.py <pre><code>[project.entry-points.\"vulcan.materializations\"]\nmy_materialization = \"my_package.my_materialization:CustomFullMaterialization\"\n</code></pre> <pre><code>setup(\n    ...,\n    entry_points={\n        \"vulcan.materializations\": [\n            \"my_materialization = my_package.my_materialization:CustomFullMaterialization\",\n        ],\n    },\n)\n</code></pre> <p>Once the package is installed, Vulcan will automatically load custom materializations from the entrypoint list.</p> <p>Refer to the Vulcan Github custom_materializations example for more details on Python packaging.</p>"},{"location":"components/advanced-features/signals/","title":"Signals","text":""},{"location":"components/advanced-features/signals/#signals","title":"Signals","text":"<p>Vulcan's built-in scheduler controls which models are evaluated when the <code>vulcan run</code> command is executed.</p> <p>It determines whether to evaluate a model based on whether the model's <code>cron</code> has elapsed since the previous evaluation. For example, if a model's <code>cron</code> was <code>@daily</code>, the scheduler would evaluate the model if its last evaluation occurred on any day before today.</p> <p>Unfortunately, the world does not always accommodate our data system's schedules. Data may land in our system after downstream daily models already ran. The scheduler did its job correctly, but today's late data will not be processed until tomorrow's scheduled run.</p> <p>You can use signals to prevent this problem.</p>"},{"location":"components/advanced-features/signals/#what-is-a-signal","title":"What is a signal?","text":"<p>The scheduler uses two criteria to determine whether a model should be evaluated: whether its <code>cron</code> elapsed since the last evaluation and whether it upstream dependencies' runs have completed.</p> <p>Signals allow you to specify additional criteria that must be met before the scheduler evaluates the model.</p> <p>A signal definition is simply a function that checks whether a criterion is met. Before describing the checking function, we provide some background information about how the scheduler works.</p> <p>The scheduler doesn't actually evaluate \"a model\" - it evaluates a model over a specific time interval. This is clearest for incremental models, where only rows in the time interval are ingested during an evaluation. However, evaluation of non-temporal model kinds like <code>FULL</code> and <code>VIEW</code> are also based on a time interval: the model's <code>cron</code> frequency.</p> <p>The scheduler's decisions are based on these time intervals. For each model, the scheduler examines a set of candidate intervals and identifies the ones that are ready for evaluation.</p> <p>It then divides those into batches (configured with the model's batch_size parameter). For incremental models, it evaluates the model once for each batch. For non-incremental models, it evaluates the model once if any batch contains an interval.</p> <p>Signal checking functions examines a batch of time intervals. The function is always called with a batch of time intervals (DateTimeRanges). It can also optionally be called with key word arguments. It may return <code>True</code> if all intervals are ready for evaluation, <code>False</code> if no intervals are ready, or the time intervals themselves if only some are ready. A checking function is defined with the <code>@signal</code> decorator.</p> <p>One model, multiple signals</p> <p>Multiple signals may be specified for a model. Vulcan categorizes a candidate interval as ready for evaluation if all the signal checking functions determine it is ready.</p>"},{"location":"components/advanced-features/signals/#defining-a-signal","title":"Defining a signal","text":"<p>To define a signal, create a <code>signals</code> directory in your project folder. Define your signal in a file named <code>__init__.py</code> in that directory (you can have additional python file names as well).</p> <p>A signal is a function that accepts a batch (<code>DateTimeRanges: t.List[t.Tuple[datetime, datetime]]</code>) and returns a batch or a boolean. It needs to use the <code>@signal</code> decorator.</p> <p>We now demonstrate signals of varying complexity.</p>"},{"location":"components/advanced-features/signals/#simple-example","title":"Simple example","text":"<p>This example defines a <code>RandomSignal</code> method.</p> <p>The method returns <code>True</code> (indicating that all intervals are ready for evaluation) if a random number is greater than a threshold specified in the model definition:</p> <pre><code>import random\nimport typing as t\nfrom vulcan import signal, DatetimeRanges\n\n\n@signal()\ndef random_signal(batch: DatetimeRanges, threshold: float) -&gt; t.Union[bool, DatetimeRanges]:\n    return random.random() &gt; threshold\n</code></pre> <p>Note that the <code>random_signal()</code> takes a mandatory user defined <code>threshold</code> argument.</p> <p>The <code>random_signal()</code> method extracts the threshold metadata and compares a random number to it. The type is inferred based on the same rules as Vulcan Macros.</p> <p>Now that we have a working signal, we need to specify that a model should use the signal by passing metadata to the model DDL's <code>signals</code> key.</p> <p>The <code>signals</code> key accepts an array delimited by brackets <code>[]</code>. Each function in the list should contain the metadata needed for one signal evaluation.</p> <p>This example specifies that the <code>random_signal()</code> should evaluate once with a threshold of 0.5:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind FULL,\n  signals (\n    random_signal(threshold := 0.5), # specify threshold value\n  )\n);\n\nSELECT 1\n</code></pre> <p>The next time this project is <code>vulcan run</code>, our signal will metaphorically flip a coin to determine whether the model should be evaluated.</p>"},{"location":"components/advanced-features/signals/#advanced-example","title":"Advanced Example","text":"<p>This example demonstrates more advanced use of signals: a signal returning a subset of intervals from a batch (rather than a single <code>True</code>/<code>False</code> value for all intervals in the batch)</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges\nfrom vulcan.utils.date import to_datetime\n\n\n# signal that returns only intervals that are &lt;= 1 week ago\n@signal()\ndef one_week_ago(batch: DatetimeRanges) -&gt; t.Union[bool, DatetimeRanges]:\n    dt = to_datetime(\"1 week ago\")\n\n    return [\n        (start, end)\n        for start, end in batch\n        if start &lt;= dt\n    ]\n</code></pre> <p>Instead of returning a single <code>True</code>/<code>False</code> value for whether a batch of intervals is ready for evaluation, the <code>one_week_ago()</code> function returns specific intervals from the batch.</p> <p>It generates a datetime argument, to which it compares the beginning of each interval in the batch. If the interval start is before that argument, the interval is ready for evaluation and included in the returned list. These signals can be added to a model like so.</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column ds,\n  ),\n  start '2 week ago',\n  signals (\n    one_week_ago(),\n  )\n);\n\n\nSELECT @start_ds AS ds\n</code></pre>"},{"location":"components/advanced-features/signals/#accessing-execution-context-engine-adapter","title":"Accessing execution context / engine adapter","text":"<p>It is possible to access the execution context in a signal and access the engine adapter (warehouse connection).</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges, ExecutionContext\n\n\n# add the context argument to your function\n@signal()\ndef one_week_ago(batch: DatetimeRanges, context: ExecutionContext) -&gt; t.Union[bool, DatetimeRanges]:\n    return len(context.engine_adapter.fetchdf(\"SELECT 1\")) &gt; 1\n</code></pre>"},{"location":"components/advanced-features/signals/#testing-signals","title":"Testing Signals","text":"<p>Signals only evaluate on <code>run</code> or with <code>check_intervals</code>.</p> <p>To test signals with the check_intervals command:</p> <ol> <li>Deploy your changes to an environment with <code>vulcan plan my_dev</code>.</li> <li>Run <code>vulcan check_intervals my_dev</code>.</li> </ol> <ul> <li>To check a subset of models use the --select-model flag.</li> <li>To turn off signals and just check missing intervals, use the --no-signals flag.</li> </ul> <ol> <li>To iterate, make changes to the signal, and redeploy with step 1.</li> </ol> <p>Note</p> <p><code>check_intervals</code> only works on remote models in an environment. Local signal changes are never run.</p>"},{"location":"components/advanced-features/macros/built_in/","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#built-in","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros behave differently than those of templating systems like Jinja.</p> <p>Macro systems are based on string substitution. The macro system scans code files, identifies special characters that signify macro content, and replaces the macro elements with other text.</p> <p>In a general sense, that is the entire functionality of templating systems. They have tools that provide control flow logic (if-then) and other functionality, but that functionality is solely to support substituting in the correct strings.</p> <p>Templating systems are intentionally agnostic to the programming language being templated, and most of them work for everything from blog posts to HTML to SQL.</p> <p>In contrast, Vulcan macros are designed specifically for generating SQL code. They have semantic understanding of the SQL code being created by analyzing it with the Python sqlglot library, and they allow use of Python code so users can tidily implement sophisticated macro logic.</p>"},{"location":"components/advanced-features/macros/built_in/#vulcan-macro-approach","title":"Vulcan macro approach","text":"<p>This section describes how Vulcan macros work under the hood. Feel free to skip over this section and return if and when it is useful. This information is not required to use Vulcan macros, but it will be useful for debugging any macros exhibiting puzzling behavior.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</li> <li>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</li> <li>Macro functions, both Vulcan's and user-defined</li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"components/advanced-features/macros/built_in/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></li> <li>Identifiers, which reference database objects like column, table, alias, and function names<ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint and local.</p> <p>Global and gateway macro variables are defined in the project configuration file and can be accessed in any project model. Blueprint and macro variables are defined in a model definition and can only be accessed in that model.</p> <p>Macro variables with the same name may be specified at any or all of the global, gateway, blueprint and local levels. When variables are specified at multiple levels, the value of the most specific level takes precedence. For example, the value of a local variable takes precedence over the value of a blueprint or gateway variable with the same name, and the value of a gateway variable takes precedence over the value of a global variable.</p>"},{"location":"components/advanced-features/macros/built_in/#global-variables","title":"Global variables","text":"<p>Global variables are defined in the project configuration file <code>variables</code> key.</p> <p>Global variable values may be any of the following data types or lists or dictionaries containing these types: <code>int</code>, <code>float</code>, <code>bool</code>, <code>str</code>.</p> <p>Access global variable values in a model definition using the <code>@&lt;VAR_NAME&gt;</code> macro or the <code>@VAR()</code> macro function. The latter function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"components/advanced-features/macros/built_in/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"components/advanced-features/macros/built_in/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system has multiple operators that allow different forms of dynamic behavior in models.</p>"},{"location":"components/advanced-features/macros/built_in/#each","title":"@EACH","text":"<p><code>@EACH</code> is used to transform a list of items by applying a function to each of them, analogous to a <code>for</code> loop.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>Vulcan macros support placing macro values at the end of a column name using <code>column_@x</code>.</p> <p>However, if you wish to substitute the variable anywhere else in the identifier, you need to use the more explicit curly brace syntax <code>@{}</code> to avoid ambiguity. For example, these are valid uses: <code>@{x}_column</code> or <code>my_@{x}_column</code>.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"components/advanced-features/macros/built_in/#if","title":"@IF","text":"<p>Vulcan's <code>@IF</code> macro allows components of a SQL query to change based on the result of a logical condition.</p> <p>It includes three elements:</p> <ol> <li>A logical condition that evaluates to <code>TRUE</code> or <code>FALSE</code></li> <li>A value to return if the condition is <code>TRUE</code></li> <li>A value to return if the condition is <code>FALSE</code> [optional]</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</li> <li>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</li> <li>Between: <code>[number] BETWEEN [low number] AND [high number]</code></li> <li>Membership: <code>[item] IN ([comma-separated list of items])</code></li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li><code>'a' = 'a'</code></li> <li><code>'a' != 'b'</code></li> <li><code>0 &lt; 1</code></li> <li><code>1 &gt;= 1</code></li> <li><code>2 BETWEEN 1 AND 3</code></li> <li><code>'a' IN ('a', 'b')</code></li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"components/advanced-features/macros/built_in/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"components/advanced-features/macros/built_in/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The relation/table whose columns are being selected</li> <li><code>alias</code> (optional): The alias of the relation (if it has one)</li> <li><code>exclude</code> (optional): A list of columns to exclude</li> <li><code>prefix</code> (optional): A string to use as a prefix for all selected column names</li> <li><code>suffix</code> (optional): A string to use as a suffix for all selected column names</li> <li><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</li> <li>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</li> <li>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</li> <li>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></li> <li>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</li> <li><code>my_column</code> is present in the query</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by: 1. <code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type) 2. Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column 3. Concatenating the column values after steps (1) and (2) 4. Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</li> <li><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>lat1</code>: Latitude of the first point</li> <li><code>lon1</code>: Longitude of the first point</li> <li><code>lat2</code>: Latitude of the second point</li> <li><code>lon2</code>: Longitude of the second point</li> <li><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>column</code>: The column to pivot</li> <li><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</li> <li><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</li> <li><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></li> <li><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></li> <li><code>prefix</code> (optional): A prefix to use for all aliases</li> <li><code>suffix</code> (optional): A suffix to use for all aliases</li> <li><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></li> <li><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></li> <li><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</li> <li><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The table or CTE name to deduplicate</li> <li><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</li> <li><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</li> <li><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</li> <li><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>. - For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</p>"},{"location":"components/advanced-features/macros/built_in/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</li> <li>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li><code>template</code> - The string template to render into an AST node</li> <li><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></li> <li><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></li> <li><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li><code>@WITH</code>: common table expression <code>WITH</code> clause</li> <li><code>@JOIN</code>: table <code>JOIN</code> clause(s)</li> <li><code>@WHERE</code>: filtering <code>WHERE</code> clause</li> <li><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</li> <li><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</li> <li><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</li> <li><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"components/advanced-features/macros/built_in/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"components/advanced-features/macros/built_in/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Vulcan supports user-defined macro functions written in two languages - SQL and Python:</p> <ul> <li>SQL macro functions must use the Jinja templating system.</li> <li>Python macro functions use the SQLGlot library to allow more complex operations than macro variables and operators provide alone.</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#python-macro-functions","title":"Python macro functions","text":""},{"location":"components/advanced-features/macros/built_in/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"components/advanced-features/macros/built_in/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li><code>@add_args(1)</code> returns 1</li> <li><code>@add_args(1, 2)</code> returns 3</li> <li><code>@add_args(1, 2, 3)</code> returns 6</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li>Numbers are parsed into <code>Literal</code> expressions</li> <li>Quoted strings are parsed into <code>Literal</code> expressions</li> <li>Unquoted strings are parsed into <code>Column</code> expressions</li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></li> <li>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in the <code>dev</code> environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in any environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</li> <li>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</li> <li>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#typed-macros","title":"Typed Macros","text":"<p>Typed macros in Vulcan bring the power of type hints from Python, enhancing readability, maintainability, and usability of your SQL macros. These macros enable developers to specify expected types for arguments, making the macros more intuitive and less error-prone.</p>"},{"location":"components/advanced-features/macros/built_in/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"components/advanced-features/macros/built_in/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"components/advanced-features/macros/built_in/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</li> <li><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li><code>exp.Table</code></li> <li><code>exp.Column</code></li> <li><code>exp.Literal</code></li> <li><code>exp.Identifier</code></li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"components/advanced-features/macros/built_in/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p> <p>See examples of the coercion function in action in the test suite here.</p>"},{"location":"components/advanced-features/macros/built_in/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"components/advanced-features/macros/built_in/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan and Jinja macro systems. We strongly recommend using only one system in a model - if both are present, they may fail or behave in unintuitive ways.</p>"},{"location":"components/advanced-features/macros/jinja/","title":"Jinja","text":""},{"location":"components/advanced-features/macros/jinja/#jinja","title":"Jinja","text":"<p>Vulcan supports macros from the Jinja templating system.</p> <p>Jinja's macro approach is pure string substitution. Unlike Vulcan macros, they assemble SQL query text without building a semantic representation.</p> <p>NOTE: Vulcan projects support the standard Jinja function library only - they do not support dbt-specific jinja functions like <code>{{ ref() }}</code>. dbt-specific functions are allowed in dbt projects being run with the Vulcan adapter.</p>"},{"location":"components/advanced-features/macros/jinja/#basics","title":"Basics","text":"<p>Jinja uses curly braces <code>{}</code> to differentiate macro from non-macro text. It uses the second character after the left brace to determine what the text inside the braces will do.</p> <p>The three curly brace symbols are:</p> <ul> <li><code>{{...}}</code> creates Jinja expressions. Expressions are replaced by text that is incorporated into the rendered SQL query; they can contain macro variables and functions.</li> <li><code>{%...%}</code> creates Jinja statements. Statements give instructions to Jinja, such as setting variable values, control flow with <code>if</code>, <code>for</code> loops, and defining macro functions.</li> <li><code>{#...#}</code> creates Jinja comments. These comments will not be included in the rendered SQL query.</li> </ul> <p>Since Jinja strings are not syntactically valid SQL expressions and cannot be parsed as such, the model query must be wrapped in a special <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code> block in order for Vulcan to detect it:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>Similarly, to use Jinja expressions as part of statements that should be evaluated before or after the model query, the <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code> block should be used:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#vulcan-predefined-variables","title":"Vulcan predefined variables","text":"<p>Vulcan provides multiple predefined macro variables you may reference in jinja code.</p> <p>Some predefined variables provide information about the Vulcan project itself, like the <code>runtime_stage</code> and <code>this_model</code> variables.</p> <p>Other predefined variables are temporal, like <code>start_ds</code> and <code>execution_date</code>. They are used to build incremental model queries and are only available in incremental model kinds.</p> <p>Access predefined macro variables by passing their unquoted name in curly braces. For example, this demonstrates how to access the <code>start_ds</code> and <code>end_ds</code> variables:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Because the two macro variables return string values, we must surround the curly braces with single quotes <code>'</code>. Other macro variables, such as <code>start_epoch</code>, return numeric values and do not require the single quotes.</p> <p>The <code>gateway</code> variable uses a slightly different syntax than other predefined variables because it is a function call. Instead of the bare name <code>{{ gateway }}</code>, it must include parentheses: <code>{{ gateway() }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports two kinds of user-defined macro variables: global and local.</p> <p>Global macro variables are defined in the project configuration file and can be accessed in any project model.</p> <p>Local macro variables are defined in a model definition and can only be accessed in that model.</p>"},{"location":"components/advanced-features/macros/jinja/#global-variables","title":"Global variables","text":"<p>Learn more about defining global variables in the Vulcan macros documentation.</p> <p>Access global variable values in a model definition using the <code>{{ var() }}</code> jinja function. The function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, a model would access a global variable named <code>int_var</code> like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>A default value can be passed as a second argument to the <code>{{ var() }}</code> jinja function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> if no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key. Learn more about defining gateway variables in the Vulcan macros documentation.</p> <p>Access gateway variables in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the configuration file's root <code>variables</code> key.</p>"},{"location":"components/advanced-features/macros/jinja/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Blueprint variables can be accessed using the <code>{{ blueprint_var() }}</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>{{ var() }}</code>).</p>"},{"location":"components/advanced-features/macros/jinja/#local-variables","title":"Local variables","text":"<p>Define your own variables with the Jinja statement <code>{% set ... %}</code>. For example, we could specify the name of the <code>num_orders</code> column in the <code>vulcan_example.full_model</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>Note that the Jinja set statement is written after the <code>MODEL</code> statement and before the SQL query.</p> <p>Jinja variables can be string, integer, or float data types. They can also be an iterable data structure, such as a list, tuple, or dictionary. Each of these data types and structures supports multiple Python methods, such as the <code>upper()</code> method for strings.</p>"},{"location":"components/advanced-features/macros/jinja/#macro-operators","title":"Macro operators","text":""},{"location":"components/advanced-features/macros/jinja/#control-flow-operators","title":"Control flow operators","text":""},{"location":"components/advanced-features/macros/jinja/#for-loops","title":"for loops","text":"<p>For loops let you iterate over a collection of items to condense repetitive code and easily change the values used by the code.</p> <p>Jinja for loops begin with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>. This example demonstrates creating indicator variables with <code>CASE WHEN</code> using a Jinja for loop:</p> <pre><code>SELECT\n  {% for vehicle_type in ['car', 'truck', 'bus']}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>Note that the <code>vehicle_type</code> values are quoted in the list <code>['car', 'truck', 'bus']</code>. Jinja removes those quotes during processing, so the reference <code>'{{ vehicle_type }}</code> in the <code>CASE WHEN</code> statement must be in quotes. The reference <code>vehicle_{{ vehicle_type }}</code> does not require quotes.</p> <p>Also note that a comma is present at the end of the <code>CASE WHEN</code> line. Trailing commas are not valid SQL and would normally require special handling, but Vulcan's semantic understanding of the query allows it to identify and remove the offending comma.</p> <p>The example renders to this after Vulcan processing:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>In general, it is a best practice to define lists of values separately from their use. We could do that like this:</p> <pre><code>{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types }\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>The rendered query would be the same as before.</p>"},{"location":"components/advanced-features/macros/jinja/#if","title":"if","text":"<p>if statements allow you to take an action (or not) based on some condition.</p> <p>Jinja if statements begin with <code>{% if ... %}</code> and end with <code>{% endif %}</code>. The starting <code>if</code> statement must contain code that evaluates to <code>True</code> or <code>False</code>. For example, all of <code>True</code>, <code>1 + 1 == 2</code>, and <code>'a' in ['a', 'b']</code> evaluate to <code>True</code>.</p> <p>As an example, you might want a model to only include a column if the model was being run for testing purposes. We can do that by setting a variable indicating whether it's a testing run that determines whether the query includes <code>testing_column</code>:</p> <pre><code>{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n</code></pre> <p>Because <code>testing</code> is <code>True</code>, the rendered query would be:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Jinja macro functions should be placed in <code>.sql</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.sql</code> file, or they can be distributed across multiple files.</p> <p>Jinja macro functions are defined with the <code>{% macro %}</code> and <code>{% endmacro %}</code> statements. The macro function name and arguments are specified in the <code>{% macro %}</code> statement.</p> <p>For example, a macro function named <code>print_text</code> that takes no arguments could be defined with:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>This macro function would be called in a SQL model with <code>{{ print_text() }}</code>, which would be substituted with <code>text</code>\" in the rendered query.</p> <p>Macro function arguments are placed in the parentheses next to the macro name. For example, this macro generates a SQL column with an alias based on the arguments <code>expression</code> and <code>alias</code>:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>We might call this macro function in a SQL query like this:</p> <pre><code>SELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Note that both argument values are quoted in the call <code>alias('item_id', 'item_id2')</code> but are not quoted in the rendered query. During the rendering process, Vulcan uses its semantic understanding of the query to build the rendered text - it recognizes that the first argument is a column name and that column aliases are unquoted by default.</p> <p>In that example, the SQL query selects the column <code>item_id</code> with the alias <code>item_id2</code>. If instead we wanted to select the string <code>'item_id'</code> with the name <code>item_id2</code>, we would pass the <code>expression</code> argument with double quotes around it: <code>\"'item_id'\"</code>:</p> <pre><code>SELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes around <code>\"'item_id'\"</code> signal to Vulcan that it is not a column name.</p> <p>Some SQL dialects interpret double and single quotes differently. We could replace the rendered single quoted <code>'item_id'</code> with double quoted <code>\"item_id\"</code> in the previous example by switching the placement of quotes in the macro function call. Instead of <code>alias(\"'item_id'\", 'item_id2')</code> we would use <code>alias('\"item_id\"', 'item_id2')</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both the Jinja and Vulcan macro systems. We strongly recommend using only one system in a single model - if both are present, they may fail or behave in unintuitive ways.</p> <p>Predefined Vulcan macro variables can be used in a query containing user-defined Jinja variables and functions. However, predefined variables passed as arguments to a user-defined Jinja macro function must use the Jinja curly brace syntax <code>{{ start_ds }}</code> instead of the Vulcan macro <code>@</code> prefix syntax <code>@start_ds</code>. Note that curly brace syntax may require quoting to generate the equivalent of the <code>@</code> syntax.</p>"},{"location":"components/advanced-features/macros/overview/","title":"Overview","text":""},{"location":"components/advanced-features/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language. It does not natively have features like variables or control flow logic (if-then, for loops) that allow SQL commands to behave differently in different situations.</p> <p>However, data pipelines are dynamic and need different behavior depending on context. SQL is made dynamic with macros.</p> <p>Vulcan supports two macro systems: Vulcan macros and the Jinja templating system.</p> <p>Learn more about macros in Vulcan:</p> <ul> <li>Pre-defined macro variables available in both macro systems</li> <li>Vulcan macros</li> <li>Jinja macros</li> </ul>"},{"location":"components/advanced-features/macros/variables/","title":"Variables","text":""},{"location":"components/advanced-features/macros/variables/#variables","title":"Variables","text":"<p>Macro variables are placeholders whose values are substituted in when the macro is rendered.</p> <p>They enable dynamic macro behavior - for example, a date parameter's value might be based on when the macro was run.</p> <p>Note</p> <p>This page discusses Vulcan's built-in macro variables. Learn more about custom, user-defined macro variables on the Vulcan macros page.</p>"},{"location":"components/advanced-features/macros/variables/#example","title":"Example","text":"<p>Consider a SQL query that filters by date in the <code>WHERE</code> clause.</p> <p>Instead of manually changing the date each time the model is run, you can use a macro variable to make the date dynamic. With the dynamic approach, the date changes automatically based on when the query is run.</p> <p>This query filters for rows where column <code>my_date</code> is after '2023-01-01':</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>To make this query's date dynamic you could use the predefined Vulcan macro variable <code>@execution_ds</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan that <code>@execution_ds</code> is a macro variable that requires substitution before the SQL is executed.</p> <p>The macro variable <code>@execution_ds</code> is predefined, so its value will be automatically set by Vulcan based on when the execution started. If the model was executed on February 1, 2023 the rendered query would be:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>This example used one of Vulcan's predefined variables, but you can also define your own macro variables.</p> <p>We describe Vulcan's predefined variables below; user-defined macro variables are discussed in the Vulcan macros and Jinja macros pages.</p>"},{"location":"components/advanced-features/macros/variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan comes with predefined variables that can be used in your queries. They are automatically set by the Vulcan runtime.</p> <p>Most predefined variables are related to time and use a combination of prefixes (start, end, etc.) and postfixes (date, ds, ts, etc.). They are described in the next section; other predefined variables are discussed in the following section.</p>"},{"location":"components/advanced-features/macros/variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses the python datetime module for handling dates and times. It uses the standard Unix epoch start of 1970-01-01.</p> <p>Important</p> <p>Predefined variables with a time component always use the UTC time zone.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes:</p> <ul> <li>start - The inclusive starting interval of a model run</li> <li>end - The inclusive end interval of a model run</li> <li>execution - The timestamp of when the execution started</li> </ul> <p>Postfixes:</p> <ul> <li>dt - A python datetime object that converts into a native SQL <code>TIMESTAMP</code> (or SQL engine equivalent)</li> <li>dtntz - A python datetime object that converts into a native SQL <code>TIMESTAMP WITHOUT TIME ZONE</code> (or SQL engine equivalent)</li> <li>date - A python date object that converts into a native SQL <code>DATE</code></li> <li>ds - A date string with the format: '%Y-%m-%d'</li> <li>ts - An ISO 8601 datetime formatted string: '%Y-%m-%d %H:%M:%S'</li> <li>tstz - An ISO 8601 datetime formatted string with timezone: '%Y-%m-%d %H:%M:%S%z'</li> <li>hour - An integer representing the hour of the day, with values 0-23</li> <li>epoch - An integer representing seconds since Unix epoch</li> <li>millis - An integer representing milliseconds since Unix epoch</li> </ul> <p>All predefined temporal macro variables:</p> <ul> <li> <p>dt</p> <ul> <li>@start_dt</li> <li>@end_dt</li> <li>@execution_dt</li> </ul> </li> <li> <p>dtntz</p> <ul> <li>@start_dtntz</li> <li>@end_dtntz</li> <li>@execution_dtntz</li> </ul> </li> <li> <p>date</p> <ul> <li>@start_date</li> <li>@end_date</li> <li>@execution_date</li> </ul> </li> <li> <p>ds</p> <ul> <li>@start_ds</li> <li>@end_ds</li> <li>@execution_ds</li> </ul> </li> <li> <p>ts</p> <ul> <li>@start_ts</li> <li>@end_ts</li> <li>@execution_ts</li> </ul> </li> <li> <p>tstz</p> <ul> <li>@start_tstz</li> <li>@end_tstz</li> <li>@execution_tstz</li> </ul> </li> <li> <p>hour</p> <ul> <li>@start_hour</li> <li>@end_hour</li> <li>@execution_hour</li> </ul> </li> <li> <p>epoch</p> <ul> <li>@start_epoch</li> <li>@end_epoch</li> <li>@execution_epoch</li> </ul> </li> <li> <p>millis</p> <ul> <li>@start_millis</li> <li>@end_millis</li> <li>@execution_millis</li> </ul> </li> </ul>"},{"location":"components/advanced-features/macros/variables/#runtime-variables","title":"Runtime variables","text":"<p>Vulcan provides additional predefined variables used to modify model behavior based on information available at runtime.</p> <ul> <li>@runtime_stage - A string value denoting the current stage of the Vulcan runtime. Typically used in models to conditionally execute pre/post-statements (learn more here). It returns one of these values:<ul> <li>'loading' - The project is being loaded into Vulcan's runtime context.</li> <li>'creating' - The model tables are being created for the first time. The data may be inserted during table creation.</li> <li>'evaluating' - The model query logic is evaluated, and the data is inserted into the existing model table.</li> <li>'promoting' - The model is being promoted in the target environment (view created during virtual layer update).</li> <li>'demoting' - The model is being demoted in the target environment (view dropped during virtual layer update).</li> <li>'auditing' - The audit is being run.</li> <li>'testing' - The model query logic is being evaluated in the context of a unit test.</li> </ul> </li> <li>@gateway - A string value containing the name of the current gateway.</li> <li>@this_model - The physical table name that the model's view selects from. Typically used to create generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</li> <li>@model_kind_name - A string value containing the name of the current model kind. Intended to be used in scenarios where you need to control the physical properties in model defaults.</li> </ul> <p>Embedding variables in strings</p> <p>Macro variable references sometimes use the curly brace syntax <code>@{variable}</code>, which serves a different purpose than the regular <code>@variable</code> syntax.</p> <p>The curly brace syntax tells Vulcan that the rendered string should be treated as an identifier, instead of simply replacing the macro variable value.</p> <p>For example, if <code>variable</code> is defined as <code>@DEF(</code>variable<code>, foo.bar)</code>, then <code>@variable</code> produces <code>foo.bar</code>, while <code>@{variable}</code> produces <code>\"foo.bar\"</code>. This is because Vulcan converts <code>foo.bar</code> into an identifier, using double quotes to correctly include the <code>.</code> character in the identifier name.</p> <p>In practice, <code>@{variable}</code> is most commonly used to interpolate a value within an identifier, e.g., <code>@{variable}_suffix</code>, whereas <code>@variable</code> is used to do plain substitutions for string literals.</p> <p>Learn more in the Vulcan macros documentation.</p>"},{"location":"components/advanced-features/macros/variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>The following variables are also available in <code>before_all</code> and <code>after_all</code> statements, as well as in macros invoked within them.</p> <ul> <li>@this_env - A string value containing the name of the current environment.</li> <li>@schemas - A list of the schema names of the virtual layer of the current environment.</li> <li>@views - A list of the view names of the virtual layer of the current environment.</li> </ul>"},{"location":"components/audits/audits/","title":"Auditing","text":""},{"location":"components/audits/audits/#auditing","title":"Auditing","text":"<p>Audits are one of the tools Vulcan provides to validate your models. Along with tests, they are a great way to ensure the quality of your data and to build trust in it across your organization.</p> <p>Unlike tests, audits are used to validate the output of a model after every run. When you apply a plan, Vulcan will automatically run each model's audits.</p> <p>All audits in Vulcan are blocking - when an audit fails, Vulcan halts plan application or run execution to prevent potentially invalid data from propagating further downstream. This ensures data quality by stopping the pipeline whenever data validation fails.</p> <p>A comprehensive suite of audits can identify data issues upstream, whether they are from your vendors or other teams. Audits also empower your data engineers and analysts to work with confidence by catching problems early as they work on new features or make updates to your models.</p> <p>NOTE: For incremental by time range models, audits are only applied to intervals being processed - not for the entire underlying table.</p>"},{"location":"components/audits/audits/#terminology-audits-and-assertions","title":"Terminology: Audits and Assertions","text":"<p>Vulcan uses two related but distinct concepts:</p> <ul> <li>AUDIT - The validation rule (the SQL query that checks for problems)</li> <li>ASSERTION - Attaching an audit to a model (claiming it should pass)</li> </ul> <p>In MODEL definitions:</p> <pre><code>-- Define the AUDIT (the rule)\nAUDIT (name check_positive_price);\nSELECT * FROM @this_model WHERE price &lt;= 0;\n\n-- Make ASSERTIONS about your model (attach the audit)\nMODEL (\n  name products,\n  assertions (check_positive_price)  -- Declaring this audit should pass\n);\n</code></pre> <p>Note: You may encounter older code that attaches audits using <code>audits</code> instead of <code>assertions</code> in MODEL definitions. While both work identically, please use <code>assertions</code> for clearer semantics. This documentation uses <code>assertions</code> throughout.</p>"},{"location":"components/audits/audits/#how-audits-work","title":"How Audits Work","text":"<p>A failed audit halts the execution of a <code>plan</code> or <code>run</code> to prevent invalid data from propagating to downstream models. The impact of a failure depends on whether you are running a <code>plan</code> or a <code>run</code>.</p> <p>Vulcan's audit process is:</p> <ol> <li>Evaluate the model (e.g., insert new data or rebuild the table)</li> <li>Run the audit query against the newly updated model table. For incremental models, the audit only runs on the processed time intervals.</li> <li>If the query returns any rows, the audit fails, halting the <code>plan</code> or <code>run</code>.</li> </ol>"},{"location":"components/audits/audits/#plan-vs-run","title":"Plan vs. Run","text":"<p>The key difference is when the model's data is promoted to the production environment:</p> <ul> <li> <p><code>plan</code>: Vulcan evaluates and audits all modified models before promoting them to production. If an audit fails, the <code>plan</code> stops, and the production table is untouched. Invalid data is contained in an isolated table and never reaches the production environment.</p> </li> <li> <p><code>run</code>: Vulcan evaluates and audits models directly against the production environment. If an audit fails, the <code>run</code> stops, but the invalid data is already present in the production table. The blocking action prevents this bad data from being used to build other downstream models.</p> </li> </ul>"},{"location":"components/audits/audits/#fixing-a-failed-audit","title":"Fixing a Failed Audit","text":"<p>If an audit fails during a <code>run</code>, you must fix the invalid data in the production table. To do so:</p> <ol> <li>Find the root cause: examine upstream models and data sources</li> <li>Fix the source<ul> <li>If the cause is an external data source, fix it there. Then, run a restatement plan on the first Vulcan model that ingests the source data. This will restate all downstream models, including the one with the failed audit.</li> <li>If the cause is a Vulcan model, update the model's logic. Then apply the change with a <code>plan</code>, which will automatically re-evaluate all downstream models.</li> </ul> </li> </ol>"},{"location":"components/audits/audits/#user-defined-audits","title":"User-Defined Audits","text":"<p>In Vulcan, user-defined audits are defined in <code>.sql</code> files in an <code>audits</code> directory in your Vulcan project. Multiple audits can be defined in a single file, so you can organize them to your liking. Alternatively, audits can be defined inline within the model definition itself.</p> <p>Audits are SQL queries that should not return any rows; in other words, they query for bad data, so returned rows indicates that something is wrong.</p> <p>In its simplest form, an audit is defined with the <code>AUDIT</code> statement along with a query, as in the following example:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  dialect spark\n);\nSELECT * from sushi.items\nWHERE\n  ds BETWEEN @start_ds AND @end_ds\n  AND price IS NULL;\n</code></pre> <p>In the example, we defined an audit named <code>assert_item_price_is_not_null</code>, ensuring that every sushi item has a price.</p> <p>Note: If the query is in a different dialect than the rest of your project, you can specify it in the <code>AUDIT</code> statement. In the example above we set it to <code>spark</code>, so SQLGlot will automatically understand how to execute the query behind the scenes.</p> <p>To run the audit, attach it to a model using <code>assertions</code> in the <code>MODEL</code> statement:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (assert_item_price_is_not_null)\n);\n</code></pre> <p>Now <code>assert_item_price_is_not_null</code> will run every time the <code>sushi.items</code> model is run.</p>"},{"location":"components/audits/audits/#generic-audits","title":"Generic audits","text":"<p>Audits can also be parameterized and implemented in a model-agnostic way so the same audit can be used for multiple models.</p> <p>Consider the following audit definition that checks whether the target column exceeds a configured threshold:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>This example utilizes macros to parameterize the audit. <code>@this_model</code> is a special macro which refers to the model that is being audited. For incremental models, this macro also ensures that only relevant data intervals are affected.</p> <p><code>@column</code> and <code>@threshold</code> are parameters whose values are specified in a model definition's <code>MODEL</code> statement.</p> <p>Apply the generic audit to a model by referencing it in the <code>MODEL</code> statement using <code>assertions</code>:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    does_not_exceed_threshold(column := id, threshold := 1000),\n    does_not_exceed_threshold(column := price, threshold := 100)\n  )\n);\n</code></pre> <p>Notice how <code>column</code> and <code>threshold</code> parameters have been set. These values will be propagated into the audit query and substituted into the <code>@column</code> and <code>@threshold</code> macro variables.</p> <p>Note that the same audit can be applied more than once to the a model using different sets of parameters.</p> <p>Generic audits can define default values as follows: </p><pre><code>AUDIT (\n  name does_not_exceed_threshold,\n  defaults (\n    threshold = 10,\n    column = id\n  )\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre><p></p> <p>Alternatively, you can apply specific audits globally by including them in the model defaults configuration:</p> <pre><code>model_defaults:\n  assertions:\n    - assert_positive_order_ids\n    - does_not_exceed_threshold(column := id, threshold := 1000)\n</code></pre> <p>Note: In <code>model_defaults</code>, you can use either <code>audits</code> or <code>assertions</code> - both are supported for backward compatibility.</p>"},{"location":"components/audits/audits/#naming","title":"Naming","text":"<p>We recommended avoiding SQL keywords when naming audit parameters. Quote any audit argument that is also a SQL keyword.</p> <p>For example, if an audit <code>my_audit</code> uses a <code>values</code> parameter, invoking it will require quotes because <code>values</code> is a SQL keyword:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    my_audit(column := a, \"values\" := (1,2,3))\n  )\n)\n</code></pre>"},{"location":"components/audits/audits/#inline-audits","title":"Inline audits","text":"<p>You can also define audits directly within a model definition using the same syntax. Multiple audits can be specified within the same SQL model file:</p> <pre><code>MODEL (\n    name sushi.items,\n    assertions(does_not_exceed_threshold(column := id, threshold := 1000), price_is_not_null)\n);\nSELECT id, price\nFROM sushi.seed;\n\nAUDIT (name does_not_exceed_threshold);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n\nAUDIT (name price_is_not_null);\nSELECT * FROM @this_model\nWHERE price IS NULL;\n</code></pre>"},{"location":"components/audits/audits/#built-in-audits","title":"Built-in audits","text":"<p>Vulcan comes with a suite of built-in generic audits that cover a broad set of common use cases. All built-in audits are blocking - when they fail, execution halts immediately.</p> <p>This section describes the audits, grouped by general purpose.</p>"},{"location":"components/audits/audits/#generic-assertion-audit","title":"Generic assertion audit","text":"<p>The <code>forall</code> audit is the most generic built-in audit, allowing arbitrary boolean SQL expressions.</p>"},{"location":"components/audits/audits/#forall","title":"forall","text":"<p>Ensures that a set of arbitrary boolean expressions evaluate to <code>TRUE</code> for all rows in the model. The boolean expressions should be written in SQL.</p> <p>This example asserts that all rows have a <code>price</code> greater than 0 and a <code>name</code> value containing at least one character:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    forall(criteria := (\n      price &gt; 0,\n      LENGTH(name) &gt; 0\n    ))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#row-counts-and-null-value-audits","title":"Row counts and NULL value audits","text":"<p>These audits concern row counts and presence of <code>NULL</code> values.</p>"},{"location":"components/audits/audits/#number_of_rows","title":"number_of_rows","text":"<p>Ensures that the number of rows in the model's table exceeds the threshold.</p> <p>This example asserts that the model has more than 10 rows:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    number_of_rows(threshold := 10)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#not_null","title":"not_null","text":"<p>Ensures that specified columns do not contain <code>NULL</code> values.</p> <p>This example asserts that none of the <code>id</code>, <code>customer_id</code>, or <code>waiter_id</code> columns contain <code>NULL</code> values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    not_null(columns := (id, customer_id, waiter_id))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#at_least_one","title":"at_least_one","text":"<p>Ensures that specified columns contain at least one non-NULL value.</p> <p>This example asserts that the <code>zip</code> column contains at least one non-NULL value:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    at_least_one(column := zip)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#not_null_proportion","title":"not_null_proportion","text":"<p>Ensures that the specified column's proportion of <code>NULL</code> values is no greater than a threshold.</p> <p>This example asserts that the <code>zip</code> column has no more than 80% <code>NULL</code> values:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    not_null_proportion(column := zip, threshold := 0.8)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#specific-data-values-audits","title":"Specific data values audits","text":"<p>These audits concern the specific set of data values present in a column.</p>"},{"location":"components/audits/audits/#not_constant","title":"not_constant","text":"<p>Ensures that the specified columns are not constant (i.e., have at least two non-NULL values).</p> <p>This example asserts that the column <code>customer_id</code> has at least two non-NULL values:</p> <pre><code>MODEL (\n  name sushi.customer_revenue_by_day,\n  assertions (\n    not_constant(column := customer_id)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#unique_values","title":"unique_values","text":"<p>Ensures that specified columns contain unique values (i.e., have no duplicated values).</p> <p>This example asserts that the <code>id</code> and <code>item_id</code> columns have unique values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_values(columns := (id, item_id))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#unique_combination_of_columns","title":"unique_combination_of_columns","text":"<p>Ensures that each row has a unique combination of values over the specified columns.</p> <p>This example asserts that the combination of <code>id</code> and <code>ds</code> columns has no duplicated values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_combination_of_columns(columns := (id, ds))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#accepted_values","title":"accepted_values","text":"<p>Ensures that all rows of the specified column contain one of the accepted values.</p> <p>Note</p> <p>Rows with <code>NULL</code> values for the column will pass this audit in most databases/engines. Use the <code>not_null</code> audit to ensure there are no <code>NULL</code> values present in a column.</p> <p>This example asserts that column <code>name</code> has a value of 'Hamachi', 'Unagi', or 'Sake':</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_values(column := name, is_in := ('Hamachi', 'Unagi', 'Sake'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#not_accepted_values","title":"not_accepted_values","text":"<p>Ensures that no rows of the specified column contain one of the not accepted values.</p> <p>Note</p> <p>This audit does not support rejecting <code>NULL</code> values. Use the <code>not_null</code> audit to ensure there are no <code>NULL</code> values present in a column.</p> <p>This example asserts that column <code>name</code> is not one of 'Hamburger' or 'French fries':</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_accepted_values(column := name, is_in := ('Hamburger', 'French fries'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#numeric-data-audits","title":"Numeric data audits","text":"<p>These audits concern the distribution of values in numeric columns.</p>"},{"location":"components/audits/audits/#sequential_values","title":"sequential_values","text":"<p>Ensures that each of an ordered numeric column's values contains the previous row's value plus <code>interval</code>.</p> <p>For example, with a column having minimum value 1 and maximum value 4 and <code>interval := 1</code>, it ensures that the rows contain values <code>[1, 2, 3, 4]</code>.</p> <p>This example asserts that column <code>item_id</code> contains sequential values that differ by <code>1</code>:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    sequential_values(column := item_id, interval := 1)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#accepted_range","title":"accepted_range","text":"<p>Ensures that a column's values are in a numeric range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that all rows have a <code>price</code> greater than or equal 1 and less than or equal to 100:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 1, max_v := 100)\n  )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that all rows have a <code>price</code> greater than 0 and less than 100:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 0, max_v := 100, inclusive := false)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#mutually_exclusive_ranges","title":"mutually_exclusive_ranges","text":"<p>Ensures that each row's numeric range does not overlap with any other row's range.</p> <p>This example asserts that each row's range [min_price, max_price] does not overlap with any other row's range:</p> <pre><code>MODEL (\n  name pricing.tier_ranges,\n  assertions (\n    mutually_exclusive_ranges(lower_bound_column := min_price, upper_bound_column := max_price)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#character-data-audits","title":"Character data audits","text":"<p>These audits concern the characteristics of values in character/string columns.</p> <p>Warning</p> <p>Databases/engines may exhibit different behavior for different character sets or languages.</p>"},{"location":"components/audits/audits/#not_empty_string","title":"not_empty_string","text":"<p>Ensures that no rows of a column contain an empty string value <code>''</code>.</p> <p>This example asserts that no <code>name</code> is an empty string:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_empty_string(column := name)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#string_length_equal","title":"string_length_equal","text":"<p>Ensures that all rows of a column contain a string with the specified number of characters.</p> <p>This example asserts that all <code>zip</code> values are 5 characters long:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_equal(column := zip, v := 5)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#string_length_between","title":"string_length_between","text":"<p>Ensures that all rows of a column contain a string with number of characters in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that all <code>name</code> values have 5 or more and 50 or fewer characters:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := name, min_v := 5, max_v := 50)\n    )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that all rows have a <code>name</code> with 5 or more and 59 or fewer characters:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := zip, min_v := 4, max_v := 60, inclusive := false)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#valid_uuid","title":"valid_uuid","text":"<p>Ensures that all non-NULL rows of a column contain a string with the UUID structure.</p> <p>UUID structure determined by matching regular expression <code>'^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$'</code>.</p> <p>This example asserts that all <code>uuid</code> values have the UUID structure:</p> <pre><code>MODEL (\n  name events.user_sessions,\n  assertions (\n    valid_uuid(column := uuid)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#valid_email","title":"valid_email","text":"<p>Ensures that all non-NULL rows of a column contain a string with the email address structure.</p> <p>Email address structure determined by matching regular expression <code>'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'</code>.</p> <p>This example asserts that all <code>email</code> values have the email address structure:</p> <pre><code>MODEL (\n  name dim.users,\n  assertions (\n    valid_email(column := email)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#valid_url","title":"valid_url","text":"<p>Ensures that all non-NULL rows of a column contain a string with the URL structure.</p> <p>URL structure determined by matching regular expression <code>'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$'</code>.</p> <p>This example asserts that all <code>url</code> values have the URL structure:</p> <pre><code>MODEL (\n  name dim.products,\n  assertions (\n    valid_url(column := url)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#valid_http_method","title":"valid_http_method","text":"<p>Ensures that all non-NULL rows of a column contain a valid HTTP method.</p> <p>Valid HTTP methods determined by matching values <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>PATCH</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.</p> <p>This example asserts that all <code>http_method</code> values are valid HTTP methods:</p> <pre><code>MODEL (\n  name logs.api_requests,\n  assertions (\n    valid_http_method(column := http_method)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#match_regex_pattern_list","title":"match_regex_pattern_list","text":"<p>Ensures that all non-NULL rows of a column match at least one of the specified regular expressions.</p> <p>This example asserts that all <code>todo</code> values match one of <code>'^\\d.*'</code> (string starts with a digit) or <code>'.*!$'</code> (ends with an exclamation mark):</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    match_regex_pattern_list(column := todo, patterns := ('^\\d.*', '.*!$'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#not_match_regex_pattern_list","title":"not_match_regex_pattern_list","text":"<p>Ensures that no non-NULL rows of a column match any of the specified regular expressions.</p> <p>This example asserts that no <code>todo</code> values match one of <code>'^!.*'</code> (string starts with an exclamation mark) or <code>'.*\\d$'</code> (ends with a digit):</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    not_match_regex_pattern_list(column := todo, patterns := ('^!.*', '.*\\d$'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#match_like_pattern_list","title":"match_like_pattern_list","text":"<p>Ensures that all non-NULL rows of a column are <code>LIKE</code> at least one of the specified patterns.</p> <p>This example asserts that all <code>name</code> values are <code>LIKE</code> one of <code>'jim%'</code> or <code>'pam%'</code>:</p> <pre><code>MODEL (\n  name sales.customers,\n  assertions (\n    match_like_pattern_list(column := name, patterns := ('jim%', 'pam%'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#not_match_like_pattern_list","title":"not_match_like_pattern_list","text":"<p>Ensures that no non-NULL rows of a column are <code>LIKE</code> any of the specified patterns.</p> <p>This example asserts that no <code>name</code> values are <code>LIKE</code> <code>'%doe'</code> or <code>'%smith'</code>:</p> <pre><code>MODEL (\n  name products.catalog,\n  assertions (\n    not_match_like_pattern_list(column := name, patterns := ('%doe', '%smith'))\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#statistical-audits","title":"Statistical audits","text":"<p>These audits concern the statistical distributions of numeric columns.</p> <p>Note</p> <p>Audit thresholds will likely require fine-tuning via trial and error for each column being audited.</p>"},{"location":"components/audits/audits/#mean_in_range","title":"mean_in_range","text":"<p>Ensures that a numeric column's mean is in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that the <code>age</code> column has a mean of at least 21 and at most 50:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 21, max_v := 50)\n    )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that <code>age</code> has a mean greater than 18 and less than 65:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 18, max_v := 65, inclusive := false)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#stddev_in_range","title":"stddev_in_range","text":"<p>Ensures that a numeric column's standard deviation is in the specified range. Range is inclusive by default, such that values equal to the range boundaries will pass the audit.</p> <p>This example asserts that the <code>age</code> column has a standard deviation of at least 2 and at most 5:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 2, max_v := 5)\n  )\n);\n</code></pre> <p>This example specifies the <code>inclusive := false</code> argument to assert that <code>age</code> has a standard deviation greater than 3 and less than 6:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 3, max_v := 6, inclusive := false)\n  )\n);\n</code></pre>"},{"location":"components/audits/audits/#z_score","title":"z_score","text":"<p>Ensures that no rows of a numeric column contain a value whose absolute z-score exceeds the threshold.</p> <p>z-score is calculated as <code>ABS(([row value] - [column mean]) / NULLIF([column standard deviation], 0))</code>.</p> <p>This example asserts that the <code>age</code> column contains no rows with z-scores greater than 3:</p> <pre><code>MODEL (\n  name sales.transactions,\n  assertions (\n    z_score(column := age, threshold := 3)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#kl_divergence","title":"kl_divergence","text":"<p>Ensures that the symmetrised Kullback-Leibler divergence (aka \"Jeffreys divergence\" or \"Population Stability Index\") between two columns does not exceed a threshold.</p> <p>This example asserts that the symmetrised KL Divergence between columns <code>age</code> and <code>reference_age</code> is less than or equal to 0.1:</p> <pre><code>MODEL (\n  name analytics.cohort_comparison,\n  assertions (\n    kl_divergence(column := age, target_column := reference_age, threshold := 0.1)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#chi_square","title":"chi_square","text":"<p>Ensures that the chi-square statistic for two categorical columns does not exceed a critical value.</p> <p>You can look up the critical value corresponding to a p-value with a table (such as this one) or by using the Python scipy library:</p> <p></p><pre><code>from scipy.stats import chi2\n\n# critical value for p-value := 0.95 and degrees of freedom := 1\nchi2.ppf(0.95, 1)\n</code></pre> This example asserts that the chi-square statistic0 for columns <code>user_state</code> and <code>user_type</code> does not exceed 6.635:<p></p> <pre><code>MODEL (\n  name analytics.user_segments,\n  assertions (\n    chi_square(column := user_state, target_column := user_type, critical_value := 6.635)\n    )\n);\n</code></pre>"},{"location":"components/audits/audits/#running-audits","title":"Running audits","text":""},{"location":"components/audits/audits/#the-cli-audit-command","title":"The CLI audit command","text":"<p>You can execute audits with the <code>vulcan audit</code> command as follows:</p> <pre><code>$ vulcan -p project audit --start 2022-01-01 --end 2022-01-02\nFound 1 audit(s).\nassert_item_price_is_not_null FAIL.\n\nFinished with 1 audit error(s).\n\nFailure in audit assert_item_price_is_not_null for model sushi.items (audits/items.sql).\nGot 3 results, expected 0.\nSELECT * FROM vulcan.sushi__items__1836721418_83893210 WHERE ds BETWEEN '2022-01-01' AND '2022-01-02' AND price IS NULL\nDone.\n</code></pre>"},{"location":"components/audits/audits/#automated-auditing","title":"Automated auditing","text":"<p>When you apply a plan, Vulcan will automatically run each model's audits.</p> <p>Vulcan will halt the pipeline when an audit fails to prevent potentially invalid data from propagating further downstream.</p>"},{"location":"components/audits/audits/#advanced-usage","title":"Advanced usage","text":""},{"location":"components/audits/audits/#skipping-audits","title":"Skipping audits","text":"<p>Audits can be skipped by setting the <code>skip</code> argument to <code>true</code> as in the following example:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  skip true\n);\nSELECT * from sushi.items\nWHERE ds BETWEEN @start_ds AND @end_ds AND\n   price IS NULL;\n</code></pre>"},{"location":"components/checks/checks/","title":"Checks","text":""},{"location":"components/checks/checks/#checks","title":"Checks","text":"<p>Quality checks are comprehensive validation rules configured in YAML files that monitor data quality over time. Unlike audits (which block pipeline execution), checks:</p> <ul> <li>Run separately from model execution (or alongside it)</li> <li>Don't block pipelines (non-blocking validation)</li> <li>Track trends and historical patterns</li> <li>Support complex statistical analysis</li> <li>Integrate with Activity API for monitoring</li> </ul> <p>Key characteristics: - Configured in <code>checks/</code> directory - Use declarative YAML syntax - Organized by data quality dimensions - Results stored for historical analysis - Integrated with Activity API</p>"},{"location":"components/checks/checks/#checks-vs-audits-vs-profiles","title":"Checks vs Audits vs Profiles","text":"<p>Understanding the three data quality mechanisms:</p> Feature Audits Checks Profiles Purpose Critical validation Monitoring &amp; analysis Observation &amp; tracking When runs With model (inline) Separately or with models With model Blocks pipeline? Yes (always) No No Configuration In MODEL DDL or .sql files YAML files (<code>checks/</code>) In MODEL DDL Output Pass/fail Pass/fail + samples Statistical metrics Best for Business rules, data integrity Trend monitoring, anomalies Understanding data Historical tracking No Yes (Activity API) Yes (<code>_check_profiles</code>) <p>The Three-Layer Strategy:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUDITS (Critical - Blocks Pipeline)   \u2502\n\u2502  \u2022 Primary keys must be unique          \u2502\n\u2502  \u2022 Revenue must be non-negative         \u2502\n\u2502  \u2022 Foreign key relationships valid      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CHECKS (Monitoring - Non-Blocking)     \u2502\n\u2502  \u2022 Row count within expected range      \u2502\n\u2502  \u2022 Anomaly detection on metrics         \u2502\n\u2502  \u2022 Cross-table consistency              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PROFILES (Observation - Metrics)       \u2502\n\u2502  \u2022 Track null percentages               \u2502\n\u2502  \u2022 Monitor column distributions         \u2502\n\u2502  \u2022 Detect data drift                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"components/checks/checks/#when-to-use-checks","title":"When to Use Checks","text":"<p>\u2705 Use Quality Checks for: - Monitoring data quality trends over time - Statistical anomaly detection - Cross-model validation (joins across models) - Non-critical validation (warnings, not blockers) - Complex validation requiring historical context - Building data quality dashboards</p> <p>\u274c Use Audits Instead for: - Critical business rules that must pass - Model-specific validation (runs inline) - Simple SQL assertions - Blocking invalid data from flowing downstream</p> <p>\u274c Use Profiles Instead for: - Understanding data characteristics - Discovering patterns (not validation) - Detecting data drift - Informing which checks/audits to add</p> <p>Example: Revenue validation strategy</p> <pre><code>-- AUDIT (Critical - blocks if fails)\nMODEL (\n  name analytics.revenue,\n  assertions (\n    not_null(columns := (customer_id, revenue)),\n    accepted_range(column := revenue, min_v := 0, max_v := 100000000)\n  )\n);\n</code></pre> <pre><code># CHECK (Monitoring - warns if unusual)\nchecks:\n  analytics.revenue:\n    accuracy:\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n      - change for row_count &gt;= -30%:\n          name: row_count_drop_alert\n</code></pre> <pre><code>-- PROFILE (Observation - tracks over time)\nMODEL (\n  name analytics.revenue,\n  profiles (revenue, order_count, customer_tier)\n);\n</code></pre>"},{"location":"components/checks/checks/#quick-start","title":"Quick Start","text":""},{"location":"components/checks/checks/#your-first-check","title":"Your First Check","text":"<p>Create <code>checks/customers.yml</code>:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_emails\n          attributes:\n            description: \"All customers must have an email address\"\n</code></pre>"},{"location":"components/checks/checks/#check-and-profile-execution","title":"Check and Profile Execution","text":"<p>Checks and profiles run automatically when models are executed, either through a plan or run command. Here's what the execution output looks like:</p> <pre><code>Check Executions (1 Models)\n\u2514\u2500\u2500 hello.subscriptions\n    \u251c\u2500\u2500 \u2713 completeness (4/4)\n    \u251c\u2500\u2500 \u2713 uniqueness (1/1)\n    \u2514\u2500\u2500 \u2713 validity (3/3)\n\nProfiled 1 model (3 columns):\n  \u2713 warehouse.hello.subscriptions: 3 columns\n</code></pre>"},{"location":"components/checks/checks/#common-check-patterns","title":"Common Check Patterns","text":""},{"location":"components/checks/checks/#pattern-1-completeness-checks","title":"Pattern 1: Completeness Checks","text":"<p>Ensure required data is present:</p> <pre><code>checks:\n  analytics.orders:\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: customer_id_required\n\n      - missing_percent(email) &lt; 5:\n          name: email_mostly_complete\n\n      - row_count &gt; 1000:\n          name: sufficient_orders\n</code></pre>"},{"location":"components/checks/checks/#pattern-2-validity-checks","title":"Pattern 2: Validity Checks","text":"<p>Validate data format and values:</p> <pre><code>checks:\n  analytics.users:\n    validity:\n      - failed rows:\n          name: invalid_emails\n          fail query: |\n            SELECT user_id, email\n            FROM analytics.users\n            WHERE email NOT LIKE '%@%'\n          samples limit: 10\n\n      - failed rows:\n          name: invalid_ages\n          fail query: |\n            SELECT user_id, age\n            FROM analytics.users\n            WHERE age &lt; 0 OR age &gt; 120\n</code></pre>"},{"location":"components/checks/checks/#pattern-3-uniqueness-checks","title":"Pattern 3: Uniqueness Checks","text":"<p>Ensure no duplicates:</p> <pre><code>checks:\n  analytics.customers:\n    uniqueness:\n      - duplicate_count(email) = 0:\n          name: unique_emails\n\n      - duplicate_count(customer_id, order_date) = 0:\n          name: unique_customer_date_combination\n</code></pre>"},{"location":"components/checks/checks/#pattern-4-anomaly-detection","title":"Pattern 4: Anomaly Detection","text":"<p>Detect unusual patterns:</p> <pre><code>checks:\n  analytics.daily_revenue:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly\n</code></pre>"},{"location":"components/checks/checks/#pattern-5-change-monitoring","title":"Pattern 5: Change Monitoring","text":"<p>Track changes over time:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - change for row_count &gt;= -50%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if row count drops more than 50%\"\n</code></pre>"},{"location":"components/checks/checks/#check-configuration","title":"Check Configuration","text":""},{"location":"components/checks/checks/#file-structure","title":"File Structure","text":"<p>Checks are YAML files in the <code>checks/</code> directory:</p> <pre><code>project/\n\u251c\u2500\u2500 models/\n\u251c\u2500\u2500 checks/\n\u2502   \u251c\u2500\u2500 users.yml           # Checks for user tables\n\u2502   \u251c\u2500\u2500 orders.yml          # Checks for order tables\n\u2502   \u251c\u2500\u2500 revenue.yml         # Checks for revenue tables\n\u2502   \u2514\u2500\u2500 cross_model.yml     # Checks spanning multiple tables\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming: - Must end with <code>.yml</code> or <code>.yaml</code> - Name doesn't matter (Vulcan reads all files) - Organize by domain or table for clarity</p>"},{"location":"components/checks/checks/#basic-check-syntax","title":"Basic Check Syntax","text":"<pre><code>checks:\n  &lt;fully_qualified_table_name&gt;:\n    &lt;dimension&gt;:\n      - &lt;check_expression&gt;:\n          name: &lt;check_name&gt;\n          attributes:\n            description: &lt;human_readable_description&gt;\n            severity: &lt;warning|error&gt;\n            tags: [&lt;tag1&gt;, &lt;tag2&gt;]\n</code></pre> <p>Example:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_customers\n          attributes:\n            description: \"At least 100 customers expected in production\"\n            severity: warning\n            tags: [critical, daily]\n</code></pre>"},{"location":"components/checks/checks/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Organize checks by 8 standard dimensions (ODPS v3.1):</p>"},{"location":"components/checks/checks/#1-completeness","title":"1. Completeness","text":"<p>No missing required data</p> <pre><code>completeness:\n  - missing_count(customer_id) = 0\n  - missing_percent(email) &lt; 5\n  - row_count &gt; 1000\n</code></pre>"},{"location":"components/checks/checks/#2-validity","title":"2. Validity","text":"<p>Data conforms to format/syntax</p> <pre><code>validity:\n  - failed rows:\n      fail query: |\n        SELECT * FROM table\n        WHERE email NOT LIKE '%@%'\n</code></pre>"},{"location":"components/checks/checks/#3-accuracy","title":"3. Accuracy","text":"<p>Data matches reality</p> <pre><code>accuracy:\n  - anomaly detection for avg(revenue)\n  - avg(age) between 18 and 65\n</code></pre>"},{"location":"components/checks/checks/#4-consistency","title":"4. Consistency","text":"<p>Data agrees across sources</p> <pre><code>consistency:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders o\n        LEFT JOIN customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n</code></pre>"},{"location":"components/checks/checks/#5-uniqueness","title":"5. Uniqueness","text":"<p>No duplicates</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0\n  - duplicate_count(order_id) = 0\n</code></pre>"},{"location":"components/checks/checks/#6-timeliness","title":"6. Timeliness","text":"<p>Data is current</p> <pre><code>timeliness:\n  - change for row_count &gt;= -30%\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders\n        WHERE updated_at &lt; CURRENT_DATE - INTERVAL '7 days'\n</code></pre>"},{"location":"components/checks/checks/#7-conformity","title":"7. Conformity","text":"<p>Follows standards</p> <pre><code>conformity:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM addresses\n        WHERE LENGTH(zip_code) != 5\n</code></pre>"},{"location":"components/checks/checks/#8-coverage","title":"8. Coverage","text":"<p>All records are present</p> <pre><code>coverage:\n  - row_count &gt;= 95% of historical_avg(row_count)\n</code></pre>"},{"location":"components/checks/checks/#filtering-checks","title":"Filtering Checks","text":"<p>Apply checks to a subset of data:</p> <pre><code>checks:\n  analytics.orders:\n    filter: \"status = 'completed' AND order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: completed_orders_have_customers\n</code></pre> <p>Multiple filters:</p> <pre><code>checks:\n  analytics.customers:\n    filter: \"country = 'US'\"\n    completeness:\n      - row_count &gt; 1000\n\n  analytics.customers:\n    filter: \"country = 'EU'\"\n    completeness:\n      - row_count &gt; 500\n</code></pre>"},{"location":"components/checks/checks/#check-attributes","title":"Check Attributes","text":"<p>Add metadata to checks:</p> <pre><code>checks:\n  analytics.revenue:\n    completeness:\n      - row_count &gt; 1000:\n          name: sufficient_revenue_data\n          attributes:\n            description: \"Revenue table must have at least 1000 rows for analysis\"\n            severity: error\n            tags: [critical, daily, revenue]\n            owner: data-team\n            jira: DATA-1234\n            sla: \"&lt; 1 hour\"\n</code></pre> <p>Standard attributes: - <code>description</code> - Human-readable explanation - <code>severity</code> - <code>error</code> (default) or <code>warning</code> - <code>tags</code> - List of tags for filtering/organization - <code>owner</code> - Team or person responsible - Custom attributes - Any key-value pairs</p>"},{"location":"components/checks/checks/#built-in-check-types","title":"Built-in Check Types","text":""},{"location":"components/checks/checks/#missing-data-checks","title":"Missing Data Checks","text":""},{"location":"components/checks/checks/#missing_countcolumn","title":"<code>missing_count(column)</code>","text":"<p>Count of NULL values:</p> <pre><code>completeness:\n  - missing_count(email) = 0:\n      name: no_missing_emails\n\n  - missing_count(phone) &lt;= 100:\n      name: phone_mostly_complete\n</code></pre>"},{"location":"components/checks/checks/#missing_percentcolumn","title":"<code>missing_percent(column)</code>","text":"<p>Percentage of NULL values:</p> <pre><code>completeness:\n  - missing_percent(email) &lt; 5:\n      name: email_95_percent_complete\n\n  - missing_percent(optional_field) &lt; 50:\n      name: optional_field_half_complete\n</code></pre>"},{"location":"components/checks/checks/#row-count-checks","title":"Row Count Checks","text":""},{"location":"components/checks/checks/#row_count","title":"<code>row_count</code>","text":"<p>Total rows in table:</p> <pre><code>completeness:\n  - row_count &gt; 1000:\n      name: sufficient_data\n\n  - row_count between 1000 and 100000:\n      name: expected_row_range\n</code></pre>"},{"location":"components/checks/checks/#row_count-with-filter","title":"<code>row_count</code> with filter","text":"<pre><code>completeness:\n  - row_count &gt; 500:\n      name: sufficient_active_users\n      filter: \"status = 'active'\"\n</code></pre>"},{"location":"components/checks/checks/#duplicate-count-checks","title":"Duplicate Count Checks","text":""},{"location":"components/checks/checks/#duplicate_countcolumn","title":"<code>duplicate_count(column)</code>","text":"<p>Count of duplicate values:</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0:\n      name: unique_emails\n\n  - duplicate_count(customer_id) = 0:\n      name: unique_customer_ids\n</code></pre>"},{"location":"components/checks/checks/#duplicate_countcolumn1-column2","title":"<code>duplicate_count(column1, column2)</code>","text":"<p>Composite key duplicates:</p> <pre><code>uniqueness:\n  - duplicate_count(customer_id, order_date) = 0:\n      name: unique_customer_date\n      attributes:\n        description: \"Each customer can have at most one order per day\"\n</code></pre>"},{"location":"components/checks/checks/#failed-rows-checks","title":"Failed Rows Checks","text":""},{"location":"components/checks/checks/#sql-based-validation-with-samples","title":"SQL-based validation with samples","text":"<p>Most flexible check type - any SQL query:</p> <pre><code>validity:\n  - failed rows:\n      name: invalid_revenue\n      fail query: |\n        SELECT customer_id, revenue, order_date\n        FROM analytics.orders\n        WHERE revenue &lt; 0 OR revenue &gt; 10000000\n      samples limit: 20\n      attributes:\n        description: \"Revenue must be between 0 and 10M\"\n</code></pre> <p>Key features: - <code>fail query</code> - SELECT statement that returns invalid rows - <code>samples limit</code> - How many example rows to capture (default: 5) - Returns empty = check passes - Returns rows = check fails (captures samples)</p> <p>Complex validation:</p> <pre><code>validity:\n  - failed rows:\n      name: orphaned_orders\n      fail query: |\n        SELECT o.order_id, o.customer_id\n        FROM analytics.orders o\n        LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n      samples limit: 10\n</code></pre>"},{"location":"components/checks/checks/#threshold-checks","title":"Threshold Checks","text":""},{"location":"components/checks/checks/#numeric-aggregations","title":"Numeric aggregations","text":"<pre><code>accuracy:\n  - avg(revenue) between 100 and 10000:\n      name: revenue_in_expected_range\n\n  - sum(amount) &gt; 1000000:\n      name: sufficient_total_revenue\n\n  - max(age) &lt;= 120:\n      name: age_within_human_range\n\n  - min(price) &gt;= 0:\n      name: non_negative_prices\n</code></pre>"},{"location":"components/checks/checks/#statistical-checks","title":"Statistical checks","text":"<pre><code>accuracy:\n  - stddev(revenue) &lt; 5000:\n      name: revenue_low_variance\n\n  - percentile(revenue, 95) &lt; 50000:\n      name: revenue_95th_percentile_check\n</code></pre>"},{"location":"components/checks/checks/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"components/checks/checks/#ml-based-anomaly-detection","title":"ML-based anomaly detection","text":"<p>Uses historical check results to detect anomalies:</p> <pre><code>accuracy:\n  - anomaly detection for row_count:\n      name: row_count_anomaly\n      attributes:\n        description: \"Detect unusual changes in row count\"\n\n  - anomaly detection for avg(revenue):\n      name: revenue_anomaly\n\n  - anomaly detection for distinct_count(customer_id):\n      name: customer_count_anomaly\n</code></pre> <p>How it works: 1. Collects historical metric values over time 2. Builds statistical model (mean, std dev, trends) 3. Compares current value to expected range 4. Flags significant deviations (typically &gt; 3 std devs)</p> <p>Requirements: - Needs historical data (runs multiple times) - Works best with regular schedules (daily, hourly) - More accurate after 30+ data points</p>"},{"location":"components/checks/checks/#change-over-time-checks","title":"Change Over Time Checks","text":""},{"location":"components/checks/checks/#monitor-changes-compared-to-previous-run","title":"Monitor changes compared to previous run","text":"<pre><code>timeliness:\n  - change for row_count &gt;= -50%:\n      name: row_count_drop_alert\n      attributes:\n        description: \"Alert if row count drops more than 50% from last week\"\n\n  - change for avg(revenue) &gt;= -20%:\n      name: revenue_drop_alert\n\n  - change for distinct_count(customer_id) &gt;= 10%:\n      name: customer_growth_check\n</code></pre> <p>Change calculation: </p><pre><code>change = (current_value - previous_value) / previous_value * 100\n</code></pre><p></p> <p>Examples: - <code>change &gt;= -30%</code> - Alert if metric drops more than 30% - <code>change &gt;= 10%</code> - Alert if metric grows more than 10% - <code>change between -10% and 10%</code> - Alert if metric changes more than 10% either way</p>"},{"location":"components/checks/checks/#data-profiling","title":"Data Profiling","text":""},{"location":"components/checks/checks/#what-is-profiling","title":"What is Profiling?","text":"<p>Profiles automatically collect statistical metrics about your data over time.</p> <p>Unlike checks (which validate), profiles observe and track data characteristics:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind FULL,\n  grains (customer_id),\n  profiles (revenue, signup_date, customer_tier, order_count)\n);\n</code></pre> <p>What gets profiled:</p> <p>Table-level metrics: - Row count</p> <p>Column-level metrics (all columns): - Null count &amp; percentage - Distinct count - Duplicate count - Uniqueness percentage</p> <p>Numeric columns: - Min, max, avg, sum - Standard deviation, variance - Histogram buckets</p> <p>Text columns: - Min, max, avg length - Most frequent values</p>"},{"location":"components/checks/checks/#profile-configuration","title":"Profile Configuration","text":"<p>Enable profiling in MODEL:</p> <pre><code>MODEL (\n  name analytics.revenue_metrics,\n  kind INCREMENTAL_BY_TIME_RANGE (time_column metric_date),\n\n  -- Profile these columns\n  profiles (\n    revenue,\n    order_count,\n    customer_tier,\n    region\n  )\n);\n</code></pre>"},{"location":"components/checks/checks/#profile-storage","title":"Profile Storage","text":"<p>Profiles are stored in the <code>_check_profiles</code> table:</p> Column Meaning <code>id</code> Unique identifier for this metric row <code>run_id</code> Identifies which profiling run this metric belongs to <code>table_name</code> Name of the table being profiled <code>column_name</code> Name of the column being profiled (NULL for table-level metrics like row_count) <code>profile_type</code> The type of metric, e.g., row_count, distinct, missing_count, frequent_values, min, max, avg_length, etc. <code>value_number</code> Numeric metric value (for metrics like row_count, distinct, min, max, avg, etc.) <code>value_text</code> Used for text values (rare) <code>value_json</code> JSON-encoded metric (for histograms, frequent values, etc.) <code>value_type</code> Type of value stored (number, json, etc.) <code>profiled_at</code> When the profiling was performed (epoch ms in your sample) <code>created_ts</code> When the row was inserted"},{"location":"components/checks/checks/#querying-profiles","title":"Querying Profiles","text":""},{"location":"components/checks/checks/#track-missing-count-over-time","title":"Track missing count over time","text":"<pre><code>SELECT\nto_timestamp(profiled_at/1000)::date AS date,\nvalue_number AS missing_count\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\nAND column_name = 'mrr'\nand profile_type = 'missing_count'\nORDER BY profiled_at DESC\nLIMIT 30;  -- Last 30 days\n</code></pre>"},{"location":"components/checks/checks/#monitor-data-drift","title":"Monitor data drift","text":"<pre><code>WITH latest_profile AS (\n  -- pick the most recent profiling timestamp for that table/column\n  SELECT profiled_at\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n  ORDER BY profiled_at DESC\n  LIMIT 1\n),\n\ncurrent AS (\n  -- get the most recent distinct count and average value from that profiling run\n  SELECT\n    MAX(CASE WHEN profile_type = 'distinct' THEN value_number END)     AS distinct_count,\n    MAX(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_value\n  FROM _check_profiles p\n  JOIN latest_profile l ON p.profiled_at = l.profiled_at\n  WHERE p.table_name = 'warehouse.hello.subscriptions'\n    AND p.column_name = 'mrr'\n),\n\nhistorical AS (\n  -- 30-day historical averages (profiled_at stored as epoch ms \u2192 convert to timestamp)\n  SELECT\n    AVG(CASE WHEN profile_type = 'distinct' THEN value_number END)      AS avg_distinct,\n    AVG(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_mrr\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n    AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days'\n)\n\nSELECT\n  c.distinct_count,\n  h.avg_distinct,\n  CASE\n    WHEN h.avg_distinct IS NULL THEN NULL\n    ELSE (c.distinct_count - h.avg_distinct) / NULLIF(h.avg_distinct, 0) * 100\n  END AS distinct_change_pct,\n  c.avg_value,\n  h.avg_mrr,\n  CASE\n    WHEN h.avg_mrr IS NULL THEN NULL\n    ELSE (c.avg_value - h.avg_mrr) / NULLIF(h.avg_mrr, 0) * 100\n  END AS mrr_change_pct\nFROM current c, historical h;\n</code></pre>"},{"location":"components/checks/checks/#using-profiles-to-inform-checks","title":"Using Profiles to Inform Checks","text":"<p>Workflow:</p> <ol> <li>Enable profiling on new models</li> <li>Observe patterns for 30+ days</li> <li>Identify anomalies in profile data</li> <li>Create checks based on observed patterns</li> </ol> <p>Example:</p> <pre><code>-- Step 1: Enable profiling\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <pre><code>-- Step 2: Query profiles after 30 days\nSELECT\n    MIN(value_number) AS min_revenue,\n    MAX(value_number) AS max_revenue,\n    AVG(value_number) AS typical_revenue,\n    STDDEV(value_number) AS revenue_stddev\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\n  AND column_name = 'mrr'\n  AND profile_type IN ('avg', 'mean', 'average', 'avg_value')\n  AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days';\n\n\n-- Results:\n-- min_revenue: 45000\n-- max_revenue: 75000\n-- typical_revenue: 58000\n-- revenue_stddev: 6000\n</code></pre> <pre><code># Step 3: Create checks based on observed patterns\nchecks:\n  analytics.orders:\n    accuracy:\n      - avg(revenue) between 40000 and 80000:\n          name: revenue_within_observed_range\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n</code></pre>"},{"location":"components/checks/checks/#profile-best-practices","title":"Profile Best Practices","text":"<p>\u2705 DO: - Profile high-value production tables - Profile columns used in downstream analysis - Use profiles to understand new data sources - Query profiles to detect data drift - Use profiles to inform check thresholds</p> <p>\u274c DON'T: - Profile sensitive/PII columns (privacy risk) - Profile every column (performance overhead) - Profile temporary/experimental models - Use profiles as a replacement for checks - Profile very high-frequency models (storage cost)</p> <p>When to use profiles: - Building new models (understand the data) - Monitoring production tables - Detecting data drift - Informing audit/check strategy - Debugging data quality issues</p> <p>When to skip profiles: - Temporary models - Models with sensitive data - Very high-frequency models (&gt; 100 runs/day) - Models where you only need pass/fail validation</p>"},{"location":"components/checks/checks/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"components/checks/checks/#cross-model-validation","title":"Cross-Model Validation","text":"<p>Validate relationships between models:</p> <pre><code># checks/cross_model.yml\nchecks:\n  analytics.orders:\n    consistency:\n      - failed rows:\n          name: orphaned_orders\n          fail query: |\n            SELECT o.order_id, o.customer_id\n            FROM analytics.orders o\n            LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n            WHERE c.customer_id IS NULL\n          samples limit: 10\n          attributes:\n            description: \"All orders must have a valid customer\"\n\n      - failed rows:\n          name: revenue_mismatch\n          fail query: |\n            SELECT\n              o.order_id,\n              o.revenue as order_revenue,\n              r.revenue as revenue_table_revenue\n            FROM analytics.orders o\n            JOIN analytics.revenue r ON o.order_id = r.order_id\n            WHERE ABS(o.revenue - r.revenue) &gt; 0.01\n</code></pre>"},{"location":"components/checks/checks/#time-based-validation","title":"Time-Based Validation","text":"<p>Ensure data timeliness:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - failed rows:\n          name: stale_data\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE updated_at &lt; CURRENT_TIMESTAMP - INTERVAL '24 hours'\n              AND status != 'completed'\n          attributes:\n            description: \"Pending orders should update within 24 hours\"\n\n      - failed rows:\n          name: future_dates\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE order_date &gt; CURRENT_DATE\n</code></pre>"},{"location":"components/checks/checks/#statistical-outlier-detection","title":"Statistical Outlier Detection","text":"<p>Custom outlier detection:</p> <pre><code>checks:\n  analytics.revenue:\n    accuracy:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            WITH stats AS (\n              SELECT\n                AVG(revenue) as mean,\n                STDDEV(revenue) as stddev\n              FROM analytics.revenue\n            )\n            SELECT r.*,\n              (r.revenue - s.mean) / s.stddev as z_score\n            FROM analytics.revenue r, stats s\n            WHERE ABS((r.revenue - s.mean) / s.stddev) &gt; 3\n          samples limit: 20\n</code></pre>"},{"location":"components/checks/checks/#best-practices","title":"Best Practices","text":""},{"location":"components/checks/checks/#check-organization","title":"Check Organization","text":"<p>By domain:</p> <pre><code>checks/\n\u251c\u2500\u2500 customers/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u251c\u2500\u2500 validity.yml\n\u2502   \u2514\u2500\u2500 consistency.yml\n\u251c\u2500\u2500 orders/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u2514\u2500\u2500 timeliness.yml\n\u2514\u2500\u2500 revenue/\n    \u2514\u2500\u2500 accuracy.yml\n</code></pre> <p>By priority:</p> <pre><code>checks/\n\u251c\u2500\u2500 critical.yml      # Must never fail\n\u251c\u2500\u2500 important.yml     # Should rarely fail\n\u251c\u2500\u2500 monitoring.yml    # Track trends\n\u2514\u2500\u2500 experimental.yml  # Testing new checks\n</code></pre>"},{"location":"components/checks/checks/#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive names:</p> <pre><code># \u274c Bad\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: check1\n\n# \u2705 Good\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_customer_emails\n          attributes:\n            description: \"All customers must have an email for marketing\"\n</code></pre> <p>Naming pattern: - <code>&lt;dimension&gt;_&lt;what&gt;_&lt;constraint&gt;</code> - Examples:   - <code>completeness_email_required</code>   - <code>validity_email_format</code>   - <code>uniqueness_email_no_duplicates</code>   - <code>timeliness_order_within_24hrs</code></p>"},{"location":"components/checks/checks/#threshold-selection","title":"Threshold Selection","text":"<p>Start conservative, adjust based on data:</p> <pre><code># Step 1: Start with wide range\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_orders_v1\n\n# Step 2: Monitor for 30 days, see actual range: 5000-10000\n\n# Step 3: Tighten based on observed patterns\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 4000 and 12000:\n          name: sufficient_orders_v2\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n</code></pre> <p>Use profiles to inform thresholds:</p> <pre><code>-- Query profiles\nSELECT\n  MIN(metric_value) as min_observed,\n  MAX(metric_value) as max_observed,\n  AVG(metric_value) as typical,\n  STDDEV(metric_value) as stddev\nFROM check_results\nWHERE check_name = 'row_count'\n  AND executed_at &gt;= CURRENT_DATE - INTERVAL '90 days';\n\n-- Set threshold as: typical \u00b1 3*stddev\n</code></pre>"},{"location":"components/checks/checks/#integration-strategy","title":"Integration Strategy","text":"<p>Layer validation:</p> <pre><code>-- LAYER 1: Audits (critical - blocks)\nMODEL (\n  name analytics.orders,\n  assertions (\n    not_null(columns := (order_id, customer_id)),\n    unique_values(columns := (order_id))\n  )\n);\n</code></pre> <pre><code># LAYER 2: Checks (monitoring - warns)\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 5000 and 15000:\n          name: order_count_in_range\n\n    timeliness:\n      - change for row_count &gt;= -30%:\n          name: order_count_stable\n</code></pre> <pre><code>-- LAYER 3: Profiles (observe - tracks)\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre>"},{"location":"components/checks/checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/checks/checks/#check-failures","title":"Check Failures","text":""},{"location":"components/checks/checks/#investigate-failed-check","title":"Investigate failed check","text":"<pre><code># Run specific check with verbose output\nvulcan check --select analytics.customers.invalid_emails --verbose\n</code></pre>"},{"location":"components/checks/checks/#query-failed-samples","title":"Query failed samples","text":"<pre><code>-- Get samples from last failed run\nSELECT *\nFROM check_samples\nWHERE check_name = 'invalid_emails'\n  AND status = 'failed'\nORDER BY executed_at DESC\nLIMIT 10;\n</code></pre>"},{"location":"components/checks/checks/#performance-issues","title":"Performance Issues","text":""},{"location":"components/checks/checks/#slow-check-queries","title":"Slow check queries","text":"<p>Problem: Check takes too long to run</p> <p>Solution 1: Add filters</p> <pre><code># \u274c Slow - scans entire table\nchecks:\n  analytics.orders:\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n\n# \u2705 Fast - filters to recent data\nchecks:\n  analytics.orders:\n    filter: \"order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n</code></pre> <p>Solution 2: Add indexes</p> <pre><code>-- Add index on frequently checked columns\nCREATE INDEX idx_orders_email ON analytics.orders(email);\nCREATE INDEX idx_orders_order_date ON analytics.orders(order_date);\n</code></pre>"},{"location":"components/checks/checks/#false-positives","title":"False Positives","text":""},{"location":"components/checks/checks/#threshold-too-strict","title":"Threshold too strict","text":"<p>Problem: Check fails during normal variance</p> <pre><code># \u274c Too strict\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count = 10000  # Exact match\n\n# \u2705 Allow variance\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 9000 and 11000  # \u00b110% variance\n</code></pre>"},{"location":"components/checks/checks/#use-anomaly-detection-instead","title":"Use anomaly detection instead","text":"<pre><code># Replace strict threshold with ML-based detection\nchecks:\n  analytics.orders:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n</code></pre>"},{"location":"components/checks/checks/#summary","title":"Summary","text":"<p>Quality checks provide a comprehensive way to monitor data quality over time:</p>"},{"location":"components/checks/checks/#core-concepts","title":"Core Concepts","text":"<p>1. Quality Checks</p> <ul> <li>YAML-configured validation rules</li> <li>Non-blocking (don't stop pipelines)</li> <li>Track trends over time</li> <li>Integrate with Activity API</li> </ul> <p>2. Check Types</p> <ul> <li>Missing data checks (<code>missing_count</code>, <code>missing_percent</code>)</li> <li>Row count checks (<code>row_count</code>)</li> <li>Duplicate checks (<code>duplicate_count</code>)</li> <li>Failed rows (SQL-based)</li> <li>Anomaly detection (ML-based)</li> <li>Change monitoring (compare to previous)</li> </ul> <p>3. Data Profiling</p> <ul> <li>Automatic statistical metric collection</li> <li>Stored in <code>_check_profiles</code> table</li> <li>Observe patterns without validation</li> <li>Inform check threshold selection</li> </ul> <p>4. Data Quality Strategy</p> <ul> <li>Audits - Critical, blocking</li> <li>Checks - Monitoring, non-blocking</li> <li>Profiles - Observation, tracking</li> </ul>"},{"location":"components/model/model_kinds/","title":"Kinds","text":""},{"location":"components/model/model_kinds/#kinds","title":"Kinds","text":"<p>This page describes the kinds of models Vulcan supports, which determine how the data for a model is loaded.</p> <p>Find information about all model kind configuration parameters in the model configuration reference page.</p>"},{"location":"components/model/model_kinds/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p>Models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are computed incrementally based on a time range. This is an optimal choice for datasets in which records are captured over time and represent immutable facts such as events, logs, or transactions. Using this kind for appropriate datasets typically results in significant cost and time savings.</p> <p>Only missing time intervals are processed during each execution for <code>INCREMENTAL_BY_TIME_RANGE</code> models. This is in contrast to the FULL model kind, where the entire dataset is recomputed every time the model is executed.</p> <p>An <code>INCREMENTAL_BY_TIME_RANGE</code> model has two requirements that other models do not: it must know which column contains the time data it will use to filter the data by time range, and it must contain a <code>WHERE</code> clause that filters the upstream data by time.</p> <p>The name of the column containing time data is specified in the model's <code>MODEL</code> DDL. It is specified in the DDL <code>kind</code> specification's <code>time_column</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_TIME_RANGE</code> model that stores time data in the \"order_date\" column:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p> In addition to specifying a time column in the <code>MODEL</code> DDL, the model's query must contain a <code>WHERE</code> clause that filters the upstream records by time range. Vulcan provides special macros that represent the start and end of the time range being processed: <code>@start_date</code> / <code>@end_date</code> and <code>@start_ds</code> / <code>@end_ds</code>. Refer to Macros for more information.</p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>This example demonstrates incremental by time range models.</p> <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incrementals_demo,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    -- How does this model kind behave?\n    --   DELETE by time range, then INSERT\n    time_column transaction_date,\n\n    -- How do I handle late-arriving data?\n    --   Handle late-arriving events for the past 2 (2*1) days based on cron\n    --   interval. Each time it runs, it will process today, yesterday, and\n    --   the day before yesterday.\n    lookback 2,\n  ),\n\n  -- Don't backfill data before this date\n  start '2024-10-25',\n\n  -- What schedule should I run these at?\n  --   Daily at Midnight UTC\n  cron '@daily',\n\n  -- Good documentation for the primary key\n  grain transaction_id,\n\n  -- How do I test this data?\n  --   Validate that the `transaction_id` primary key values are both unique\n  --   and non-null. Data audit tests only run for the processed intervals,\n  --   not for the entire table.\n  -- audits (\n  --   UNIQUE_VALUES(columns = (transaction_id)),\n  --   NOT_NULL(columns = (transaction_id))\n  -- )\n);\n\nWITH sales_data AS (\n  SELECT\n    transaction_id,\n    product_id,\n    customer_id,\n    transaction_amount,\n    -- How do I account for UTC vs. PST (California baby) timestamps?\n    --   Make sure all time columns are in UTC and convert them to PST in the\n    --   presentation layer downstream.\n    transaction_timestamp,\n    payment_method,\n    currency\n  FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data\n  -- How do I make this run fast and only process the necessary intervals?\n  --   Use our date macros that will automatically run the necessary intervals.\n  --   Because Vulcan manages state, it will know what needs to run each time\n  --   you invoke `vulcan run`.\n  WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B\n  -- Include usage data from the 30 days before the interval\n  WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n)\n\nSELECT\n  s.transaction_id,\n  s.product_id,\n  s.customer_id,\n  s.transaction_amount,\n  -- Extract the date from the timestamp to partition by day\n  DATE(s.transaction_timestamp) as transaction_date,\n  -- Convert timestamp to PST using a SQL function in the presentation layer for end users\n  DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,\n  s.payment_method,\n  s.currency,\n  -- Product usage metrics\n  p.last_usage_date,\n  p.usage_count,\n  p.feature_utilization_score,\n  p.user_segment,\n  -- Derived metrics\n  CASE\n    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.8 THEN 'Power User'\n    WHEN p.usage_count &gt; 50 THEN 'Regular User'\n    WHEN p.usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type,\n  -- Time since last usage\n  DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage\nFROM sales_data s\nLEFT JOIN product_usage p\n  ON s.product_id = p.product_id\n  AND s.customer_id = p.customer_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>50975949</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (\n  `transaction_id` STRING,\n  `product_id` STRING,\n  `customer_id` STRING,\n  `transaction_amount` NUMERIC,\n  `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),\n  `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),\n  `payment_method` STRING,\n  `currency` STRING,\n  `last_usage_date` TIMESTAMP,\n  `usage_count` INT64,\n  `feature_utilization_score` FLOAT64,\n  `user_segment` STRING,\n  `user_type` STRING OPTIONS (description='Derived metrics'),\n  `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')\n  )\n  PARTITION BY `transaction_date`\n</code></pre> <p>Vulcan will validate the SQL before processing data (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps).</p> <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND\n    FALSE\n),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)\n    ) AND\n    FALSE\n)\n\nSELECT\n  `s`.`transaction_id` AS `transaction_id`,\n  `s`.`product_id` AS `product_id`,\n  `s`.`customer_id` AS `customer_id`,\n  CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n  DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n  DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n  `s`.`payment_method` AS `payment_method`,\n  `s`.`currency` AS `currency`,\n  `p`.`last_usage_date` AS `last_usage_date`,\n  `p`.`usage_count` AS `usage_count`,\n  `p`.`feature_utilization_score` AS `feature_utilization_score`,\n  `p`.`user_segment` AS `user_segment`,\n  CASE\n    WHEN `p`.`feature_utilization_score` &gt; 0.8 AND `p`.`usage_count` &gt; 100 THEN 'Power User'\n    WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n    WHEN `p`.`usage_count` IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END AS `user_type`,\n  DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\nFROM `sales_data` AS `s`\nLEFT JOIN `product_usage` AS `p`\n  ON `p`.`customer_id` = `s`.`customer_id` AND\n  `p`.`product_id` = `s`.`product_id`\nWHERE FALSE\nLIMIT 0\n</code></pre> <p>Vulcan will merge data into the empty table.</p> <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `transaction_id`,\n      `product_id`,\n      `customer_id`,\n      `transaction_amount`,\n      `transaction_timestamp`,\n      `payment_method`,\n      `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_id`,\n      `customer_id`,\n      `last_usage_date`,\n      `usage_count`,\n      `feature_utilization_score`,\n      `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  )\n\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`usage_count` &gt; 100 AND `p`.`feature_utilization_score` &gt; 0.8 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n      END AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p`\n      ON `s`.`product_id` = `p`.`product_id`\n      AND `s`.`customer_id` = `p`.`customer_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN\n  INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n  VALUES (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer to pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`\n</code></pre> <p>Important</p> <p>A model's <code>time_column</code> should be in the UTC time zone to ensure correct interaction with Vulcan's scheduler and predefined macro variables.</p> <p>This requirement aligns with the data engineering best practice of converting datetime/timestamp columns to UTC as soon as they are ingested into the data system and only converting them to local timezones when they exit the system for downstream uses. The <code>cron_tz</code> flag does not change this requirement.</p> <p>Placing all timezone conversion code in the system's first/last transformation models prevents inadvertent timezone-related errors as data flows between models.</p> <p>If a model must use a different timezone, parameters like lookback, allow_partials, and cron with offset time can be used to try to account for misalignment between the model's timezone and the UTC timezone used by Vulcan.</p> <p>This example implements a complete <code>INCREMENTAL_BY_TIME_RANGE</code> model that specifies the time column name <code>order_date</code> in the <code>MODEL</code> DDL and includes a SQL <code>WHERE</code> clause to filter records by time range:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_time_range,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  grains (order_date, product_id),\n  cron '@daily'\n);\n\nSELECT\n  o.order_date,\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  COUNT(DISTINCT o.order_id) AS order_count,\n  SUM(oi.quantity) AS total_quantity,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p\n  ON oi.product_id = p.product_id\nWHERE\n  o.order_date BETWEEN @start_ds AND @end_ds\nGROUP BY\n  o.order_date, p.product_id, p.name, p.category\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_time_range_py\",\n    columns={\n        \"order_date\": \"date\",\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"total_sales_amount\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_date\", \"product_id\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\", \"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT o.order_date, p.product_id, p.name AS product_name,\n           SUM(oi.quantity * oi.unit_price) AS total_sales_amount\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    JOIN vulcan_demo.products p ON oi.product_id = p.product_id\n    WHERE o.order_date BETWEEN '{start}' AND '{end}'\n    GROUP BY o.order_date, p.product_id, p.name\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/model_kinds/#time-column","title":"Time column","text":"<p>Vulcan needs to know which column in the model's output represents the timestamp or date associated with each record.</p> <p>Important</p> <p>The <code>time_column</code> variable should be in the UTC time zone - learn more above.</p> <p>The time column is used to determine which records will be overwritten during data restatement and provides a partition key for engines that support partitioning (such as Apache Spark). The name of the time column is specified in the <code>MODEL</code> DDL <code>kind</code> specification:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p>By default, Vulcan assumes the time column is in the <code>%Y-%m-%d</code> format. For other formats, the default can be overridden with a formatting string:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (order_date, '%Y-%m-%d')\n  )\n);\n</code></pre> <p>Note</p> <p>The time format should be defined using the same SQL dialect as the one used to define the model's query.</p> <p>Vulcan also uses the time column to automatically append a time range filter to the model's query at runtime, which prevents records that are not part of the target interval from being stored. This is a safety mechanism that prevents unintentionally overwriting unrelated records when handling late-arriving data.</p> <p>The required filter you write in the model query's <code>WHERE</code> clause filters the input data as it is read from upstream tables, reducing the amount of data processed by the model. The automatically appended time range filter is applied to the model query's output data to prevent data leakage.</p> <p>Consider the following model definition, which specifies a <code>WHERE</code> clause filter with the <code>shipped_date</code> column. The model's <code>time_column</code> is a different column <code>order_date</code>, whose filter is automatically added to the model query. This approach is useful when an upstream model's time column is different from the model's time column:</p> <pre><code>MODEL (\n  name vulcan_demo.shipment_events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- `order_date` is model's time column\n  )\n);\n\nSELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds; -- Filter is based on the user-supplied `shipped_date` column\n</code></pre> <p>At runtime, Vulcan will automatically modify the model's query to look like this:</p> <pre><code>SELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds\n  AND o.order_date BETWEEN @start_ds AND @end_ds; -- `order_date` time column filter automatically added by Vulcan\n</code></pre>"},{"location":"components/model/model_kinds/#partitioning","title":"Partitioning","text":"<p>By default, we ensure that the <code>time_column</code> is part of the partitioned_by property of the model so that it forms part of the partition key and allows the database engine to do partition pruning. If it is not explicitly listed in the Model definition, we will automatically add it.</p> <p>However, this may be undesirable if you want to exclusively partition on another column or you want to partition on something like <code>month(time_column)</code> but the engine you're using doesnt support partitioning based on expressions.</p> <p>To opt out of this behaviour, you can set <code>partition_by_time_column false</code> like so:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    partition_by_time_column false\n  ),\n  partitioned_by (warehouse_id) -- order_date will no longer be automatically added here and the partition key will just be 'warehouse_id'\n);\n</code></pre>"},{"location":"components/model/model_kinds/#idempotency","title":"Idempotency","text":"<p>We recommend making sure incremental by time range model queries are idempotent to prevent unexpected results during data restatement.</p> <p>Note, however, that upstream models and tables can impact a model's idempotency. For example, referencing an upstream model of kind FULL in the model query automatically causes the model to be non-idempotent because its data could change on every model execution.</p>"},{"location":"components/model/model_kinds/#materialization-strategy","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE by time column partition Databricks INSERT OVERWRITE by time column partition Snowflake DELETE by time range, then INSERT BigQuery DELETE by time range, then INSERT Redshift DELETE by time range, then INSERT Postgres DELETE by time range, then INSERT DuckDB DELETE by time range, then INSERT"},{"location":"components/model/model_kinds/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p>Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are computed incrementally based on a key.</p> <p>They insert or update rows based on these rules:</p> <ul> <li>If a key in newly loaded data is not present in the model table, the new data row is inserted.</li> <li>If a key in newly loaded data is already present in the model table, the existing row is updated with the new data.</li> <li>If a key is present in the model table but not present in the newly loaded data, its row is not modified and remains in the model table.</li> </ul> <p>Prevent duplicated keys</p> <p>If you do not want duplicated keys in the model table, you must ensure the model query does not return rows with duplicate keys.</p> <p>Vulcan does not automatically detect or prevent duplicates.</p> <p>This kind is a good fit for datasets that have the following traits:</p> <ul> <li>Each record has a unique key associated with it.</li> <li>There is at most one record associated with each unique key.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their keys match.</li> </ul> <p>A Slowly Changing Dimension (SCD) is one approach that fits this description well. See the SCD Type 2 model kind for a specific model kind for SCD Type 2 models.</p> <p>The name of the unique key column must be provided as part of the <code>MODEL</code> DDL, as in this example:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_unique_key,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  MAX(o.order_date) AS last_order_date\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nWHERE\n  o.order_date IS NULL OR o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_unique_key_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"last_order_date\": \"date\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id,\n           SUM(oi.quantity * oi.unit_price) as total_spent,\n           MAX(o.order_date) as last_order_date\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Composite keys are also supported:</p> <pre><code>MODEL (\n  name vulcan_demo.order_items_agg,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, product_id)\n  )\n);\n</code></pre> <p><code>INCREMENTAL_BY_UNIQUE_KEY</code> model kinds can also filter upstream records by time range using a SQL <code>WHERE</code> clause and the <code>@start_date</code>, <code>@end_date</code> or other macro variables (similar to the INCREMENTAL_BY_TIME_RANGE kind). Note that Vulcan macro time variables are in the UTC time zone.</p> <pre><code>SELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(o.order_id) AS total_orders\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nWHERE\n  o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incremental_by_unique_key_example,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key id\n  ),\n  start '2020-01-01',\n  cron '@daily',\n);\n\nSELECT\n  id,\n  item_id,\n  event_date\nFROM demo.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>1161945221</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>FALSE LIMIT 0</code> in the <code>WHERE</code> statement and the placeholder dates).</p> <pre><code>SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE (`seed_model`.`event_date` &lt;= CAST('1970-01-01' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('1970-01-01' AS DATE)) AND FALSE LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE `seed_model`.`event_date` &lt;= CAST('2024-10-30' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('2020-01-01' AS DATE)) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incremental_by_unique_key_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221`\n</code></pre> <p>Note: Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are inherently non-idempotent, which should be taken into consideration during data restatement. As a result, partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated.</p>"},{"location":"components/model/model_kinds/#unique-key-expressions","title":"Unique Key Expressions","text":"<p>The <code>unique_key</code> values can either be column names or SQL expressions. For example, if you wanted to create a key that is based on the coalesce of a value then you could do the following:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_unique,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key COALESCE(\"email\", '')\n  )\n);\n</code></pre>"},{"location":"components/model/model_kinds/#when-matched-expression","title":"When Matched Expression","text":"<p>The logic to use when updating columns when a match occurs (the source and target match on the given keys) by default updates all the columns. This can be overriden with custom logic like below:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched (\n      WHEN MATCHED THEN UPDATE SET target.email = COALESCE(source.email, target.email)\n    )\n  )\n);\n</code></pre> <p>The <code>source</code> and <code>target</code> aliases are required when using the <code>when_matched</code> expression in order to distinguish between the source and target columns.</p> <p>Multiple <code>WHEN MATCHED</code> expressions can also be provided. Ex:</p> <pre><code>MODEL (\n  name vulcan_demo.products_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key product_id,\n    when_matched (\n      WHEN MATCHED AND source.price IS NULL THEN UPDATE SET target.price = target.price\n      WHEN MATCHED THEN UPDATE SET target.category = COALESCE(source.category, target.category)\n    )\n  )\n);\n</code></pre> <p>Note: <code>when_matched</code> is only available on engines that support the <code>MERGE</code> statement. Currently supported engines include:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> </ul> <p>In Redshift's case, to enable the use of the native <code>MERGE</code> statement, you need to pass the <code>enable_merge</code> flag in the connection and set it to <code>true</code>. It is disabled by default.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      enable_merge: true\n</code></pre> <p>Redshift supports only the <code>UPDATE</code> or <code>DELETE</code> actions for the <code>WHEN MATCHED</code> clause and does not allow multiple <code>WHEN MATCHED</code> expressions. For further information, refer to the Redshift documentation.</p>"},{"location":"components/model/model_kinds/#merge-filter-expression","title":"Merge Filter Expression","text":"<p>The <code>MERGE</code> statement typically induces a full table scan of the existing table, which can be problematic with large data volumes.</p> <p>Prevent a full table scan by passing filtering conditions to the <code>merge_filter</code> parameter.</p> <p>The <code>merge_filter</code> accepts a single or a conjunction of predicates to be used in the <code>ON</code> clause of the <code>MERGE</code> operation:</p> <pre><code>MODEL (\n  name vulcan_demo.orders_recent,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key order_id,\n    merge_filter source._operation IS NULL AND target.order_date &gt; dateadd(day, -7, current_date)\n  )\n);\n</code></pre> <p>Similar to <code>when_matched</code>, the <code>source</code> and <code>target</code> aliases are used to distinguish between the source and target tables.</p> <p>If an existing dbt project uses the incremental_predicates functionality, Vulcan will automatically convert them into the equivalent <code>merge_filter</code> specification.</p>"},{"location":"components/model/model_kinds/#materialization-strategy_1","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are materialized using the following strategies:</p> Engine Strategy Spark not supported Databricks MERGE ON unique key Snowflake MERGE ON unique key BigQuery MERGE ON unique key Redshift MERGE ON unique key Postgres MERGE ON unique key DuckDB DELETE ON matched + INSERT new rows"},{"location":"components/model/model_kinds/#full","title":"FULL","text":"<p>Models of the <code>FULL</code> kind cause the dataset associated with a model to be fully refreshed (rewritten) upon each model evaluation.</p> <p>The <code>FULL</code> model kind is somewhat easier to use than incremental kinds due to the lack of special settings or additional query considerations. This makes it suitable for smaller datasets, where recomputing data from scratch is relatively cheap and doesn't require preservation of processing history. However, using this kind with datasets containing a large volume of records will result in significant runtime and compute costs.</p> <p>This kind can be a good fit for aggregate tables that lack a temporal dimension. For aggregate tables with a temporal dimension, consider the INCREMENTAL_BY_TIME_RANGE kind instead.</p> <p>This example specifies a <code>FULL</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  start '2025-01-01',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\nORDER BY total_spent DESC\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT p.product_id, p.name AS product_name, p.category,\n           COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.full_model_example,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM demo.incremental_model\nGROUP BY\n  item_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>2345651858</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` (`item_id` INT64, `num_orders` INT64)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>WHERE FALSE</code> and <code>LIMIT 0</code>).</p> <pre><code>SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nWHERE FALSE\nGROUP BY `incremental_model`.`item_id` LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` AS\nSELECT CAST(`item_id` AS INT64) AS `item_id`, CAST(`num_orders` AS INT64) AS `num_orders`\nFROM (SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nGROUP BY `incremental_model`.`item_id`) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`full_model_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858`\n</code></pre>"},{"location":"components/model/model_kinds/#materialization-strategy_2","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>FULL</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE Databricks INSERT OVERWRITE Snowflake CREATE OR REPLACE TABLE BigQuery CREATE OR REPLACE TABLE Redshift DROP TABLE, CREATE TABLE, INSERT Postgres DROP TABLE, CREATE TABLE, INSERT DuckDB CREATE OR REPLACE TABLE"},{"location":"components/model/model_kinds/#view","title":"VIEW","text":"<p>The model kinds described so far cause the output of a model query to be materialized and stored in a physical table.</p> <p>The <code>VIEW</code> kind is different, because no data is actually written during model execution. Instead, a non-materialized view (or \"virtual table\") is created or replaced based on the model's query.</p> <p>Note: <code>VIEW</code> is the default model kind if kind is not specified.</p> <p>Note: Python models do not support the <code>VIEW</code> model kind - use a SQL model instead.</p> <p>Note: With this kind, the model's query is evaluated every time the model is referenced in a downstream query. This may incur undesirable compute cost and time in cases where the model's query is compute-intensive, or when the model is referenced in many downstream queries.</p> <p>This example specifies a <code>VIEW</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.view_model,\n  kind VIEW,\n  grains (warehouse_performance_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  r.region_name,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', o.order_date::TEXT) AS warehouse_performance_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.warehouses AS w\nLEFT JOIN vulcan_demo.regions AS r\n  ON w.region_id = r.region_id\nLEFT JOIN vulcan_demo.orders AS o\n  ON w.warehouse_id = o.warehouse_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY w.warehouse_id, w.name, r.region_name, o.order_date\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.example_view,\n  kind VIEW,\n  cron '@daily',\n);\n\nSELECT\n  'hello there' as a_column\n</code></pre> <p>Vulcan will execute this SQL to create a versioned view in the physical layer. Note that the view's version fingerprint, <code>1024042926</code>, is part of the view name.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n(`a_column`) AS SELECT 'hello there' AS `a_column`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned view in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`example_view` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n</code></pre>"},{"location":"components/model/model_kinds/#materialized-views","title":"Materialized Views","text":"<p>The <code>VIEW</code> model kind can be configured to represent a materialized view by setting the <code>materialized</code> flag to <code>true</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.sales_summary,\n  kind VIEW (\n    materialized true\n  )\n);\n</code></pre> <p>Note: This flag only applies to engines that support materialized views and is ignored by other engines. Supported engines include:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Snowflake</li> </ul> <p>During the evaluation of a model of this kind, the view will be replaced or recreated only if the model's query rendered during evaluation does not match the query used during the previous view creation for this model, or if the target view does not exist. Thus, views are recreated only when necessary in order to realize all the benefits provided by materialized views.</p>"},{"location":"components/model/model_kinds/#embedded","title":"EMBEDDED","text":"<p>Embedded models are a way to share common logic between different models of other kinds.</p> <p>There are no data assets (tables or views) associated with <code>EMBEDDED</code> models in the data warehouse. Instead, an <code>EMBEDDED</code> model's query is injected directly into the query of each downstream model that references it, as a subquery.</p> <p>Note: Python models do not support the <code>EMBEDDED</code> model kind - use a SQL model instead.</p> <p>This example specifies an <code>EMBEDDED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.unique_customers,\n  kind EMBEDDED\n);\n\nSELECT DISTINCT\n  customer_id,\n  name AS customer_name,\n  email\nFROM vulcan_demo.customers\n</code></pre>"},{"location":"components/model/model_kinds/#seed","title":"SEED","text":"<p>The <code>SEED</code> model kind is used to specify seed models for using static CSV datasets in your Vulcan project.</p> <p>Notes:</p> <ul> <li>Seed models are loaded only once unless the SQL model and/or seed file is updated.</li> <li>Python models do not support the <code>SEED</code> model kind - use a SQL model instead.</li> </ul> <p>This example specifies a <code>SEED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INT,\n    item_id INT,\n    event_date DATE\n  ),\n  grains (id),\n  assertions (\n    UNIQUE_COMBINATION_OF_COLUMNS(columns := (id, event_date)),\n    NOT_NULL(columns := (id, item_id, event_date))\n  )\n)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.seed_example,\n  kind SEED (\n    path '../../seeds/seed_example.csv'\n  ),\n  columns (\n    id INT64,\n    item_id INT64,\n    event_date DATE\n  ),\n  grain (id, event_date)\n)\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>3038173937</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will upload the seed as a temp table in the physical layer.</p> <pre><code>vulcan-public-demo.vulcan__demo.__temp_demo__seed_example__3038173937_9kzbpld7\n</code></pre> <p>Vulcan will create a versioned table in the physical layer from the temp table.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `id`, `item_id`, `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`) AS `_subquery`\n</code></pre> <p>Vulcan will drop the temp table in the physical layer.</p> <pre><code>DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`seed_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937`\n</code></pre>"},{"location":"components/model/model_kinds/#scd-type-2","title":"SCD Type 2","text":"<p>SCD Type 2 is a model kind that supports slowly changing dimensions (SCDs) in your Vulcan project. SCDs are a common pattern in data warehousing that allow you to track changes to records over time.</p> <p>Vulcan achieves this by adding a <code>valid_from</code> and <code>valid_to</code> column to your model. The <code>valid_from</code> column is the timestamp that the record became valid (inclusive) and the <code>valid_to</code> column is the timestamp that the record became invalid (exclusive). The <code>valid_to</code> column is set to <code>NULL</code> for the latest record.</p> <p>Therefore, you can use these models to not only tell you what the latest value is for a given record but also what the values were anytime in the past. Note that maintaining this history does come at a cost of increased storage and compute and this may not be a good fit for sources that change frequently since the history could get very large.</p> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss, so data restatement is disabled for models of this kind by default.</p> <p>There are two ways to tracking changes: By Time (Recommended) or By Column.</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-recommended","title":"SCD Type 2 By Time (Recommended)","text":"<p>SCD Type 2 By Time supports sourcing from tables that have an \"Updated At\" timestamp defined in the table that tells you when a given record was last updated. This is the recommended way since this \"Updated At\" gives you a precise time when the record was last updated and therefore improves the accuracy of the SCD Type 2 table that is produced.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_TIME</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_time,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key dt\n  ),\n  grains (dt)\n);\n\nSELECT\n  dd.dt,\n  dd.year,\n  dd.month,\n  dd.day_of_week,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity) AS total_quantity_sold,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  CURRENT_TIMESTAMP AS updated_at\nFROM vulcan_demo.dim_dates AS dd\nLEFT JOIN vulcan_demo.orders AS o\n  ON dd.dt = o.order_date\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY dd.dt, dd.year, dd.month, dd.day_of_week\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_time_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"region_name\": \"string\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.regions\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id, c.name as customer_name, c.email, r.region_name\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.regions r ON c.region_id = r.region_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p> <p>The <code>updated_at</code> column name can also be changed by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    updated_at_name my_updated_at -- Name for `updated_at` column\n  )\n);\n\nSELECT\n  id,\n  name,\n  price,\n  my_updated_at\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  my_updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>SCD Type 2 By Column supports sourcing from tables that do not have an \"Updated At\" timestamp defined in the table. Instead, it will check the columns defined in the <code>columns</code> field to see if their value has changed and if so it will record the <code>valid_from</code> time as the execution time when the change was detected.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_COLUMN</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_column,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key ARRAY[product_id],\n    columns ARRAY[product_name, category, price]\n  ),\n  grains (product_id)\n);\n\nSELECT\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  p.price,\n  s.name AS supplier_name,\n  r.region_name\nFROM vulcan_demo.products AS p\nLEFT JOIN vulcan_demo.suppliers AS s\n  ON p.supplier_id = s.supplier_id\nLEFT JOIN vulcan_demo.regions AS r\n  ON s.region_id = r.region_id\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_column_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"price\": \"decimal(10,2)\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"product_name\", \"category\", \"price\"],\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT product_id, name as product_name, category, price\n    FROM vulcan_demo.products\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#change-column-names","title":"Change Column Names","text":"<p>Vulcan will automatically add the <code>valid_from</code> and <code>valid_to</code> columns to your table. If you would like to specify the names of these columns you can do so by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    valid_from_name my_valid_from, -- Name for `valid_from` column\n    valid_to_name my_valid_to -- Name for `valid_to` column\n  )\n);\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  my_valid_from TIMESTAMP,\n  my_valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#deletes","title":"Deletes","text":"<p>A hard delete is when a record no longer exists in the source table. When this happens,</p> <p>If <code>invalidate_hard_deletes</code> is set to <code>false</code> (default):</p> <ul> <li><code>valid_to</code> column will continue to be set to <code>NULL</code> (therefore still considered \"valid\")</li> <li>If the record is added back, then the <code>valid_to</code> column will be set to the <code>valid_from</code> of the new record.</li> </ul> <p>When a record is added back, the new record will be inserted into the table with <code>valid_from</code> set to:</p> <ul> <li>SCD_TYPE_2_BY_TIME: the largest of either the <code>updated_at</code> timestamp of the new record or the <code>valid_from</code> timestamp of the deleted record in the SCD Type 2 table</li> <li>SCD_TYPE_2_BY_COLUMN: the <code>execution_time</code> when the record was detected again</li> </ul> <p>If <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> <ul> <li><code>valid_to</code> column will be set to the time when the Vulcan run started that detected the missing record (called <code>execution_time</code>).</li> <li>If the record is added back, then the <code>valid_to</code> column will remain unchanged.</li> </ul> <p>One way to think about <code>invalidate_hard_deletes</code> is that, if <code>invalidate_hard_deletes</code> is set to <code>true</code>, deletes are most accurately tracked in the SCD Type 2 table since it records when the delete occurred. As a result though, you can have gaps between records if the there is a gap of time between when it was deleted and added back. If you would prefer to not have gaps, and a result consider missing records in source as still \"valid\", then you can leave the default value or set <code>invalidate_hard_deletes</code> to <code>false</code>.</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-time-in-action","title":"Example of SCD Type 2 By Time in Action","text":"<p>Lets say that you started with the following data in your source table and <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> ID Name Price Updated At 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 2 Cheeseburger 8.99 2020-01-01 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Chocolate Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 4 Chocolate Milkshake 3.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL <p>Note: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 00:00:00</code> meaning if you queried the table during that time range then you would not see <code>Cheeseburger</code> in the menu. This is the most accurate representation of the menu based on the source data provided. If <code>Cheeseburger</code> were added back to the menu with it's original updated at timestamp of <code>2020-01-01 00:00:00</code> then the <code>valid_from</code> timestamp of the new record would have been <code>2020-01-02 11:00:00</code> resulting in no period of time where the item was deleted. Since in this case the updated at timestamp did not change it is likely the item was removed in error and this again most accurately represents the menu based on the source data.</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-column-in-action","title":"Example of SCD Type 2 By Column in Action","text":"<p>Lets say that you started with the following data in your source table and <code>invalidate_hard_deletes</code> is set to <code>true</code>:</p> ID Name Price 1 Chicken Sandwich 10.99 2 Cheeseburger 8.99 3 French Fries 4.99 <p>We configure the SCD Type 2 By Column model to check the columns <code>Name</code> and <code>Price</code> for changes</p> <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 12.99 3 French Fries 4.99 4 Milkshake 3.99 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 14.99 2 Cheeseburger 8.99 3 French Fries 4.99 4 Chocolate Milkshake 3.99 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-03 11:00:00</code>, Target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 2020-01-03 11:00:00 1 Chicken Sandwich 14.99 2020-01-03 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 11:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 2020-01-03 11:00:00 4 Chocolate Milkshake 3.99 2020-01-03 11:00:00 NULL <p>Note: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 11:00:00</code> meaning if you queried the table during that time range then you would not see <code>Cheeseburger</code> in the menu. This is the most accurate representation of the menu based on the source data provided.</p>"},{"location":"components/model/model_kinds/#shared-configuration-options","title":"Shared Configuration Options","text":"Name Description Type unique_key Unique key used for identifying rows between source and target List of strings or string valid_from_name The name of the <code>valid_from</code> column to create in the target table. Default: <code>valid_from</code> string valid_to_name The name of the <code>valid_to</code> column to create in the target table. Default: <code>valid_to</code> string invalidate_hard_deletes If set to <code>true</code>, when a record is missing from the source table it will be marked as invalid. Default: <code>false</code> bool batch_size The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int <p>Important</p> <p>If using BigQuery, the default data type of the valid_from/valid_to columns is DATETIME. If you want to use TIMESTAMP, you can specify the data type in the model definition.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    time_data_type TIMESTAMP\n  )\n);\n</code></pre> <p>This could likely be used on other engines to change the expected data type but has only been tested on BigQuery.</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-configuration-options","title":"SCD Type 2 By Time Configuration Options","text":"Name Description Type updated_at_name The name of the column containing a timestamp to check for new or updated records. Default: <code>updated_at</code> string updated_at_as_valid_from By default, for new rows <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code>. This changes the behavior to set it to the valid of <code>updated_at</code> when the row is inserted. Default: <code>false</code> bool"},{"location":"components/model/model_kinds/#scd-type-2-by-column-configuration-options","title":"SCD Type 2 By Column Configuration Options","text":"Name Description Type columns The name of the columns to check for changes. <code>*</code> to represent that all columns should be checked. List of strings or string execution_time_as_valid_from By default, when the model is first loaded <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code> and future new rows will have <code>execution_time</code> of when the pipeline ran. This changes the behavior to always use <code>execution_time</code>. Default: <code>false</code> bool updated_at_name If sourcing from a table that includes as timestamp to use as valid_from, set this property to that column. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int"},{"location":"components/model/model_kinds/#processing-source-table-with-historical-data","title":"Processing Source Table with Historical Data","text":"<p>The most common case for SCD Type 2 is creating history for a table that it doesn't have it already.  In the example of the restaurant menu, the menu just tells you what is offered right now, but you want to know what was offered over time. In this case, the default setting of <code>None</code> for <code>batch_size</code> is the best option.</p> <p>Another use case though is processing a source table that already has history in it.  A common example of this is a \"daily snapshot\" table that is created by a source system that takes a snapshot of the data at the end of each day. If your source table has historical records, like a \"daily snapshot\" table, then set <code>batch_size</code> to <code>1</code> to process each interval (each day if a <code>@daily</code> cron) in sequential order. That way the historical records will be properly captured in the SCD Type 2 table.</p>"},{"location":"components/model/model_kinds/#example-source-from-daily-snapshot-table","title":"Example - Source from Daily Snapshot Table","text":"<pre><code>MODEL (\n    name db.table,\n    kind SCD_TYPE_2_BY_COLUMN (\n        unique_key id,\n        columns [some_value],\n        updated_at_name ds,\n        batch_size 1\n    ),\n    start '2025-01-01',\n    cron '@daily'\n);\nSELECT\n    id,\n    some_value,\n    ds\nFROM\n    source_table\nWHERE\n    ds between @start_ds and @end_ds\n</code></pre> <p>This will process each day of the source table in sequential order (if more than one day to process), checking <code>some_value</code> column to see if it changed. If it did change, <code>valid_from</code> will be set to match the <code>ds</code> column (except for first value which would be <code>1970-01-01 00:00:00</code>).</p> <p>If the source data was the following:</p> id some_value ds 1 1 2025-01-01 1 2 2025-01-02 1 3 2025-01-03 1 3 2025-01-04 <p>Then the resulting SCD Type 2 table would be:</p> id some_value ds valid_from valid_to 1 1 2025-01-01 1970-01-01 00:00:00 2025-01-02 00:00:00 1 2 2025-01-02 2025-01-02 00:00:00 2025-01-03 00:00:00 1 3 2025-01-03 2025-01-03 00:00:00 NULL"},{"location":"components/model/model_kinds/#querying-scd-type-2-models","title":"Querying SCD Type 2 Models","text":""},{"location":"components/model/model_kinds/#querying-the-current-version-of-a-record","title":"Querying the current version of a record","text":"<p>Although SCD Type 2 models support history, it is still very easy to query for just the latest version of a record. Simply query the model as you would any other table. For example, if you wanted to query the latest version of the <code>menu_items</code> table you would simply run:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  valid_to IS NULL;\n</code></pre> <p>One could also create a view on top of the SCD Type 2 model that creates a new <code>is_current</code> column to make it easy for consumers to identify the current record.</p> <pre><code>SELECT\n  *,\n  valid_to IS NULL AS is_current\nFROM\n  menu_items;\n</code></pre>"},{"location":"components/model/model_kinds/#querying-for-a-specific-version-of-a-record-at-a-give-point-in-time","title":"Querying for a specific version of a record at a give point in time","text":"<p>If you wanted to query the <code>menu_items</code> table as it was on <code>2020-01-02 01:00:00</code> you would simply run:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  id = 1\n  AND '2020-01-02 01:00:00' &gt;= valid_from\n  AND '2020-01-02 01:00:00' &lt; COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>Example in a join:</p> <pre><code>SELECT\n  *\nFROM\n  orders\nINNER JOIN\n  menu_items\n  ON orders.menu_item_id = menu_items.id\n  AND orders.created_at &gt;= menu_items.valid_from\n  AND orders.created_at &lt; COALESCE(menu_items.valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>A view can be created to do the <code>COALESCE</code> automatically. This, combined with the <code>is_current</code> flag, makes it easier to query for a specific version of a record.</p> <pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP)) AS valid_to\n  valid_to IS NULL AS is_current,\nFROM\n  menu_items;\n</code></pre> <p>Furthermore if you want to make it so users can use <code>BETWEEN</code> when querying by making <code>valid_to</code> inclusive you can do the following: </p><pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2200-01-01 00:00:00+00:00' AS TIMESTAMP)) - INTERVAL 1 SECOND AS valid_to\n  valid_to IS NULL AS is_current,\n</code></pre><p></p> <p>Note: The precision of the timestamps in this example is second so I subtract 1 second. Make sure to subtract a value equal to the precision of your timestamps.</p>"},{"location":"components/model/model_kinds/#querying-for-deleted-records","title":"Querying for deleted records","text":"<p>One way to identify deleted records is to query for records that do not have a <code>valid_to</code> record of <code>NULL</code>. For example, if you wanted to query for all deleted ids in the <code>menu_items</code> table you would simply run:</p> <pre><code>SELECT\n  id,\n  MAX(CASE WHEN valid_to IS NULL THEN 0 ELSE 1 END) AS is_deleted\nFROM\n  menu_items\nGROUP BY\n  id\n</code></pre>"},{"location":"components/model/model_kinds/#reset-scd-type-2-model-clearing-history","title":"Reset SCD Type 2 Model (clearing history)","text":"<p>SCD Type 2 models are designed by default to protect the data that has been captured because it is not possible to recreate the history once it has been lost. However, there are cases where you may want to clear the history and start fresh. For this use use case you will want to start by setting <code>disable_restatement</code> to <code>false</code> in the model definition.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    disable_restatement false\n  )\n);\n</code></pre> <p>Plan/apply this change to production. Then you will want to restate the model.</p> <pre><code>vulcan plan --restate-model db.menu_items\n</code></pre> <p>Warning</p> <p>This will remove the historical data on the model which in most situations cannot be recovered.</p> <p>Once complete you will want to remove <code>disable_restatement</code> on the model definition which will set it back to <code>true</code> and prevent accidental data loss.</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n  )\n);\n</code></pre> <p>Plan/apply this change to production.</p>"},{"location":"components/model/model_kinds/#external","title":"EXTERNAL","text":"<p>The EXTERNAL model kind is used to specify external models that store metadata about external tables. External models are special; they are not specified in .sql files like the other model kinds. They are optional but useful for propagating column and type information for external tables queried in your Vulcan project.</p>"},{"location":"components/model/model_kinds/#managed","title":"MANAGED","text":"<p>Warning</p> <p>Managed models are still under development and the API / semantics may change as support for more engines is added</p> <p>Note: Python models do not support the <code>MANAGED</code> model kind - use a SQL model instead.</p> <p>The <code>MANAGED</code> model kind is used to create models where the underlying database engine manages the data lifecycle.</p> <p>These models don't get updated with new intervals or refreshed when <code>vulcan run</code> is called. Responsibility for keeping the data up to date falls on the engine.</p> <p>You can control how the engine creates the managed model by using the <code>physical_properties</code> to pass engine-specific parameters for adapter to use when issuing commands to the underlying database.</p> <p>Due to there being no standard, each vendor has a different implementation with different semantics and different configuration parameters. Therefore, <code>MANAGED</code> models are not as portable between database engines as other Vulcan model types. In addition, due to their black-box nature, Vulcan has limited visibility into the integrity and state of the model.</p> <p>We would recommend using standard Vulcan model types in the first instance. However, if you do need to use Managed models, you still gain other Vulcan benefits like the ability to use them in virtual environments.</p> <p>See Managed Models for more information on which engines are supported and which properties are available.</p>"},{"location":"components/model/model_kinds/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Models of the <code>INCREMENTAL_BY_PARTITION</code> kind are computed incrementally based on partition. A set of columns defines the model's partitioning key, and a partition is the group of rows with the same partitioning key value.</p> <p>Should you use this model kind?</p> <p>Any model kind can use a partitioned table by specifying the <code>partitioned_by</code> key in the <code>MODEL</code> DDL.</p> <p>The \"partition\" in <code>INCREMENTAL_BY_PARTITION</code> is about how the data is loaded when the model runs.</p> <p><code>INCREMENTAL_BY_PARTITION</code> models are inherently non-idempotent, so restatements and other actions can cause data loss. This makes them more complex to manage than other model kinds.</p> <p>In most scenarios, an <code>INCREMENTAL_BY_TIME_RANGE</code> model can meet your needs and will be easier to manage. The <code>INCREMENTAL_BY_PARTITION</code> model kind should only be used when the data must be loaded by partition (usually for performance reasons).</p> <p>This model kind is designed for the scenario where data rows should be loaded and updated as a group based on their shared value for the partitioning key.</p> <p>It may be used with any SQL engine. Vulcan will automatically create partitioned tables on engines that support explicit table partitioning (e.g., BigQuery, Databricks).</p> <p>New rows are loaded based on their partitioning key value:</p> <ul> <li>If a partitioning key in newly loaded data is not present in the model table, the new partitioning key and its data rows are inserted.</li> <li>If a partitioning key in newly loaded data is already present in the model table, all the partitioning key's existing data rows in the model table are replaced with the partitioning key's data rows in the newly loaded data.</li> <li>If a partitioning key is present in the model table but not present in the newly loaded data, the partitioning key's existing data rows are not modified and remain in the model table.</li> </ul> <p>This kind should only be used for datasets that have the following traits:</p> <ul> <li>The dataset's records can be grouped by a partitioning key.</li> <li>Each record has a partitioning key associated with it.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their partitioning keys match.</li> <li>All existing records associated with a given partitioning key can be removed or overwritten when any new record has the partitioning key value.</li> </ul> <p>The column defining the partitioning key is specified in the model's <code>MODEL</code> DDL <code>partitioned_by</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_PARTITION</code> model:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.partition,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by ARRAY[warehouse_id, category],\n  grains (partitioned_analysis_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  p.category,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', p.category, '_', o.order_date::TEXT) AS partitioned_analysis_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p ON oi.product_id = p.product_id\nJOIN vulcan_demo.warehouses AS w ON o.warehouse_id = w.warehouse_id\nGROUP BY w.warehouse_id, w.name, p.category, o.order_date\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.partition_py\",\n    columns={\n        \"warehouse_id\": \"int\",\n        \"order_date\": \"date\",\n        \"daily_revenue\": \"decimal(10,2)\",\n    },\n    partitioned_by=[\"warehouse_id\"],\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_PARTITION,\n    ),\n    grains=[\"warehouse_id\", \"order_date\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT o.warehouse_id, o.order_date,\n           SUM(oi.quantity * oi.unit_price) as daily_revenue\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY o.warehouse_id, o.order_date\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Compound partition keys are also supported:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by (warehouse_id, category)\n);\n</code></pre> <p>Date and/or timestamp column expressions are also supported (varies by SQL engine). This BigQuery example's partition key is based on the month each row's <code>order_date</code> occurred:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by DATETIME_TRUNC(order_date, MONTH)\n);\n</code></pre> <p>Only full restatements supported</p> <p>Partial data restatements are used to reprocess part of a table's data (usually a limited time range).</p> <p>Partial data restatement is not supported for <code>INCREMENTAL_BY_PARTITION</code> models. If you restate an <code>INCREMENTAL_BY_PARTITION</code> model, its entire table will be recreated from scratch.</p> <p>Restating <code>INCREMENTAL_BY_PARTITION</code> models may lead to data loss and should be performed with care.</p>"},{"location":"components/model/model_kinds/#example","title":"Example","text":"<p>This is a fuller example of how you would use this model kind in practice. It limits the number of partitions to backfill based on time range in the <code>partitions_to_update</code> CTE.</p> <pre><code>MODEL (\n  name demo.incremental_by_partition_demo,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by user_segment,\n);\n\n-- This is the source of truth for what partitions need to be updated and will join to the product usage data\n-- This could be an INCREMENTAL_BY_TIME_RANGE model that reads in the user_segment values last updated in the past 30 days to reduce scope\n-- Use this strategy to reduce full restatements\nWITH partitions_to_update AS (\n  SELECT DISTINCT\n    user_segment\n  FROM demo.incremental_by_time_range_demo  -- upstream table tracking which user segments to update\n  WHERE last_updated_at BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage\n  WHERE user_segment IN (SELECT user_segment FROM partitions_to_update) -- partition filter applied here\n)\n\nSELECT\n  product_id,\n  customer_id,\n  last_usage_date,\n  usage_count,\n  feature_utilization_score,\n  user_segment,\n  CASE\n    WHEN usage_count &gt; 100 AND feature_utilization_score &gt; 0.7 THEN 'Power User'\n    WHEN usage_count &gt; 50 THEN 'Regular User'\n    WHEN usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type\nFROM product_usage\n</code></pre> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss.</p>"},{"location":"components/model/model_kinds/#materialization-strategy_3","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_PARTITION</code> kind are materialized using the following strategies:</p> Engine Strategy Databricks REPLACE WHERE by partitioning key Spark INSERT OVERWRITE by partitioning key Snowflake DELETE by partitioning key, then INSERT BigQuery DELETE by partitioning key, then INSERT Redshift DELETE by partitioning key, then INSERT Postgres DELETE by partitioning key, then INSERT DuckDB DELETE by partitioning key, then INSERT"},{"location":"components/model/model_kinds/#incremental_unmanaged","title":"INCREMENTAL_UNMANAGED","text":"<p>The <code>INCREMENTAL_UNMANAGED</code> model kind exists to support append-only tables. It's \"unmanaged\" in the sense that Vulcan doesnt try to manage how the data is loaded. Vulcan will just run your query on the configured cadence and append whatever it gets into the table.</p> <p>Should you use this model kind?</p> <p>Some patterns for data management, such as Data Vault, may rely on append-only tables. In this situation, <code>INCREMENTAL_UNMANAGED</code> is the correct type to use.</p> <p>In most other situations, you probably want <code>INCREMENTAL_BY_TIME_RANGE</code> or <code>INCREMENTAL_BY_UNIQUE_KEY</code> because they give you much more control over how the data is loaded.</p> <p>Usage of the <code>INCREMENTAL_UNMANAGED</code> model kind is straightforward:</p> <pre><code>MODEL (\n  name vulcan_demo.incremental_unmanaged,\n  kind INCREMENTAL_UNMANAGED,\n  cron '@daily',\n  start '2025-01-01',\n  grains (shipment_id)\n);\n\n/* Append-only shipment event log */\nSELECT\n  s.shipment_id,\n  s.order_id,\n  s.shipped_date,\n  s.carrier,\n  o.customer_id,\n  c.name AS customer_name,\n  o.order_date,\n  (s.shipped_date - o.order_date::DATE)::INT AS days_to_ship,\n  CURRENT_TIMESTAMP AS shipment_event_timestamp\nFROM vulcan_demo.shipments AS s\nJOIN vulcan_demo.orders AS o ON s.order_id = o.order_id\nJOIN vulcan_demo.customers AS c ON o.customer_id = c.customer_id\nORDER BY s.shipped_date DESC\n</code></pre> <p>Since it's unmanaged, it doesnt support the <code>batch_size</code> and <code>batch_concurrency</code> properties to control how data is loaded like the other incremental model types do.</p> <p>Only full restatements supported</p> <p>Similar to <code>INCREMENTAL_BY_PARTITION</code>, attempting to restate an <code>INCREMENTAL_UNMANAGED</code> model will trigger a full restatement. That is, the model will be rebuilt from scratch rather than from a time slice you specify.</p> <p>This is because an append-only table is inherently non-idempotent. Restating <code>INCREMENTAL_UNMANAGED</code> models may lead to data loss and should be performed with care.</p>"},{"location":"components/model/overview/","title":"Overview","text":""},{"location":"components/model/overview/#overview","title":"Overview","text":"<p>Models are made up of metadata and queries that create tables and views, which can be used by other models or even outside of Vulcan. They are defined in the <code>models/</code> directory of your Vulcan project and live in <code>.sql</code> files.</p> <p>Vulcan will automatically determine the relationships among and lineage of your models by parsing SQL, so you don't have to worry about manually configuring dependencies.</p> <p>A model consists of two core components:</p> <ul> <li>DDL (Data Definition Language): The <code>MODEL</code> block that defines the structure, metadata, and behavior of the model</li> <li>DML (Data Manipulation Language): The <code>SELECT</code> query that contains the transformation logic</li> </ul>"},{"location":"components/model/overview/#model-structure","title":"Model Structure","text":"<p>Models can be defined in SQL or Python. Both formats follow the same DDL/DML pattern.</p> SQL ModelPython Model <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> Component Lines Purpose DDL (MODEL block) 1-6 Defines model name, kind, schedule, and grain DML (SELECT query) 8-17 Contains the transformation logic <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n  \"sales.daily_sales_py\",\n  columns={\n    \"order_date\": \"timestamp\",\n    \"total_orders\": \"int\",\n    \"total_revenue\": \"decimal(18,2)\",\n    \"last_order_id\": \"string\",\n  },\n  kind=dict(name=ModelKindName.FULL),\n  grains=[\"order_date\"],\n  depends_on=[\"raw.raw_orders\"],\n  cron='@daily',\n)\ndef execute(\n  context: ExecutionContext,\n  start: datetime,\n  end: datetime,\n  execution_time: datetime,\n  **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n  query = \"\"\"\n  SELECT\n    CAST(order_date AS TIMESTAMP) AS order_date,\n    COUNT(order_id)::INTEGER AS total_orders,\n    SUM(total_amount)::NUMERIC(18,2) AS total_revenue,\n    MAX(order_id)::VARCHAR AS last_order_id\n  FROM raw.raw_orders\n  GROUP BY order_date\n  ORDER BY order_date\n  \"\"\"\n\n  return context.fetchdf(query)\n</code></pre> Component Lines Purpose DDL (<code>@model</code> decorator) 7-20 Defines model name, columns, kind, schedule, and dependencies DML (function body) 21-40 Contains the transformation logic and returns a DataFrame"},{"location":"components/model/overview/#ddl-the-model-block","title":"DDL: The MODEL Block","text":"<p>The <code>MODEL</code> block is the DDL component that defines the structure and metadata of your model. It is the first non-comment statement in the file and uses a DDL-like syntax to declare model properties.</p>"},{"location":"components/model/overview/#ddl-syntax","title":"DDL Syntax","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n</code></pre>"},{"location":"components/model/overview/#common-ddl-properties","title":"Common DDL Properties","text":"Property Description Example <code>name</code> Fully qualified model name (schema.table) <code>sales.daily_sales</code> <code>kind</code> Materialization strategy (FULL, INCREMENTAL, VIEW, etc.) <code>FULL</code> <code>cron</code> Scheduling expression <code>'@daily'</code> <code>grain</code> Column(s) that define row uniqueness <code>order_date</code> <code>owner</code> Model owner for governance <code>analytics_team</code> <code>description</code> Human-readable description <code>'Daily sales aggregates'</code> <p>More DDL Properties</p> <p>For a complete list of all available model properties and their configurations, see the Model Properties reference .</p>"},{"location":"components/model/overview/#dml-the-select-query","title":"DML: The SELECT Query","text":"<p>The <code>SELECT</code> query is the DML component that contains the transformation logic. It defines what data is selected, how it's transformed, and what columns appear in the final output.</p>"},{"location":"components/model/overview/#dml-syntax","title":"DML Syntax","text":"<pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"components/model/overview/#conventions","title":"Conventions","text":"<p>Vulcan attempts to infer as much as possible about your pipelines to reduce the cognitive overhead of switching to another format such as YAML. The DML portion of a model must follow certain conventions for Vulcan to detect the necessary metadata.</p>"},{"location":"components/model/overview/#sql-model-conventions","title":"SQL Model Conventions","text":""},{"location":"components/model/overview/#unique-column-names","title":"Unique Column Names","text":"<p>The final <code>SELECT</code> of a model's query must contain unique column names.</p> <pre><code>-- \u2713 Good: Each column has a unique name\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre>"},{"location":"components/model/overview/#explicit-types","title":"Explicit Types","text":"<p>Vulcan encourages explicit type casting in the final <code>SELECT</code> of a model's query. It is considered a best practice to prevent unexpected types in the schema of a model's table.</p> <p>Vulcan uses the postgres <code>x::int</code> syntax for casting; the casts are automatically transpiled to the appropriate format for the execution engine.</p> <pre><code>-- Explicit casting ensures consistent schema\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,  -- explicit timestamp\n  COUNT(order_id)::INTEGER AS total_orders,                 -- explicit integer\n  SUM(total_amount)::FLOAT AS total_revenue,                -- explicit float\n  MAX(order_id)::VARCHAR AS last_order_id                   -- explicit varchar\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre>"},{"location":"components/model/overview/#inferrable-names","title":"Inferrable Names","text":"<p>The final <code>SELECT</code> of a model's query must have inferrable names or aliases.</p> <p>Explicit aliases are recommended, but not required. The Vulcan formatter will automatically add aliases to columns without them when the model SQL is rendered.</p> <pre><code>SELECT\n  1,                              -- not inferrable\n  total_amount + 1,               -- not inferrable\n  SUM(total_amount),              -- not inferrable\n  order_date,                     -- inferrable as order_date\n  order_date::TIMESTAMP,          -- inferrable as order_date\n  total_amount + 1 AS adjusted,   -- explicitly adjusted\n  SUM(total_amount) AS revenue    -- explicitly revenue\n</code></pre>"},{"location":"components/model/overview/#column-descriptions","title":"Column Descriptions","text":"<p>You can explicitly define column descriptions in the DDL using the <code>column_descriptions</code> property. This is the recommended approach for comprehensive documentation.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Aggregated daily sales metrics',\n  column_descriptions (\n    order_date = 'The date of the sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts for this date',\n    last_order_id = 'The most recent order ID for this date'\n  )\n);\n</code></pre> <p>Priority</p> <p>If <code>column_descriptions</code> is present in the DDL, Vulcan will use these descriptions and not detect inline comments from the query.</p>"},{"location":"components/model/overview/#inline-column-comments","title":"Inline Column Comments","text":"<p>If the <code>column_descriptions</code> key is not present in the DDL, Vulcan will automatically detect comments in a query's column selections and register each column's final comment in the underlying SQL engine.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  order_date::TIMESTAMP AS order_date,           -- The date of sales transactions\n  COUNT(order_id)::INTEGER AS total_orders,      -- Number of orders placed\n  SUM(total_amount)::FLOAT AS total_revenue,     -- Total revenue for the day\n  MAX(order_id)::VARCHAR AS last_order_id        -- Most recent order ID\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>The physical table created would have:</p> <ol> <li>Each inline comment registered as a column comment for the respective column</li> <li>If a comment exists before the <code>MODEL</code> block, it will be registered as the table comment</li> </ol>"},{"location":"components/model/overview/#python-model-conventions","title":"Python Model Conventions","text":""},{"location":"components/model/overview/#explicit-column-definitions","title":"Explicit Column Definitions","text":"<p>Python models require explicit column definitions in the <code>@model</code> decorator since types cannot be inferred from the code.</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(name=ModelKindName.FULL),\n)\n</code></pre>"},{"location":"components/model/overview/#explicit-dependencies","title":"Explicit Dependencies","text":"<p>Unlike SQL models where dependencies are automatically inferred, Python models must explicitly declare their dependencies using the <code>depends_on</code> parameter.</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={...},\n    depends_on=[\"raw.raw_orders\"],  # Must explicitly list upstream models\n)\n</code></pre>"},{"location":"components/model/overview/#column-descriptions_1","title":"Column Descriptions","text":"<p>Python models cannot use inline comments for column descriptions. Instead, specify them in the <code>@model</code> decorator's <code>column_descriptions</code> key.</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n    },\n    column_descriptions={\n        \"order_date\": \"The date of sales transactions\",\n        \"total_orders\": \"Number of orders placed on this date\",\n        \"total_revenue\": \"Total revenue for the day\",\n    },\n)\n</code></pre> <p>Column name validation</p> <p>Vulcan will error if a column name in <code>column_descriptions</code> is not also present in the <code>columns</code> key.</p>"},{"location":"components/model/overview/#return-type","title":"Return Type","text":"<p>The <code>execute</code> function must return a pandas DataFrame with columns matching the <code>columns</code> definition.</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:  # Must return a DataFrame\n    query = \"SELECT ...\"\n    return context.fetchdf(query)\n</code></pre> <p>Learn more</p> <p>For detailed information about Python models, see Python Models.</p>"},{"location":"components/model/overview/#comment-registration","title":"Comment Registration","text":""},{"location":"components/model/overview/#model-comment","title":"Model Comment","text":"<p>Vulcan will register a comment specified before the <code>MODEL</code> DDL block as the table comment in the underlying SQL engine. If the DDL <code>description</code> field is also specified, Vulcan will register it with the engine instead.</p>"},{"location":"components/model/overview/#comment-registration-by-object-type","title":"Comment Registration by Object Type","text":"<p>Only some tables/views have comments registered:</p> <ul> <li>Temporary tables are not registered</li> <li>Non-temporary tables and views in the physical layer (i.e., the schema named <code>vulcan__[project schema name]</code>) are registered</li> <li>Views in non-prod environments are not registered</li> <li>Views in the <code>prod</code> environment are registered</li> </ul> <p>Some engines automatically pass comments from physical tables through to views that select from them. In those engines, views may display comments even if Vulcan did not explicitly register them.</p>"},{"location":"components/model/overview/#engine-comment-support","title":"Engine Comment Support","text":"<p>Engines vary in their support for comments and their method(s) of registering comments. Engines may support one or both registration methods: in the <code>CREATE</code> command that creates the object or with specific post-creation commands.</p> <p>In the former method, column comments are embedded in the <code>CREATE</code> schema definition - for example: <code>CREATE TABLE my_table (my_col INTEGER COMMENT 'comment on my_col') COMMENT 'comment on my_table'</code>. This means that all table and column comments can be registered in a single command.</p> <p>In the latter method, separate commands are required for every comment. This may result in many commands: one for the table comment and one for each column comment.</p> Engine <code>TABLE</code> comments <code>VIEW</code> comments Athena N N BigQuery Y Y ClickHouse Y Y Databricks Y Y DuckDB &lt;=0.9 N N DuckDB &gt;=0.10 Y Y MySQL Y Y MSSQL N N Postgres Y Y GCP Postgres Y Y Redshift Y N Snowflake Y Y Spark Y Y Trino Y Y"},{"location":"components/model/overview/#macros","title":"Macros","text":"<p>Macros can be used for passing in parameterized arguments such as dates, as well as for making SQL less repetitive. By default, Vulcan provides several predefined macro variables that can be used. Macros are used by prefixing with the <code>@</code> symbol. For more information, refer to macros.</p>"},{"location":"components/model/properties/","title":"Properties","text":""},{"location":"components/model/properties/#properties","title":"Properties","text":"<p>The <code>MODEL</code> DDL statement accepts various properties that control model metadata and behavior. This page provides a complete reference for all available model properties.</p>"},{"location":"components/model/properties/#quick-reference","title":"Quick Reference","text":"Property Description Type Required <code>name</code> Fully qualified model name (<code>schema.model</code> or <code>catalog.schema.model</code>) <code>str</code> N* <code>project</code> Project name for multi-repo deployments <code>str</code> N <code>kind</code> Model kind (VIEW, FULL, INCREMENTAL, etc.) <code>str</code> | <code>dict</code> N <code>cron</code> Schedule expression for model refresh <code>str</code> N <code>cron_tz</code> Timezone for the cron schedule <code>str</code> N <code>interval_unit</code> Temporal granularity of data intervals <code>str</code> N <code>start</code> Earliest date/time to process <code>str</code> | <code>int</code> N <code>end</code> Latest date/time to process <code>str</code> | <code>int</code> N <code>grain</code> Column(s) defining row uniqueness <code>str</code> | <code>array</code> N <code>grains</code> Multiple unique key definitions <code>array</code> N <code>owner</code> Model owner for governance <code>str</code> N <code>description</code> Model description (registered as table comment) <code>str</code> N <code>column_descriptions</code> Column-level comments <code>dict</code> N <code>columns</code> Explicit column names and types <code>array</code> N <code>dialect</code> SQL dialect of the model <code>str</code> N <code>tags</code> Labels for organizing models <code>array[str]</code> N <code>assertions</code> Audits to run after model evaluation <code>array</code> N <code>profiles</code> Columns to track statistical metrics over time <code>array</code> N <code>depends_on</code> Explicit model dependencies <code>array[str]</code> N <code>references</code> Non-unique join relationship columns <code>array</code> N <code>partitioned_by</code> Partition key column(s) <code>str</code> | <code>array</code> N <code>clustered_by</code> Clustering column(s) <code>str</code> N <code>table_format</code> Table format (iceberg, hive, delta) <code>str</code> N <code>storage_format</code> Storage format (parquet, orc) <code>str</code> N <code>physical_properties</code> Engine-specific table/view properties <code>dict</code> N <code>virtual_properties</code> Engine-specific view layer properties <code>dict</code> N <code>session_properties</code> Engine session properties <code>dict</code> N <code>stamp</code> Arbitrary version string <code>str</code> N <code>enabled</code> Whether model is enabled <code>bool</code> N <code>allow_partials</code> Allow partial data intervals <code>bool</code> N <code>gateway</code> Specific gateway for execution <code>str</code> N <code>optimize_query</code> Enable query optimization <code>bool</code> N <code>formatting</code> Enable model formatting <code>bool</code> N <code>ignored_rules</code> Linter rules to ignore <code>str</code> | <code>array</code> N <p>*Required unless name inference is enabled.</p>"},{"location":"components/model/properties/#general-properties","title":"General Properties","text":""},{"location":"components/model/properties/#name","title":"name","text":"<p>The model name, representing the production view/table name. Must include at least a qualifying schema (<code>schema.model</code>) and may include a catalog (<code>catalog.schema.model</code>).</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,        -- schema.model format\n);\n\n-- Or with catalog\nMODEL (\n  name catalog.sales.daily_sales -- catalog.schema.model format\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",  # schema.model format\n)\ndef execute(context, **kwargs):\n    ...\n\n# Or with catalog\n@model(\n    \"catalog.sales.daily_sales\",  # catalog.schema.model format\n)\n</code></pre> <p>Environment Prefixing</p> <p>In non-production environments, Vulcan automatically prefixes names. For example, <code>sales.daily_sales</code> becomes <code>sales__dev.daily_sales</code> in dev.</p>"},{"location":"components/model/properties/#project","title":"project","text":"<p>Specifies the project name for multi-repo Vulcan deployments.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  project 'analytics_project',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    project=\"analytics_project\",\n)\n</code></pre>"},{"location":"components/model/properties/#kind","title":"kind","text":"<p>Determines how the model is computed and stored. See Model Kinds for details.</p> SQLPython <pre><code>-- VIEW (default for SQL)\nMODEL (\n  name sales.daily_sales,\n  kind VIEW,\n);\n\n-- FULL\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n);\n\n-- Incremental with properties\nMODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n  ),\n);\n\n-- SEED\nMODEL (\n  name raw.holidays,\n  kind SEED (\n    path 'seeds/holidays.csv',\n  ),\n);\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n# FULL (default for Python)\n@model(\n    \"sales.daily_sales\",\n    kind=dict(name=ModelKindName.FULL),\n)\n\n# Incremental\n@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n    ),\n)\n\n# SCD Type 2\n@model(\n    \"dim.customers\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#cron","title":"cron","text":"<p>Schedules when the model processes or refreshes data. Accepts cron expressions or shortcuts.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',          -- Daily at midnight UTC\n);\n\nMODEL (\n  name sales.hourly_metrics,\n  cron '@hourly',         -- Every hour\n);\n\nMODEL (\n  name sales.custom_schedule,\n  cron '0 6 * * *',       -- Custom: every day at 6 AM UTC\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n)\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"@hourly\",\n)\n\n@model(\n    \"sales.custom_schedule\",\n    cron=\"0 6 * * *\",  # Every day at 6 AM UTC\n)\n</code></pre> <p>Cron shortcuts: <code>@hourly</code>, <code>@daily</code>, <code>@weekly</code>, <code>@monthly</code></p>"},{"location":"components/model/properties/#cron_tz","title":"cron_tz","text":"<p>Specifies the timezone for the cron schedule. Only affects scheduling, not the interval boundaries passed to incremental models.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',\n  cron_tz 'America/Los_Angeles',  -- Runs at midnight Pacific time\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n    cron_tz=\"America/Los_Angeles\",\n)\n</code></pre>"},{"location":"components/model/properties/#interval_unit","title":"interval_unit","text":"<p>Determines the temporal granularity for calculating time intervals. By default, inferred from the <code>cron</code> expression.</p> <p>Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code></p> SQLPython <pre><code>MODEL (\n  name sales.hourly_metrics,\n  cron '30 7 * * *',      -- Run daily at 7:30 AM\n  interval_unit 'hour',   -- Process hourly intervals (not daily)\n  );\n</code></pre> <pre><code>from vulcan import IntervalUnit\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"30 7 * * *\",\n    interval_unit=IntervalUnit.HOUR,\n)\n</code></pre>"},{"location":"components/model/properties/#start","title":"start","text":"<p>The earliest date/time for processing. Accepts absolute dates, epoch milliseconds, or relative expressions.</p> SQLPython <pre><code>-- Absolute date\nMODEL (\n  name sales.daily_sales,\n  start '2024-01-01',\n);\n\n-- Relative expression\nMODEL (\n  name sales.recent_sales,\n  start '1 year ago',\n);\n\n-- Epoch milliseconds\nMODEL (\n  name sales.events,\n  start 1704067200000,\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    start=\"2024-01-01\",\n)\n\n@model(\n    \"sales.recent_sales\",\n    start=\"1 year ago\",\n)\n</code></pre>"},{"location":"components/model/properties/#end","title":"end","text":"<p>The latest date/time for processing. Same format as <code>start</code>.</p> SQLPython <pre><code>MODEL (\n  name sales.historical_sales,\n  start '2020-01-01',\n  end '2023-12-31',\n  );\n</code></pre> <pre><code>@model(\n    \"sales.historical_sales\",\n    start=\"2020-01-01\",\n    end=\"2023-12-31\",\n)\n</code></pre>"},{"location":"components/model/properties/#grain-grains","title":"grain / grains","text":"<p>Defines the column(s) that uniquely identify each row. Used by tools like <code>table_diff</code>.</p> SQLPython <pre><code>-- Single column grain\nMODEL (\n  name sales.daily_sales,\n  grain order_date,\n);\n\n-- Composite grain\nMODEL (\n  name sales.customer_daily,\n  grain (customer_id, order_date),\n);\n\n-- Multiple grains\nMODEL (\n  name sales.orders,\n  grains (\n    order_id,\n    (customer_id, order_date)\n  ),\n);\n</code></pre> <pre><code># Single grain\n@model(\n    \"sales.daily_sales\",\n    grains=[\"order_date\"],\n)\n\n# Composite grain\n@model(\n    \"sales.customer_daily\",\n    grains=[(\"customer_id\", \"order_date\")],\n)\n\n# Multiple grains\n@model(\n    \"sales.orders\",\n    grains=[\n        \"order_id\",\n        (\"customer_id\", \"order_date\"),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#owner","title":"owner","text":"<p>Specifies the main point of contact for governance and notifications.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  owner 'analytics_team',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    owner=\"analytics_team\",\n)\n</code></pre>"},{"location":"components/model/properties/#description","title":"description","text":"<p>Model description automatically registered as a table comment in the SQL engine (if supported).</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  description 'Aggregated daily sales metrics including total orders and revenue',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    description=\"Aggregated daily sales metrics including total orders and revenue\",\n)\n</code></pre>"},{"location":"components/model/properties/#column_descriptions","title":"column_descriptions","text":"<p>Explicit column descriptions registered as column comments.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  column_descriptions (\n    order_date = 'The date of sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts',\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n    },\n    column_descriptions={\n        \"order_date\": \"The date of sales transactions\",\n        \"total_orders\": \"Count of orders placed on this date\",\n        \"total_revenue\": \"Sum of all order amounts\",\n    },\n)\n</code></pre> <p>Priority</p> <p>If <code>column_descriptions</code> is present, inline column comments will not be registered.</p>"},{"location":"components/model/properties/#columns","title":"columns","text":"<p>Explicitly specifies column names and data types, disabling automatic inference.</p> SQLPython <pre><code>MODEL (\n  name sales.national_holidays,\n  kind SEED (path 'holidays.csv'),\n  columns (\n    holiday_name VARCHAR,\n    holiday_date DATE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n)\ndef execute(context, **kwargs) -&gt; pd.DataFrame:\n    ...\n</code></pre> <p>Python Models</p> <p>Required for Python models that return DataFrames.</p>"},{"location":"components/model/properties/#dialect","title":"dialect","text":"<p>The SQL dialect of the model. Defaults to the project's <code>model_defaults</code> dialect. Supports all SQLGlot dialects.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  dialect 'snowflake',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    dialect=\"snowflake\",\n)\n</code></pre>"},{"location":"components/model/properties/#tags","title":"tags","text":"<p>Labels for organizing and filtering models.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  tags ['sales', 'daily', 'core'],\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    tags=[\"sales\", \"daily\", \"core\"],\n)\n</code></pre>"},{"location":"components/model/properties/#assertions","title":"assertions","text":"<p>Attaching audits to the model, declaring that these validations should pass after each model evaluation.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    not_null(columns := (order_date, customer_id)),\n    unique_values(columns := (order_id)),\n    accepted_range(column := price, min_v := 0, max_v := 1000),\n    forall(criteria := (price &gt; 0, quantity &gt;= 1))\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    assertions=[\n        (\"not_null\", {\"columns\": [\"order_date\", \"customer_id\"]}),\n        (\"unique_values\", {\"columns\": [\"order_id\"]}),\n        (\"accepted_range\", {\"column\": \"price\", \"min_v\": 0, \"max_v\": 1000}),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#profiles","title":"profiles","text":"<p>Specifies columns for which statistical metrics should be tracked over time. Profiles provide observational data about your columns\u2014tracking distributions, null percentages, and patterns\u2014without blocking pipeline execution.</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  grains (customer_id),\n  profiles (customer_id, customer_name, email, total_orders, total_spent, avg_order_value)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"avg_order_value\": \"decimal(10,2)\",\n    },\n    kind=\"full\",\n    grains=[\"customer_id\"],\n    profiles=[\"customer_id\", \"customer_name\", \"email\", \"total_orders\", \"total_spent\", \"avg_order_value\"],\n)\ndef execute(context, **kwargs):\n    ...\n</code></pre> <p>Profile results are stored in the <code>_check_profiles</code> table and can be used to:</p> <ul> <li>Detect data drift over time</li> <li>Understand column distributions</li> <li>Inform which audits or checks to add</li> <li>Build data quality dashboards</li> </ul>"},{"location":"components/model/properties/#depends_on","title":"depends_on","text":"<p>Explicitly declares dependencies in addition to those inferred from the query.</p> SQLPython <pre><code>MODEL (\n  name sales.summary,\n  depends_on ['sales.daily_sales', 'sales.products'],\n);\n</code></pre> <pre><code>@model(\n    \"sales.summary\",\n    depends_on=[\"sales.daily_sales\", \"sales.products\"],\n)\n</code></pre> <p>Python Models</p> <p>Python models require <code>depends_on</code> since dependencies cannot be automatically inferred from the code.</p>"},{"location":"components/model/properties/#references","title":"references","text":"<p>Non-unique columns that define join relationships to other models.</p> SQLPython <pre><code>MODEL (\n  name sales.orders,\n  references (\n    customer_id,\n    guest_id AS account_id,  -- Alias for joining to account_id grain\n  ),\n);\n</code></pre> <pre><code>@model(\n    \"sales.orders\",\n    references=[\n        \"customer_id\",\n        (\"guest_id\", \"account_id\"),  # Alias\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#storage-properties","title":"Storage Properties","text":""},{"location":"components/model/properties/#partitioned_by","title":"partitioned_by","text":"<p>Defines partition key for engines supporting table partitioning (Spark, BigQuery, etc.).</p> SQLPython <pre><code>-- Single column partition\nMODEL (\n  name sales.events,\n  partitioned_by event_date,\n);\n\n-- Partition with transformation (BigQuery)\nMODEL (\n  name sales.events,\n  partitioned_by TIMESTAMP_TRUNC(event_ts, DAY),\n);\n\n-- Multi-column partition\nMODEL (\n  name sales.events,\n  partitioned_by (year, month, day),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n)\n\n# Multi-column\n@model(\n    \"sales.events\",\n    partitioned_by=[\"year\", \"month\", \"day\"],\n)\n</code></pre>"},{"location":"components/model/properties/#clustered_by","title":"clustered_by","text":"<p>Clustering column(s) for engines supporting clustering (BigQuery, etc.).</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  partitioned_by event_date,\n  clustered_by (customer_id, product_id),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n    clustered_by=[\"customer_id\", \"product_id\"],\n)\n</code></pre>"},{"location":"components/model/properties/#table_format","title":"table_format","text":"<p>Table format for engines supporting multiple formats: <code>iceberg</code>, <code>hive</code>, <code>delta</code>.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  table_format 'iceberg',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    table_format=\"iceberg\",\n)\n</code></pre>"},{"location":"components/model/properties/#storage_format","title":"storage_format","text":"<p>Physical file format: <code>parquet</code>, <code>orc</code>, etc.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  storage_format 'parquet',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    storage_format=\"parquet\",\n)\n</code></pre>"},{"location":"components/model/properties/#engine-properties","title":"Engine Properties","text":""},{"location":"components/model/properties/#physical_properties","title":"physical_properties","text":"<p>Engine-specific properties applied to the physical table/view.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  physical_properties (\n    partition_expiration_days = 7,\n    require_partition_filter = true,\n    creatable_type = TRANSIENT,  -- Creates TRANSIENT TABLE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    physical_properties={\n        \"partition_expiration_days\": 7,\n        \"require_partition_filter\": True,\n        \"creatable_type\": \"TRANSIENT\",\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#virtual_properties","title":"virtual_properties","text":"<p>Engine-specific properties applied to the virtual layer view.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  virtual_properties (\n    creatable_type = SECURE,  -- Creates SECURE VIEW\n    labels = [('team', 'analytics')]\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    virtual_properties={\n        \"creatable_type\": \"SECURE\",\n        \"labels\": [(\"team\", \"analytics\")],\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#session_properties","title":"session_properties","text":"<p>Engine-specific session properties applied during execution.</p> SQLPython <pre><code>MODEL (\n  name sales.large_query,\n  session_properties (\n    query_timeout = 3600,\n    max_parallel_workers = 8,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.large_query\",\n    session_properties={\n        \"query_timeout\": 3600,\n        \"max_parallel_workers\": 8,\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#gateway","title":"gateway","text":"<p>Specifies a specific gateway for model execution (when not using the default).</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  gateway 'warehouse_gateway',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    gateway=\"warehouse_gateway\",\n)\n</code></pre>"},{"location":"components/model/properties/#behavior-properties","title":"Behavior Properties","text":""},{"location":"components/model/properties/#stamp","title":"stamp","text":"<p>Arbitrary string to create a new model version without changing the definition.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  stamp 'v2.1.0',  -- Force new version\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    stamp=\"v2.1.0\",\n)\n</code></pre>"},{"location":"components/model/properties/#enabled","title":"enabled","text":"<p>Whether the model is active. Set to <code>false</code> to ignore during project loading. (Default: <code>true</code>)</p> SQLPython <pre><code>MODEL (\n  name sales.deprecated_model,\n  enabled false,  -- Model will be ignored\n);\n</code></pre> <pre><code>@model(\n    \"sales.deprecated_model\",\n    enabled=False,\n)\n</code></pre>"},{"location":"components/model/properties/#allow_partials","title":"allow_partials","text":"<p>Allows processing of incomplete data intervals. (Default: <code>false</code>)</p> SQLPython <pre><code>MODEL (\n  name sales.realtime_events,\n  cron '@hourly',\n  allow_partials true,  -- Process incomplete intervals\n);\n</code></pre> <pre><code>@model(\n    \"sales.realtime_events\",\n    cron=\"@hourly\",\n    allow_partials=True,\n)\n</code></pre> <p>Use with caution</p> <p>When enabled, you cannot distinguish between missing data due to pipeline issues vs. partial backfills.</p>"},{"location":"components/model/properties/#optimize_query","title":"optimize_query","text":"<p>Whether to optimize the model's query. (Default: <code>true</code>)</p> SQLPython <pre><code>MODEL (\n  name sales.complex_query,\n  optimize_query false,  -- Disable optimization\n);\n</code></pre> <pre><code>@model(\n    \"sales.complex_query\",\n    optimize_query=False,\n)\n</code></pre>"},{"location":"components/model/properties/#formatting","title":"formatting","text":"<p>Whether the model is formatted during <code>vulcan format</code>. (Default: <code>true</code>)</p> SQLPython <pre><code>MODEL (\n  name sales.legacy_model,\n  formatting false,  -- Skip formatting\n);\n</code></pre> <pre><code>@model(\n    \"sales.legacy_model\",\n    formatting=False,\n)\n</code></pre>"},{"location":"components/model/properties/#ignored_rules","title":"ignored_rules","text":"<p>Linter rules to ignore for this model.</p> SQLPython <pre><code>-- Ignore specific rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules ['rule_name', 'another_rule'],\n);\n\n-- Ignore all rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules 'ALL',\n);\n</code></pre> <pre><code># Ignore specific rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=[\"rule_name\", \"another_rule\"],\n)\n\n# Ignore all rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=\"ALL\",\n)\n</code></pre>"},{"location":"components/model/properties/#incremental-model-properties","title":"Incremental Model Properties","text":"<p>Properties specified within the <code>kind</code> definition for incremental models. See Model Kinds for detailed documentation.</p>"},{"location":"components/model/properties/#common-incremental-properties","title":"Common Incremental Properties","text":"<p>These properties apply to all incremental model kinds.</p> Property Description Type Default <code>forward_only</code> All changes should be forward-only <code>bool</code> <code>false</code> <code>on_destructive_change</code> Behavior for destructive schema changes <code>str</code> <code>error</code> <code>on_additive_change</code> Behavior for additive schema changes <code>str</code> <code>allow</code> <code>disable_restatement</code> Disable data restatement <code>bool</code> <code>false</code> <code>auto_restatement_cron</code> Cron expression for automatic restatement <code>str</code> - <p>Values for <code>on_destructive_change</code> / <code>on_additive_change</code>: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code></p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    forward_only true,\n    on_destructive_change 'warn',\n    on_additive_change 'allow',\n    disable_restatement false,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        forward_only=True,\n        on_destructive_change=\"warn\",\n        on_additive_change=\"allow\",\n        disable_restatement=False,\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p>Incrementally updates data based on a time column. See INCREMENTAL_BY_TIME_RANGE.</p> Property Description Type Required <code>time_column</code> Column containing each row's timestamp (should be UTC) <code>str</code> Y <code>format</code> Format of the time column's data <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>batch_concurrency</code> Maximum concurrent batches <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N <code>auto_restatement_intervals</code> Number of last intervals to auto-restate <code>int</code> N SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    time_column (event_ts, '%Y-%m-%d'),  -- With format\n    batch_size 12,\n    batch_concurrency 4,\n    lookback 7,\n    auto_restatement_cron '@weekly',\n    auto_restatement_intervals 7,\n  )\n);\n\nSELECT\n  event_ts::TIMESTAMP AS event_ts,\n  event_type::VARCHAR AS event_type,\n  user_id::INTEGER AS user_id\nFROM raw.events\nWHERE event_ts BETWEEN @start_ts AND @end_ts;\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n@model(\n    \"sales.events\",\n    columns={\n        \"event_ts\": \"timestamp\",\n        \"event_type\": \"varchar\",\n        \"user_id\": \"int\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        batch_size=12,\n        batch_concurrency=4,\n        lookback=7,\n    ),\n    depends_on=[\"raw.events\"],\n)\ndef execute(context, start, end, **kwargs) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT event_ts, event_type, user_id\n    FROM raw.events\n    WHERE event_ts BETWEEN '{start}' AND '{end}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Important</p> <p>The <code>time_column</code> should be in UTC timezone.</p>"},{"location":"components/model/properties/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p>Incrementally updates data based on a unique key using MERGE operations. See INCREMENTAL_BY_UNIQUE_KEY.</p> Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>str</code> | <code>array</code> Y <code>when_matched</code> SQL logic to update columns on match (MERGE engines only) <code>str</code> N <code>merge_filter</code> Predicates for ON clause of MERGE operation <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N SQLPython <pre><code>-- Single unique key\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n  )\n);\n\n-- Composite unique key\nMODEL (\n  name sales.order_items,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, item_id),\n  )\n);\n\n-- With MERGE options\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched WHEN MATCHED THEN UPDATE SET \n      name = source.name,\n      updated_at = source.updated_at,\n    auto_restatement_cron '@weekly',\n  )\n);\n</code></pre> <pre><code># Single unique key\n@model(\n    \"sales.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=\"customer_id\",\n    ),\n    depends_on=[\"raw.customers\"],\n)\n\n# Composite unique key\n@model(\n    \"sales.order_items\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"order_id\", \"item_id\"],\n    ),\n)\n</code></pre> <p>Batch Concurrency</p> <p><code>batch_concurrency</code> cannot be set for this kind because these models cannot safely run in parallel.</p>"},{"location":"components/model/properties/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Incrementally updates data based on partition key. See INCREMENTAL_BY_PARTITION.</p> <p>This kind uses the <code>partitioned_by</code> general property as its partition key and does not have additional kind-specific properties.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by event_date,\n);\n\nSELECT\n  event_date::DATE AS event_date,\n  event_type::VARCHAR AS event_type,\n  COUNT(*)::INTEGER AS event_count\nFROM raw.events\nGROUP BY event_date, event_type;\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    columns={\n        \"event_date\": \"date\",\n        \"event_type\": \"varchar\",\n        \"event_count\": \"int\",\n    },\n    kind=dict(name=ModelKindName.INCREMENTAL_BY_PARTITION),\n    partitioned_by=[\"event_date\"],\n    depends_on=[\"raw.events\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2","title":"SCD_TYPE_2","text":"<p>Slowly Changing Dimension Type 2 models track historical changes. See SCD_TYPE_2.</p>"},{"location":"components/model/properties/#common-scd-type-2-properties","title":"Common SCD Type 2 Properties","text":"Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>array</code> Y <code>valid_from_name</code> Column for valid from date <code>str</code> N (default: <code>valid_from</code>) <code>valid_to_name</code> Column for valid to date <code>str</code> N (default: <code>valid_to</code>) <code>invalidate_hard_deletes</code> Mark missing records as invalid <code>bool</code> N (default: <code>true</code>)"},{"location":"components/model/properties/#scd_type_2_by_time","title":"SCD_TYPE_2_BY_TIME","text":"<p>Tracks changes based on an <code>updated_at</code> timestamp column.</p> Property Description Type Required <code>updated_at_name</code> Column containing updated at date <code>str</code> N (default: <code>updated_at</code>) <code>updated_at_as_valid_from</code> Use <code>updated_at</code> value as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>MODEL (\n  name dim.customers,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key customer_id,\n    updated_at_name last_modified,\n    valid_from_name effective_from,\n    valid_to_name effective_to,\n    invalidate_hard_deletes true,\n    updated_at_as_valid_from true,\n  )\n);\n\nSELECT\n  customer_id::INTEGER AS customer_id,\n  name::VARCHAR AS name,\n  email::VARCHAR AS email,\n  last_modified::TIMESTAMP AS last_modified\nFROM raw.customers;\n</code></pre> <pre><code>@model(\n    \"dim.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"last_modified\": \"timestamp\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n        updated_at_name=\"last_modified\",\n        valid_from_name=\"effective_from\",\n        valid_to_name=\"effective_to\",\n        invalidate_hard_deletes=True,\n    ),\n    depends_on=[\"raw.customers\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2_by_column","title":"SCD_TYPE_2_BY_COLUMN","text":"<p>Tracks changes by comparing column values (no <code>updated_at</code> column needed).</p> Property Description Type Required <code>columns</code> Columns to check for changes (<code>*</code> for all) <code>str</code> | <code>array</code> Y <code>execution_time_as_valid_from</code> Use execution time as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>-- Track specific columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns (name, price, category),\n    execution_time_as_valid_from true,\n  )\n);\n\n-- Track all columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns '*',\n  )\n);\n</code></pre> <pre><code># Track specific columns\n@model(\n    \"dim.products\",\n    columns={\n        \"product_id\": \"int\",\n        \"name\": \"varchar\",\n        \"price\": \"decimal(10,2)\",\n        \"category\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"name\", \"price\", \"category\"],\n        execution_time_as_valid_from=True,\n    ),\n    depends_on=[\"raw.products\"],\n)\n\n# Track all columns\n@model(\n    \"dim.products\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=\"*\",\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#model-naming","title":"Model Naming","text":"<p>Enable automatic name inference from directory structure:</p> <pre><code>model_defaults:\n  dialect: snowflake\n\n# Enable name inference\ninfer_names: true\n</code></pre> <p>With <code>infer_names: true</code>, a model at <code>models/sales/daily_sales.sql</code> automatically gets the name <code>sales.daily_sales</code>.</p>"},{"location":"components/model/statements/","title":"Statements","text":""},{"location":"components/model/statements/#statements","title":"Statements","text":"<p>Statements are auxiliary SQL commands that extend a model's capabilities beyond its main query. They allow you to execute custom logic at specific points during model execution\u2014before the query runs (pre-statements), after the query completes (post-statements), or when views are created/updated (on-virtual-update). Common use cases include setting session parameters, loading UDFs, creating indexes, enforcing data quality checks, logging anomalies, and granting access permissions. Statements can be defined at the model level for specific needs or at the project level via <code>model_defaults</code> for consistent behavior across all models.</p> <p>Statement types:</p> <ul> <li>Pre-statements: Run before the main model query</li> <li>Post-statements: Run after the main model query</li> <li>On-virtual-update statements: Run when a virtual update occurs (view creation/update)</li> </ul> <p>Concurrency considerations</p> <p>Pre-statements should generally only be used for preparing the main query. Avoid creating or altering physical tables in pre-statements, as this could lead to unpredictable behavior if multiple models run concurrently.</p>"},{"location":"components/model/statements/#model-defaults","title":"Model defaults","text":"<p>You can define statements at the project level using <code>model_defaults</code> in your configuration. These default statements are merged with any model-specific statements, with default statements executing first, followed by model-specific statements.</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  pre_statements:\n    - \"SET query_timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre>"},{"location":"components/model/statements/#pre-statements","title":"Pre-statements","text":"<p>Pre-statements run before the main model query. They are useful for:</p> <ul> <li>Loading JARs or UDFs</li> <li>Creating temporary tables or caching data</li> <li>Setting session parameters</li> <li>Initializing variables</li> </ul> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily'\n);\n\n/* Pre-statement: Create table for anomaly tracking */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* Pre-statement: Set session variables using Jinja */\nJINJA_STATEMENT_BEGIN;\n{% if start_date is none or end_date is none %}\n  SET start_date = DATE '{{ start }}';\n  SET end_date = CURRENT_DATE;\n{% endif %}\nJINJA_END;\n\n/* Main model query */\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    pre_statements=[\n        \"SET query_timeout = 300000\",\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR\n        )\"\"\",\n        exp.Cache(this=exp.table_(\"orders_cache\"), expression=exp.select(\"*\").from_(\"demo.raw_data.orders\")),\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#post-statements","title":"Post-statements","text":"<p>Post-statements run after each evaluation of the model query. The model query must end with a semicolon when post-statements are present in SQL models.</p> <p>Post-statements are useful for:</p> <ul> <li>Creating indexes or clustering</li> <li>Running data quality checks</li> <li>Inserting anomalies into tracking tables</li> <li>Conditional table alterations</li> </ul> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  )\n);\n\nSELECT\n  order_id,\n  order_date,\n  customer_id,\n  quantity,\n  unit_price,\n  total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* Post-statement: Conditional retention policy (only on table creation) */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Post-statement: Add clustering for query performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Post-statement: Capture data anomalies - negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'NEGATIVE_QUANTITY',\n  CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Post-statement: Capture data anomalies - total mismatch */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calculated=', ROUND(unit_price * quantity, 2),\n    '; actual=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS((unit_price * quantity) - total_amount) &gt; 0.01;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#on-virtual-update-statements","title":"On-virtual-update statements","text":"<p>On-virtual-update statements run when a virtual update occurs (when views are created or updated). They are useful for:</p> <ul> <li>Granting permissions on views</li> <li>Setting up access controls</li> <li>Applying column masking policies</li> </ul> SQLPython <p>Use <code>ON_VIRTUAL_UPDATE_BEGIN</code> and <code>ON_VIRTUAL_UPDATE_END</code> to define these statements:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  )\n);\n\nSELECT\n  customer_id,\n  full_name,\n  email,\n  customer_segment\nFROM demo.raw_data.customers;\n\n/* Post-statement: Apply masking policy */\nJINJA_STATEMENT_BEGIN;\nALTER TABLE {{ this_model }} MODIFY COLUMN email SET MASKING POLICY demo.security.mask_email_policy;\nJINJA_END;\n\n/* On-virtual-update: Grant permissions when view is created/updated */\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Use the <code>on_virtual_update</code> argument in the <code>@model</code> decorator:</p> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.customers_py\",\n    columns={\n        \"customer_id\": \"varchar\",\n        \"full_name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"customer_segment\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 7)\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT customer_id, CONCAT(first_name, ' ', last_name) AS full_name,\n           email, customer_segment\n    FROM demo.raw_data.customers\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#complete-example","title":"Complete example","text":"<p>Here's a complete example showing all statement types:</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grains (order_id),\n  description 'Orders fact table with incremental loading'\n);\n\n/* ============ PRE-STATEMENTS ============ */\n\n/* Create anomaly tracking table */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* ============ MAIN QUERY ============ */\n\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  product_id::VARCHAR AS product_id,\n  quantity::INT AS quantity,\n  unit_price::FLOAT AS unit_price,\n  discount::FLOAT AS discount,\n  tax::FLOAT AS tax,\n  shipping_cost::FLOAT AS shipping_cost,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* ============ POST-STATEMENTS ============ */\n\n/* Conditional: Set retention only on table creation */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Add clustering for performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Data quality: Log negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Data quality: Log total mismatches */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calc=', ROUND(unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0), 2),\n    '; total=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS(\n  (unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0))\n  - total_amount\n) &gt; 0.01;\n\n/* ============ ON-VIRTUAL-UPDATE ============ */\n\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"product_id\": \"varchar\",\n        \"quantity\": \"int\",\n        \"unit_price\": \"float\",\n        \"discount\": \"float\",\n        \"tax\": \"float\",\n        \"shipping_cost\": \"float\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_id\"],\n    depends_on=[\"demo.raw_data.orders\"],\n    description=\"Orders fact table with incremental loading\",\n    pre_statements=[\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR,\n            captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\"\"\",\n    ],\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT\n        order_id, order_date, customer_id, product_id,\n        quantity, unit_price, discount, tax, shipping_cost, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#useful-macros-and-variables","title":"Useful macros and variables","text":"Macro/Variable Description <code>@this_model</code> References the current model's table/view <code>@runtime_stage</code> Current execution stage: <code>'creating'</code>, <code>'evaluating'</code>, or <code>'testing'</code> <code>@IF(condition, statement)</code> Conditionally execute a statement <code>@start_date</code>, <code>@end_date</code> Time range macros for incremental models <code>{{ this_model }}</code> Jinja equivalent of <code>@this_model</code> <p>For more information on macros, see the Macro Variables documentation.</p>"},{"location":"components/model/types/external_models/","title":"External models","text":""},{"location":"components/model/types/external_models/#external-models","title":"External models","text":"<p>Vulcan model queries may reference \"external\" tables that are created and managed outside the Vulcan project. For example, a model might ingest data from a third party's read-only data system.</p> <p>Vulcan does not manage external tables, but it can use information about the tables' columns and data types to make features more useful. For example, column information allows column-level lineage to include external tables' columns.</p> <p>Vulcan stores external tables' column information as <code>EXTERNAL</code> models.</p>"},{"location":"components/model/types/external_models/#how-external-models-work","title":"How external models work","text":"<p><code>EXTERNAL</code> models consist solely of an external table's column information, so there is no query for Vulcan to run.</p> <p>Vulcan has no information about the data contained in the table represented by an <code>EXTERNAL</code> model. The table could be altered or have all its data deleted, and Vulcan will not detect it. All Vulcan knows about the table is that it contains the columns specified in the <code>EXTERNAL</code> model's file (more information below).</p> <p>Vulcan will not take any actions based on an <code>EXTERNAL</code> model - its actions are solely determined by the model whose query selects from the <code>EXTERNAL</code> model.</p> <p>The querying model's <code>kind</code>, <code>cron</code>, and previously loaded time intervals determine when Vulcan will query the <code>EXTERNAL</code> model.</p>"},{"location":"components/model/types/external_models/#generating-an-external-models-schema-file","title":"Generating an external models schema file","text":"<p>External models can be defined in the <code>external_models.yaml</code> file in the Vulcan project's root folder. The alternative name for this file is <code>schema.yaml</code>.</p> <p>You can create this file by either writing the YAML by hand or allowing Vulcan to fetch information about external tables with the <code>create_external_models</code> CLI command.</p> <p>Consider this example model that queries external tables <code>vulcan_demo.customers</code>, <code>vulcan_demo.orders</code>, and <code>vulcan_demo.order_items</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <p>The following sections demonstrate how to create external models containing the column information for these tables.</p> <p>All of a Vulcan project's external models are defined in a single <code>external_models.yaml</code> file, so the files created below might also include column information for other external models.</p> <p>Alternatively, additional external models can also be defined in the external_models/ folder.</p>"},{"location":"components/model/types/external_models/#using-cli","title":"Using CLI","text":"<p>Instead of creating the <code>external_models.yaml</code> file manually, Vulcan can generate it for you with the create_external_models CLI command.</p> <p>The command identifies all external tables referenced in your Vulcan project, fetches their column information from the SQL engine's metadata, and then stores the information in the <code>external_models.yaml</code> file.</p> <p>If Vulcan does not have access to an external table's metadata, the table will be omitted from the file and Vulcan will issue a warning.</p> <p><code>create_external_models</code> solely queries SQL engine metadata and does not query external tables themselves.</p>"},{"location":"components/model/types/external_models/#gateway-specific-external-models","title":"Gateway-specific external models","text":"<p>In some use-cases such as isolated systems with multiple gateways, there are external models that only exist on a certain gateway.</p> <p>Gateway names are case-insensitive in external model configurations. You can specify the gateway name using any case (e.g., <code>gateway: dev</code>, <code>gateway: DEV</code>, <code>gateway: Dev</code>) and Vulcan will handle the matching correctly.</p> <p>Consider the following model that queries an external table with a dynamic database based on the current gateway:</p> <pre><code>MODEL (\n  name vulcan_demo.customer_summary,\n  kind FULL\n);\n\nSELECT\n  *\nFROM\n  @{gateway}_db.customers;\n</code></pre> <p>This table will be named differently depending on which <code>--gateway</code> Vulcan is run with (learn more about the curly brace <code>@{gateway}</code> syntax here).</p> <p>For example:</p> <ul> <li><code>vulcan --gateway dev plan</code> - Vulcan will try to query <code>dev_db.customers</code></li> <li><code>vulcan --gateway prod plan</code> - Vulcan will try to query <code>prod_db.customers</code></li> </ul> <p>To ensure Vulcan can look up the correct schema when the relevant gateway is set, run <code>create_external_models</code> with the <code>--gateway</code> argument. For example:</p> <ul> <li><code>vulcan --gateway dev create_external_models</code></li> </ul> <p>This will set <code>gateway: dev</code> on the external model and ensure that it is only loaded when the current gateway is set to <code>dev</code>.</p>"},{"location":"components/model/types/external_models/#writing-yaml-by-hand","title":"Writing YAML by hand","text":"<p>This example demonstrates the structure of a <code>external_models.yaml</code> file based on a typical e-commerce data model:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n- name: '\"warehouse\".\"vulcan_demo\".\"shipments\"'\n  columns:\n    shipment_id: INT\n    order_id: INT\n    shipped_date: DATE\n    carrier: TEXT\n</code></pre> <p>It contains each <code>EXTERNAL</code> model's name and each of the external table's columns' name and data type. An optional description and gateway can also be specified.</p> <p>The file can be constructed by hand using a standard text editor or IDE.</p>"},{"location":"components/model/types/external_models/#using-the-external_models-directory","title":"Using the <code>external_models</code> directory","text":"<p>Sometimes, Vulcan cannot infer the structure of a model and you need to add it manually.</p> <p>However, since <code>vulcan create_external_models</code> replaces the <code>external_models.yaml</code> file, any manual changes you made to that file will be overwritten.</p> <p>The solution is to create the manual model definition files in the <code>external_models/</code> directory, like so:</p> <pre><code>external_models.yaml\nexternal_models/more_external_models.yaml\nexternal_models/even_more_external_models.yaml\n</code></pre> <p>Files in the <code>external_models</code> directory must be <code>.yaml</code> files that follow the same structure as the <code>external_models.yaml</code> file.</p> <p>When Vulcan loads the definitions, it will first load the models defined in <code>external_models.yaml</code> (or <code>schema.yaml</code>) and  any models found in <code>external_models/*.yaml</code>.</p> <p>Therefore, you can use <code>vulcan create_external_models</code> to manage the <code>external_models.yaml</code> file and then put any models that need to be defined manually inside the <code>external_models/</code> directory.</p>"},{"location":"components/model/types/external_models/#external-assertions","title":"External assertions","text":"<p>It is possible to define assertions on external models. This can be useful to check the data quality of upstream dependencies before your internal models evaluate.</p> <p>This example shows an external model with assertions:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  description: Table containing customer information\n  assertions:\n    - name: not_null\n      columns: \"[customer_id, email]\"\n    - name: unique_values\n      columns: \"[customer_id]\"\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  description: Table containing order transactions\n  assertions:\n    - name: not_null\n      columns: \"[order_id, customer_id, order_date]\"\n    - name: accepted_range\n      column: order_id\n      min_v: \"1\"\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n</code></pre>"},{"location":"components/model/types/managed_models/","title":"Managed models","text":""},{"location":"components/model/types/managed_models/#managed-models","title":"Managed models","text":"<p>Unlike normal tables where the user is responsible for managing the data within the table, some database engines have a concept of a table where the engine itself ensures that the data within the table is up to date. These tables are typically based on a query that reads from other tables within the database. Each time these other tables are updated, the database will ensure that the managed table reflects the changes without the user having to do anything special (such as issue a <code>REFRESH</code> command).</p> <p>Under the hood, each supported database engine achieves this in a slightly different way but most of them have background processes that run and automatically keep the tables up to date, within the parameters you define when you create the table.</p> <p>For supported engines, we expose this functionality through Managed models. This indicates to Vulcan that the underlying database engine will ensure that the data remains up to date and all Vulcan needs to do is maintain the schema.</p> <p>Due to this, managed models would typically be built off an External Model rather than another Vulcan model. Since Vulcan already ensures that models it's tracking are kept up to date, the main benefit of managed models comes when they read from external tables that arent tracked by Vulcan.</p> <p>Not supported in Python models</p> <p>Python models do not support the <code>MANAGED</code> model kind - use a SQL model isntead.</p>"},{"location":"components/model/types/managed_models/#difference-from-materialized-views","title":"Difference from materialized views","text":"<p>The difference between an Managed model and a materialized view is down to semantics and in some engines there is no difference.</p> <p>Vulcan has support for materialized views already. However, depending on the engine, these are subject to some limitations, such as:</p> <ul> <li>A Materialized View query can only be derived from a single base table</li> <li>The Materialized View is not automatically maintained by the engine. To refresh the data, a <code>REFRESH MATERIALIZED VIEW</code> or equivalent command must be issued</li> </ul> <p>Managed models are different in that:</p> <ul> <li>The engine updates the table data automatically when a base table changes</li> <li>When performing updates, the engine has a semantic understanding of the query and can decide if an incremental or full refresh should be applied</li> <li>There is no need to issue manual <code>REFRESH</code> commands. The engine maintains the table transparently in a background process</li> </ul>"},{"location":"components/model/types/managed_models/#lifecycle-in-vulcan","title":"Lifecycle in Vulcan","text":"<p>Managed models follow the same lifecycle as other models:</p> <ul> <li>Creating a Virtual Environment creates a pointer to the current model snapshot</li> <li>Modifying the model causes a new snapshot to be created</li> <li>Any upstream changes cause a new snapshot to be created</li> <li>The model can be deployed and rolled back via the usual pointer swap mechanism</li> <li>Once the TTL expires, model snapshots are cleaned up</li> </ul> <p>However, there is usually extra vendor-imposed costs associated with Managed models. For example, Snowflake has additional costs for Dynamic Tables.</p> <p>Therefore, we try to not create managed tables unnecessarily. For example, in forward-only plans we just create a normal table to preview the changes and only re-create the managed table on deployment to prod.</p> <p>Warning</p> <p>Due to the use of normal tables for dev previews, it is possible to write a query that uses features that are available to normal tables in the target engine but not managed tables. This could result in a scenario where a plan works in a dev environment but fails when deployed to production.</p> <p>We believe the cost savings are worth it, however please reach out if this causes problems for you.</p>"},{"location":"components/model/types/managed_models/#supported-engines","title":"Supported Engines","text":"<p>Vulcan supports managed models in the following database engines:</p> Engine Implementatation Snowflake Dynamic Table <p>To define a managed model, you can use the <code>MANAGED</code> model Kind.</p>"},{"location":"components/model/types/managed_models/#snowflake","title":"Snowflake","text":"<p>Managed Models are in Snowflake are implemented as Dynamic Tables.</p> <p>Here is an example of a Vulcan model that will result in a dynamic table being created:</p> <pre><code>MODEL (\n  name db.events,\n  kind MANAGED,\n  physical_properties (\n    warehouse = datalake,\n    target_lag = '2 minutes',\n    data_retention_time_in_days = 2\n  )\n);\n\nSELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>results in:</p> <pre><code>CREATE OR REPLACE DYNAMIC TABLE db.events\n  WAREHOUSE = \"datalake\",\n  TARGET_LAG = '2 minutes'\n  DATA_RETENTION_TIME_IN_DAYS = 2\nAS SELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>Note</p> <p>Vulcan will not create intervals and run this model for each interval, so there is no need to add a WHERE clause with date filters like you would for a normal incremental model. How the data in this model is refreshed is completely up to Snowflake.</p>"},{"location":"components/model/types/managed_models/#table-properties","title":"Table properties","text":"<p>Dynamic Tables have some properties that affect things like how often the data is refreshed by Snowflake, when the initial data is populated, how long data is retained for etc. The list of available properties is located in the Snowflake documentation.</p> <p>In Vulcan, these properties are set on the model definition.</p> <p>The following Dynamic Table properties are set on the model <code>physical_properties</code>:</p> Snowflake Property Required Notes target_lag Y warehouse N In Snowflake, this is a required property. However, if not specified, then Vulcan will use the result of <code>select current_warehouse()</code>. refresh_mode N initialize N data_retention_time_in_days N max_data_extension_time_in_days N <p>The following Dynamic Table properties can be set directly on the model:</p> Snowflake Property Required Notes cluster by N <code>clustered_by</code> is a standard model property, so set <code>clustered_by</code> on the model to add a <code>CLUSTER BY</code> clause to the Dynamic Table"},{"location":"components/model/types/python_models/","title":"Python models","text":""},{"location":"components/model/types/python_models/#python-models","title":"Python models","text":"<p>Although SQL is a powerful tool, some use cases are better handled by Python. For example, Python may be a better option in pipelines that involve machine learning, interacting with external APIs, or complex business logic that cannot be expressed in SQL.</p> <p>Vulcan has first-class support for models defined in Python; there are no restrictions on what can be done in the Python model as long as it returns a Pandas or Spark DataFrame instance.</p> <p>Unsupported model kinds</p> <p>Python models do not support these model kinds - use a SQL model instead.</p> <ul> <li><code>VIEW</code></li> <li><code>SEED</code></li> <li><code>MANAGED</code></li> <li><code>EMBEDDED</code></li> </ul>"},{"location":"components/model/types/python_models/#definition","title":"Definition","text":"<p>To create a Python model, add a new file with the <code>*.py</code> extension to the <code>models/</code> directory. Inside the file, define a function named <code>execute</code>. For example:</p> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"order_date\"],\n    depends_on=[\"raw.raw_orders\"],\n    cron='@daily',\n    description=\"Daily sales aggregated by order_date.\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"FULL model - rebuilds entire daily_sales table each run\"\"\"\n\n    query = \"\"\"\n    SELECT\n      CAST(order_date AS TIMESTAMP) AS order_date,\n      COUNT(order_id) AS total_orders,\n      SUM(total_amount) AS total_revenue,\n      MAX(order_id) AS last_order_id\n    FROM raw.raw_orders\n    GROUP BY order_date\n    ORDER BY order_date\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>The <code>execute</code> function is wrapped with the <code>@model</code> decorator, which is used to capture the model's metadata (similar to the <code>MODEL</code> DDL statement in SQL models).</p> <p>Because Vulcan creates tables before evaluating models, the schema of the output DataFrame is a required argument. The <code>@model</code> argument <code>columns</code> contains a dictionary of column names to types.</p> <p>The function takes an <code>ExecutionContext</code> that is able to run queries and to retrieve the current time interval that is being processed, along with arbitrary key-value arguments passed in at runtime. The function can either return a Pandas, PySpark, Bigframe, or Snowpark Dataframe instance.</p> <p>If the function output is too large, it can also be returned in chunks using Python generators.</p>"},{"location":"components/model/types/python_models/#model-specification","title":"<code>@model</code> specification","text":"<p>The arguments provided in the <code>@model</code> specification have the same names as those provided in a SQL model's <code>MODEL</code> DDL.</p> <p>Python model <code>kind</code>s are specified with a Python dictionary containing the kind's name and arguments. All model kind arguments are listed in the models configuration reference page.</p> <p>The model <code>kind</code> dictionary must contain a <code>name</code> key whose value is a member of the <code>ModelKindName</code> enum class. The <code>ModelKindName</code> class must be imported at the beginning of the model definition file before being used in the <code>@model</code> specification.</p> <p>Supported <code>kind</code> dictionary <code>name</code> values are:</p> <ul> <li><code>ModelKindName.VIEW</code></li> <li><code>ModelKindName.FULL</code></li> <li><code>ModelKindName.SEED</code></li> <li><code>ModelKindName.INCREMENTAL_BY_TIME_RANGE</code></li> <li><code>ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</code></li> <li><code>ModelKindName.INCREMENTAL_BY_PARTITION</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_TIME</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_COLUMN</code></li> <li><code>ModelKindName.EMBEDDED</code></li> <li><code>ModelKindName.CUSTOM</code></li> <li><code>ModelKindName.MANAGED</code></li> <li><code>ModelKindName.EXTERNAL</code></li> </ul>"},{"location":"components/model/types/python_models/#execution-context","title":"Execution context","text":"<p>Python models can do anything you want, but it is strongly recommended for all models to be idempotent. Python models can fetch data from upstream models or even data outside of Vulcan.</p> <p>Given an execution <code>ExecutionContext</code> \"context\", you can fetch a DataFrame with the <code>fetchdf</code> method:</p> <pre><code>df = context.fetchdf(\"SELECT * FROM vulcan_demo.products\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-prepost-statements","title":"Optional pre/post-statements","text":"<p>Optional pre/post-statements allow you to execute SQL commands before and after a model runs, respectively.</p> <p>For example, pre/post-statements might modify settings or create indexes. However, be careful not to run any statement that could conflict with the execution of another statement if models run concurrently, such as creating a physical table.</p> <p>You can set the <code>pre_statements</code> and <code>post_statements</code> arguments to a list of SQL strings, SQLGlot expressions, or macro calls to define the model's pre/post-statements.</p> <p>Project-level defaults: You can also define pre/post-statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_statements\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    pre_statements=[\n        \"SET GLOBAL parameter = 'value';\",\n        exp.Cache(this=exp.table_(\"x\"), expression=exp.select(\"1\")),\n    ],\n    post_statements=[\"@CREATE_INDEX(@this_model, id)\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>The previous example's <code>post_statements</code> called user-defined Vulcan macro <code>@CREATE_INDEX(@this_model, id)</code>.</p> <p>We could define the <code>CREATE_INDEX</code> macro in the project's <code>macros</code> directory like this. The macro creates a table index on a single column, conditional on the runtime stage being <code>creating</code> (table creation time).</p> <pre><code>@macro()\ndef create_index(\n    evaluator: MacroEvaluator,\n    model_name: str,\n    column: str,\n):\n    if evaluator.runtime_stage == \"creating\":\n        return f\"CREATE INDEX idx ON {model_name}({column});\"\n    return None\n</code></pre> <p>Alternatively, pre- and post-statements can be issued with the Vulcan <code>fetchdf</code> method described above.</p> <p>Pre-statements may be specified anywhere in the function body before it <code>return</code>s or <code>yield</code>s. Post-statements must execute after the function completes, so instead of <code>return</code>ing a value the function must <code>yield</code> the value. The post-statement must be specified after the <code>yield</code>.</p> <p>This example function includes both pre- and post-statements:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    # pre-statement\n    context.engine_adapter.execute(\"SET GLOBAL parameter = 'value';\")\n\n    # post-statement requires using `yield` instead of `return`\n    yield pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n\n    # post-statement\n    context.engine_adapter.execute(\"CREATE INDEX idx ON vulcan_demo.model_with_statements (id);\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-on-virtual-update-statements","title":"Optional on-virtual-update statements","text":"<p>The optional on-virtual-update statements allow you to execute SQL commands after the completion of the Virtual Update.</p> <p>These can be used, for example, to grant privileges on views of the virtual layer.</p> <p>Similar to pre/post-statements you can set the <code>on_virtual_update</code> argument in the <code>@model</code> decorator to a list of SQL strings, SQLGlot expressions, or macro calls.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project (including Python models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_grants\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    on_virtual_update=[\"GRANT SELECT ON VIEW @this_model TO ROLE dev_role\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>Note</p> <p>Table resolution for these statements occurs at the virtual layer. This means that table names, including <code>@this_model</code> macro, are resolved to their qualified view names. For instance, when running the plan in an environment named <code>dev</code>, <code>vulcan_demo.model_with_grants</code> and <code>@this_model</code> would resolve to <code>vulcan_demo__dev.model_with_grants</code> and not to the physical table name.</p>"},{"location":"components/model/types/python_models/#dependencies","title":"Dependencies","text":"<p>In order to fetch data from an upstream model, you first get the table name using <code>context</code>'s <code>resolve_table</code> method. This returns the appropriate table name for the current runtime environment:</p> <pre><code>table = context.resolve_table(\"vulcan_demo.products\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>The <code>resolve_table</code> method will automatically add the referenced model to the Python model's dependencies.</p> <p>The only other way to set dependencies of models in Python models is to define them explicitly in the <code>@model</code> decorator using the keyword <code>depends_on</code>. The dependencies defined in the model decorator take precedence over any dynamic references inside the function.</p> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # Dependencies are explicitly declared above\n    query = \"\"\"\n    SELECT \n        p.product_id,\n        p.name AS product_name,\n        p.category,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>User-defined global variables or blueprint variables can also be used in <code>resolve_table</code> calls, as shown in the following example (similarly for <code>blueprint_var()</code>):</p> <pre><code>@model(\n    \"@schema_name.test_model2\",\n    kind=\"FULL\",\n    columns={\"id\": \"INT\"},\n)\ndef execute(context, **kwargs):\n    table = context.resolve_table(f\"{context.var('schema_name')}.test_model1\")\n    select_query = exp.select(\"*\").from_(table)\n    return context.fetchdf(select_query)\n</code></pre>"},{"location":"components/model/types/python_models/#returning-empty-dataframes","title":"Returning empty dataframes","text":"<p>Python models may not return an empty dataframe.</p> <p>If your model could possibly return an empty dataframe, conditionally <code>yield</code> the dataframe or an empty generator instead of <code>return</code>ing:</p> <pre><code>@model(\n    \"vulcan_demo.empty_df_model\"\n)\ndef execute(\n    context: ExecutionContext,\n) -&gt; pd.DataFrame:\n\n    [...code creating df...]\n\n    if df.empty:\n        yield from ()\n    else:\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#user-defined-variables","title":"User-defined variables","text":"<p>User-defined global variables can be accessed from within the Python model with the <code>context.var</code> method.</p> <p>For example, this model access the user-defined variables <code>var</code> and <code>var_with_default</code>. It specifies a default value of <code>default_value</code> if <code>variable_with_default</code> resolves to a missing value.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    var_value = context.var(\"var\")\n    var_with_default_value = context.var(\"var_with_default\", \"default_value\")\n    ...\n</code></pre> <p>Alternatively, you can access global variables via <code>execute</code> function arguments, where the name of the argument corresponds to the name of a variable key.</p> <p>For example, this model specifies <code>my_var</code> as an argument to the <code>execute</code> method. The model code can reference the <code>my_var</code> object directly:</p> <pre><code>@model(\n    \"vulcan_demo.model_with_arg_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    my_var: Optional[str] = None,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    my_var_plus1 = my_var + 1\n    ...\n</code></pre> <p>Make sure the argument has a default value if it's possible for the variable to be missing.</p> <p>Note that arguments must be specified explicitly - variables cannot be accessed using <code>kwargs</code>.</p>"},{"location":"components/model/types/python_models/#python-model-blueprinting","title":"Python model blueprinting","text":"<p>A Python model can also serve as a template for creating multiple models, or blueprints, by specifying a list of key-value dicts in the <code>blueprints</code> property. In order to achieve this, the model's name must be parameterized with a variable that exists in this mapping.</p> <p>For instance, the following model will result into two new models, each using the corresponding mapping in the <code>blueprints</code> property:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"@{customer}.some_table\",\n    kind=\"FULL\",\n    blueprints=[\n        {\"customer\": \"customer1\", \"field_a\": \"x\", \"field_b\": \"y\"},\n        {\"customer\": \"customer2\", \"field_a\": \"z\", \"field_b\": \"w\"},\n    ],\n    columns={\n        \"field_a\": \"text\",\n        \"field_b\": \"text\",\n        \"customer\": \"text\",\n    },\n)\ndef entrypoint(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        {\n            \"field_a\": [context.blueprint_var(\"field_a\")],\n            \"field_b\": [context.blueprint_var(\"field_b\")],\n            \"customer\": [context.blueprint_var(\"customer\")],\n        }\n    )\n</code></pre> <p>Note the use of curly brace syntax <code>@{customer}</code> in the model name above. It is used to ensure Vulcan can combine the macro variable into the model name identifier correctly - learn more here.</p> <p>Blueprint variable mappings can also be constructed dynamically, e.g., by using a macro: <code>blueprints=\"@gen_blueprints()\"</code>. This is useful in cases where the <code>blueprints</code> list needs to be sourced from external sources, such as CSV files.</p> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@EACH(@values, x -&gt; (customer := schema_@x))\",\n    ...\n)\n...\n</code></pre>"},{"location":"components/model/types/python_models/#using-macros-in-model-properties","title":"Using macros in model properties","text":"<p>Python models support macro variables in model properties. However, special care must be taken when the macro variable appears within a string.</p> <p>For example when using macro variables inside cron expressions, you need to wrap the entire expression in quotes and prefix it with <code>@</code> to ensure proper parsing:</p> <pre><code># Correct: Wrap the cron expression containing a macro variable\n@model(\n    \"vulcan_demo.scheduled_model\",\n    cron=\"@'*/@{mins} * * * *'\",  # Note the @'...' syntax\n    ...\n)\n\n# This also works with blueprint variables\n@model(\n    \"@{customer}.scheduled_model\",\n    cron=\"@'0 @{hour} * * *'\",\n    blueprints=[\n        {\"customer\": \"customer_1\", \"hour\": 2}, # Runs at 2 AM\n        {\"customer\": \"customer_2\", \"hour\": 8}, # Runs at 8 AM\n    ],\n    ...\n)\n</code></pre> <p>This is necessary because cron expressions often use <code>@</code> for aliases (like <code>@daily</code>, <code>@hourly</code>), which can conflict with Vulcan's macro syntax.</p>"},{"location":"components/model/types/python_models/#examples","title":"Examples","text":""},{"location":"components/model/types/python_models/#basic","title":"Basic","text":"<p>The following is an example of a Python model returning a static Pandas DataFrame.</p> <p>Note: All of the metadata field names are the same as those in the SQL <code>MODEL</code> DDL.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom sqlglot.expressions import to_column\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.basic_model\",\n    owner=\"data_team\",\n    cron=\"@daily\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    column_descriptions={\n        \"id\": \"Unique ID\",\n        \"name\": \"Name corresponding to the ID\",\n    },\n    audits=[\n        (\"not_null\", {\"columns\": [to_column(\"id\")]}),\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre>"},{"location":"components/model/types/python_models/#sql-query-and-pandas","title":"SQL Query and Pandas","text":"<p>The following is a more complex example that queries an upstream model and outputs a Pandas DataFrame:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.sql_pandas_model\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"text\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's name and register it as a dependency\n    products_table = context.resolve_table(\"vulcan_demo.products\")\n    order_items_table = context.resolve_table(\"vulcan_demo.order_items\")\n\n    # fetch data from the model as a pandas DataFrame\n    df = context.fetchdf(f\"\"\"\n        SELECT \n            p.product_id,\n            p.name AS product_name,\n            SUM(oi.quantity * oi.unit_price) as total_sales\n        FROM {products_table} p\n        LEFT JOIN {order_items_table} oi ON p.product_id = oi.product_id\n        GROUP BY p.product_id, p.name\n    \"\"\")\n\n    # do some pandas stuff\n    df['total_sales'] = df['total_sales'].fillna(0)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#pyspark","title":"PySpark","text":"<p>This example demonstrates using the PySpark DataFrame API. If you use Spark, the DataFrame API is preferred to Pandas since it allows you to compute in a distributed fashion.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom pyspark.sql import DataFrame, functions\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.pyspark_model\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"text\",\n        \"region\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # get the upstream model's name and register it as a dependency\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    # use the spark DataFrame api to add the region column\n    df = context.spark.table(table).withColumn(\"region\", functions.lit(\"North\"))\n\n    # returns the pyspark DataFrame directly, so no data is computed locally\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#snowpark","title":"Snowpark","text":"<p>This example demonstrates using the Snowpark DataFrame API. If you use Snowflake, the DataFrame API is preferred to Pandas since it allows you to compute in a distributed fashion.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom snowflake.snowpark.dataframe import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.snowpark_model\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n        \"country\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # returns the snowpark DataFrame directly, so no data is computed locally\n    df = context.snowpark.create_dataframe([[1, \"a\", \"usa\"], [2, \"b\", \"cad\"]], schema=[\"id\", \"name\", \"country\"])\n    df = df.filter(df.id &gt; 1)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#bigframe","title":"Bigframe","text":"<p>This example demonstrates using the Bigframe DataFrame API. If you use Bigquery, the Bigframe API is preferred to Pandas as all computation is done in Bigquery.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nfrom bigframes.pandas import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n\ndef get_bucket(num: int):\n    if not num:\n        return \"NA\"\n    boundary = 10\n    return \"at_or_above_10\" if num &gt;= boundary else \"below_10\"\n\n\n@model(\n    \"vulcan_demo.bigframe_model\",\n    columns={\n        \"title\": \"text\",\n        \"views\": \"int\",\n        \"bucket\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # Create a remote function to be used in the Bigframe DataFrame\n    remote_get_bucket = context.bigframe.remote_function([int], str)(get_bucket)\n\n    # Returns the Bigframe DataFrame handle, no data is computed locally\n    df = context.bigframe.read_gbq(\"bigquery-samples.wikipedia_pageviews.200809h\")\n\n    df = (\n        # This runs entirely on the BigQuery engine lazily\n        df[df.title.str.contains(r\"[Gg]oogle\")]\n        .groupby([\"title\"], as_index=False)[\"views\"]\n        .sum(numeric_only=True)\n        .sort_values(\"views\", ascending=False)\n    )\n\n    return df.assign(bucket=df[\"views\"].apply(remote_get_bucket))\n</code></pre>"},{"location":"components/model/types/python_models/#batching","title":"Batching","text":"<p>If the output of a Python model is very large and you cannot use Spark, it may be helpful to split the output into multiple batches.</p> <p>With Pandas or other single machine DataFrame libraries, all data is stored in memory. Instead of returning a single DataFrame instance, you can return multiple instances using the Python generator API. This minimizes the memory footprint by reducing the size of data loaded into memory at any given time.</p> <p>This examples uses the Python generator <code>yield</code> to batch the model output:</p> <pre><code>@model(\n    \"vulcan_demo.batching_model\",\n    columns={\n        \"customer_id\": \"int\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's table name\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    for i in range(3):\n        # run 3 queries to get chunks of data and not run out of memory\n        df = context.fetchdf(f\"SELECT customer_id from {table} WHERE customer_id = {i}\")\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#serialization","title":"Serialization","text":"<p>Vulcan executes Python code locally where Vulcan is running by using our custom serialization framework.</p>"},{"location":"components/model/types/sql_models/","title":"SQL models","text":""},{"location":"components/model/types/sql_models/#sql-models","title":"SQL models","text":"<p>SQL models are the main type of models used by Vulcan. These models can be defined using either SQL or Python that generates SQL.</p>"},{"location":"components/model/types/sql_models/#sql-based-definition","title":"SQL-based definition","text":"<p>The SQL-based definition of SQL models is the most common one, and consists of the following sections:</p> <ul> <li>The <code>MODEL</code> DDL</li> <li>Optional pre-statements</li> <li>A single query</li> <li>Optional post-statements</li> <li>Optional on-virtual-update-statements</li> </ul> <p>These models are designed to look and feel like you're simply using SQL, but they can be customized for advanced use cases.</p> <p>To create a SQL-based model, add a new file with the <code>.sql</code> suffix into the <code>models/</code> directory (or a subdirectory of <code>models/</code>) within your Vulcan project. Although the name of the file doesn't matter, it is customary to use the model's name (without the schema) as the file name. For example, the file containing the model <code>sales.daily_sales</code> would be named <code>daily_sales.sql</code>.</p>"},{"location":"components/model/types/sql_models/#example","title":"Example","text":"<pre><code>-- This is the MODEL DDL, where you specify model metadata and configuration information.\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\n/*\n  This is the single query that defines the model's logic.\n  Although it is not required, it is considered best practice to explicitly\n  specify the type for each one of the model's columns through casting.\n*/\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"components/model/types/sql_models/#model-ddl","title":"<code>MODEL</code> DDL","text":"<p>The <code>MODEL</code> DDL is used to specify metadata about the model such as its name, kind, owner, cron, and others. This should be the first statement in your SQL-based model's file.</p> <p>Refer to <code>MODEL</code> properties for the full list of allowed properties.</p>"},{"location":"components/model/types/sql_models/#optional-prepost-statements","title":"Optional pre/post-statements","text":"<p>Optional pre/post-statements allow you to execute SQL commands before and after a model runs, respectively.</p> <p>For example, pre/post-statements might modify settings or create a table index. However, be careful not to run any statement that could conflict with the execution of another model if they are run concurrently, such as creating a physical table.</p> <p>Pre/post-statements are just standard SQL commands located before/after the model query. They must end with a semi-colon, and the model query must end with a semi-colon if a post-statement is present.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\n-- Pre-statement: Cache a table for use in the query\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\n-- The model query (must end with semi-colon when post-statements are present)\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date;\n\n-- Post-statement: Clean up the cached table\nUNCACHE TABLE countries;\n</code></pre> <p>Project-level defaults: You can also define pre/post-statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>Warning</p> <p>Pre/post-statements are evaluated twice: when a model's table is created and when its query logic is evaluated. Executing statements more than once can have unintended side-effects, so you can conditionally execute them based on Vulcan's runtime stage.</p> <p>The pre/post-statements in the example above will run twice because they are not conditioned on runtime stage.</p> <p>We can condition the post-statement to only run after the model query is evaluated using the <code>@IF</code> macro operator and <code>@runtime_stage</code> macro variable like this:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  UNCACHE TABLE countries\n);\n</code></pre> <p>Note that the SQL command <code>UNCACHE TABLE countries</code> inside the <code>@IF()</code> macro does not end with a semi-colon. Instead, the semi-colon comes after the <code>@IF()</code> macro's closing parenthesis.</p>"},{"location":"components/model/types/sql_models/#optional-on-virtual-update-statements","title":"Optional on-virtual-update statements","text":"<p>The optional on-virtual-update statements allow you to execute SQL commands after the completion of the Virtual Update.</p> <p>These can be used, for example, to grant privileges on views of the virtual layer.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>These SQL statements must be enclosed within an <code>ON_VIRTUAL_UPDATE_BEGIN;</code> ...; <code>ON_VIRTUAL_UPDATE_END;</code> block like this:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nSELECT\n  order_date::TIMESTAMP,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\nON_VIRTUAL_UPDATE_BEGIN;\nGRANT SELECT ON VIEW @this_model TO ROLE role_name;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE admin;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Jinja expressions can also be used within them, as demonstrated in the example above. These expressions must be properly nested within a <code>JINJA_STATEMENT_BEGIN;</code> and <code>JINJA_END;</code> block.</p> <p>Note</p> <p>Table resolution for these statements occurs at the virtual layer. This means that table names, including <code>@this_model</code> macro, are resolved to their qualified view names. For instance, when running the plan in an environment named <code>dev</code>, <code>sales.daily_sales</code> and <code>@this_model</code> would resolve to <code>sales__dev.daily_sales</code> and not to the physical table name.</p>"},{"location":"components/model/types/sql_models/#the-model-query","title":"The model query","text":"<p>The model must contain a standalone query, which can be a single <code>SELECT</code> expression, or multiple <code>SELECT</code> expressions combined with the <code>UNION</code>, <code>INTERSECT</code>, or <code>EXCEPT</code> operators. The result of this query will be used to populate the model's table or view.</p>"},{"location":"components/model/types/sql_models/#sql-model-blueprinting","title":"SQL model blueprinting","text":"<p>A SQL model can also serve as a template for creating multiple models, or blueprints, by specifying a list of key-value mappings in the <code>blueprints</code> property. In order to achieve this, the model's name must be parameterized with a variable that exists in this mapping.</p> <p>For instance, the following model will result into four new models, each using the corresponding mapping in the <code>blueprints</code> property:</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints (\n    (region := 'north'),\n    (region := 'south'),\n    (region := 'east'),\n    (region := 'west')\n  ),\n  grains region_id\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre> <p>The four models produced from this template are:</p> <pre><code>-- This uses the first variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__north,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('north')\n\n-- This uses the second variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__south,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('south')\n</code></pre> <p>Note the use of curly brace syntax <code>@{region}</code> in the model name above. It is used to tell Vulcan that the rendered variable value should be treated as a SQL identifier instead of a string literal.</p> <p>You can see the different behavior in the WHERE clause. <code>@region</code> (without braces) is resolved to the string literal <code>'north'</code> (with single quotes) because the blueprint value is quoted. Learn more about the curly brace syntax here.</p> <p>Blueprint variable mappings can also be constructed dynamically, e.g., by using a macro: <code>blueprints @gen_blueprints()</code>. This is useful in cases where the <code>blueprints</code> list needs to be sourced from external sources, such as CSV files.</p> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((region := 'north'),\"\n        \" (region := 'south'),\"\n        \" (region := 'east'),\"\n        \" (region := 'west'))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints @EACH(@values, x -&gt; (region := @x)),\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre>"},{"location":"components/model/types/sql_models/#python-based-definition","title":"Python-based definition","text":"<p>SQL models can also be defined using Python. This approach is beneficial when the query is too complex to express cleanly in SQL, or when you need dynamic components that would require heavy use of macros.</p> <p>For comprehensive documentation on Python-based models, including the <code>@model</code> decorator, execution context, pre/post-statements, blueprinting, and examples, see the Python models page.</p>"},{"location":"components/model/types/sql_models/#automatic-dependencies","title":"Automatic dependencies","text":"<p>Vulcan parses your SQL, so it understands what the code does and how it relates to other models. There is no need for you to manually specify dependencies to other models with special tags or commands.</p> <p>For example, consider a model with this query:</p> <pre><code>SELECT\n  order_date,\n  COUNT(order_id) AS total_orders,\n  SUM(total_amount) AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan will detect that the model depends on <code>raw.raw_orders</code>. When executing this model, it will ensure that <code>raw.raw_orders</code> is executed first.</p> <p>External dependencies not defined in Vulcan are also supported. Vulcan can either depend on them implicitly through the order in which they are executed, or through signals.</p> <p>Although automatic dependency detection works most of the time, there may be specific cases for which you want to define dependencies manually. You can do so in the <code>MODEL</code> DDL with the dependencies property.</p>"},{"location":"components/model/types/sql_models/#conventions","title":"Conventions","text":"<p>Vulcan encourages explicitly specifying the data types of a model's columns through casting. This allows Vulcan to understand the data types in your models, and it prevents incorrect type inference. Vulcan supports the casting format <code>&lt;column name&gt;::&lt;data type&gt;</code> in models of any SQL dialect.</p>"},{"location":"components/model/types/sql_models/#explicit-selects","title":"Explicit SELECTs","text":"<p>Although <code>SELECT *</code> is convenient, it is dangerous because a model's results can change due to external factors (e.g., an upstream source adding or removing a column). In general, we encourage listing out every column you need or using <code>create_external_models</code> to capture the schema of an external data source.</p> <p>If you select from an external source, <code>SELECT *</code> will prevent Vulcan from performing some optimization steps and from determining upstream column-level lineage. Use an <code>external</code> model kind to enable optimizations and upstream column-level lineage for external sources.</p>"},{"location":"components/model/types/sql_models/#encoding","title":"Encoding","text":"<p>Vulcan expects files containing SQL models to be encoded according to the UTF-8 standard. Using a different encoding may lead to unexpected behavior.</p>"},{"location":"components/model/types/sql_models/#transpilation","title":"Transpilation","text":"<p>Vulcan leverages SQLGlot to parse and transpile SQL. Therefore, you can write your SQL in any supported dialect and transpile it into another supported dialect.</p> <p>You can also use advanced syntax that may not be available in your engine of choice. For example, <code>x::int</code> is equivalent to <code>CAST(x as INT)</code>, but is only supported in some dialects. SQLGlot allows you to use this feature regardless of what engine you're using.</p> <p>Additionally, you won't have to worry about minor formatting differences such as trailing commas, as SQLGlot will remove them at parse time.</p>"},{"location":"components/model/types/sql_models/#macros","title":"Macros","text":"<p>Although standard SQL is very powerful, complex data systems often require running SQL queries with dynamic components such as date filters. For example, you may want to change the date ranges in a <code>between</code> statement so that you can get the latest batch of data. Vulcan provides these dates automatically through macro variables.</p> <p>Additionally, large queries can be difficult to read and maintain. In order to make queries more compact, Vulcan supports a powerful macro syntax as well as Jinja, allowing you to write macros that make your SQL queries easier to manage.</p>"},{"location":"components/semantics/business_metrics/","title":"Business Metrics","text":""},{"location":"components/semantics/business_metrics/#business-metrics","title":"Business Metrics","text":"<p>Business metrics combine measures with dimensions and time to create complete analytical definitions ready for time-series analysis.</p>"},{"location":"components/semantics/business_metrics/#what-are-business-metrics","title":"What are Business Metrics?","text":"<p>Business metrics are complete analytical definitions that:</p> <ul> <li>Combine measures with time: Enable time-series analysis at different granularities</li> <li>Include dimensions: Allow slicing and dicing by business attributes</li> <li>Ready for analysis: Pre-configured for dashboards, reports, and APIs</li> <li>Examples: <code>monthly_revenue_by_tier</code>, <code>daily_active_users</code>, <code>customer_acquisition_trend</code></li> </ul>"},{"location":"components/semantics/business_metrics/#basic-structure","title":"Basic Structure","text":"<p>A business metric combines:</p> <ul> <li>Measure: The calculation to perform (e.g., <code>orders.total_revenue</code>)</li> <li>Time: The time dimension for analysis (e.g., <code>orders.order_date</code>)</li> <li>Dimensions: Optional attributes for grouping (e.g., <code>customers.customer_tier</code>)</li> </ul> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue      # Which measure to calculate\n    time: orders.order_date            # Time dimension for analysis\n    description: \"Monthly revenue trends\"\n</code></pre>"},{"location":"components/semantics/business_metrics/#simple-metric","title":"Simple Metric","text":"<p>A basic metric with just a measure and time:</p> <pre><code>metrics:\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n</code></pre> <p>This metric can be queried at different time granularities (day, week, month, quarter, year) without redefinition.</p>"},{"location":"components/semantics/business_metrics/#metric-with-dimensions","title":"Metric with Dimensions","text":"<p>Add dimensions for slicing and grouping:</p> <pre><code>metrics:\n  revenue_by_tier:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier      # Group by tier\n      - customers.country            # And country\n    description: \"Revenue trends by customer tier and country\"\n</code></pre> <p>This enables queries like: - Revenue by tier over time - Revenue by country over time - Revenue by tier and country over time</p>"},{"location":"components/semantics/business_metrics/#cross-model-metrics","title":"Cross-Model Metrics","text":"<p>Combine measures and dimensions from multiple models:</p> <pre><code>metrics:\n  product_revenue_by_customer_segment:\n    measure: orders.total_revenue      # From orders\n    time: orders.order_date            # From orders\n    dimensions:\n      - products.category              # From products\n      - products.brand\n      - customers.customer_tier        # From customers\n      - customers.region\n    description: \"Product revenue segmented by customer demographics\"\n</code></pre> <p>Requirement: Proper joins must be defined between models for cross-model metrics to work.</p>"},{"location":"components/semantics/business_metrics/#reference-format","title":"Reference Format","text":"<p>Always use dot notation with semantic model aliases:</p> <pre><code># \u2705 Good: Use aliases\nmeasure: orders.total_revenue     # alias.measure_name\ntime: orders.order_date           # alias.column_name\ndimensions:\n  - customers.customer_tier       # alias.column_name\n\n# \u274c Bad: Don't use physical names\nmeasure: analytics.fact_orders.revenue\ntime: order_date  # Missing alias\n</code></pre>"},{"location":"components/semantics/business_metrics/#time-granularity","title":"Time Granularity","text":"<p>Metrics support different time granularities at query time:</p> <pre><code>metrics:\n  revenue_trends:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Revenue at any time granularity\"\n</code></pre> <p>The same metric can be queried with different granularities: - Daily: <code>granularity=day</code> - Weekly: <code>granularity=week</code> - Monthly: <code>granularity=month</code> - Quarterly: <code>granularity=quarter</code> - Yearly: <code>granularity=year</code></p>"},{"location":"components/semantics/business_metrics/#complete-example","title":"Complete Example","text":"<pre><code>metrics:\n  # Simple revenue metric\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n    tags: [revenue, financial, kpi]\n\n  # Customer acquisition\n  customer_acquisition_trend:\n    measure: customers.new_signups\n    time: customers.signup_date\n    dimensions:\n      - customers.signup_channel\n      - customers.customer_tier\n      - customers.country\n    description: \"Customer acquisition by channel, tier, and geography\"\n    tags: [acquisition, growth, customer]\n\n  # Cross-model metric\n  product_performance:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - products.category\n      - products.brand\n      - customers.customer_tier\n    description: \"Product revenue by category, brand, and customer segment\"\n    tags: [revenue, products, segmentation]\n</code></pre>"},{"location":"components/semantics/business_metrics/#benefits","title":"Benefits","text":""},{"location":"components/semantics/business_metrics/#time-series-analysis","title":"Time-Series Analysis","text":"<p>Metrics are designed for time-series analysis:</p> <ul> <li>Flexible granularity: Query the same metric at different time intervals</li> <li>Consistent definitions: Same calculation logic across all time periods</li> <li>Trend analysis: Built-in support for comparing periods</li> </ul>"},{"location":"components/semantics/business_metrics/#self-service-analytics","title":"Self-Service Analytics","text":"<p>Business users can query metrics without SQL:</p> <ul> <li>Simple API: Query metrics by name with time range and dimensions</li> <li>Consistent results: Same metric definition used everywhere</li> <li>No SQL required: Abstract away complex joins and aggregations</li> </ul>"},{"location":"components/semantics/business_metrics/#single-source-of-truth","title":"Single Source of Truth","text":"<p>Centralized metric definitions:</p> <ul> <li>Define once: Create metric definitions in YAML files</li> <li>Use everywhere: Same metrics power dashboards, reports, and APIs</li> <li>Version controlled: Metric definitions live alongside your code</li> </ul>"},{"location":"components/semantics/business_metrics/#best-practices","title":"Best Practices","text":""},{"location":"components/semantics/business_metrics/#descriptive-names","title":"Descriptive Names","text":"<pre><code># \u2705 Good: Self-explanatory\nmetrics:\n  monthly_revenue_by_tier: ...\n  daily_active_users: ...\n\n# \u274c Bad: Vague\nmetrics:\n  metric_1: ...\n  rev: ...\n</code></pre>"},{"location":"components/semantics/business_metrics/#include-essential-dimensions","title":"Include Essential Dimensions","text":"<pre><code># \u2705 Good: Key business dimensions\nmetrics:\n  revenue_analysis:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n      - customers.region\n      - products.category\n\n# \u274c Too few: Limited analysis\nmetrics:\n  revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    # Missing dimensions\n</code></pre>"},{"location":"components/semantics/business_metrics/#document-business-context","title":"Document Business Context","text":"<pre><code>metrics:\n  net_revenue_retention:\n    measure: subscriptions.nrr\n    time: subscriptions.cohort_month\n    description: \"Net Revenue Retention: expansion minus churn\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation: \"(Starting MRR + Expansion - Churn) / Starting MRR\"\n      benchmark: \"&gt;110% is good for SaaS\"\n</code></pre>"},{"location":"components/semantics/business_metrics/#integration-with-semantic-models","title":"Integration with Semantic Models","text":"<p>Metrics build on semantic models:</p> <ol> <li>Semantic models define measures, dimensions, and joins</li> <li>Metrics combine these components with time for analysis</li> <li>APIs expose metrics for querying and visualization</li> </ol> <p>The semantic layer provides the foundation, and metrics add the time-series analytical capabilities.</p>"},{"location":"components/semantics/business_metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Semantic Models that provide the foundation for metrics</li> <li>See the Semantics Overview for the complete picture</li> <li>Explore metric definitions in your project's <code>semantics/</code> directory</li> </ul>"},{"location":"components/semantics/models/","title":"Semantic Models","text":""},{"location":"components/semantics/models/#semantic-models","title":"Semantic Models","text":"<p>Semantic models map physical Vulcan models to business concepts, providing business-friendly names and exposing analytical capabilities through dimensions, measures, segments, and joins.</p>"},{"location":"components/semantics/models/#what-are-semantic-models","title":"What are Semantic Models?","text":"<p>Semantic models bridge the gap between technical table structures and business understanding:</p> <ul> <li>Reference physical models: Each semantic model references a Vulcan model defined in your <code>models/</code> directory</li> <li>Provide business aliases: Hide technical naming (like <code>dim_customers</code> or <code>fact_orders</code>) behind consumer-friendly names</li> <li>Expose analytical capabilities: Define dimensions, measures, segments, and joins for each model</li> </ul>"},{"location":"components/semantics/models/#basic-structure","title":"Basic Structure","text":"<p>A semantic model maps a physical Vulcan model to a semantic representation:</p> <pre><code>models:\n  analytics.customers:  # Physical model name (dictionary key)\n    alias: customers     # Business-friendly semantic alias\n    description: \"Customer master data\"\n    dimensions: {...}    # Optional: control which columns are exposed\n    measures: {...}      # Optional: aggregated calculations\n    segments: {...}      # Optional: reusable filter conditions\n    joins: {...}         # Optional: relationships to other models\n</code></pre>"},{"location":"components/semantics/models/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes for grouping and filtering:</p> <ul> <li>Automatically exposed: All columns from your Vulcan model become dimensions automatically</li> <li>Answer \"by what?\" questions: Use dimensions to slice and dice your data</li> <li>Examples: <code>customer_tier</code>, <code>country</code>, <code>order_date</code>, <code>product_category</code></li> <li>Enhancements: Add granularities to time dimensions for cohort analysis and time-based grouping</li> </ul> <pre><code># All columns from analytics.customers automatically become dimensions:\n# - customers.customer_id\n# - customers.customer_tier\n# - customers.signup_date\n# - customers.country\n\n# You can control which columns are exposed:\ndimensions:\n  excludes:\n    - password_hash       # Hide sensitive data\n    - internal_notes\n\n  # Enhance dimensions with additional capabilities:\n  enhancements:\n    - name: start_date\n      granularities:\n        - name: monthly\n          interval: \"1 month\"\n          description: \"Monthly subscription cohorts\"\n        - name: quarterly\n          interval: \"3 months\"\n          description: \"Quarterly cohorts\"\n</code></pre>"},{"location":"components/semantics/models/#measures","title":"Measures","text":"<p>Measures are aggregated calculations:</p> <ul> <li>Answer \"how much?\" or \"how many?\" questions: Calculate totals, averages, counts, etc.</li> <li>SQL expressions: Use aggregations like <code>SUM(amount)</code>, <code>COUNT(*)</code>, <code>AVG(value)</code></li> <li>Examples: <code>total_revenue</code>, <code>customer_count</code>, <code>avg_order_value</code></li> </ul> <pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{customers.amount}\"\n    description: \"Total revenue from all orders\"\n    format: currency\n\n  avg_order_value:\n    type: number\n    expression: \"SUM({customers.total_revenue}) / NULLIF(COUNT(*), 0)\"\n    format: currency\n    description: \"Average order value\"\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"\n    filters:\n      - \"{customers.status} = 'active'\"\n    description: \"Number of active customers\"\n</code></pre>"},{"location":"components/semantics/models/#segments","title":"Segments","text":"<p>Segments are reusable filter conditions:</p> <ul> <li>Answer \"which ones?\" questions: Define meaningful subsets of data</li> <li>Reusable filters: Use segments across multiple queries and metrics</li> <li>Examples: <code>active_customers</code>, <code>high_value</code>, <code>recent_signups</code></li> </ul> <pre><code>segments:\n  active_customers:\n    expression: \"{customers.status} = 'active'\"\n    description: \"Customers with active subscriptions\"\n\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"\n    description: \"Customers who spent over $10K\"\n\n  recent_signups:\n    expression: \"{customers.signup_date} &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    description: \"Customers who signed up in last 30 days\"\n</code></pre>"},{"location":"components/semantics/models/#joins","title":"Joins","text":"<p>Joins define relationships between semantic models:</p> <ul> <li>Connect models: Enable cross-model analysis</li> <li>Relationship types: <code>one_to_one</code>, <code>one_to_many</code>, <code>many_to_one</code></li> <li>Examples: <code>orders \u2192 customers</code>, <code>orders \u2192 products</code></li> </ul> <pre><code>joins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"\n    description: \"Order's customer\"\n\n  products:\n    type: many_to_one\n    expression: \"{orders.product_id} = {products.product_id}\"\n    description: \"Ordered product\"\n</code></pre>"},{"location":"components/semantics/models/#cross-model-analysis","title":"Cross-Model Analysis","text":"<p>Once models are joined, you can reference columns and measures from other models in your current model's definitions.</p>"},{"location":"components/semantics/models/#referencing-joined-model-fields","title":"Referencing Joined Model Fields","text":"<p>You can use columns from joined models in measure expressions and filters:</p> <pre><code>measures:\n  enterprise_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    filters:\n      - \"{customers.customer_tier} = 'Enterprise'\"\n    description: \"Revenue from Enterprise customers\"\n</code></pre>"},{"location":"components/semantics/models/#proxy-dimensions","title":"Proxy Dimensions","text":"<p>Proxy dimensions expose measures from joined models as dimensions on the current model, enabling you to filter and group by aggregated values from other models.</p> <pre><code>dimensions:\n  proxies:\n    - name: plan_average_monthly_price\n      measure: subscription_plans.avg_monthly_price\n\n    - name: plan_average_annual_price\n      measure: subscription_plans.avg_annual_price\n</code></pre> <p>Requirements: - The referenced model must be joined to the current model - The measure must exist on the target model - Format: <code>model_alias.measure_name</code></p>"},{"location":"components/semantics/models/#complete-example","title":"Complete Example","text":"<pre><code>models:\n  analytics.customers:\n    alias: customers\n\n    dimensions:\n      excludes:\n        - password_hash\n        - internal_notes\n      enhancements:\n        - name: signup_date\n          granularities:\n            - name: monthly\n              interval: \"1 month\"\n              description: \"Monthly signup cohorts\"\n            - name: quarterly\n              interval: \"3 months\"\n              description: \"Quarterly signup cohorts\"\n\n    measures:\n      total_customers:\n        type: count\n        expression: \"{customers.customer_id}\"\n        description: \"Total number of customers\"\n\n      active_customers:\n        type: count_distinct\n        expression: \"{customers.customer_id}\"\n        filters:\n          - \"{customers.status} = 'active'\"\n        description: \"Number of active customers\"\n\n    segments:\n      active:\n        expression: \"{customers.status} = 'active'\"\n        description: \"Active customers\"\n\n      high_value:\n        expression: \"{customers.total_spent} &gt; 10000\"\n        description: \"High-value customers\"\n\n    joins:\n      orders:\n        type: one_to_many\n        expression: \"{customers.customer_id} = {orders.customer_id}\"\n        description: \"Customer's orders\"\n</code></pre>"},{"location":"components/semantics/models/#best-practices","title":"Best Practices","text":""},{"location":"components/semantics/models/#use-business-friendly-aliases","title":"Use Business-Friendly Aliases","text":"<pre><code># \u2705 Good: Consumer-friendly\nalias: customers\nalias: orders\nalias: subscriptions\n\n# \u274c Bad: Technical naming\nalias: dim_customers\nalias: fact_orders\n</code></pre>"},{"location":"components/semantics/models/#design-models-with-semantics-in-mind","title":"Design Models with Semantics in Mind","text":"<pre><code>-- \u2705 Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name\n  signup_date,        -- Good time dimension\n  total_spent         -- Good for segments\nFROM raw.customers;\n</code></pre>"},{"location":"components/semantics/models/#document-business-logic","title":"Document Business Logic","text":"<pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    description: \"Total revenue from all completed orders\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation_method: \"Sum of order amounts excluding refunds\"\n</code></pre>"},{"location":"components/semantics/models/#use-curly-braces-for-references","title":"Use Curly Braces for References","text":"<p>When referencing any column or measure anywhere in your semantic model definitions, always use curly braces <code>{}</code> as a best practice:</p> <pre><code># \u2705 Good: Use curly braces for all references\nmeasures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"  # Column reference with curly braces\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"  # Column reference with curly braces\n    filters:\n      - \"{customers.status} = 'active'\"  # Column reference in filter\n\nsegments:\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"  # Column reference with curly braces\n\njoins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"  # Both references use curly braces\n</code></pre> <p>Why use curly braces? - \u2705 Clear distinction between semantic references and SQL functions - \u2705 Consistent syntax across all semantic model definitions - \u2705 Prevents ambiguity in complex expressions - \u2705 Required for cross-model references (e.g., <code>{customers.customer_tier}</code>)</p>"},{"location":"components/semantics/models/#validation","title":"Validation","text":"<p>Vulcan automatically validates semantic model definitions during <code>plan</code> creation:</p> <ul> <li>\u2705 All column references in measures exist</li> <li>\u2705 All column references in segments exist</li> <li>\u2705 Join expressions reference valid columns</li> <li>\u2705 Cross-model references have valid join paths</li> <li>\u2705 Semantic aliases are properly defined</li> </ul>"},{"location":"components/semantics/models/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Business Metrics that combine measures with time and dimensions</li> <li>Explore semantic model examples in your project's <code>semantics/</code> directory</li> <li>See the Semantics Overview for the complete picture</li> </ul>"},{"location":"components/semantics/overview/","title":"Overview","text":""},{"location":"components/semantics/overview/#overview","title":"Overview","text":"<p>The semantic layer adds business context to your Vulcan data models, transforming technical table structures into business-friendly definitions that can be queried without SQL knowledge.</p>"},{"location":"components/semantics/overview/#what-is-the-semantic-layer","title":"What is the Semantic Layer?","text":"<p>The semantic layer bridges the gap between physical tables and business understanding. It provides a consistent, business-friendly interface to your data that enables self-service analytics while maintaining a single source of truth for business logic.</p>"},{"location":"components/semantics/overview/#key-benefits","title":"Key Benefits","text":"<p>For Developers:</p> <ul> <li>\u2705 Define metrics once, use everywhere</li> </ul> <ul> <li>\u2705 Version-controlled business logic</li> <li>\u2705 Consistent calculations across tools</li> </ul> <p>For Business Users:</p> <ul> <li>\u2705 Self-service analytics without SQL</li> <li>\u2705 Consistent metric definitions</li> <li>\u2705 Trusted, validated data</li> <li>\u2705 Works across BI tools and APIs</li> </ul> <p>For Organizations:</p> <ul> <li>\u2705 Single source of truth for metrics</li> <li>\u2705 Faster time to insights</li> <li>\u2705 Reduced data team bottleneck</li> <li>\u2705 Better data governance</li> </ul>"},{"location":"components/semantics/overview/#core-components","title":"Core Components","text":"<p>The semantic layer consists of two main components:</p>"},{"location":"components/semantics/overview/#semantic-models","title":"Semantic Models","text":"<p>Semantic models map physical Vulcan models to business concepts:</p> <ul> <li>Map physical models: Reference your Vulcan models defined in <code>models/</code> directory</li> <li>Expose dimensions: All model columns automatically become dimensions for filtering and grouping</li> <li>Define measures: Aggregated calculations like <code>SUM(amount)</code>, <code>COUNT(*)</code></li> <li>Create segments: Reusable filter conditions for meaningful data subsets</li> <li>Establish joins: Relationships between models for cross-model analysis</li> </ul> <pre><code>models:\n  analytics.customers:\n    alias: customers\n    measures:\n      total_customers:\n        type: count\n        expression: \"COUNT(*)\"\n</code></pre>"},{"location":"components/semantics/overview/#business-metrics","title":"Business Metrics","text":"<p>Business metrics combine measures with dimensions and time to create complete analytical definitions:</p> <ul> <li>Time-series analysis: Metrics include time dimensions for trend analysis</li> <li>Flexible granularity: Query the same metric at different time intervals (day, week, month, etc.)</li> <li>Multi-dimensional: Slice and dice by business attributes</li> <li>Ready for dashboards: Pre-configured for visualization tools</li> </ul> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n</code></pre>"},{"location":"components/semantics/overview/#how-it-works","title":"How It Works","text":"<ol> <li>Define semantic models: Create YAML files that reference your Vulcan models</li> <li>Add measures and dimensions: Define what can be calculated and filtered</li> <li>Create joins: Connect models for cross-model analysis</li> <li>Define metrics: Combine measures with time and dimensions for analysis</li> <li>Validate: Vulcan automatically validates semantic definitions during <code>plan</code> creation</li> <li>Query: Use the semantic layer via APIs or export to BI tools</li> </ol>"},{"location":"components/semantics/overview/#file-organization","title":"File Organization","text":"<p>Semantic layer definitions are YAML files in the <code>semantics/</code> directory:</p> <pre><code>project/\n\u251c\u2500\u2500 models/           # Vulcan data models (.sql files)\n\u2502   \u251c\u2500\u2500 customers.sql\n\u2502   \u251c\u2500\u2500 orders.sql\n\u2502   \u2514\u2500\u2500 events.sql\n\u2502\n\u251c\u2500\u2500 semantics/        # Semantic layer definitions (YAML)\n\u2502   \u251c\u2500\u2500 customers.yml\n\u2502   \u251c\u2500\u2500 orders.yml\n\u2502   \u2514\u2500\u2500 metrics.yml\n\u2502\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>Vulcan automatically merges all YAML files in the <code>semantics/</code> directory. File naming doesn't matter - organize by domain or model for clarity.</p>"},{"location":"components/semantics/overview/#integration-with-models","title":"Integration with Models","text":"<p>Key Insight: Model columns automatically become dimensions. The semantic layer adds measures, segments, joins, and metrics on top.</p> <p>When designing Vulcan models, keep the semantic layer in mind:</p> <pre><code>-- \u2705 Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name\n  signup_date,        -- Good time dimension\n  total_spent         -- Good for segments\nFROM raw.customers;\n</code></pre>"},{"location":"components/semantics/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Semantic Models - mapping physical models to business concepts</li> <li>Explore Business Metrics - time-series analytical definitions</li> <li>Learn about Transpiling Semantic Queries - converting semantic queries to SQL</li> <li>See examples in your Vulcan project's <code>semantics/</code> directory</li> </ul>"},{"location":"components/tests/tests/","title":"Testing","text":""},{"location":"components/tests/tests/#testing","title":"Testing","text":"<p>Testing is a critical practice in data engineering that ensures your data transformations produce correct, reliable results. Just as software engineers write unit tests to verify code behavior, data practitioners use tests to validate that models transform data as expected\u2014catching bugs before they reach production and preventing costly data quality issues.</p>"},{"location":"components/tests/tests/#why-testing-matters","title":"Why testing matters","text":"<p>Data pipelines are complex systems where small errors can cascade into significant business impacts. Testing provides several key benefits:</p> <ul> <li>Regression prevention: Catch breaking changes before they affect downstream consumers</li> <li>Confidence in changes: Refactor models knowing that tests will flag unintended behavior changes</li> <li>Documentation: Tests serve as executable specifications of expected model behavior</li> <li>Faster debugging: When something breaks, tests help pinpoint the exact transformation that failed</li> <li>Data quality assurance: Verify that aggregations, joins, and calculations produce correct results</li> </ul> <p>Unlike audits which validate data quality at runtime, tests verify the logic of your models against predefined inputs and expected outputs. Tests run either on demand (e.g., in CI/CD pipelines) or automatically when creating a new plan.</p>"},{"location":"components/tests/tests/#creating-tests","title":"Creating tests","text":"<p>A test suite is a YAML file in the <code>tests/</code> folder of your Vulcan project. The filename must begin with <code>test</code> and end with <code>.yaml</code> or <code>.yml</code>. Each file can contain multiple uniquely named unit tests.</p> <p>At minimum, a unit test must specify:</p> <ul> <li>model: The model being tested</li> <li>inputs: Mock data for upstream dependencies</li> <li>outputs: Expected results from the model's query</li> </ul>"},{"location":"components/tests/tests/#basic-example","title":"Basic example","text":"<p>Consider this <code>sales.daily_sales</code> model that aggregates orders by date:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP) AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>Here's a test that verifies the aggregation logic:</p> <pre><code>test_daily_sales_aggregation:\n  model: sales.daily_sales\n  description: &gt;\n    Test that daily_sales correctly aggregates orders by date.\n\n  inputs:\n    raw.raw_orders:\n      rows:\n        - order_id: O001\n          order_date: '2024-03-15'\n          customer_id: C001\n          product_id: P001\n          total_amount: 50.00\n        - order_id: O002\n          order_date: '2024-03-15'\n          customer_id: C002\n          product_id: P002\n          total_amount: 75.00\n        - order_id: O003\n          order_date: '2024-03-16'\n          customer_id: C001\n          product_id: P003\n          total_amount: 100.00\n\n  outputs:\n    query:\n      rows:\n        - order_date: \"2024-03-15\"\n          total_orders: 2\n          total_revenue: 125.00\n          last_order_id: \"O002\"\n        - order_date: \"2024-03-16\"\n          total_orders: 1\n          total_revenue: 100.00\n          last_order_id: \"O003\"\n</code></pre> <p>This test provides three input orders (two on March 15, one on March 16) and verifies that:</p> <ul> <li>Orders are correctly grouped by date</li> <li><code>total_orders</code> counts distinct orders per day</li> <li><code>total_revenue</code> sums the amounts correctly</li> <li><code>last_order_id</code> returns the maximum order ID per day</li> </ul>"},{"location":"components/tests/tests/#testing-models-with-multiple-dependencies","title":"Testing models with multiple dependencies","text":"<p>Real-world models often join multiple tables. Here's a test for a customer summary model that joins customers, orders, and order items:</p> <pre><code>test_full_model_basic:\n  model: vulcan_demo.full_model\n  description: |\n    Validates aggregates and averages:\n    - DISTINCT order counting\n    - SUM(quantity * unit_price)\n    - avg_order_value = total_spent / total_orders, or NULL when total_orders = 0\n\n  inputs:\n    vulcan_demo.customers:\n      - customer_id: 1\n        name: Alice\n        email: alice@example.com\n      - customer_id: 2\n        name: Bob\n        email: bob@example.com\n      - customer_id: 3\n        name: Charlie\n        email: charlie@example.com\n\n    vulcan_demo.orders:\n      # Alice has 2 orders\n      - order_id: 1001\n        customer_id: 1\n      - order_id: 1002\n        customer_id: 1\n      # Bob has 1 order\n      - order_id: 2001\n        customer_id: 2\n      # Charlie has 0 orders (no rows)\n\n    vulcan_demo.order_items:\n      # Order 1001: 2*50 + 1*25 = 125\n      - order_id: 1001\n        product_id: 501\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 502\n        quantity: 1\n        unit_price: 25\n      # Order 1002: 1*200 = 200 \u2192 Alice total = 325\n      - order_id: 1002\n        product_id: 503\n        quantity: 1\n        unit_price: 200\n      # Order 2001: 2*5 = 10 \u2192 Bob total = 10\n      - order_id: 2001\n        product_id: 504\n        quantity: 2\n        unit_price: 5\n\n  outputs:\n    query:\n      rows:\n        - customer_id: 1\n          customer_name: Alice\n          email: alice@example.com\n          total_orders: 2\n          total_spent: 325\n          avg_order_value: 162.5\n        - customer_id: 2\n          customer_name: Bob\n          email: bob@example.com\n          total_orders: 1\n          total_spent: 10\n          avg_order_value: 10.0\n        - customer_id: 3\n          customer_name: Charlie\n          email: charlie@example.com\n          total_orders: 0\n          total_spent: 0\n          avg_order_value: null  # Division by zero handled\n</code></pre>"},{"location":"components/tests/tests/#testing-incremental-models","title":"Testing incremental models","text":"<p>Incremental models require special attention because they filter data by time range. Use the <code>vars</code> attribute to set <code>start</code> and <code>end</code> dates:</p> <pre><code>test_incremental_by_time_range_basic:\n  model: vulcan_demo.incremental_by_time_range\n  description: |\n    Validates per-(order_date, product_id) aggregates over a fixed two-day window.\n    Checks DISTINCT order counts, quantity and revenue sums, and AVG(unit_price).\n  vars:\n    start: '2025-01-01'\n    end: '2025-01-02'\n\n  inputs:\n    vulcan_demo.products:\n      - product_id: 10\n        name: Widget\n        category: Electronics\n      - product_id: 20\n        name: Gizmo\n        category: Home\n\n    vulcan_demo.orders:\n      - order_id: 1001\n        customer_id: 9001\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1002\n        customer_id: 9002\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1003\n        customer_id: 9003\n        warehouse_id: 2\n        order_date: '2025-01-02'\n\n    vulcan_demo.order_items:\n      # 2025-01-01\n      - order_id: 1001\n        product_id: 10\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 20\n        quantity: 1\n        unit_price: 200\n      - order_id: 1002\n        product_id: 10\n        quantity: 1\n        unit_price: 60\n      # 2025-01-02\n      - order_id: 1003\n        product_id: 10\n        quantity: 5\n        unit_price: 40\n\n  outputs:\n    query:\n      rows:\n        - order_date: '2025-01-01'\n          product_id: 20\n          product_name: Gizmo\n          category: Home\n          order_count: 1\n          total_quantity: 1\n          total_sales_amount: 200\n          avg_unit_price: 200\n        - order_date: '2025-01-01'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 2\n          total_quantity: 3\n          total_sales_amount: 160\n          avg_unit_price: 55\n        - order_date: '2025-01-02'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 1\n          total_quantity: 5\n          total_sales_amount: 200\n          avg_unit_price: 40\n</code></pre>"},{"location":"components/tests/tests/#testing-ctes","title":"Testing CTEs","text":"<p>Individual CTEs within the model's query can also be tested. Given a model with a CTE:</p> <pre><code>WITH filtered_orders_cte AS (\n  SELECT id, item_id\n  FROM vulcan_demo.incremental_model\n  WHERE item_id = 1\n)\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM filtered_orders_cte\nGROUP BY item_id\n</code></pre> <p>Test both the CTE and final query:</p> <pre><code>test_model_with_cte:\n  model: vulcan_demo.full_model\n  inputs:\n    vulcan_demo.incremental_model:\n      rows:\n        - id: 1\n          item_id: 1\n        - id: 2\n          item_id: 1\n        - id: 3\n          item_id: 2\n  outputs:\n    ctes:\n      filtered_orders_cte:\n        rows:\n          - id: 1\n            item_id: 1\n          - id: 2\n            item_id: 1\n    query:\n      rows:\n        - item_id: 1\n          num_orders: 2\n</code></pre>"},{"location":"components/tests/tests/#supported-data-formats","title":"Supported data formats","text":"<p>Vulcan supports multiple ways to define input and output data:</p>"},{"location":"components/tests/tests/#yaml-dictionaries-default","title":"YAML dictionaries (default)","text":"<pre><code>inputs:\n  vulcan_demo.orders:\n    rows:\n      - order_id: 1001\n        customer_id: 1\n        order_date: '2025-01-01'\n</code></pre>"},{"location":"components/tests/tests/#csv-format","title":"CSV format","text":"<pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    rows: |\n      order_id,customer_id,order_date\n      1001,1,2025-01-01\n      1002,2,2025-01-01\n</code></pre>"},{"location":"components/tests/tests/#sql-queries","title":"SQL queries","text":"<pre><code>inputs:\n  vulcan_demo.orders:\n    query: |\n      SELECT 1001 AS order_id, 1 AS customer_id, '2025-01-01' AS order_date\n      UNION ALL\n      SELECT 1002 AS order_id, 2 AS customer_id, '2025-01-01' AS order_date\n</code></pre>"},{"location":"components/tests/tests/#external-files","title":"External files","text":"<pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    path: fixtures/orders_test_data.csv\n</code></pre>"},{"location":"components/tests/tests/#omitting-columns","title":"Omitting columns","text":"<p>For wide tables, you can omit columns (treated as <code>NULL</code>) or use partial matching:</p> <pre><code>outputs:\n  query:\n    partial: true  # Only test specified columns\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre> <p>To apply partial matching to all outputs:</p> <pre><code>outputs:\n  partial: true\n  query:\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre>"},{"location":"components/tests/tests/#freezing-time","title":"Freezing time","text":"<p>For models using <code>CURRENT_TIMESTAMP</code> or similar functions, set <code>execution_time</code> to make tests deterministic:</p> <pre><code>test_with_timestamp:\n  model: vulcan_demo.audit_log\n  outputs:\n    query:\n      - event: \"login\"\n        created_at: \"2023-01-01 12:05:03\"\n  vars:\n    execution_time: \"2023-01-01 12:05:03\"\n</code></pre>"},{"location":"components/tests/tests/#running-tests","title":"Running tests","text":""},{"location":"components/tests/tests/#command-line","title":"Command line","text":"<pre><code># Run all tests\nvulcan test\n\n# Run specific test file\nvulcan test tests/test_daily_sales.yaml\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_aggregation\n\n# Run tests matching a pattern\nvulcan test tests/test_*\n</code></pre>"},{"location":"components/tests/tests/#example-output","title":"Example output","text":"<pre><code>$ vulcan test\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.024s\n\nOK\n</code></pre> <p>When tests fail:</p> <pre><code>$ vulcan test\nF\n======================================================================\nFAIL: test_daily_sales_aggregation (tests/test_daily_sales.yaml)\n----------------------------------------------------------------------\nAssertionError: Data mismatch (exp: expected, act: actual)\n\n  total_orders\n         exp  act\n0        3.0  2.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.012s\n\nFAILED (failures=1)\n</code></pre> <p>&lt;!-- ### Notebook magic</p> <pre><code>import vulcan\n%run_test\n``` --&gt;\n\n## Automatic test generation\n\nGenerate tests automatically using the `create_test` command:\n\n```bash\nvulcan create_test vulcan_demo.daily_sales \\\n  --query raw.raw_orders \"SELECT * FROM raw.raw_orders WHERE order_date BETWEEN '2025-01-01' AND '2025-01-02' LIMIT 10\" \n</code></pre> <p>This creates a test file with actual data from your warehouse, making it easy to bootstrap your test suite.</p> <p>&lt;!-- ## Using a different testing connection</p> <p>Override the testing connection for specific tests:</p> <pre><code>test_with_spark:\n  gateway: spark_testing\n  model: vulcan_demo.complex_model\n  # ... rest of test\n</code></pre> <p>Configure the gateway in <code>config.yaml</code>:</p> <pre><code>gateways:\n  spark_testing:\n    test_connection:\n      type: spark\n      config:\n        \"spark.master\": \"local\"\n``` --&gt;\n\n## Troubleshooting\n\n### Preserving fixtures\n\nUse `--preserve-fixtures` to keep test fixtures for debugging:\n\n```bash\nvulcan test --preserve-fixtures\n</code></pre> <p>Fixtures are created as views in a schema named <code>vulcan_test_&lt;random_ID&gt;</code>.</p>"},{"location":"components/tests/tests/#type-mismatches","title":"Type mismatches","text":"<p>If Vulcan can't infer column types correctly, specify them explicitly:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    columns:\n      order_id: INT\n      order_date: DATE\n      total_amount: DECIMAL(10,2)\n    rows:\n      - order_id: 1001\n        order_date: '2025-01-01'\n        total_amount: 99.99\n</code></pre>"},{"location":"components/tests/tests/#unit-test-structure","title":"Unit test structure","text":""},{"location":"components/tests/tests/#test_name","title":"<code>&lt;test_name&gt;</code>","text":"<p>The unique name of the test.</p>"},{"location":"components/tests/tests/#test_namemodel","title":"<code>&lt;test_name&gt;.model</code>","text":"<p>The name of the model being tested. This model must be defined in the project's <code>models/</code> folder.</p>"},{"location":"components/tests/tests/#test_namedescription","title":"<code>&lt;test_name&gt;.description</code>","text":"<p>An optional description of the test, which can be used to provide additional context.</p>"},{"location":"components/tests/tests/#test_nameschema","title":"<code>&lt;test_name&gt;.schema</code>","text":"<p>The name of the schema that will contain the views that are necessary to run this unit test.</p>"},{"location":"components/tests/tests/#test_namegateway","title":"<code>&lt;test_name&gt;.gateway</code>","text":"<p>The gateway whose <code>test_connection</code> will be used to run this test. If not specified, the default gateway is used.</p>"},{"location":"components/tests/tests/#test_nameinputs","title":"<code>&lt;test_name&gt;.inputs</code>","text":"<p>The inputs that will be used to test the target model. If the model has no dependencies, this can be omitted.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_model","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;</code>","text":"<p>A model that the target model depends on.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelrows","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>","text":"<p>The rows of the upstream model, defined as an array of dictionaries that map columns to their values:</p> <pre><code>    &lt;upstream_model&gt;:\n      rows:\n        - &lt;column_name&gt;: &lt;column_value&gt;\n        ...\n</code></pre> <p>If <code>rows</code> is the only key under <code>&lt;upstream_model&gt;</code>, then it can be omitted:</p> <pre><code>    &lt;upstream_model&gt;:\n      - &lt;column_name&gt;: &lt;column_value&gt;\n      ...\n</code></pre> <p>When the input format is <code>csv</code>, the data can be specified inline under <code>rows</code> :</p> <pre><code>    &lt;upstream_model&gt;:\n      rows: |\n        &lt;column1_name&gt;,&lt;column2_name&gt;\n        &lt;row1_value&gt;,&lt;row1_value&gt;\n        &lt;row2_value&gt;,&lt;row2_value&gt;\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelformat","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.format</code>","text":"<p>The optional <code>format</code> key allows for control over how the input data is loaded.</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n</code></pre> <p>Currently, the following formats are supported: <code>yaml</code> (default), <code>csv</code>.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcsv_settings","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.csv_settings</code>","text":"<p>When the<code>format</code> is CSV, you can control the behaviour of data loading under <code>csv_settings</code>:</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n      csv_settings: \n        sep: \"#\"\n        skip_blank_lines: true\n      rows: |\n        &lt;column1_name&gt;#&lt;column2_name&gt;\n        &lt;row1_value&gt;#&lt;row1_value&gt;\n        &lt;row2_value&gt;#&lt;row2_value&gt;\n</code></pre> <p>Learn more about the supported CSV settings.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelpath","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.path</code>","text":"<p>The optional <code>path</code> key specifies the pathname of the data to be loaded.</p> <pre><code>    &lt;upstream_model&gt;:\n      path: filepath/test_data.yaml\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcolumns","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.columns</code>","text":"<p>An optional dictionary that maps columns to their types:</p> <pre><code>    &lt;upstream_model&gt;:\n      columns:\n        &lt;column_name&gt;: &lt;column_type&gt;\n        ...\n</code></pre> <p>This can be used to help Vulcan interpret the row values correctly in the context of SQL.</p> <p>Any number of columns may be omitted from this mapping, in which case their types will be inferred on a best-effort basis. Explicitly casting the corresponding columns in the model's query will enable Vulcan to infer their types more accurately.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelquery","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the input rows:</p> <pre><code>    &lt;upstream_model&gt;:\n      query: &lt;sql_query&gt;\n</code></pre> <p>This provides more control over how the input data must be interpreted.</p> <p>The <code>query</code> key can't be used together with the <code>rows</code> key.</p>"},{"location":"components/tests/tests/#test_nameoutputs","title":"<code>&lt;test_name&gt;.outputs</code>","text":"<p>The target model's expected outputs.</p> <p>Note: the columns in each row of an expected output must appear in the same relative order as they are selected in the corresponding query.</p>"},{"location":"components/tests/tests/#test_nameoutputspartial","title":"<code>&lt;test_name&gt;.outputs.partial</code>","text":"<p>A boolean flag that indicates whether only a subset of the output columns will be tested. When set to <code>true</code>, only the columns referenced in the corresponding expected rows will be tested.</p> <p>See also: Omitting columns.</p>"},{"location":"components/tests/tests/#test_nameoutputsquery","title":"<code>&lt;test_name&gt;.outputs.query</code>","text":"<p>The expected output of the target model's query. This is optional, as long as <code>&lt;test_name&gt;.outputs.ctes</code> is present.</p>"},{"location":"components/tests/tests/#test_nameoutputsquerypartial","title":"<code>&lt;test_name&gt;.outputs.query.partial</code>","text":"<p>Same as <code>&lt;test_name&gt;.outputs.partial</code>, but applies only to the output of the target model's query.</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryrows","title":"<code>&lt;test_name&gt;.outputs.query.rows</code>","text":"<p>The expected rows of the target model's query.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>.</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryquery","title":"<code>&lt;test_name&gt;.outputs.query.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the expected rows for the target model's query.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>.</p>"},{"location":"components/tests/tests/#test_nameoutputsctes","title":"<code>&lt;test_name&gt;.outputs.ctes</code>","text":"<p>The expected output per each individual top-level Common Table Expression (CTE) defined in the target model's query. This is optional, as long as <code>&lt;test_name&gt;.outputs.query</code> is present.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_name","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;</code>","text":"<p>The expected output of the CTE with name <code>&lt;cte_name&gt;</code>.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namepartial","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.partial</code>","text":"<p>Same as <code>&lt;test_name&gt;.outputs.partial</code>, but applies only to the output of the CTE with name <code>&lt;cte_name&gt;</code>.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namerows","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.rows</code>","text":"<p>The expected rows of the CTE with name <code>&lt;cte_name&gt;</code>.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namequery","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.query</code>","text":"<p>An optional SQL query that will be executed against the testing connection to generate the expected rows for the CTE with name <code>&lt;cte_name&gt;</code>.</p> <p>See also: <code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>.</p>"},{"location":"components/tests/tests/#test_namevars","title":"<code>&lt;test_name&gt;.vars</code>","text":"<p>An optional dictionary that assigns values to macro variables:</p> <pre><code>  vars:\n    start: 2022-01-01\n    end: 2022-01-01\n    execution_time: 2022-01-01\n    &lt;macro_variable_name&gt;: &lt;macro_variable_value&gt;\n</code></pre> <p>There are three special macro variables: <code>start</code>, <code>end</code>, and <code>execution_time</code>. If these are set, they will override the corresponding date macros of the target model. For example, <code>@execution_ds</code> will render to <code>2022-01-01</code> if <code>execution_time</code> is set to this value.</p> <p>Additionally, SQL expressions like <code>CURRENT_DATE</code> and <code>CURRENT_TIMESTAMP</code> will produce the same datetime value as <code>execution_time</code>, when it is set.</p>"},{"location":"concepts-old/environments/","title":"Environments","text":""},{"location":"concepts-old/environments/#environments","title":"Environments","text":"<p>Environments are isolated namespaces that allow you to test and preview your changes.</p> <p>Vulcan differentiates between production and development environments. Currently, only the environment with the name <code>prod</code> is treated by Vulcan as the production one. Environments with other names are considered to be development ones.</p> <p>Models in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name <code>db.model_a</code> in the target environment <code>my_dev</code>, the <code>db__my_dev.model_a</code> table name should be used in a query. Models in the production environment are referred to by their original names.</p>"},{"location":"concepts-old/environments/#why-use-environments","title":"Why use environments","text":"<p>Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.</p> <p>Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment do not impact other environments. At the same time, any computation that was done in this environment can be safely reused in other environments.</p>"},{"location":"concepts-old/environments/#how-to-use-environments","title":"How to use environments","text":"<p>When running the plan command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is <code>prod</code>, which refers to the production environment. Environment with names other than <code>prod</code> are considered to be development environments.</p> <p>By default, the <code>vulcan plan</code> command targets the production (<code>prod</code>) environment.</p>"},{"location":"concepts-old/environments/#example","title":"Example","text":"<p>A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name <code>my_dev</code>, run:</p> <p></p><pre><code>vulcan plan my_dev\n</code></pre> A new environment is created automatically the first time a plan is applied to it.<p></p>"},{"location":"concepts-old/environments/#how-environments-work","title":"How environments work","text":"<p>Whenever a model definition changes, a new model snapshot is created with a unique fingerprint. This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model snapshots.</p> <p>Refer to plans for additional details.</p>"},{"location":"concepts-old/environments/#date-range","title":"Date range","text":"<p>A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the plan creation.</p>"},{"location":"concepts-old/glossary/","title":"Glossary","text":""},{"location":"concepts-old/glossary/#glossary","title":"Glossary","text":""},{"location":"concepts-old/glossary/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.</p>"},{"location":"concepts-old/glossary/#backfill","title":"Backfill","text":"<p>Load or refresh model data, triggered by a vulcan plan command.</p>"},{"location":"concepts-old/glossary/#catalog","title":"Catalog","text":"<p>A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.</p>"},{"location":"concepts-old/glossary/#cicd","title":"CI/CD","text":"<p>An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with tests and audits.</p>"},{"location":"concepts-old/glossary/#cte","title":"CTE","text":"<p>A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to tests.</p>"},{"location":"concepts-old/glossary/#dag","title":"DAG","text":"<p>Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.</p>"},{"location":"concepts-old/glossary/#data-modeling","title":"Data modeling","text":"<p>Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.</p>"},{"location":"concepts-old/glossary/#data-pipeline","title":"Data pipeline","text":"<p>The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.</p>"},{"location":"concepts-old/glossary/#data-transformation","title":"Data transformation","text":"<p>Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.</p>"},{"location":"concepts-old/glossary/#data-warehouse","title":"Data warehouse","text":"<p>The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.</p>"},{"location":"concepts-old/glossary/#direct-modification","title":"Direct Modification","text":"<p>A change to a model's definition from the user instead of being inherited from an upstream dependency like Indirect Modification.</p>"},{"location":"concepts-old/glossary/#elt","title":"ELT","text":"<p>Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.</p>"},{"location":"concepts-old/glossary/#etl","title":"ETL","text":"<p>Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.</p>"},{"location":"concepts-old/glossary/#full-refresh","title":"Full refresh","text":"<p>In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.</p>"},{"location":"concepts-old/glossary/#idempotency","title":"Idempotency","text":"<p>The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.</p>"},{"location":"concepts-old/glossary/#incremental-loads","title":"Incremental Loads","text":"<p>Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See Model Kinds for more information.</p>"},{"location":"concepts-old/glossary/#indirect-modification","title":"Indirect Modification","text":"<p>A change to model's upstream dependency and not to the model itself like a Direct Modification.</p>"},{"location":"concepts-old/glossary/#integration","title":"Integration","text":"<p>Combining data from various sources (such as from a data warehouse) into one unified view.</p>"},{"location":"concepts-old/glossary/#lineage","title":"Lineage","text":"<p>The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.</p>"},{"location":"concepts-old/glossary/#physical-layer","title":"Physical Layer","text":"<p>The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the Vulcan virtual layer's views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.</p>"},{"location":"concepts-old/glossary/#plan-summaries","title":"Plan Summaries","text":"<p>An upcoming feature that allows users to see a summary of changes applied to a given environment.</p>"},{"location":"concepts-old/glossary/#semantic-understanding","title":"Semantic Understanding","text":"<p>Vulcan, by leveraging SQLGlot, understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.</p>"},{"location":"concepts-old/glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.</p>"},{"location":"concepts-old/glossary/#table","title":"Table","text":"<p>A table is the visual representation of data stored in rows and columns.</p>"},{"location":"concepts-old/glossary/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.</p>"},{"location":"concepts-old/glossary/#view","title":"View","text":"<p>A view is the result of a SQL query on a database.</p>"},{"location":"concepts-old/glossary/#virtual-environments","title":"Virtual Environments","text":"<p>Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See plan application for more information.</p>"},{"location":"concepts-old/glossary/#virtual-layer","title":"Virtual Layer","text":"<p>The virtual layer is Vulcan's abstraction layer over the physical layer and physical data storage. While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.</p>"},{"location":"concepts-old/glossary/#virtual-update","title":"Virtual Update","text":"<p>Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See Virtual Update for more information.</p>"},{"location":"concepts-old/glossary/#virtual-preview","title":"Virtual Preview","text":"<p>Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.</p>"},{"location":"concepts-old/overview/","title":"Overview","text":""},{"location":"concepts-old/overview/#overview","title":"Overview","text":"<p>This page provides a conceptual overview of what Vulcan does and how its components fit together.</p>"},{"location":"concepts-old/overview/#what-vulcan-is","title":"What Vulcan is","text":"<p>Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of execution engines.</p> <p>It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.</p> <p>You can use Vulcan with the CLI.</p>"},{"location":"concepts-old/overview/#how-vulcan-works","title":"How Vulcan works","text":""},{"location":"concepts-old/overview/#create-models","title":"Create models","text":"<p>You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.</p>"},{"location":"concepts-old/overview/#make-a-plan","title":"Make a plan","text":"<p>Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.</p> <p>Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change before you expend the time and resources to actually perform the computations.</p> <p>Vulcan automatically identifies all affected models and the computations a change entails by creating a \"Vulcan plan.\" When you execute the <code>plan</code> command, Vulcan generates the plan for the environment specified in the command (e.g., dev, test, prod).</p> <p>The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.</p> <p>Learn more about plans.</p>"},{"location":"concepts-old/overview/#apply-the-plan","title":"Apply the plan","text":"<p>After using <code>plan</code> to understand the impacts of a change in an environment, Vulcan offers to execute the computations by <code>apply</code>ing the plan. However, you must provide additional information that determines the scope of what computations are executed.</p> <p>The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.</p> <p>\"Backfilling\" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.</p> <p>Most business data is temporal \u2014 each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.</p> <p>The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.</p>"},{"location":"concepts-old/overview/#build-a-virtual-environment","title":"Build a Virtual Environment","text":"<p>Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.</p> <p>One challenge with using multiple data environments is that backfill and other computations must happen twice \u2014 once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.</p> <p>Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.</p> <p>Vulcan uses its knowledge of equivalent outputs to create a Virtual Environment. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but without computation or data movement.</p> <p>Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.</p>"},{"location":"concepts-old/overview/#test-your-code-and-data","title":"Test your code and data","text":"<p>Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.</p>"},{"location":"concepts-old/overview/#tests","title":"Tests","text":"<p>Vulcan \"tests\" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model code \u2014 you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.</p> <p>Vulcan automatically runs tests when you apply a <code>plan</code>, or you can run them on demand with the <code>test</code> command.</p>"},{"location":"concepts-old/overview/#audits","title":"Audits","text":"<p>In contrast to tests, Vulcan \"audits\" validate the results of model code applied to your actual data.</p> <p>You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure <code>your_field</code> has no <code>NULL</code> values would include <code>WHERE your_field IS NULL</code>. If any NULLs are detected, the query will return at least one row and the audit will fail.</p> <p>Audits are flexible \u2014 they can be tied to a specific model's contents, or you can use macros to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.</p> <p>You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.</p> <p>Vulcan automatically runs audits when you apply a <code>plan</code> to an environment, or you can run them on demand with the <code>audit</code> command.</p>"},{"location":"concepts-old/overview/#infrastructure-and-orchestration","title":"Infrastructure and orchestration","text":"<p>Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use \u2014 its only requirement is access to the target SQL/analytics engine.</p> <p>Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a <code>vulcan</code> schema in your data warehouse for its internal metadata.</p>"},{"location":"concepts-old/plans/","title":"Plans","text":""},{"location":"concepts-old/plans/#plans","title":"Plans","text":"<p>A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target environment. In order for any model changes to take effect in a target environment, a plan needs to be created and applied.</p>"},{"location":"concepts-old/plans/#plan-architecture-overview","title":"Plan Architecture Overview","text":"<p>The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:</p> <pre><code>flowchart TD\n    subgraph \"1\ufe0f\u20e3 Local Development\"\n        A[\ud83d\udc68\u200d\ud83d\udcbb Developer modifies model files&lt;br/&gt;\ud83d\udcdd Edit SQL/Python models]\n        B[\ud83d\udcc1 Local project state&lt;br/&gt;\u2728 Your changes ready]\n    end\n\n    subgraph \"2\ufe0f\u20e3 Plan Creation\"\n        C[\u26a1 vulcan plan&lt;br/&gt;\ud83d\ude80 Command execution]\n        D[\ud83d\udd0e Compare local vs environment&lt;br/&gt;\ud83d\udcca State comparison]\n        E{\ud83d\udd0d Changes detected?}\n        F[\ud83d\udccb Generate plan summary&lt;br/&gt;\u2728 Plan ready for review]\n        G[\ud83c\udff7\ufe0f Change categorization&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking / \ud83d\udfe1 Forward-only]\n    end\n\n    subgraph \"3\ufe0f\u20e3 Plan Review\"\n        H[\ud83d\udc40 Review plan output&lt;br/&gt;\ud83d\udcca Check changes &amp; impacts]\n        I{\u2705 Apply plan?}\n        J[\u274c Cancel&lt;br/&gt;\ud83d\udeab No changes applied]\n    end\n\n    subgraph \"4\ufe0f\u20e3 Plan Application\"\n        K[\ud83d\udd37 Create model variants&lt;br/&gt;\ud83d\udd11 With unique fingerprints]\n        L[\ud83d\uddc4\ufe0f Create physical tables&lt;br/&gt;\ud83d\udcbe In data warehouse]\n        M[\ud83d\udd04 Backfill data&lt;br/&gt;\ud83d\udcc8 Process historical data]\n        N[\ud83d\udc41\ufe0f Update virtual layer&lt;br/&gt;\ud83d\udd0d Create/update views]\n        O[\ud83c\udf0d Update environment references&lt;br/&gt;\ud83d\udd17 Point to new variants]\n    end\n\n    subgraph \"5\ufe0f\u20e3 Result\"\n        P[\u2705 Environment updated&lt;br/&gt;\ud83c\udf89 Changes deployed]\n        Q[\ud83d\udd0d Models accessible via views&lt;br/&gt;\ud83d\udcca Ready for queries]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\u27a1\ufe0f\"| C\n    C --&gt;|\"\ud83d\udd0d\"| D\n    D --&gt;|\"\ud83d\udd0e\"| E\n    E --&gt;|\"\u2705 Yes\"| F\n    E --&gt;|\"\u274c No\"| P\n    F --&gt;|\"\ud83c\udff7\ufe0f\"| G\n    G --&gt;|\"\ud83d\udccb\"| H\n    H --&gt;|\"\ud83d\udc40\"| I\n    I --&gt;|\"\u2705 Yes\"| K\n    I --&gt;|\"\u274c No\"| J\n    K --&gt;|\"\ud83d\udd37\"| L\n    L --&gt;|\"\ud83d\udcbe\"| M\n    M --&gt;|\"\ud83d\udd04\"| N\n    N --&gt;|\"\ud83d\udc41\ufe0f\"| O\n    O --&gt;|\"\ud83d\udd17\"| P\n    P --&gt;|\"\u2728\"| Q\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre>"},{"location":"concepts-old/plans/#plan-components","title":"Plan Components","text":"<pre><code>graph LR\n    subgraph \"\ud83d\udccb Plan Contents\"\n        PC1[\u2795 Added Models&lt;br/&gt;\u2728 New models to create]\n        PC2[\u2796 Removed Models&lt;br/&gt;\ud83d\uddd1\ufe0f Models to delete]\n        PC3[\u270f\ufe0f Modified Models&lt;br/&gt;\ud83d\udcdd With diffs]\n        PC4[\ud83d\udd17 Indirectly Affected&lt;br/&gt;\ud83d\udcca Downstream models]\n        PC5[\ud83d\udcc5 Backfill Requirements&lt;br/&gt;\ud83d\udcc6 Date ranges]\n    end\n\n    subgraph \"\ud83c\udff7\ufe0f Change Types\"\n        CT1[\ud83d\udd34 Breaking Change&lt;br/&gt;\u26a0\ufe0f Requires downstream backfill&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        CT2[\ud83d\udfe2 Non-Breaking Change&lt;br/&gt;\u2705 Only direct model backfill&lt;br/&gt;\ud83c\udfaf Isolated impact]\n        CT3[\ud83d\udfe1 Forward-Only&lt;br/&gt;\u267b\ufe0f Reuses existing tables&lt;br/&gt;\ud83d\udcb0 Cost-effective]\n    end\n\n    PC3 --&gt;|\"\ud83d\udd34\"| CT1\n    PC3 --&gt;|\"\ud83d\udfe2\"| CT2\n    PC3 --&gt;|\"\ud83d\udfe1\"| CT3\n    PC4 --&gt;|\"\ud83d\udd34\"| CT1\n\n    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000</code></pre> <p>During plan creation:</p> <ul> <li>The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.</li> <li>Users may be prompted to categorize changes to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through configuration.</li> <li>Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.</li> </ul> <p>The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:</p> <ul> <li>A list of added models</li> <li>A list of removed models</li> <li>A list of directly modified models and a text diff of changes that have been made</li> <li>A list of indirectly modified models</li> <li>Missing data intervals for affected models</li> <li>A date range that will be affected by the plan application</li> </ul> <p>To create a new plan, run the following command: </p><pre><code>vulcan plan [environment name]\n</code></pre><p></p> <p>If no environment name is specified, the plan is generated for the <code>prod</code> environment.</p>"},{"location":"concepts-old/plans/#change-categories","title":"Change categories","text":"<p>Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.</p> <p>If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.</p>"},{"location":"concepts-old/plans/#change-propagation-flow","title":"Change Propagation Flow","text":"<p>The following diagram illustrates how changes propagate through the dependency graph:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Dependencies\"\n        A[\ud83d\udce5 raw.raw_orders&lt;br/&gt;\u2b06\ufe0f Upstream]\n        B[\ud83d\udcca sales.daily_sales&lt;br/&gt;\ud83d\udd04 Midstream]\n        C[\ud83d\udcc8 sales.weekly_sales&lt;br/&gt;\u2b07\ufe0f Downstream]\n        D[\ud83d\udcc9 analytics.revenue_report&lt;br/&gt;\u2b07\ufe0f Downstream]\n    end\n\n    subgraph \"\ud83d\udfe2 Scenario 1: Non-Breaking Change\"\n        NB1[\u2795 Add column to daily_sales&lt;br/&gt;\u2728 New column added]\n        NB2[\u2705 Only daily_sales backfilled&lt;br/&gt;\ud83d\udd04 Single model update]\n        NB3[\u23ed\ufe0f weekly_sales NOT affected&lt;br/&gt;\u2705 No cascade]\n        NB4[\u23ed\ufe0f revenue_report NOT affected&lt;br/&gt;\u2705 No cascade]\n    end\n\n    subgraph \"\ud83d\udd34 Scenario 2: Breaking Change\"\n        BC1[\ud83d\udd0d Add WHERE clause to daily_sales&lt;br/&gt;\u26a0\ufe0f Filter logic changed]\n        BC2[\ud83d\udd04 daily_sales backfilled&lt;br/&gt;\ud83d\udcca Data reprocessed]\n        BC3[\ud83d\udd04 weekly_sales backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        BC4[\ud83d\udd04 revenue_report backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\ud83d\udce4\"| C\n    B --&gt;|\"\ud83d\udce4\"| D\n\n    NB1 --&gt;|\"\u270f\ufe0f\"| B\n    B --&gt;|\"\u2705\"| NB2\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| C\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| D\n\n    BC1 --&gt;|\"\u26a0\ufe0f\"| B\n    B --&gt;|\"\ud83d\udd04\"| BC2\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC3\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC4\n    BC3 --&gt;|\"\ud83d\udd04\"| C\n    BC4 --&gt;|\"\ud83d\udd04\"| D\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000</code></pre>"},{"location":"concepts-old/plans/#breaking-change","title":"Breaking change","text":"<p>If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.</p> <p>In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to backfilling).</p> <p>Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's <code>WHERE</code> clause is a breaking change because downstream models contain rows that would now be filtered out.</p>"},{"location":"concepts-old/plans/#non-breaking-change","title":"Non-breaking change","text":"<p>A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.</p> <p>This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.</p> <p>If any downstream models contain a <code>select *</code> from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.</p>"},{"location":"concepts-old/plans/#summary","title":"Summary","text":"Change Category Change Type Behaviour Breaking Direct or Indirect Backfill Non-breaking Direct Backfill Non-breaking Indirect No Backfill"},{"location":"concepts-old/plans/#forward-only-change","title":"Forward-only change","text":"<p>In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.</p> <p>A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the <code>prod</code> environment). This means that no backfill will take place.</p> <p>While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.</p> <p>In either case the data produced this way in the development environment can only be used for preview and will not be reused once the change is deployed to production. See Forward-only Plans for more details.</p> <p>This category is assigned by Vulcan automatically either when a user opts into using a forward-only plan or when a model is explicitly configured to be forward-only.</p>"},{"location":"concepts-old/plans/#plan-application","title":"Plan application","text":"<p>Once a plan has been created and reviewed, it is then applied to the target environment in order for its changes to take effect.</p> <p>Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a snapshot with a unique fingerprint is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for forward-only plans.</p> <p>When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.</p>"},{"location":"concepts-old/plans/#model-versioning-architecture","title":"Model Versioning Architecture","text":"<p>The following diagram shows how model variants, physical tables, and environments relate:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcdd Model Definitions\"\n        M1[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 1&lt;br/&gt;\u2728 Original]\n        M2[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 2 - Modified&lt;br/&gt;\u270f\ufe0f Updated]\n    end\n\n    subgraph \"\ud83d\udd37 Model Variants &amp; Snapshots\"\n        V1[\ud83d\udd37 Variant 1&lt;br/&gt;\ud83d\udd11 Fingerprint: abc123&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        V2[\ud83d\udd37 Variant 2&lt;br/&gt;\ud83d\udd11 Fingerprint: def456&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        S1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd10 Immutable state]\n        S2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd10 Immutable state]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Tables\"\n        T1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__abc123&lt;br/&gt;\ud83d\udcbe Actual data storage]\n        T2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__def456&lt;br/&gt;\ud83d\udcbe Actual data storage]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer Views\"\n        VL1[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 1&lt;br/&gt;\ud83d\udd17 Reference mapping]\n        VL2[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 2&lt;br/&gt;\ud83d\udd17 Reference mapping]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;\u2705 References Variant 1&lt;br/&gt;\ud83c\udf10 Live production data]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;\ud83d\udd2c References Variant 2&lt;br/&gt;\ud83e\uddea Testing environment]\n    end\n\n    M1 --&gt;|\"\u2728\"| V1\n    M2 --&gt;|\"\u270f\ufe0f\"| V2\n    V1 --&gt;|\"\ud83d\udcf8\"| S1\n    V2 --&gt;|\"\ud83d\udcf8\"| S2\n    S1 --&gt;|\"\ud83d\udcbe\"| T1\n    S2 --&gt;|\"\ud83d\udcbe\"| T2\n    T1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    T2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n    PROD --&gt;|\"\ud83d\udd17\"| V1\n    DEV --&gt;|\"\ud83d\udd17\"| V2\n\n    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000</code></pre> <p></p> <p>Each model variant gets its own physical table while environments only contain references to these tables.</p> <p>This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.</p> <p>Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to Virtual Update) as long as its physical table hasn't been garbage collected by the janitor process.</p> <p>Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.</p>"},{"location":"concepts-old/plans/#backfilling","title":"Backfilling","text":"<p>Despite all the benefits, the approach described above is not without trade-offs.</p> <p>When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.</p> <p>We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:</p> <ul> <li>When a VIEW model is created</li> <li>When a FULL model is built</li> <li>When an INCREMENTAL model is built for the first time</li> <li>When an INCREMENTAL model has recent data appended to it</li> <li>When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)</li> </ul> <p>Note for incremental models: despite the fact that backfilling can happen incrementally (see <code>batch_size</code> parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a forward-only plan instead.</p>"},{"location":"concepts-old/plans/#virtual-update","title":"Virtual Update","text":"<p>A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.</p> <p>With this approach, the process of promoting a change to production is reduced to reference swapping.</p> <p>If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.</p>"},{"location":"concepts-old/plans/#start-and-end-dates","title":"Start and end dates","text":"<p>The <code>plan</code> command provides two temporal options: <code>--start</code> and <code>--end</code>. These options are only applicable to plans for non-prod environments.</p> <p>For context, every model has a start date. The start can be specified in the model definition, in the project configuration's <code>model_defaults</code>, or by Vulcan's default value of yesterday.</p> <p>Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the <code>plan</code> command's <code>--start</code> and <code>--end</code> options are not supported for regular plans against prod. The options are supported for restatement plans against prod to allow re-processing a subset of existing data.</p> <p>Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the <code>--start</code> and <code>--end</code> options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.</p>"},{"location":"concepts-old/plans/#model-kind-limitations","title":"Model kind limitations","text":"<p>Some model kinds do not support backfilling a limited date range.</p> <p>For context, Vulcan strives to make models idempotent, meaning that if we ran them multiple times we would get the same correct result every time.</p> <p>However, some model kinds are inherently non-idempotent:</p> <ul> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>SCD_TYPE_2_BY_TIME</li> <li>SCD_TYPE_2_BY_COLUMN</li> <li>Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)</li> </ul> <p>Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:</p> <ul> <li>If the <code>--start</code> option date is the same as or before the model's start date, the model is fully refreshed for all of time</li> <li>If the <code>--start</code> option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production</li> </ul>"},{"location":"concepts-old/plans/#example","title":"Example","text":"<p>Consider a Vulcan project with a default start date of 2024-09-20.</p> <p>It contains the following <code>INCREMENTAL_BY_UNIQUE_KEY</code> model that specifies an explicit start date of 2024-09-23:</p> <pre><code>MODEL (\n  name vulcan_example.start_end_model,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key item_id\n  ),\n  start '2024-09-23'\n);\n\nSELECT\n  item_id,\n  num_orders\nFROM\n  vulcan_example.full_model\n</code></pre> <p>When we run the project's first plan, we see that Vulcan correctly detected a different start date for our <code>start_end_model</code> than the other models (which have the project default start of 2024-09-20):</p> <pre><code>\u276f vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u251c\u2500\u2500 vulcan_example.seed_model\n    \u2514\u2500\u2500 vulcan_example.start_end_model\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example.full_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.incremental_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.seed_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example.start_end_model: 2024-09-23 - 2024-09-26\nApply - Backfill Tables [y/n]:\n</code></pre> <p>After executing that plan, we add columns to both the <code>incremental_model</code> and <code>start_end_model</code> queries.</p> <p>We then execute <code>vulcan plan dev</code> to create the new <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre> <p>Note two things about the output:</p> <ol> <li>As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for <code>incremental_model</code> and 2024-09-23 for <code>start_end_model</code></li> <li>Vulcan prompted us for a backfill start date because we didn't pass the <code>--start</code> option to the <code>vulcan plan dev</code> command</li> </ol> <p>Let's cancel that plan and start a new one, passing a start date of 2024-09-24.</p> <p>The <code>start_end_model</code> is of kind <code>INCREMENTAL_BY_UNIQUE_KEY</code>, which is non-idempotent and cannot be backfilled for a limited time range.</p> <p>Because the command's <code>--start</code> of 2024-09-24 is after <code>start_end_model</code>'s start date 2024-09-23, <code>start_end_model</code> is marked as preview:</p> <pre><code>\u276f vulcan plan dev --start 2024-09-24\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':\n</code></pre>"},{"location":"concepts-old/plans/#minimum-intervals","title":"Minimum intervals","text":"<p>When you run a plan with a fixed <code>--start</code> or <code>--end</code> date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.</p> <p>For example, if you have a model like so:</p> <pre><code>MODEL(\n    name vulcan_example.monthly_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column month\n    ),\n    cron '@monthly'\n);\n\nSELECT SUM(a) AS sum_a, MONTH(day) AS month\nFROM vulcan_example.upstream_model\nWHERE day BETWEEN @start_ds AND @end_ds\n</code></pre> <p>make a change to it and run the following:</p> <pre><code>$ vulcan plan dev --start '1 day ago' \n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\nSKIP: No model batches to execute\n</code></pre> <p>No data will be backfilled because <code>1 day ago</code> does not contain a complete month. However, you can use the <code>--min-intervals</code> option to override this behaviour like so:</p> <pre><code>$ vulcan plan dev --start '1 day ago' --min-intervals 1\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\n[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   \nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00                                                             \n\n\u2714 Model batches executed\n</code></pre> <p>This will ensure that regardless of the plan <code>--start</code> date, all added or modified models will have at least <code>--min-intervals</code> intervals considered for backfill.</p> <p>Info</p> <p>If you are running plans manually you can just adjust the <code>--start</code> date to be wide enough to cover the models in question.</p> <p>The <code>--min-intervals</code> option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) \"2 weeks worth of data\" in the target environment.</p>"},{"location":"concepts-old/plans/#data-preview-for-forward-only-changes","title":"Data preview for forward-only changes","text":"<p>As mentioned earlier, the data output produced by forward-only changes in a development environment can only be used for preview and will not be reused in production.</p> <p>The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.</p> <p>Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with <code>(preview)</code> in the plan summary: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28\n\u251c\u2500\u2500 sushi__dev.top_waiters: 2023-12-22 - 2023-12-28\n\u2514\u2500\u2500 sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)\n</code></pre><p></p>"},{"location":"concepts-old/plans/#forward-only-plans","title":"Forward-only plans","text":"<p>Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.</p> <p>The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and restatements.</p> <p>Note that once a forward-only change is applied to <code>prod</code>, all development environments that referred to the previous versions of the updated models will be impacted.</p> <p>A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, <code>vulcan plan [environment name]</code> evaluates code in non-<code>prod</code> environments while targeting shallow (a.k.a. \"zero-copy\") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.</p> <p>This means that only a limited preview of changes is available in the development environment before the change is promoted to <code>prod</code>. The date range of the preview is provided as part of plan creation command.</p> <p>Engines for which table cloning is supported include:</p> <ul> <li><code>BigQuery</code></li> <li><code>Databricks</code></li> <li><code>Snowflake</code></li> </ul> <p>Note that all changes made as part of a forward-only plan automatically get a forward-only category assigned to them. These types of changes can't be mixed together with breaking and non-breaking changes within the same plan.</p> <p>To create a forward-only plan, add the <code>--forward-only</code> option to the <code>plan</code> command: </p><pre><code>vulcan plan [environment name] --forward-only\n</code></pre><p></p> <p>Note</p> <p>The <code>--forward-only</code> flag is not required when applying changes to models that have been explicitly configured as forward-only.</p> <p>Use it only if you need to provide a time range for the preview window or the effective date.</p>"},{"location":"concepts-old/plans/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to forward-only models - learn more here.</p> <p>Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy: </p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models.  Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. </p> <p>For example, to allow destructive changes to all models in the <code>analytics</code> schema: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"analytics.*\"\n</code></pre><p></p> <p>Or to allow destructive changes to multiple specific models: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"sales.revenue_model\" --allow-destructive-model \"marketing.campaign_model\"\n</code></pre><p></p> <p>Learn more about model selectors here.</p>"},{"location":"concepts-old/plans/#effective-date","title":"Effective date","text":"<p>Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:</p> <pre><code>vulcan plan --forward-only --effective-from 2023-01-01\n</code></pre> <p>This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.</p>"},{"location":"concepts-old/plans/#restatement-plans","title":"Restatement plans","text":"<p>Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.</p> <p>For example, these scenarios all require re-evaluating model data that already exists:</p> <ul> <li>Correcting an upstream data issue by reprocessing some of a model's existing data</li> <li>Retroactively applying a forward-only plan change to some historical data</li> <li>Fully refreshing a model</li> </ul> <p>In Vulcan, reprocessing existing data is called a \"restatement.\"</p> <p>Restate one or more models' data with the <code>plan</code> command's <code>--restate-model</code> selector. The selector lets you specify which models to restate by name, wildcard, or tag (syntax below).</p> <p>No changes allowed</p> <p>Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.</p> <p>You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running <code>vulcan plan</code> without the <code>--restate-model</code> option.</p> <p>Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.</p> <p>You may restate external models. An external model is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.</p> <p>The plan's <code>--start</code> and <code>--end</code> date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more below.</p> <p>Just catching up</p> <p>Restatement plans \"catch models up\" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.</p> <p>If you pass an <code>--end</code> date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.</p> <p>To prevent models from ever being restated, set the disable_restatement attribute to <code>true</code>.</p> <p> These examples demonstrate how to select which models to restate based on model names or model tags.</p> Names OnlyUpstreamWildcardsUpstream + WildcardsSpecific Date Range <pre><code>vulcan plan --restate-model \"db.model_a\" --restate-model \"tag:expensive\"\n</code></pre> <pre><code># All selected models (including upstream models) will also include their downstream models\nvulcan plan --restate-model \"+db.model_a\" --restate-model \"+tag:expensive\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db*\" --restate-model \"tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"+db*\" --restate-model \"+tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre>"},{"location":"concepts-old/plans/#restating-production-vs-development","title":"Restating production vs development","text":"<p>Restatement plans behave differently depending on if you're targeting the <code>prod</code> environment or a development environment.</p> <p>If you target a development environment by including an environment name like <code>dev</code>:</p> <pre><code>vulcan plan dev --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the requested intervals for the specified model in the <code>dev</code> environment. In other environments, the model will be unaffected.</p> <p>However, if you target the <code>prod</code> environment by omitting an environment name:</p> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the intervals in the <code>prod</code> table and clear the model's time intervals from state in every other environment.</p> <p>The next time you do a run in <code>dev</code>, the intervals already reprocessed in <code>prod</code> are reprocessed in <code>dev</code> as well. This is to prevent old data from getting promoted to <code>prod</code> in the future.</p> <p>This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:</p> <ul> <li>Table <code>A</code> exists in <code>prod</code></li> <li>A virtual environment <code>dev</code> is created with new tables <code>B</code> and <code>C</code> downstream of <code>A</code><ul> <li>the DAG in <code>prod</code> looks like <code>A</code></li> <li>the DAG in <code>dev</code> looks like <code>A &lt;- B &lt;- C</code></li> </ul> </li> <li>A restatement plan is executed against table <code>A</code> in <code>prod</code></li> <li>Vulcan will clear the affected intervals for <code>B</code> and <code>C</code> in <code>dev</code> even though those tables do not exist in <code>prod</code></li> </ul> <p>Bringing development environments up to date</p> <p>A restatement plan against <code>prod</code> clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.</p> <p>Execute <code>vulcan run &lt;environment name&gt;</code> to trigger reprocessing in the development environment.</p> <p>This is necessary because a <code>prod</code> restatement plan only does work in the <code>prod</code> environment for speed and efficiency.</p>"},{"location":"concepts-old/state/","title":"State","text":""},{"location":"concepts-old/state/#state","title":"State","text":"<p>Vulcan stores information about your project in a state database that is usually separate from your main warehouse.</p> <p>The Vulcan state database contains:</p> <ul> <li>Information about every Model Version in your project (query, loaded intervals, dependencies)</li> <li>A list of every Virtual Data Environment in the project</li> <li>Which model versions are promoted into each Virtual Data Environment</li> <li>Information about any auto restatements present in your project</li> <li>Other metadata about your project such as current Vulcan / SQLGlot version</li> </ul> <p>The state database is how Vulcan \"remembers\" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for incremental models so you dont need to add branching logic into the model query to handle this.</p> <p>State database performance</p> <p>The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.</p> <p>For the best experience, we recommend databases designed for OLTP workloads such as PostgreSQL.</p> <p>Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and will lead to poor performance and consistency.</p> <p>For more information on engines suitable for the Vulcan state database, see the configuration guide.</p>"},{"location":"concepts-old/state/#exporting-importing-state","title":"Exporting / Importing State","text":"<p>Vulcan supports exporting the state database to a <code>.json</code> file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.</p>"},{"location":"concepts-old/state/#exporting-state","title":"Exporting state","text":"<p>Vulcan can export the state database to a file like so:</p> <pre><code>$ vulcan state export -o state.json\nExporting state to 'state.json' from the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\nContinue? [y/n]: y\n\n    Exporting versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Exporting snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nExporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState exported successfully to 'state.json'\n</code></pre> <p>This will produce a file <code>state.json</code> in the current directory containing the Vulcan state.</p> <p>The state file is a simple <code>json</code> file that looks like:</p> <pre><code>{\n    /* State export metadata */\n    \"metadata\": {\n        \"timestamp\": \"2025-03-16 23:09:00+00:00\", /* UTC timestamp of when the file was produced */\n        \"file_version\": 1, /* state export file format version */\n        \"importable\": true /* whether or not this file can be imported with `vulcan state import` */\n    },\n    /* Library versions used to produce this state export file */\n    \"versions\": {\n        \"schema_version\": 76 /* vulcan state database schema version */,\n        \"sqlglot_version\": \"26.10.1\" /* version of SQLGlot used to produce the state file */,\n        \"vulcan_version\": \"0.165.1\" /* version of Vulcan used to produce the state file */,\n    },\n    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */\n    \"snapshots\": [\n        { \"name\": \"...\" }\n    ],\n    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */\n    \"environments\": {\n        \"prod\": {\n            /* information about the environment itself */\n            \"environment\": {\n                \"...\"\n            },\n            /* information about any before_all / after_all statements for this environment */\n            \"statements\": [\n                \"...\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"concepts-old/state/#specific-environments","title":"Specific environments","text":"<p>You can export a specific environment like so:</p> <pre><code>$ vulcan state export --environment my_dev -o my_dev_state.json\n</code></pre> <p>Note that every snapshot that is part of the environment will be exported, not just the differences from <code>prod</code>. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.</p>"},{"location":"concepts-old/state/#local-state","title":"Local state","text":"<p>You can export local state like so:</p> <pre><code>$ vulcan state export --local -o local_state.json\n</code></pre> <p>This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.</p> <p>Therefore, a local state export will only have <code>snapshots</code> populated. <code>environments</code> will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as not importable so it cannot be used with a subsequent <code>vulcan state import</code> command.</p>"},{"location":"concepts-old/state/#importing-state","title":"Importing state","text":"<p>Back up your state database first!</p> <p>Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.</p> <p>Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means a import error has the potential to leave the state database in an inconsistent state.</p> <p>Vulcan can import a state file into the state database like so:</p> <pre><code>$ vulcan state import -i state.json --replace\nLoading state from 'state.json' into the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\n[WARNING] This destructive operation will delete all existing state against the 'dev' gateway\nand replace it with what\\'s in the 'state.json' file.\n\nAre you sure? [y/n]: y\n\nState File Information:\n\u251c\u2500\u2500 Creation Timestamp: 2025-03-31 02:15:00+00:00\n\u251c\u2500\u2500 File Version: 1\n\u251c\u2500\u2500 Vulcan version: 0.170.1.dev0\n\u251c\u2500\u2500 Vulcan migration version: 76\n\u2514\u2500\u2500 SQLGlot version: 26.12.0\n\n    Importing versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Importing snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nImporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState imported successfully from 'state.json'\n</code></pre> <p>Note that the state database structure needs to be present and up to date, so run <code>vulcan migrate</code> before running <code>vulcan state import</code> if you get a version mismatch error.</p> <p>If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the <code>--replace</code> parameter:</p> <pre><code>$ vulcan state import -i state.json\n...\n\n[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.\nMatching snapshots or environments will be replaced.\nNon-matching snapshots or environments will be ignored.\n\nAre you sure? [y/n]: y\n\n...\nState imported successfully from 'state.json'\n</code></pre>"},{"location":"concepts-old/state/#specific-gateways","title":"Specific gateways","text":"<p>If your project has multiple gateways with different state connections per gateway, you can target the state_connection of a specific gateway like so:</p> <pre><code># state export\n$ vulcan --gateway &lt;gateway&gt; state export -o state.json\n\n# state import\n$ vulcan --gateway &lt;gateway&gt; state import -i state.json\n</code></pre>"},{"location":"concepts-old/state/#version-compatibility","title":"Version Compatibility","text":"<p>When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.</p> <p>If you attempt to import state with an incompatible version, you will get the following error:</p> <pre><code>$ vulcan state import -i state.json\n...SNIP...\n\nState import failed!\nError: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.\nPlease upgrade/downgrade your Vulcan version to match the state file before performing the import.\n</code></pre>"},{"location":"concepts-old/state/#upgrading-a-state-file","title":"Upgrading a state file","text":"<p>You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:</p> <ul> <li>Loading it into a local database using the older Vulcan version</li> <li>Installing the newer Vulcan version</li> <li>Running <code>vulcan migrate</code> to upgrade the state within the local database</li> <li>Running <code>vulcan state export</code> to export it back out again. The new export is now compatible with the newer version of Vulcan.</li> </ul> <p>Below is an example of how to upgrade a state file created with Vulcan <code>0.164.1</code> to be compatible with Vulcan <code>0.165.1</code>.</p> <p>First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:</p> <pre><code>$ python -m venv migration-env\n\n$ . ./migration-env/bin/activate\n\n(migration-env)$\n</code></pre> <p>Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg <code>the state file was created with '0.164.1'</code> means you need to install Vulcan <code>0.164.1</code>:</p> <pre><code>(migration-env)$ pip install \"vulcan==0.164.1\"\n</code></pre> <p>Add a gateway to your <code>config.yaml</code> like so:</p> <pre><code>gateways:\n  migration:\n    connection:\n      type: duckdb\n      database: ./state-migration.duckdb\n</code></pre> <p>The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the <code>model_defaults</code> from your project in order to migrate state correctly which is why we have not used an isolated directory.</p> <p>Warning</p> <p>From here on, be sure to specify <code>--gateway migration</code> to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway</p> <p>You can now import your state export using the same version of Vulcan it was created with:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n\n(migration-env)$ vulcan --gateway migration state import -i state.json\n...\nState imported successfully from 'state.json'\n</code></pre> <p>Now we have the state imported, we can upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, eg <code>You are running '0.165.1'</code></p> <p>To upgrade Vulcan, simply install the new version:</p> <pre><code>(migration-env)$ pip install --upgrade \"vulcan==0.165.1\"\n</code></pre> <p>Migrate the state to the new version:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n</code></pre> <p>And finally, create a new state file which is now compatible with the new Vulcan version:</p> <pre><code> (migration-env)$ vulcan --gateway migration state export -o state-migrated.json\n</code></pre> <p>The <code>state-migrated.json</code> file is now compatible with the newer version of Vulcan. You can then transfer it to the place you originally needed it and import it in:</p> <pre><code>$ vulcan state import -i state-migrated.json\n...\nState imported successfully from 'state-migrated.json'\n</code></pre>"},{"location":"concepts-old/architecture/serialization/","title":"Serialization","text":""},{"location":"concepts-old/architecture/serialization/#serialization","title":"Serialization","text":"<p>Vulcan executes Python code through macros and Python models. Each Python model is stored as a standalone snapshot, which includes all of the Python code necessary to generate it.</p>"},{"location":"concepts-old/architecture/serialization/#serialization-format","title":"Serialization format","text":"<p>Rather than using Python's <code>pickle</code> format, Vulcan has its own serialization format. This is because <code>pickle</code> is not compatible across Python versions, and would, for example, prevent you from developing on Python 3.9 and then running Python 3.10 in production.</p> <p>Instead, Vulcan stores the string representation of your Python implementation and then re-evaluates it. Given a custom Python function or macro, Vulcan reads the Abstract Syntax Tree (AST) of the function and converts that into a string representation, along with all dependencies and global variables. For more information, refer to snapshot fingerprinting.</p>"},{"location":"concepts-old/architecture/serialization/#limitations","title":"Limitations","text":"<p>Vulcan only serializes the Python code you write and does not include libraries, which means the module of your code must match your Vulcan config path. In addition, any references to libraries will be converted to imports, so you must ensure that any libraries you are using are installed everywhere that Vulcan is running.</p>"},{"location":"concepts-old/architecture/snapshots/","title":"Snapshots","text":""},{"location":"concepts-old/architecture/snapshots/#snapshots","title":"Snapshots","text":"<p>A snapshot is a record of a model at a given time. Along with a copy of the model, a snapshot contains everything needed to evaluate the model and render its query. This allows Vulcan to have a consistent view of your project's history and its data as the project and its models evolve and change. Since model queries can have macros, each snapshot stores a copy of all macro definitions and global variables at the time the snapshot is taken. Additionally, snapshots store the intervals of time for which they have data.</p>"},{"location":"concepts-old/architecture/snapshots/#fingerprinting","title":"Fingerprinting","text":"<p>Snapshots have unique fingerprints that are derived from their models. Vulcan uses these fingerprints to determine when existing tables can be reused, or whether a backfill is needed as a model's query has changed.</p> <p>Because Vulcan can understand SQL with SQLGlot, it can generate fingerprints such that superficial changes to a model, such as applying formatting to its query, will not return a new fingerprint.</p>"},{"location":"concepts-old/architecture/snapshots/#change-categories","title":"Change categories","text":"<p>Refer to change categories.</p>"},{"location":"concepts-old/macros/jinja_macros/","title":"Jinja","text":""},{"location":"concepts-old/macros/jinja_macros/#jinja","title":"Jinja","text":"<p>Vulcan supports macros from the Jinja templating system.</p> <p>Jinja's macro approach is pure string substitution. Unlike Vulcan macros, they assemble SQL query text without building a semantic representation.</p> <p>NOTE: Vulcan projects support the standard Jinja function library only - they do not support dbt-specific jinja functions like <code>{{ ref() }}</code>. dbt-specific functions are allowed in dbt projects being run with the Vulcan adapter.</p>"},{"location":"concepts-old/macros/jinja_macros/#basics","title":"Basics","text":"<p>Jinja uses curly braces <code>{}</code> to differentiate macro from non-macro text. It uses the second character after the left brace to determine what the text inside the braces will do.</p> <p>The three curly brace symbols are:</p> <ul> <li><code>{{...}}</code> creates Jinja expressions. Expressions are replaced by text that is incorporated into the rendered SQL query; they can contain macro variables and functions.</li> <li><code>{%...%}</code> creates Jinja statements. Statements give instructions to Jinja, such as setting variable values, control flow with <code>if</code>, <code>for</code> loops, and defining macro functions.</li> <li><code>{#...#}</code> creates Jinja comments. These comments will not be included in the rendered SQL query.</li> </ul> <p>Since Jinja strings are not syntactically valid SQL expressions and cannot be parsed as such, the model query must be wrapped in a special <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code> block in order for Vulcan to detect it:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>Similarly, to use Jinja expressions as part of statements that should be evaluated before or after the model query, the <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code> block should be used:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#vulcan-predefined-variables","title":"Vulcan predefined variables","text":"<p>Vulcan provides multiple predefined macro variables you may reference in jinja code.</p> <p>Some predefined variables provide information about the Vulcan project itself, like the <code>runtime_stage</code> and <code>this_model</code> variables.</p> <p>Other predefined variables are temporal, like <code>start_ds</code> and <code>execution_date</code>. They are used to build incremental model queries and are only available in incremental model kinds.</p> <p>Access predefined macro variables by passing their unquoted name in curly braces. For example, this demonstrates how to access the <code>start_ds</code> and <code>end_ds</code> variables:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Because the two macro variables return string values, we must surround the curly braces with single quotes <code>'</code>. Other macro variables, such as <code>start_epoch</code>, return numeric values and do not require the single quotes.</p> <p>The <code>gateway</code> variable uses a slightly different syntax than other predefined variables because it is a function call. Instead of the bare name <code>{{ gateway }}</code>, it must include parentheses: <code>{{ gateway() }}</code>.</p>"},{"location":"concepts-old/macros/jinja_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports two kinds of user-defined macro variables: global and local.</p> <p>Global macro variables are defined in the project configuration file and can be accessed in any project model.</p> <p>Local macro variables are defined in a model definition and can only be accessed in that model.</p>"},{"location":"concepts-old/macros/jinja_macros/#global-variables","title":"Global variables","text":"<p>Learn more about defining global variables in the Vulcan macros documentation.</p> <p>Access global variable values in a model definition using the <code>{{ var() }}</code> jinja function. The function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, a model would access a global variable named <code>int_var</code> like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>A default value can be passed as a second argument to the <code>{{ var() }}</code> jinja function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> if no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key. Learn more about defining gateway variables in the Vulcan macros documentation.</p> <p>Access gateway variables in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the configuration file's root <code>variables</code> key.</p>"},{"location":"concepts-old/macros/jinja_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Blueprint variables can be accessed using the <code>{{ blueprint_var() }}</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>{{ var() }}</code>).</p>"},{"location":"concepts-old/macros/jinja_macros/#local-variables","title":"Local variables","text":"<p>Define your own variables with the Jinja statement <code>{% set ... %}</code>. For example, we could specify the name of the <code>num_orders</code> column in the <code>vulcan_example.full_model</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>Note that the Jinja set statement is written after the <code>MODEL</code> statement and before the SQL query.</p> <p>Jinja variables can be string, integer, or float data types. They can also be an iterable data structure, such as a list, tuple, or dictionary. Each of these data types and structures supports multiple Python methods, such as the <code>upper()</code> method for strings.</p>"},{"location":"concepts-old/macros/jinja_macros/#macro-operators","title":"Macro operators","text":""},{"location":"concepts-old/macros/jinja_macros/#control-flow-operators","title":"Control flow operators","text":""},{"location":"concepts-old/macros/jinja_macros/#for-loops","title":"for loops","text":"<p>For loops let you iterate over a collection of items to condense repetitive code and easily change the values used by the code.</p> <p>Jinja for loops begin with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>. This example demonstrates creating indicator variables with <code>CASE WHEN</code> using a Jinja for loop:</p> <pre><code>SELECT\n  {% for vehicle_type in ['car', 'truck', 'bus']}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>Note that the <code>vehicle_type</code> values are quoted in the list <code>['car', 'truck', 'bus']</code>. Jinja removes those quotes during processing, so the reference <code>'{{ vehicle_type }}</code> in the <code>CASE WHEN</code> statement must be in quotes. The reference <code>vehicle_{{ vehicle_type }}</code> does not require quotes.</p> <p>Also note that a comma is present at the end of the <code>CASE WHEN</code> line. Trailing commas are not valid SQL and would normally require special handling, but Vulcan's semantic understanding of the query allows it to identify and remove the offending comma.</p> <p>The example renders to this after Vulcan processing:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>In general, it is a best practice to define lists of values separately from their use. We could do that like this:</p> <pre><code>{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types }\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>The rendered query would be the same as before.</p>"},{"location":"concepts-old/macros/jinja_macros/#if","title":"if","text":"<p>if statements allow you to take an action (or not) based on some condition.</p> <p>Jinja if statements begin with <code>{% if ... %}</code> and end with <code>{% endif %}</code>. The starting <code>if</code> statement must contain code that evaluates to <code>True</code> or <code>False</code>. For example, all of <code>True</code>, <code>1 + 1 == 2</code>, and <code>'a' in ['a', 'b']</code> evaluate to <code>True</code>.</p> <p>As an example, you might want a model to only include a column if the model was being run for testing purposes. We can do that by setting a variable indicating whether it's a testing run that determines whether the query includes <code>testing_column</code>:</p> <pre><code>{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n</code></pre> <p>Because <code>testing</code> is <code>True</code>, the rendered query would be:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Jinja macro functions should be placed in <code>.sql</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.sql</code> file, or they can be distributed across multiple files.</p> <p>Jinja macro functions are defined with the <code>{% macro %}</code> and <code>{% endmacro %}</code> statements. The macro function name and arguments are specified in the <code>{% macro %}</code> statement.</p> <p>For example, a macro function named <code>print_text</code> that takes no arguments could be defined with:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>This macro function would be called in a SQL model with <code>{{ print_text() }}</code>, which would be substituted with <code>text</code>\" in the rendered query.</p> <p>Macro function arguments are placed in the parentheses next to the macro name. For example, this macro generates a SQL column with an alias based on the arguments <code>expression</code> and <code>alias</code>:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>We might call this macro function in a SQL query like this:</p> <pre><code>SELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Note that both argument values are quoted in the call <code>alias('item_id', 'item_id2')</code> but are not quoted in the rendered query. During the rendering process, Vulcan uses its semantic understanding of the query to build the rendered text - it recognizes that the first argument is a column name and that column aliases are unquoted by default.</p> <p>In that example, the SQL query selects the column <code>item_id</code> with the alias <code>item_id2</code>. If instead we wanted to select the string <code>'item_id'</code> with the name <code>item_id2</code>, we would pass the <code>expression</code> argument with double quotes around it: <code>\"'item_id'\"</code>:</p> <pre><code>SELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes around <code>\"'item_id'\"</code> signal to Vulcan that it is not a column name.</p> <p>Some SQL dialects interpret double and single quotes differently. We could replace the rendered single quoted <code>'item_id'</code> with double quoted <code>\"item_id\"</code> in the previous example by switching the placement of quotes in the macro function call. Instead of <code>alias(\"'item_id'\", 'item_id2')</code> we would use <code>alias('\"item_id\"', 'item_id2')</code>.</p>"},{"location":"concepts-old/macros/jinja_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both the Jinja and Vulcan macro systems. We strongly recommend using only one system in a single model - if both are present, they may fail or behave in unintuitive ways.</p> <p>Predefined Vulcan macro variables can be used in a query containing user-defined Jinja variables and functions. However, predefined variables passed as arguments to a user-defined Jinja macro function must use the Jinja curly brace syntax <code>{{ start_ds }}</code> instead of the Vulcan macro <code>@</code> prefix syntax <code>@start_ds</code>. Note that curly brace syntax may require quoting to generate the equivalent of the <code>@</code> syntax.</p>"},{"location":"concepts-old/macros/macro_variables/","title":"Variables","text":""},{"location":"concepts-old/macros/macro_variables/#variables","title":"Variables","text":"<p>Macro variables are placeholders whose values are substituted in when the macro is rendered.</p> <p>They enable dynamic macro behavior - for example, a date parameter's value might be based on when the macro was run.</p> <p>Note</p> <p>This page discusses Vulcan's built-in macro variables. Learn more about custom, user-defined macro variables on the Vulcan macros page.</p>"},{"location":"concepts-old/macros/macro_variables/#example","title":"Example","text":"<p>Consider a SQL query that filters by date in the <code>WHERE</code> clause.</p> <p>Instead of manually changing the date each time the model is run, you can use a macro variable to make the date dynamic. With the dynamic approach, the date changes automatically based on when the query is run.</p> <p>This query filters for rows where column <code>my_date</code> is after '2023-01-01':</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>To make this query's date dynamic you could use the predefined Vulcan macro variable <code>@execution_ds</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan that <code>@execution_ds</code> is a macro variable that requires substitution before the SQL is executed.</p> <p>The macro variable <code>@execution_ds</code> is predefined, so its value will be automatically set by Vulcan based on when the execution started. If the model was executed on February 1, 2023 the rendered query would be:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>This example used one of Vulcan's predefined variables, but you can also define your own macro variables.</p> <p>We describe Vulcan's predefined variables below; user-defined macro variables are discussed in the Vulcan macros and Jinja macros pages.</p>"},{"location":"concepts-old/macros/macro_variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan comes with predefined variables that can be used in your queries. They are automatically set by the Vulcan runtime.</p> <p>Most predefined variables are related to time and use a combination of prefixes (start, end, etc.) and postfixes (date, ds, ts, etc.). They are described in the next section; other predefined variables are discussed in the following section.</p>"},{"location":"concepts-old/macros/macro_variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses the python datetime module for handling dates and times. It uses the standard Unix epoch start of 1970-01-01.</p> <p>Important</p> <p>Predefined variables with a time component always use the UTC time zone.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes:</p> <ul> <li>start - The inclusive starting interval of a model run</li> <li>end - The inclusive end interval of a model run</li> <li>execution - The timestamp of when the execution started</li> </ul> <p>Postfixes:</p> <ul> <li>dt - A python datetime object that converts into a native SQL <code>TIMESTAMP</code> (or SQL engine equivalent)</li> <li>dtntz - A python datetime object that converts into a native SQL <code>TIMESTAMP WITHOUT TIME ZONE</code> (or SQL engine equivalent)</li> <li>date - A python date object that converts into a native SQL <code>DATE</code></li> <li>ds - A date string with the format: '%Y-%m-%d'</li> <li>ts - An ISO 8601 datetime formatted string: '%Y-%m-%d %H:%M:%S'</li> <li>tstz - An ISO 8601 datetime formatted string with timezone: '%Y-%m-%d %H:%M:%S%z'</li> <li>hour - An integer representing the hour of the day, with values 0-23</li> <li>epoch - An integer representing seconds since Unix epoch</li> <li>millis - An integer representing milliseconds since Unix epoch</li> </ul> <p>All predefined temporal macro variables:</p> <ul> <li> <p>dt</p> <ul> <li>@start_dt</li> <li>@end_dt</li> <li>@execution_dt</li> </ul> </li> <li> <p>dtntz</p> <ul> <li>@start_dtntz</li> <li>@end_dtntz</li> <li>@execution_dtntz</li> </ul> </li> <li> <p>date</p> <ul> <li>@start_date</li> <li>@end_date</li> <li>@execution_date</li> </ul> </li> <li> <p>ds</p> <ul> <li>@start_ds</li> <li>@end_ds</li> <li>@execution_ds</li> </ul> </li> <li> <p>ts</p> <ul> <li>@start_ts</li> <li>@end_ts</li> <li>@execution_ts</li> </ul> </li> <li> <p>tstz</p> <ul> <li>@start_tstz</li> <li>@end_tstz</li> <li>@execution_tstz</li> </ul> </li> <li> <p>hour</p> <ul> <li>@start_hour</li> <li>@end_hour</li> <li>@execution_hour</li> </ul> </li> <li> <p>epoch</p> <ul> <li>@start_epoch</li> <li>@end_epoch</li> <li>@execution_epoch</li> </ul> </li> <li> <p>millis</p> <ul> <li>@start_millis</li> <li>@end_millis</li> <li>@execution_millis</li> </ul> </li> </ul>"},{"location":"concepts-old/macros/macro_variables/#runtime-variables","title":"Runtime variables","text":"<p>Vulcan provides additional predefined variables used to modify model behavior based on information available at runtime.</p> <ul> <li>@runtime_stage - A string value denoting the current stage of the Vulcan runtime. Typically used in models to conditionally execute pre/post-statements (learn more here). It returns one of these values:<ul> <li>'loading' - The project is being loaded into Vulcan's runtime context.</li> <li>'creating' - The model tables are being created for the first time. The data may be inserted during table creation.</li> <li>'evaluating' - The model query logic is evaluated, and the data is inserted into the existing model table.</li> <li>'promoting' - The model is being promoted in the target environment (view created during virtual layer update).</li> <li>'demoting' - The model is being demoted in the target environment (view dropped during virtual layer update).</li> <li>'auditing' - The audit is being run.</li> <li>'testing' - The model query logic is being evaluated in the context of a unit test.</li> </ul> </li> <li>@gateway - A string value containing the name of the current gateway.</li> <li>@this_model - The physical table name that the model's view selects from. Typically used to create generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</li> <li>@model_kind_name - A string value containing the name of the current model kind. Intended to be used in scenarios where you need to control the physical properties in model defaults.</li> </ul> <p>Embedding variables in strings</p> <p>Macro variable references sometimes use the curly brace syntax <code>@{variable}</code>, which serves a different purpose than the regular <code>@variable</code> syntax.</p> <p>The curly brace syntax tells Vulcan that the rendered string should be treated as an identifier, instead of simply replacing the macro variable value.</p> <p>For example, if <code>variable</code> is defined as <code>@DEF(</code>variable<code>, foo.bar)</code>, then <code>@variable</code> produces <code>foo.bar</code>, while <code>@{variable}</code> produces <code>\"foo.bar\"</code>. This is because Vulcan converts <code>foo.bar</code> into an identifier, using double quotes to correctly include the <code>.</code> character in the identifier name.</p> <p>In practice, <code>@{variable}</code> is most commonly used to interpolate a value within an identifier, e.g., <code>@{variable}_suffix</code>, whereas <code>@variable</code> is used to do plain substitutions for string literals.</p> <p>Learn more in the Vulcan macros documentation.</p>"},{"location":"concepts-old/macros/macro_variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>The following variables are also available in <code>before_all</code> and <code>after_all</code> statements, as well as in macros invoked within them.</p> <ul> <li>@this_env - A string value containing the name of the current environment.</li> <li>@schemas - A list of the schema names of the virtual layer of the current environment.</li> <li>@views - A list of the view names of the virtual layer of the current environment.</li> </ul>"},{"location":"concepts-old/macros/overview/","title":"Overview","text":""},{"location":"concepts-old/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language. It does not natively have features like variables or control flow logic (if-then, for loops) that allow SQL commands to behave differently in different situations.</p> <p>However, data pipelines are dynamic and need different behavior depending on context. SQL is made dynamic with macros.</p> <p>Vulcan supports two macro systems: Vulcan macros and the Jinja templating system.</p> <p>Learn more about macros in Vulcan:</p> <ul> <li>Pre-defined macro variables available in both macro systems</li> <li>Vulcan macros</li> <li>Jinja macros</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/","title":"Built In","text":""},{"location":"concepts-old/macros/vulcan_macros/#built-in","title":"Built In","text":""},{"location":"concepts-old/macros/vulcan_macros/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros behave differently than those of templating systems like Jinja.</p> <p>Macro systems are based on string substitution. The macro system scans code files, identifies special characters that signify macro content, and replaces the macro elements with other text.</p> <p>In a general sense, that is the entire functionality of templating systems. They have tools that provide control flow logic (if-then) and other functionality, but that functionality is solely to support substituting in the correct strings.</p> <p>Templating systems are intentionally agnostic to the programming language being templated, and most of them work for everything from blog posts to HTML to SQL.</p> <p>In contrast, Vulcan macros are designed specifically for generating SQL code. They have semantic understanding of the SQL code being created by analyzing it with the Python sqlglot library, and they allow use of Python code so users can tidily implement sophisticated macro logic.</p>"},{"location":"concepts-old/macros/vulcan_macros/#vulcan-macro-approach","title":"Vulcan macro approach","text":"<p>This section describes how Vulcan macros work under the hood. Feel free to skip over this section and return if and when it is useful. This information is not required to use Vulcan macros, but it will be useful for debugging any macros exhibiting puzzling behavior.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</li> <li>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</li> <li>Macro functions, both Vulcan's and user-defined</li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"concepts-old/macros/vulcan_macros/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></li> <li>Identifiers, which reference database objects like column, table, alias, and function names<ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint and local.</p> <p>Global and gateway macro variables are defined in the project configuration file and can be accessed in any project model. Blueprint and macro variables are defined in a model definition and can only be accessed in that model.</p> <p>Macro variables with the same name may be specified at any or all of the global, gateway, blueprint and local levels. When variables are specified at multiple levels, the value of the most specific level takes precedence. For example, the value of a local variable takes precedence over the value of a blueprint or gateway variable with the same name, and the value of a gateway variable takes precedence over the value of a global variable.</p>"},{"location":"concepts-old/macros/vulcan_macros/#global-variables","title":"Global variables","text":"<p>Global variables are defined in the project configuration file <code>variables</code> key.</p> <p>Global variable values may be any of the following data types or lists or dictionaries containing these types: <code>int</code>, <code>float</code>, <code>bool</code>, <code>str</code>.</p> <p>Access global variable values in a model definition using the <code>@&lt;VAR_NAME&gt;</code> macro or the <code>@VAR()</code> macro function. The latter function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"concepts-old/macros/vulcan_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"concepts-old/macros/vulcan_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"concepts-old/macros/vulcan_macros/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system has multiple operators that allow different forms of dynamic behavior in models.</p>"},{"location":"concepts-old/macros/vulcan_macros/#each","title":"@EACH","text":"<p><code>@EACH</code> is used to transform a list of items by applying a function to each of them, analogous to a <code>for</code> loop.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>Vulcan macros support placing macro values at the end of a column name using <code>column_@x</code>.</p> <p>However, if you wish to substitute the variable anywhere else in the identifier, you need to use the more explicit curly brace syntax <code>@{}</code> to avoid ambiguity. For example, these are valid uses: <code>@{x}_column</code> or <code>my_@{x}_column</code>.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"concepts-old/macros/vulcan_macros/#if","title":"@IF","text":"<p>Vulcan's <code>@IF</code> macro allows components of a SQL query to change based on the result of a logical condition.</p> <p>It includes three elements:</p> <ol> <li>A logical condition that evaluates to <code>TRUE</code> or <code>FALSE</code></li> <li>A value to return if the condition is <code>TRUE</code></li> <li>A value to return if the condition is <code>FALSE</code> [optional]</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</li> <li>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</li> <li>Between: <code>[number] BETWEEN [low number] AND [high number]</code></li> <li>Membership: <code>[item] IN ([comma-separated list of items])</code></li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li><code>'a' = 'a'</code></li> <li><code>'a' != 'b'</code></li> <li><code>0 &lt; 1</code></li> <li><code>1 &gt;= 1</code></li> <li><code>2 BETWEEN 1 AND 3</code></li> <li><code>'a' IN ('a', 'b')</code></li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"concepts-old/macros/vulcan_macros/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"concepts-old/macros/vulcan_macros/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The relation/table whose columns are being selected</li> <li><code>alias</code> (optional): The alias of the relation (if it has one)</li> <li><code>exclude</code> (optional): A list of columns to exclude</li> <li><code>prefix</code> (optional): A string to use as a prefix for all selected column names</li> <li><code>suffix</code> (optional): A string to use as a suffix for all selected column names</li> <li><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</li> <li>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</li> <li>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</li> <li>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></li> <li>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</li> <li><code>my_column</code> is present in the query</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by: 1. <code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type) 2. Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column 3. Concatenating the column values after steps (1) and (2) 4. Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</li> <li><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>lat1</code>: Latitude of the first point</li> <li><code>lon1</code>: Longitude of the first point</li> <li><code>lat2</code>: Latitude of the second point</li> <li><code>lon2</code>: Longitude of the second point</li> <li><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>column</code>: The column to pivot</li> <li><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</li> <li><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</li> <li><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></li> <li><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></li> <li><code>prefix</code> (optional): A prefix to use for all aliases</li> <li><code>suffix</code> (optional): A suffix to use for all aliases</li> <li><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></li> <li><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></li> <li><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</li> <li><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The table or CTE name to deduplicate</li> <li><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</li> <li><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</li> <li><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</li> <li><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>. - For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</p>"},{"location":"concepts-old/macros/vulcan_macros/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</li> <li>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li><code>template</code> - The string template to render into an AST node</li> <li><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></li> <li><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></li> <li><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li><code>@WITH</code>: common table expression <code>WITH</code> clause</li> <li><code>@JOIN</code>: table <code>JOIN</code> clause(s)</li> <li><code>@WHERE</code>: filtering <code>WHERE</code> clause</li> <li><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</li> <li><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</li> <li><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</li> <li><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"concepts-old/macros/vulcan_macros/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"concepts-old/macros/vulcan_macros/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"concepts-old/macros/vulcan_macros/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Vulcan supports user-defined macro functions written in two languages - SQL and Python:</p> <ul> <li>SQL macro functions must use the Jinja templating system.</li> <li>Python macro functions use the SQLGlot library to allow more complex operations than macro variables and operators provide alone.</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#python-macro-functions","title":"Python macro functions","text":""},{"location":"concepts-old/macros/vulcan_macros/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"concepts-old/macros/vulcan_macros/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"concepts-old/macros/vulcan_macros/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li><code>@add_args(1)</code> returns 1</li> <li><code>@add_args(1, 2)</code> returns 3</li> <li><code>@add_args(1, 2, 3)</code> returns 6</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li>Numbers are parsed into <code>Literal</code> expressions</li> <li>Quoted strings are parsed into <code>Literal</code> expressions</li> <li>Unquoted strings are parsed into <code>Column</code> expressions</li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></li> <li>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in the <code>dev</code> environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in any environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</li> <li>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</li> <li>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#typed-macros","title":"Typed Macros","text":"<p>Typed macros in Vulcan bring the power of type hints from Python, enhancing readability, maintainability, and usability of your SQL macros. These macros enable developers to specify expected types for arguments, making the macros more intuitive and less error-prone.</p>"},{"location":"concepts-old/macros/vulcan_macros/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"concepts-old/macros/vulcan_macros/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"concepts-old/macros/vulcan_macros/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</li> <li><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li><code>exp.Table</code></li> <li><code>exp.Column</code></li> <li><code>exp.Literal</code></li> <li><code>exp.Identifier</code></li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"concepts-old/macros/vulcan_macros/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p> <p>See examples of the coercion function in action in the test suite here.</p>"},{"location":"concepts-old/macros/vulcan_macros/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"concepts-old/macros/vulcan_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan and Jinja macro systems. We strongly recommend using only one system in a model - if both are present, they may fail or behave in unintuitive ways.</p>"},{"location":"configurations/overview/","title":"Overview","text":""},{"location":"configurations/overview/#overview","title":"Overview","text":"<p>Vulcan projects are configured through a central configuration file that defines how your data pipeline connects to databases, manages environments, and handles model execution.</p>"},{"location":"configurations/overview/#configuration-file","title":"Configuration File","text":"<p>Every Vulcan project requires a configuration file in the project root directory:</p> <ul> <li><code>config.yaml</code> - YAML format (recommended for most users)</li> <li><code>config.py</code> - Python format (for advanced use cases requiring dynamic configuration)</li> </ul>"},{"location":"configurations/overview/#quick-start-example","title":"Quick Start Example","text":"<p>Here's a practical example of a Vulcan configuration file:</p> <pre><code># Project metadata\nname: orders360\ntenant: sales\ndescription: Daily sales analytics pipeline\n\n# Gateway Connection\ngateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: \"{{ env_var('DB_PASSWORD') }}\"\n    state_connection:\n      type: postgres\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: \"{{ env_var('STATE_DB_PASSWORD') }}\"\n\ndefault_gateway: default\n\n# Model Defaults (required)\nmodel_defaults:\n  dialect: postgres\n  start: 2024-01-01\n  cron: '@daily'\n\n# Linting Rules\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre>"},{"location":"configurations/overview/#configuration-structure","title":"Configuration Structure","text":"<pre><code>graph TB\n    Config[config.yaml]\n    Config --&gt; Project[Project Settings]\n    Config --&gt; Gateways[Gateways]\n    Config --&gt; ModelDefaults[Model Defaults]\n    Config --&gt; Options[Optional Features]\n    Gateways --&gt; Connection[connection]\n    Gateways --&gt; StateConn[state_connection]\n    Gateways --&gt; TestConn[test_connection]\n    Options --&gt; Linter[linter]\n    Options --&gt; Notifications[notifications]\n    Options --&gt; Variables[variables]\n    Options --&gt; Format[format]</code></pre>"},{"location":"configurations/overview/#configuration-sections","title":"Configuration Sections","text":""},{"location":"configurations/overview/#project-settings","title":"Project Settings","text":"<p>Basic project metadata for identification.</p> Option Description Type <code>name</code> Project name string <code>tenant</code> Tenant or organization name string <code>description</code> Human-readable project description string"},{"location":"configurations/overview/#gateways","title":"Gateways","text":"<p>Gateways define connections to your data warehouse, state backend, and other services.</p> Component Description Default <code>connection</code> Primary data warehouse connection Required <code>state_connection</code> Where Vulcan stores internal state Uses <code>connection</code> <code>test_connection</code> Connection for running tests Uses <code>connection</code> <code>scheduler</code> Scheduler configuration <code>builtin</code> <code>state_schema</code> Schema name for state tables <code>vulcan</code> <p>\u2192 See Configuration Reference for detailed gateway options.</p>"},{"location":"configurations/overview/#model-defaults-required","title":"Model Defaults (Required)","text":"<p>The <code>model_defaults</code> section is required and must include at least the <code>dialect</code> key.</p> <pre><code>model_defaults:\n  dialect: postgres     # Required\n  owner: data-team\n  start: 2024-01-01\n  cron: '@daily'\n</code></pre> <p>\u2192 See Model Defaults for all available options.</p>"},{"location":"configurations/overview/#variables","title":"Variables","text":"<p>Configure environment variables, <code>.env</code> files, and configuration overrides.</p> <p>\u2192 See Variables for details.</p>"},{"location":"configurations/overview/#execution-hooks","title":"Execution Hooks","text":"<p>Execute SQL statements at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands using <code>before_all</code> and <code>after_all</code>.</p> <p>\u2192 See Execution Hooks for detailed examples and use cases.</p>"},{"location":"configurations/overview/#linter","title":"Linter","text":"<p>Enable automatic code quality checks for your models.</p> <p>\u2192 See Linter for rules and custom linter configuration.</p>"},{"location":"configurations/overview/#notifications","title":"Notifications","text":"<p>Configure alerts via Slack or email for pipeline events.</p> <p>\u2192 See Notifications for Slack webhooks, API, and email setup.</p>"},{"location":"configurations/overview/#supported-engines","title":"Supported Engines","text":"<p>Vulcan supports connecting to various data warehouses:</p> <ul> <li>PostgreSQL - Open-source relational database</li> <li>Snowflake - Cloud data warehouse</li> </ul>"},{"location":"configurations/overview/#configuration-reference","title":"Configuration Reference","text":"Topic Description Configuration Reference Complete list of all configuration parameters Variables Environment variables and <code>.env</code> files Model Defaults Default settings for all models Execution Hooks <code>before_all</code> and <code>after_all</code> statements Linter Code quality rules and custom linters Notifications Slack and email notification setup"},{"location":"configurations/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for sensitive data like passwords and API keys</li> <li>Set meaningful defaults in <code>model_defaults</code> to reduce boilerplate</li> <li>Enable linting to catch common errors early in development</li> <li>Separate state connection from data warehouse for better isolation</li> <li>Use multiple gateways for different environments (dev, staging, prod)</li> </ol>"},{"location":"configurations/ci_cd/","title":"CI/CD","text":""},{"location":"configurations/ci_cd/#cicd","title":"CI/CD","text":"<p>Coming soon...</p>"},{"location":"configurations/engines/postgres/postgres/","title":"Postgres","text":""},{"location":"configurations/engines/postgres/postgres/#postgres","title":"Postgres","text":""},{"location":"configurations/engines/postgres/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"configurations/engines/postgres/postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"configurations/engines/snowflake/snowflake/","title":"Snowflake","text":""},{"location":"configurations/engines/snowflake/snowflake/#snowflake","title":"Snowflake","text":"<p>This page provides information about how to use Vulcan with the Snowflake SQL engine.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the built-in.</p>"},{"location":"configurations/engines/snowflake/snowflake/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.</p> <p>It demonstrates connecting to Snowflake with the <code>snowflake-connector-python</code> library bundled with Vulcan.</p> <p>Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are described below.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"configurations/engines/snowflake/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Snowflake account and know your username and password</li> <li>Your Snowflake account has at least one warehouse available for running computations</li> <li>Your computer has Vulcan installed with the Snowflake extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[snowflake]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init snowflake</code></li> </ul> </li> </ol>"},{"location":"configurations/engines/snowflake/snowflake/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access different types of database objects.</p> <p>Vulcan's core functionality requires relatively broad permissions, including:</p> <ol> <li>Ability to create and delete schemas in a database</li> <li>Ability to create, modify, delete, and query tables and views in the schemas it creates</li> </ol> <p>If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.</p> <p>We now describe how to grant Vulcan appropriate permissions.</p>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-roles","title":"Snowflake roles","text":"<p>Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a \"role\" that you then grant to the user.</p> <p>Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.</p> <p>The role must be granted <code>USAGE</code> on a warehouse so it can execute computations. We describe other permissions below.</p>"},{"location":"configurations/engines/snowflake/snowflake/#database-permissions","title":"Database permissions","text":"<p>The top-level object container in Snowflake is a \"database\" (often called a \"catalog\" in other engines). Vulcan does not need permission to create databases; it may use an existing one.</p> <p>The simplest way to grant Vulcan sufficient permissions for a database is to give it <code>OWNERSHIP</code> of the database, which includes all the necessary permissions.</p> <p>Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.</p>"},{"location":"configurations/engines/snowflake/snowflake/#granting-the-permissions","title":"Granting the permissions","text":"<p>This section provides example code for creating a <code>vulcan</code> role, granting it sufficient permissions, and granting it to a user.</p> <p>The code must be executed by a user with <code>USERADMIN</code> level permissions or higher. We provide two versions of the code, one that grants database <code>OWNERSHIP</code> to the role and another that does not.</p> <p>Both examples create a role named <code>vulcan</code>, grant it usage of the warehouse <code>compute_wh</code>, create a database named <code>demo_db</code>, and assign the role to the user <code>demo_user</code>. The step that creates the database can be omitted if the database already exists.</p> With database ownershipWithout database ownership <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\n\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\nGRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre> <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\n\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\nGRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database\n\nGRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates\nGRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas\nGRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas\nGRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas\nGRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#get-connection-info","title":"Get connection info","text":"<p>Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.</p>"},{"location":"configurations/engines/snowflake/snowflake/#account-name","title":"Account name","text":"<p>Snowflake connection configurations require the <code>account</code> parameter that identifies the Snowflake account Vulcan should connect to.</p> <p>Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a <code>/</code>.</p> <p>This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:</p> <p></p> <p>In this example, our organization name is <code>idapznw</code>, and our account name is <code>wq29399</code>.</p> <p>We concatenate the two components, separated by a <code>-</code>, for the Vulcan <code>account</code> parameter: <code>idapznw-wq29399</code>.</p>"},{"location":"configurations/engines/snowflake/snowflake/#warehouse-name","title":"Warehouse name","text":"<p>Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.</p> <p>Some Snowflake user accounts may have a default warehouse they automatically use when connecting.</p> <p>The connection configuration's <code>warehouse</code> parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.</p>"},{"location":"configurations/engines/snowflake/snowflake/#database-name","title":"Database name","text":"<p>Snowflake user accounts may have a \"Default Namespace\" that includes a default database they automatically use when connecting.</p> <p>The connection configuration's <code>database</code> parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.</p>"},{"location":"configurations/engines/snowflake/snowflake/#configure-the-connection","title":"Configure the connection","text":"<p>We now have the information we need to configure Vulcan's connection to Snowflake.</p> <p>We start the configuration by adding a gateway named <code>snowflake</code> to our example project's config.yaml file and making it our <code>default_gateway</code>:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>And we specify the <code>account</code>, <code>user</code>, <code>password</code>, <code>database</code>, and <code>warehouse</code> connection parameters using the information from above:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Warning</p> <p>Best practice for storing secrets like passwords is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>SNOWFLAKE_PASSWORD</code> for the configuration's <code>password</code> parameter:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      password: {{ env_var('SNOWFLAKE_PASSWORD') }}\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>snowflake</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Snowflake SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"configurations/engines/snowflake/snowflake/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>snowflake</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>snowflake_state.db</code> file:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; your password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n    state_connection:\n      type: duckdb\n      database: snowflake_state.db\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Now we no longer see the warning when running <code>vulcan info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"configurations/engines/snowflake/snowflake/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in Snowflake:</p> <p></p> <p>And confirm that our schemas and objects exist in the Snowflake catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Snowflake!</p>"},{"location":"configurations/engines/snowflake/snowflake/#where-are-the-row-counts","title":"Where are the row counts?","text":"<p>Vulcan reports the number of rows processed by each model in its <code>plan</code> and <code>run</code> terminal output.</p> <p>However, due to limitations in the Snowflake Python connector, row counts cannot be determined for <code>CREATE TABLE AS</code> statements. Therefore, Vulcan does not report row counts for certain model kinds, such as <code>FULL</code> models.</p> <p>Learn more about the connector limitation on Github.</p>"},{"location":"configurations/engines/snowflake/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"configurations/engines/snowflake/snowflake/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[snowflake]\"\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account name string Y <code>user</code> The Snowflake username string N <code>password</code> The Snowflake password string N <code>authenticator</code> The Snowflake authenticator method string N <code>warehouse</code> The Snowflake warehouse name string N <code>database</code> The Snowflake database name string N <code>role</code> The Snowflake role name string N <code>token</code> The Snowflake OAuth 2.0 access token string N <code>private_key</code> The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). string N <code>private_key_path</code> The optional path to the private key to use for authentication. This would be used instead of <code>private_key</code>. string N <code>private_key_passphrase</code> The optional passphrase to use to decrypt <code>private_key</code> (if in PEM format) or <code>private_key_path</code>. Keys can be created without encryption so only provide this if needed. string N <code>session_parameters</code> The optional session parameters to set for the connection. dict N"},{"location":"configurations/engines/snowflake/snowflake/#lowercase-object-names","title":"Lowercase object names","text":"<p>Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command <code>CREATE SCHEMA vulcan</code> will generate a schema named <code>VULCAN</code> in Snowflake.</p> <p>If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.</p> <p>For example, a connection to the database <code>\"my_db\"</code> would include:</p> <pre><code>connection:\n  type: snowflake\n  &lt;other connection options&gt;\n  database: '\"my_db\"' # outer single and inner double quotes\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-authorization-methods","title":"Snowflake authorization methods","text":"<p>The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.</p> <p>This section describes how to configure other authorization methods.</p>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-sso-authorization","title":"Snowflake SSO Authorization","text":"<p>Vulcan supports Snowflake SSO authorization connections using the <code>externalbrowser</code> authenticator method. For example:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: ************\n      user: ************\n      authenticator: externalbrowser\n      warehouse: ************\n      database: ************\n      role: ************\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-oauth-authorization","title":"Snowflake OAuth Authorization","text":"<p>Vulcan supports Snowflake OAuth authorization connections using the <code>oauth</code> authenticator method. For example:</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      authenticator: oauth\n      token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                authenticator=\"oauth\",\n                token=\"eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-private-key-authorization","title":"Snowflake Private Key Authorization","text":"<p>Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).</p> <p>The <code>account</code> and <code>user</code> parameters are required for each of these methods.</p> <p>Private Key Path</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key_path: '/path/to/key.key'\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key_path=\"/path/to/key.key\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key PEM</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: |\n        -----BEGIN PRIVATE KEY-----\n        ...\n        -----END PRIVATE KEY-----\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"\"\"\n                -----BEGIN PRIVATE KEY-----\n                ...\n                -----END PRIVATE KEY-----\"\"\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#private-key-base64","title":"Private Key Base64","text":"<p>Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Bytes</p> YAMLPython <p>Base64 encode the bytes and follow Private Key Base64 instructions.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    GatewayConfig,\n    ModelDefaultsConfig,\n    SnowflakeConnectionConfig,\n)\n\nfrom cryptography.hazmat.primitives import serialization\n\nkey = \"\"\"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\"\"\".encode()\n\np_key= serialization.load_pem_private_key(key, password=None)\n\npkb = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=pkb,\n            ),\n        ),\n    }\n)\n</code></pre> <p>The authenticator method is assumed to be <code>snowflake_jwt</code> when <code>private_key</code> is provided, but it can also be explicitly provided in the connection configuration.</p>"},{"location":"configurations/engines/snowflake/snowflake/#configuring-virtual-warehouses","title":"Configuring Virtual Warehouses","text":"<p>The Snowflake Virtual Warehouse a model should use can be specified in the <code>session_properties</code> attribute of the model definition:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  session_properties (\n    'warehouse' = TEST_WAREHOUSE,\n  ),\n);\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#custom-view-and-table-types","title":"Custom View and Table types","text":"<p>Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the <code>physical_properties</code> and <code>virtual_properties</code> attributes respectively. For example:</p>"},{"location":"configurations/engines/snowflake/snowflake/#secure-views","title":"Secure Views","text":"<p>A table can be exposed through a <code>SECURE</code> view in the virtual layer by specifying the <code>creatable_type</code> property and setting it to <code>SECURE</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  virtual_properties (\n      creatable_type = SECURE\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#transient-tables","title":"Transient Tables","text":"<p>A model can use a <code>TRANSIENT</code> table in the physical layer by specifying the <code>creatable_type</code> property and setting it to <code>TRANSIENT</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  physical_properties (\n      creatable_type = TRANSIENT\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#iceberg-tables","title":"Iceberg Tables","text":"<p>In order for Snowflake to be able to create an Iceberg table, there must be an External Volume configured to store the Iceberg table data on.</p> <p>Once that is configured, you can create a model backed by an Iceberg table by using <code>table_format iceberg</code> like so:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = '&lt;external volume name&gt;'\n  )\n);\n</code></pre> <p>To prevent having to specify <code>catalog = 'snowflake'</code> and <code>external_volume = '&lt;external volume name&gt;'</code> on every model, see the Snowflake documentation for:</p> <ul> <li>Configuring a default Catalog</li> <li>Configuring a default External Volume</li> </ul> <p>Alternatively you can also use model defaults to set defaults at the Vulcan level instead.</p> <p>To utilize the wide variety of optional properties that Snowflake makes available for Iceberg tables, simply specify them as <code>physical_properties</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = 'my_external_volume',\n    base_location = 'my/product_reviews/'\n  )\n);\n</code></pre> <p>External catalogs</p> <p>Setting <code>catalog = 'snowflake'</code> to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake does not support writing to Iceberg tables configured under external catalogs.</p> <p>You can however still reference a table from an external catalog in your model as a normal external table.</p>"},{"location":"configurations/engines/snowflake/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configurations/engines/snowflake/snowflake/#frequent-authentication-prompts","title":"Frequent Authentication Prompts","text":"<p>When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.</p> <p>To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:</p> <ul> <li>For general authentication, see Connection Caching Documentation</li> <li>For MFA specifically, see MFA Token Caching Documentation.</li> </ul>"},{"location":"configurations/options/execution_hooks/","title":"Execution Hooks","text":""},{"location":"configurations/options/execution_hooks/#execution-hooks","title":"Execution Hooks","text":"<p>Vulcan provides <code>before_all</code> and <code>after_all</code> hooks that execute SQL statements or macros at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands. These hooks are powerful tools for automating setup, cleanup, and privilege management across your data pipeline.</p>"},{"location":"configurations/options/execution_hooks/#overview","title":"Overview","text":"Hook When it Runs Common Use Cases <code>before_all</code> Before any model is processed Setup tables, initialize logging, validate prerequisites <code>after_all</code> After all models are processed Grant privileges, cleanup, send notifications, update metadata"},{"location":"configurations/options/execution_hooks/#basic-configuration","title":"Basic Configuration","text":"YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\n  - INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\n\nafter_all:\n  - \"@grant_select_privileges()\"\n  - UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all=[\n        \"CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\",\n        \"INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\"\n    ],\n    after_all=[\n        \"@grant_select_privileges()\",\n        \"UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\"\n    ],\n)\n</code></pre>"},{"location":"configurations/options/execution_hooks/#using-macros-in-hooks","title":"Using Macros in Hooks","text":"<p>Hooks can execute Vulcan macros using the <code>@macro_name()</code> syntax. Macros provide access to runtime context like view names, schemas, and the current environment.</p>"},{"location":"configurations/options/execution_hooks/#available-context-variables","title":"Available Context Variables","text":"<p>Macros invoked in hooks have access to:</p> Property Type Description <code>evaluator.views</code> <code>list[str]</code> All view names created in the virtual layer <code>evaluator.schemas</code> <code>list[str]</code> All schema names used by models <code>evaluator.this_env</code> <code>str</code> Current environment name (e.g., <code>prod</code>, <code>dev</code>) <code>evaluator.gateway</code> <code>str</code> Current gateway name"},{"location":"configurations/options/execution_hooks/#use-cases","title":"Use Cases","text":""},{"location":"configurations/options/execution_hooks/#1-granting-privileges-on-views","title":"1. Granting Privileges on Views","text":"<p>Instead of adding privilege grants to each model individually, use <code>after_all</code> to grant access to all views at once:</p> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    \"\"\"Grant SELECT on all views to the analytics role.\"\"\"\n    if not evaluator.views:\n        return []\n\n    return [\n        f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE analytics_role;\"\n        for view_name in evaluator.views\n    ]\n</code></pre> config.yaml<pre><code>after_all:\n  - \"@grant_select_privileges()\"\n</code></pre> <p>Preventing Name Replacement</p> <p>The comment <code>/* sqlglot.meta replace=false */</code> ensures Vulcan doesn't replace the view name with the physical table name during SQL rendering.</p>"},{"location":"configurations/options/execution_hooks/#2-environment-specific-execution","title":"2. Environment-Specific Execution","text":"<p>Use the <code>@IF</code> macro to conditionally execute statements based on the environment:</p> config.yaml<pre><code>after_all:\n  # Only grant schema usage in production\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n\n  # Only run cleanup in development\n  - \"@IF(@this_env != 'prod', @cleanup_dev_tables())\"\n</code></pre> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    \"\"\"Grant USAGE on all schemas to admin role (production only).\"\"\"\n    if evaluator.this_env != \"prod\" or not evaluator.schemas:\n        return []\n\n    return [\n        f\"GRANT USAGE ON SCHEMA {schema} TO ROLE admin_role;\"\n        for schema in evaluator.schemas\n    ]\n\n@macro()\ndef cleanup_dev_tables(evaluator):\n    \"\"\"Clean up temporary tables in development environments.\"\"\"\n    return [\n        \"DROP TABLE IF EXISTS temp_debug_output;\",\n        \"DROP TABLE IF EXISTS temp_test_data;\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#3-audit-logging","title":"3. Audit Logging","text":"<p>Track pipeline execution with audit tables:</p> config.yaml<pre><code>before_all:\n  - |\n    CREATE TABLE IF NOT EXISTS pipeline_audit (\n      run_id VARCHAR,\n      environment VARCHAR,\n      started_at TIMESTAMP,\n      completed_at TIMESTAMP,\n      status VARCHAR\n    )\n  - \"@log_pipeline_start()\"\n\nafter_all:\n  - \"@log_pipeline_end()\"\n</code></pre> macros/audit.py<pre><code>from vulcan.core.macros import macro\nimport uuid\n\n@macro()\ndef log_pipeline_start(evaluator):\n    run_id = str(uuid.uuid4())[:8]\n    return [\n        f\"\"\"\n        INSERT INTO pipeline_audit (run_id, environment, started_at, status)\n        VALUES ('{run_id}', '{evaluator.this_env}', CURRENT_TIMESTAMP, 'running')\n        \"\"\"\n    ]\n\n@macro()\ndef log_pipeline_end(evaluator):\n    return [\n        f\"\"\"\n        UPDATE pipeline_audit \n        SET completed_at = CURRENT_TIMESTAMP, status = 'completed'\n        WHERE environment = '{evaluator.this_env}' \n          AND status = 'running'\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#4-schema-and-database-setup","title":"4. Schema and Database Setup","text":"<p>Ensure required schemas exist before models run:</p> config.yaml<pre><code>before_all:\n  - CREATE SCHEMA IF NOT EXISTS staging\n  - CREATE SCHEMA IF NOT EXISTS analytics\n  - CREATE SCHEMA IF NOT EXISTS reporting\n  - \"@setup_external_tables()\"\n</code></pre> macros/setup.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef setup_external_tables(evaluator):\n    \"\"\"Create external tables for data ingestion.\"\"\"\n    return [\n        \"\"\"\n        CREATE EXTERNAL TABLE IF NOT EXISTS staging.raw_events (\n            event_id VARCHAR,\n            event_type VARCHAR,\n            event_data VARCHAR,\n            created_at TIMESTAMP\n        )\n        LOCATION 's3://data-lake/events/'\n        FILE_FORMAT = (TYPE = 'PARQUET')\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#5-data-quality-gates","title":"5. Data Quality Gates","text":"<p>Run data quality checks before processing:</p> config.yaml<pre><code>before_all:\n  - \"@validate_source_data()\"\n</code></pre> macros/validation.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef validate_source_data(evaluator):\n    \"\"\"Validate that source data meets quality requirements.\"\"\"\n    return [\n        \"\"\"\n        DO $$\n        DECLARE\n            row_count INTEGER;\n        BEGIN\n            SELECT COUNT(*) INTO row_count FROM raw_data.events WHERE created_at &gt;= CURRENT_DATE;\n            IF row_count = 0 THEN\n                RAISE EXCEPTION 'No data found for today in raw_data.events';\n            END IF;\n        END $$;\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#6-refresh-materialized-views","title":"6. Refresh Materialized Views","text":"<p>Refresh dependent materialized views after models are updated:</p> config.yaml<pre><code>after_all:\n  - \"@refresh_materialized_views()\"\n</code></pre> macros/refresh.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef refresh_materialized_views(evaluator):\n    \"\"\"Refresh all materialized views that depend on our models.\"\"\"\n    materialized_views = [\n        \"reporting.daily_summary_mv\",\n        \"reporting.weekly_trends_mv\",\n        \"analytics.user_metrics_mv\"\n    ]\n\n    return [\n        f\"REFRESH MATERIALIZED VIEW {mv};\"\n        for mv in materialized_views\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#7-notification-integration","title":"7. Notification Integration","text":"<p>Send notifications after pipeline completion:</p> config.yaml<pre><code>after_all:\n  - \"@notify_completion()\"\n</code></pre> macros/notify.py<pre><code>from vulcan.core.macros import macro\nimport os\n\n@macro()\ndef notify_completion(evaluator):\n    \"\"\"Log completion status (integrate with your notification system).\"\"\"\n    # This example logs to a table; you could also call an external API\n    view_count = len(evaluator.views) if evaluator.views else 0\n    schema_count = len(evaluator.schemas) if evaluator.schemas else 0\n\n    return [\n        f\"\"\"\n        INSERT INTO notifications_log (\n            environment, \n            message, \n            view_count, \n            schema_count, \n            created_at\n        )\n        VALUES (\n            '{evaluator.this_env}',\n            'Pipeline completed successfully',\n            {view_count},\n            {schema_count},\n            CURRENT_TIMESTAMP\n        )\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#execution-order","title":"Execution Order","text":"<pre><code>graph TD\n    A[Start] --&gt; B[before_all]\n    B --&gt; C[Process Models]\n    C --&gt; D[after_all]\n    D --&gt; E[Complete]</code></pre>"},{"location":"configurations/options/execution_hooks/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use macros for complex logic - Keep YAML configuration clean by moving complex SQL generation to Python macros</p> </li> <li> <p>Make hooks idempotent - Hooks may run multiple times; use <code>IF NOT EXISTS</code>, <code>ON CONFLICT</code>, or similar patterns</p> </li> <li> <p>Use environment checks - Gate production-only operations with <code>@IF(@this_env = 'prod', ...)</code></p> </li> <li> <p>Handle failures gracefully - Consider what happens if a hook fails; use transactions where appropriate</p> </li> <li> <p>Document your hooks - Add comments explaining why each hook exists and what it does</p> </li> <li> <p>Test in development first - Always test hooks in a development environment before running in production</p> </li> </ol>"},{"location":"configurations/options/execution_hooks/#comparison-with-model-level-hooks","title":"Comparison with Model-Level Hooks","text":"Feature <code>before_all</code> / <code>after_all</code> Model <code>pre_statements</code> / <code>post_statements</code> Scope Entire pipeline Single model Runs Once per plan/run Once per model execution Access to All views, schemas, environment Model-specific context Use for Global setup, cleanup, privileges Model-specific operations <p>Use <code>before_all</code>/<code>after_all</code> for operations that apply to the entire pipeline. Use model-level hooks for operations specific to individual models.</p>"},{"location":"configurations/options/linter/","title":"Linter","text":""},{"location":"configurations/options/linter/#linter","title":"Linter","text":"<p>Linting is a powerful tool for improving code quality and consistency. It enables you to automatically validate model definition, ensuring they adhere to your team's best practices.</p> <p>When a Vulcan plan is created, each model's code is checked for compliance with a set of rules you choose.</p> <p>Vulcan provides built-in rules, and you can define custom rules. This improves code quality and helps detect issues early in the development cycle when they are simpler to debug.</p>"},{"location":"configurations/options/linter/#rules","title":"Rules","text":"<p>Each linting rule is responsible for identifying a pattern in a model's code.</p> <p>Some rules validate that a pattern is not present, such as not allowing <code>SELECT *</code> in a model's outermost query. Other rules validate that a pattern is present, like ensuring that every model's <code>owner</code> field is specified. We refer to both of these below as \"validating a pattern\".</p> <p>Rules are defined in Python. Each rule is an individual Python class that inherits from Vulcan's <code>Rule</code> base class and defines the logic for validating a pattern.</p> <p>We display a portion of the <code>Rule</code> base class's code below (full source code). Its methods and properties illustrate the most important components of the subclassed rules you define.</p> <p>Each rule class you create has four vital components:</p> <ol> <li>Name: the class's name is used as the rule's name.</li> <li>Description: the class should define a docstring that provides a short explanation of the rule's purpose.</li> <li>Pattern validation logic: the class should define a <code>check_model()</code> method containing the core logic that validates the rule's pattern. The method can access any <code>Model</code> attribute.</li> <li>Rule violation logic: if a rule's pattern is not validated, the rule is \"violated\" and the class should return a <code>RuleViolation</code> object. The <code>RuleViolation</code> object should include the contextual information a user needs to understand and fix the problem.</li> </ol> <pre><code># Class name used as rule's name\nclass Rule:\n    # Docstring provides rule's description\n    \"\"\"The base class for a rule.\"\"\"\n\n    # Pattern validation logic goes in `check_model()` method\n    @abc.abstractmethod\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        \"\"\"The evaluation function that checks for a violation of this rule.\"\"\"\n\n    # Rule violation object returned by `violation()` method\n    def violation(self, violation_msg: t.Optional[str] = None) -&gt; RuleViolation:\n        \"\"\"Return a RuleViolation instance if this rule is violated\"\"\"\n        return RuleViolation(rule=self, violation_msg=violation_msg or self.summary)\n</code></pre>"},{"location":"configurations/options/linter/#built-in-rules","title":"Built-in rules","text":"<p>Vulcan includes a set of predefined rules that check for potential SQL errors or enforce code style.</p> <p>An example of the latter is the <code>NoSelectStar</code> rule, which prohibits a model from using <code>SELECT *</code> in its query's outer-most select statement.</p> <p>Here is code for the built-in <code>NoSelectStar</code> rule class, with the different components annotated:</p> <pre><code># Rule's name is the class name `NoSelectStar`\nclass NoSelectStar(Rule):\n    # Docstring explaining rule\n    \"\"\"Query should not contain SELECT * on its outer most projections, even if it can be expanded.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # If this model does not contain a SQL query, there is nothing to validate\n        if not isinstance(model, SqlModel):\n            return None\n\n        # Use the query's `is_star` property to detect the `SELECT *` pattern.\n        # If present, call the `violation()` method to return a `RuleViolation` object.\n        return self.violation() if model.query.is_star else None\n</code></pre> <p>Here are all of Vulcan's built-in linting rules:</p> Name Check type Explanation <code>ambiguousorinvalidcolumn</code> Correctness Vulcan found duplicate columns or was unable to determine whether a column is duplicated or not <code>invalidselectstarexpansion</code> Correctness The query's top-level selection may be <code>SELECT *</code>, but only if Vulcan can expand the <code>SELECT *</code> into individual columns <code>noselectstar</code> Stylistic The query's top-level selection may not be <code>SELECT *</code>, even if Vulcan can expand the <code>SELECT *</code> into individual columns <code>nomissingaudits</code> Governance Vulcan did not find any <code>audits</code> in the model's configuration to test data quality."},{"location":"configurations/options/linter/#user-defined-rules","title":"User-defined rules","text":"<p>You may define custom rules to implement your team's best practices.</p> <p>For instance, you could ensure all models have an <code>owner</code> by defining the following linting rule:</p> linter/user.py<pre><code>import typing as t\n\nfrom vulcan.core.linter.rule import Rule, RuleViolation\nfrom vulcan.core.model import Model\n\nclass NoMissingOwner(Rule):\n    \"\"\"Model owner should always be specified.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # Rule violated if the model's owner field (`model.owner`) is not specified\n        return self.violation() if not model.owner else None\n</code></pre> <p>Place a rule's code in the project's <code>linter/</code> directory. Vulcan will load all subclasses of <code>Rule</code> from that directory.</p> <p>If the rule is specified in the project's configuration file, Vulcan will run it when: - A plan is created during <code>vulcan plan</code> - The command <code>vulcan lint</code> is ran</p> <p>Vulcan will error if a model violates the rule, informing you which model(s) violated the rule. In this example, <code>full_model.sql</code> violated the <code>NoMissingOwner</code> rule, essentially halting execution:</p> <pre><code>$ vulcan plan\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Or through the standalone command, for faster iterations:</p> <pre><code>$ vulcan lint\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Use <code>vulcan lint --help</code> for more information.</p>"},{"location":"configurations/options/linter/#applying-linting-rules","title":"Applying linting rules","text":"<p>Specify which linting rules a project should apply in the project's configuration file.</p> <p>Rules are specified as lists of rule names under the <code>linter</code> key. Globally enable or disable linting with the <code>enabled</code> key, which is <code>false</code> by default.</p> <p>NOTE: you must set the <code>enabled</code> key to <code>true</code> key to apply the project's linting rules.</p>"},{"location":"configurations/options/linter/#specific-linting-rules","title":"Specific linting rules","text":"<p>This example specifies that the <code>\"ambiguousorinvalidcolumn\"</code> and <code>\"invalidselectstarexpansion\"</code> linting rules should be enforced:</p> YAMLPython <pre><code>linter:\n  enabled: true\n  rules: [\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=[\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#all-linting-rules","title":"All linting rules","text":"<p>Apply every built-in and user-defined rule by specifying <code>\"ALL\"</code> instead of a list of rules:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\"\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=\"all\",\n    )\n)\n</code></pre> <p>If you want to apply all rules except for a few, you can specify <code>\"ALL\"</code> and list the rules to ignore in the <code>ignored_rules</code> key:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\" # apply all built-in and user-defined rules and error if violated\n  ignored_rules: [\"noselectstar\"] # but don't run the `noselectstar` rule\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # apply all built-in and user-defined linting rules and error if violated\n        rules=\"all\",\n         # but don't run the `noselectstar` rule\n        ignored_rules=[\"noselectstar\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#exclude-a-model-from-linting","title":"Exclude a model from linting","text":"<p>You can specify that a specific model ignore a linting rule by specifying <code>ignored_rules</code> in its <code>MODEL</code> block.</p> <p>This example specifies that the model <code>docs_example.full_model</code> should not run the <code>invalidselectstarexpansion</code> rule:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  ignored_rules [\"invalidselectstarexpansion\"] # or \"ALL\" to turn off linting completely\n);\n</code></pre>"},{"location":"configurations/options/linter/#rule-violation-behavior","title":"Rule violation behavior","text":"<p>Linting rule violations raise an error by default, preventing the project from running until the violation is addressed.</p> <p>You may specify that a rule's violation should not error and only log a warning by specifying it in the <code>warn_rules</code> key instead of the <code>rules</code> key.</p> YAMLPython <pre><code>linter:\n  enabled: True\n  # error if `ambiguousorinvalidcolumn` rule violated\n  rules: [\"ambiguousorinvalidcolumn\"]\n  # but only warn if \"invalidselectstarexpansion\" is violated\n  warn_rules: [\"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # error if `ambiguousorinvalidcolumn` rule violated\n        rules=[\"ambiguousorinvalidcolumn\"],\n        # but only warn if \"invalidselectstarexpansion\" is violated\n        warn_rules=[\"invalidselectstarexpansion\"],\n    )\n)\n</code></pre> <p>Vulcan will raise an error if the same rule is included in more than one of the <code>rules</code>, <code>warn_rules</code>, and <code>ignored_rules</code> keys since they should be mutually exclusive.</p>"},{"location":"configurations/options/model_defaults/","title":"Model defaults","text":""},{"location":"configurations/options/model_defaults/#model-defaults","title":"Model defaults","text":"<p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed. Other values are set automatically unless explicitly overridden in the model definition.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference page.</p>"},{"location":"configurations/options/model_defaults/#basic-configuration","title":"Basic configuration","text":"<p>Example configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless overridden with the <code>kind</code> key. For more information on model kinds, refer to model concepts page.</p>"},{"location":"configurations/options/model_defaults/#identifier-resolution","title":"Identifier resolution","text":"<p>When a SQL engine receives a query such as <code>SELECT id FROM \"some_table\"</code>, it eventually needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This process is usually referred to as identifier (or name) resolution.</p> <p>Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.</p> <p>Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it normalizes and quotes all identifiers in those queries, respecting each dialect's resolution rules.</p> <p>The \"normalization strategy\", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.</p> <p>See here to learn more about the supported normalization strategies.</p>"},{"location":"configurations/options/model_defaults/#gateway-specific-model-defaults","title":"Gateway-specific model defaults","text":"<p>You can also define gateway specific <code>model_defaults</code> in the <code>gateways</code> section, which override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This allows you to tailor the behavior of models for each gateway without affecting the global <code>model_defaults</code>.</p> <p>For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.</p> <p>Gateway-specific <code>model_defaults</code> allow you to change how Vulcan performs identifier normalization by engine to align the different engines' behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway configuration overrides that global default dialect with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>That value tells Vulcan that the <code>redshift</code> gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"configurations/options/notifications/","title":"Notifications","text":""},{"location":"configurations/options/notifications/#notifications","title":"Notifications","text":"<p>Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.</p>"},{"location":"configurations/options/notifications/#notification-targets","title":"Notification targets","text":"<p>Notifications are configured with <code>notification targets</code>. Targets are specified in a project's configuration file (<code>config.yml</code> or <code>config.py</code>), and multiple targets can be specified for a project.</p> <p>A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each event type (e.g., notifications for <code>run</code> will be sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification target, corresponding to the two Slack notification methods and email notification. They are specified in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"configurations/options/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter may be useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"configurations/options/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"configurations/options/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"configurations/options/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n      - apply_failure\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#api-configuration","title":"API Configuration","text":"<p>If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n      - apply_end\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"configurations/options/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, new notification targets can be configured to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>). See the definitions of these classes on Github here.</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/variables/","title":"Variables","text":""},{"location":"configurations/options/variables/#variables","title":"Variables","text":"<p>This page covers environment variables and configuration overrides for your Vulcan project.</p>"},{"location":"configurations/options/variables/#environment-variables","title":"Environment Variables","text":"<p>Vulcan can access environment variables during configuration, enabling you to store secrets outside configuration files and dynamically change settings based on the user running Vulcan.</p>"},{"location":"configurations/options/variables/#using-env-files","title":"Using .env Files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>Security</p> <p>Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information.</p>"},{"location":"configurations/options/variables/#custom-env-file-location","title":"Custom .env File Location","text":"<p>Specify a custom path using the <code>--dotenv</code> CLI flag:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Or set the <code>VULCAN_DOTENV_PATH</code> environment variable:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag must be placed before the subcommand (e.g., <code>plan</code>, <code>run</code>).</p>"},{"location":"configurations/options/variables/#accessing-variables-in-configuration","title":"Accessing Variables in Configuration","text":"YAMLPython <p>Use <code>{{ env_var('VARIABLE_NAME') }}</code> syntax:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: admin\n      password: \"{{ env_var('SNOWFLAKE_PW') }}\"\n      account: my_account\n</code></pre> <p>Use <code>os.environ</code>:</p> <pre><code>import os\nfrom vulcan.core.config import Config, GatewayConfig, SnowflakeConnectionConfig\n\nconfig = Config(\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"admin\",\n                password=os.environ['SNOWFLAKE_PW'],\n                account=\"my_account\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/options/variables/#configuration-overrides","title":"Configuration Overrides","text":"<p>Environment variables have the highest precedence and will override configuration file values if they follow the <code>VULCAN__</code> naming convention.</p>"},{"location":"configurations/options/variables/#override-naming-structure","title":"Override Naming Structure","text":"<p>Use double underscores <code>__</code> to navigate the configuration hierarchy:</p> <pre><code>VULCAN__&lt;ROOT_KEY&gt;__&lt;NESTED_KEY&gt;__&lt;FIELD&gt;=value\n</code></pre> <p>Example: Override a gateway connection password:</p> <pre><code># config.yaml\ngateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      password: dummy_pw  # This will be overridden\n</code></pre> <pre><code># Override with environment variable\nexport VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre>"},{"location":"configurations/options/variables/#dynamic-configuration","title":"Dynamic Configuration","text":""},{"location":"configurations/options/variables/#user-based-target-environment","title":"User-based Target Environment","text":"<p>Use the <code>{{ user() }}</code> function to dynamically set configuration based on the current user:</p> YAMLPython <pre><code># Each user gets their own dev environment\ndefault_target_environment: dev_{{ user() }}\n</code></pre> <pre><code>import getpass\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre> <p>This allows running <code>vulcan plan</code> instead of <code>vulcan plan dev_username</code>.</p>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#cookbook","title":"Cookbook","text":"<p>Coming soon...</p>"},{"location":"examples/overview/","title":"Orders360 - Postgres Mini Example","text":""},{"location":"examples/overview/#orders360-postgres-mini-example","title":"Orders360 - Postgres Mini Example","text":"<p>A minimal e-commerce data product demonstrating Vulcan's core capabilities with a simple orders, customers, and products domain.</p>"},{"location":"examples/overview/#objective","title":"Objective","text":"<p>Build a lightweight sales analytics pipeline that: - Ingests raw transactional data (customers, orders, products) - Transforms into daily sales aggregations - Exposes semantic layer for BI tools and analytics</p>"},{"location":"examples/overview/#user-story","title":"User Story","text":"<p>As a Sales Operations Manager, I want a daily view of order volumes and revenue, So that I can track sales performance and identify trends.</p>"},{"location":"examples/overview/#business-context","title":"Business Context","text":"<p>The marketing team runs campaigns across multiple channels and needs to measure their impact on daily sales. Currently, they rely on manual spreadsheet exports that are error-prone and outdated by the time they're reviewed.</p>"},{"location":"examples/overview/#key-questions-this-data-product-answers","title":"Key Questions This Data Product Answers","text":"<ol> <li> <p>How many orders did we process yesterday?     Query <code>total_orders</code> from <code>daily_sales</code></p> </li> <li> <p>What was our revenue for last week?     Use weekly granularity on <code>order_date</code> with <code>total_daily_revenue</code> measure</p> </li> <li> <p>Which days exceeded our $100 daily target?     Apply the <code>high_revenue_days</code> segment</p> </li> <li> <p>Are there any data quality issues in incoming orders?     Automated checks validate completeness, uniqueness, and value ranges</p> </li> </ol>"},{"location":"examples/overview/#stakeholders","title":"Stakeholders","text":"Role Usage Sales Manager Daily dashboard for revenue tracking Marketing Analyst Campaign performance correlation Finance Team Monthly revenue reconciliation Data Engineer Pipeline health monitoring"},{"location":"examples/overview/#project-structure","title":"Project Structure","text":"<pre><code>postgres-mini/\n seeds/                  # Raw CSV data files\n models/\n seeds/              # Seed model definitions (raw data ingestion)\n daily_sales.sql     # Aggregated daily sales model\n semantics/              # Semantic layer definitions\n checks/                 # Data quality checks (Soda-style)\n tests/                  # Unit tests for models\n audits/                 # Custom audit definitions\n config.yaml             # Project configuration\n</code></pre>"},{"location":"examples/overview/#data-models","title":"Data Models","text":""},{"location":"examples/overview/#seed-models-raw-layer","title":"Seed Models (Raw Layer)","text":"Model Description Grain <code>raw.raw_customers</code> Customer master data <code>customer_id</code> <code>raw.raw_orders</code> Order transactions <code>order_id</code> <code>raw.raw_products</code> Product catalog <code>product_id</code>"},{"location":"examples/overview/#transformation-models","title":"Transformation Models","text":"Model Description Grain Schedule <code>sales.daily_sales</code> Daily aggregated sales metrics <code>order_date</code> <code>@daily</code> <p>Daily Sales aggregates orders by date with: - <code>total_orders</code> - Count of orders per day - <code>total_revenue</code> - Sum of order amounts per day - <code>last_order_id</code> - Latest order ID processed</p>"},{"location":"examples/overview/#semantic-layer","title":"Semantic Layer","text":"<p>The semantic layer (<code>semantics/daily_sales.yml</code>) provides:</p> <p>Dimensions with time granularities: - <code>order_date</code> (weekly, monthly, quarterly rollups)</p> <p>Measures: - <code>total_daily_orders</code> - Sum of orders across date range - <code>total_daily_revenue</code> - Sum of revenue across date range</p> <p>Segments: - <code>high_revenue_days</code> - Days with $100+ revenue</p>"},{"location":"examples/overview/#data-quality","title":"Data Quality","text":""},{"location":"examples/overview/#assertions-model-level","title":"Assertions (Model-level)","text":"<ul> <li>Unique grain validation</li> <li>Not-null constraints on required columns</li> <li>Positive value checks on amounts</li> </ul>"},{"location":"examples/overview/#checks-soda-style","title":"Checks (Soda-style)","text":"<ul> <li>Completeness checks (missing counts, row counts)</li> <li>Validity checks (negative values, data consistency)</li> <li>Anomaly detection on row counts</li> </ul>"},{"location":"examples/overview/#quick-start","title":"Quick Start","text":"<pre><code># Start infrastructure\nmake setup\n\n# plan to see changes\nvulcan plan\n</code></pre>"},{"location":"examples/overview/#configuration","title":"Configuration","text":"<ul> <li>Dialect: PostgreSQL</li> <li>State Store: PostgreSQL</li> </ul>"},{"location":"examples/exhaustive_model/","title":"B2B SaaS Example - Split Docker Compose Setup","text":""},{"location":"examples/exhaustive_model/#b2b-saas-example-split-docker-compose-setup","title":"B2B SaaS Example - Split Docker Compose Setup","text":"<p>This project uses split Docker Compose files for better service management and scalability. Services are organized into separate compose files that share a common external Docker network.</p>"},{"location":"examples/exhaustive_model/#setup","title":"Setup","text":"<p>You can use <code>make</code> commands for convenience, or run the docker compose commands directly.</p> <p>make setup </p> <p>make vulcan-up</p> <p>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev vulcan\"</p> <p>vulcan plan </p> <p>make vulcan-down </p> <p>make all-down</p> <p>vulcan transpile --format sql \"select measure(total_order_lines) from orders\"</p> <p>vulcan create_test ANALYTICS.ORDERS --query DEMO.RAW_DATA.ORDERS \"select ORDER_ID ,ORDER_DATE ,CUSTOMER_ID ,PRODUCT_ID ,QUANTITY ,UNIT_PRICE ,DISCOUNT ,TAX ,SHIPPING_COST ,TOTAL_AMOUNT FROM DEMO.RAW_DATA.ORDERS WHERE ORDER_DATE  BETWEEN '2025-01-01' and '2025-01-15'\"</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Coming soon...</p>"},{"location":"getting_started/","title":"Quickstart","text":""},{"location":"getting_started/#quickstart","title":"Quickstart","text":"<p>Welcome to the Vulcan quickstart! This guide will help you get up and running with Vulcan quickly.</p>"},{"location":"getting_started/#choose-your-path","title":"Choose Your Path","text":"<p>Vulcan can be run in different ways. Choose the option that works best for you:</p>"},{"location":"getting_started/#docker-recommended","title":"Docker (Recommended)","text":"<p>Best for: Getting started quickly, consistent environments, production-like setup</p> <p>The Docker approach uses Docker Compose to set up all necessary infrastructure and services. This is the fastest way to start working with Vulcan and provides a production-like environment.</p> <p>\ud83d\udc49 Start with Docker Quickstart</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure your system meets the prerequisites for using Vulcan.</p>"},{"location":"getting_started/cli/","title":"CLI","text":""},{"location":"getting_started/cli/#cli","title":"CLI","text":"<p>In this quickstart, you'll use the Vulcan command line interface (CLI) to get up and running with Vulcan's scaffold generator.</p> <p>It will create an example project that runs locally on your computer using DuckDB as an embedded SQL engine.</p> <p>Before beginning, ensure that you meet all the prerequisites for using Vulcan.</p> <p>Using Docker?</p> <p>If you're using the Docker installation method (recommended), you'll need to run the <code>vulcan</code> commands inside the Docker shell. </p> <p>Start the Docker shell with: </p><pre><code>make vulcan-shell\n</code></pre> or <pre><code>docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell\n</code></pre><p></p> <p>Alternatively, you can create a temporary alias: </p><pre><code>alias vulcan=\"docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell vulcan\"\n</code></pre><p></p> <p>For a complete Docker setup guide, see the Docker Quickstart.</p> Learn more about the quickstart project structure <p>This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.</p> <p>The project contains three models with a CSV file as the only data source:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502seed_data.csv\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n             \u2502\n            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502seed_model.sql\u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                          \u2502\n                         \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502incremental_model.sql\u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                                              \u2502\n                                             \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                             \u2502full_model.sql\u2502\n                                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.</p>"},{"location":"getting_started/cli/#1-create-the-vulcan-project","title":"1. Create the Vulcan project","text":"<p>First, create a project directory and navigate to it:</p> <p></p><pre><code>mkdir vulcan-example\n</code></pre> <pre><code>cd vulcan-example\n</code></pre><p></p> <p>Docker Users</p> <p>If you're using Docker, make sure your Docker infrastructure is set up and running. See the Docker Quickstart for setup instructions. Then access the Vulcan shell before running the commands below.</p> <p>Python Library Users (Coming Soon)</p> <p>If using a Python virtual environment (when available), ensure it's activated first by running the <code>source .venv/bin/activate</code> command.</p>"},{"location":"getting_started/cli/#11-initialize-the-project","title":"1.1 Initialize the project","text":"<p>Vulcan includes a scaffold generator to initialize a new Vulcan project.</p> <p>The scaffold generator will ask you some questions and create a Vulcan configuration file based on your responses.</p> <p>Depending on your answers, it will also create multiple files for the Vulcan example project used in this quickstart.</p> <p>Start the scaffold generator by executing the <code>vulcan init</code> command:</p> <pre><code>vulcan init\n</code></pre> Skip the questions <p>If you don't want to use the interactive scaffold generator, you can initialize your project with arguments to the <code>vulcan init</code> command.</p> <p>The only required argument is <code>engine</code>, which specifies the SQL engine your project will use. Specify one of the engine <code>type</code>s from the supported execution engines.</p> <p>In this example, we specify the <code>duckdb</code> engine:</p> <pre><code>vulcan init duckdb\n</code></pre> <p>The scaffold will include a Vulcan configuration file and example project directories and files. You're now ready to continue the quickstart below.</p>"},{"location":"getting_started/cli/#project-type","title":"Project type","text":"<p>The first question asks about the type of project you want to create. Enter the number corresponding to the type of project you want to create and press <code>Enter</code>.</p> <pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nWelcome to Vulcan!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nWhat type of project do you want to set up?\n\n    [1] DEFAULT - Create Vulcan example project models and files\n    [2] dbt     - You have an existing dbt project and want to run it with Vulcan\n    [3] EMPTY   - Create a Vulcan configuration file and project directories only\n\nEnter a number: 1\n</code></pre> <p>For this quickstart, choose the <code>DEFAULT</code> option <code>1</code> so the example project files are included in the project directories.</p>"},{"location":"getting_started/cli/#sql-engine","title":"SQL engine","text":"<p>The second question asks which SQL engine your project will use. Vulcan will include that engine's connection settings in the configuration file, which you will fill in later to connect your project to the engine.</p> <p>For this quickstart, choose the <code>DuckDB</code> option <code>1</code> so we can run the example project with the built-in DuckDB engine that doesn't need additional configuration.</p> <pre><code>Choose your SQL engine:\n\n    [1]  DuckDB\n    [2]  Snowflake\n    [3]  Databricks\n    [4]  BigQuery\n    [5]  MotherDuck\n    [6]  ClickHouse\n    [7]  Redshift\n    [8]  Spark\n    [9]  Trino\n    [10] Azure SQL\n    [11] MSSQL\n    [12] Postgres\n    [13] GCP Postgres\n    [14] MySQL\n    [15] Athena\n    [16] RisingWave\n\nEnter a number: 1\n</code></pre>"},{"location":"getting_started/cli/#cli-mode","title":"CLI mode","text":"<p>Vulcan's core commands have multiple options that alter their behavior. Some of those options streamline the Vulcan <code>plan</code> workflow and CLI output.</p> <p>If you prefer a streamlined workflow (no prompts, no file diff previews, auto-apply changes), choose the <code>FLOW</code> CLI mode to automatically include those options in your project configuration file.</p> <p>If you prefer to see all the output Vulcan provides, choose <code>DEFAULT</code> mode, which we will use in this quickstart:</p> <pre><code>Choose your Vulcan CLI experience:\n\n    [1] DEFAULT - See and control every detail\n    [2] FLOW    - Automatically run changes and show summary output\n\nEnter a number: 1\n</code></pre>"},{"location":"getting_started/cli/#ready-to-go","title":"Ready to go","text":"<p>Your project is now ready to go, and Vulcan displays a message with some good next steps.</p> <p>If you chose the DuckDB engine, you're ready to move forward and run the example project with DuckDB.</p> <p>If you chose a different engine, add your engine's connection information to the <code>config.yaml</code> file before you run any additional Vulcan commands.</p> <pre><code>Your Vulcan project is ready!\n\nNext steps:\n- Update your gateway connection settings (e.g., username/password) in the project configuration file:\n    /vulcan-example/config.yaml\n- Run command in CLI: vulcan plan\n- (Optional) Explain a plan: vulcan plan --explain\n\nQuickstart guide:\nhttps://vulcan.readthedocs.io/en/stable/quickstart/cli/\n\nNeed help?\n- Docs:   https://vulcan.readthedocs.io\n- Slack:  https://www.tobikodata.com/slack\n- GitHub: https://github.com/TobikoData/vulcan/issues\n</code></pre> Learn more about the project's configuration: <code>config.yaml</code> <p>Vulcan project-level configuration parameters are specified in the <code>config.yaml</code> file in the project directory.</p> <p>This example project uses the embedded DuckDB SQL engine, so its configuration specifies <code>duckdb</code> as the gateway's connection type. All available configuration settings are included in the file, with optional settings set to their default value and commented out.</p> <p>Vulcan requires a default model SQL dialect. Vulcan automatically specifies the SQL dialect for your project's SQL engine, which it places in the config <code>model_defaults</code> <code>dialect</code> key. In this example, we specified the DuckDB engine, so <code>duckdb</code> is the default SQL dialect:</p> <pre><code># --- Gateway Connection ---\ngateways:\n  duckdb:\n    connection:\n      # For more information on configuring the connection to your execution engine, visit:\n      # https://vulcan.readthedocs.io/en/stable/reference/configuration/#connection\n      # https://vulcan.readthedocs.io/en/stable/integrations/engines/duckdb/#connection-options\n      #\n      type: duckdb               # &lt;-- DuckDB engine\n      database: db.db\n      # concurrent_tasks: 1\n      # register_comments: True  # &lt;-- Optional setting `register_comments` has a default value of True\n      # pre_ping: False\n      # pretty_sql: False\n      # catalogs:                # &lt;-- Optional setting `catalogs` has no default value\n      # extensions:\n      # connector_config:\n      # secrets:\n      # token:\n\ndefault_gateway: duckdb\n\n# --- Model Defaults ---\n# https://vulcan.readthedocs.io/en/stable/reference/model_configuration/#model-defaults\n\nmodel_defaults:\n  dialect: duckdb                # &lt;-- Models written in DuckDB SQL dialect by default\n  start: 2025-06-12 # Start date for backfill history\n  cron: '@daily'    # Run models daily at 12am UTC (can override per model)\n\n# --- Linting Rules ---\n# Enforce standards for your team\n# https://vulcan.readthedocs.io/en/stable/guides/linter/\n\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre> <p>Learn more about Vulcan project configuration here.</p> <p>The scaffold generator creates multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).</p> Learn more about the project directories and files <p>Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.</p> <p>The scaffold generator will create the following configuration file and directories:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. More info about configuration here.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. More info about models here.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. More info about seeds here.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. More info about audits here.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. More info about tests here.</li> </ul> </li> <li>./macros<ul> <li>Macro files. More info about macros here.</li> </ul> </li> </ul> <p>It will also create the files needed for this quickstart example:</p> <ul> <li>./models<ul> <li>full_model.sql</li> <li>incremental_model.sql</li> <li>seed_model.sql</li> </ul> </li> <li>./seeds<ul> <li>seed_data.csv</li> </ul> </li> <li>./audits<ul> <li>assert_positive_order_ids.sql</li> </ul> </li> <li>./tests<ul> <li>test_full_model.yaml</li> </ul> </li> </ul> <p>Finally, the scaffold generator creates data for the example project to use.</p> Learn more about the project's data <p>The data used in this example project is contained in the <code>seed_data.csv</code> file in the <code>/seeds</code> project directory. The data reflects sales of 3 items over 7 days in January 2020.</p> <p>The file contains three columns, <code>id</code>, <code>item_id</code>, and <code>event_date</code>, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.</p> <p>This is the complete dataset:</p> id item_id event_date 1 2 2020-01-01 2 1 2020-01-01 3 3 2020-01-03 4 1 2020-01-04 5 1 2020-01-05 6 1 2020-01-06 7 1 2020-01-07"},{"location":"getting_started/cli/#2-create-a-prod-environment","title":"2. Create a prod environment","text":"<p>Vulcan's key actions are creating and applying plans to environments. At this point, the only environment is the empty <code>prod</code> environment.</p> Learn more about Vulcan plans and environments <p>Vulcan's key actions are creating and applying plans to environments.</p> <p>A Vulcan environment is an isolated namespace containing models and the data they generated.</p> <p>The most important environment is <code>prod</code> (\"production\"), which consists of the databases behind the applications your business uses to operate each day. Environments other than <code>prod</code> provide a place where you can test and preview changes to model code before they go live and affect business operations.</p> <p>A Vulcan plan contains a comparison of one environment to another and the set of changes needed to bring them into alignment.</p> <p>For example, if a new SQL model was added, tested, and run in the <code>dev</code> environment, it would need to be added and run in the <code>prod</code> environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.</p> <p>Breaking changes are those that invalidate data already existing in an environment. For example, if a <code>WHERE</code> clause was added to a model in the <code>dev</code> environment, existing data created by that model in the <code>prod</code> environment are now invalid because they may contain rows that would be filtered out by the new <code>WHERE</code> clause.</p> <p>Other changes, like adding a new column to a model in <code>dev</code>, are non-breaking because all the existing data in <code>prod</code> are still valid to use - only new data must be added to align the environments.</p> <p>After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to \"backfill\" data to apply the plan. (In this context, backfill is a generic term for updating or adding to a table's data, including an initial load or full refresh.)</p> Learn more about a plan's actions: <code>vulcan plan --explain</code> <p>Before applying a plan, you can view a detailed description of the actions it will take by passing the explain flag in your <code>vulcan plan</code> command:</p> <pre><code>vulcan plan --explain\n</code></pre> <p>Passing the explain flag for the quickstart example project above adds the following information to the output:</p> <pre><code>Explained plan\n\u251c\u2500\u2500 Validate SQL and create physical layer tables and views if they do not exist\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502       \u2514\u2500\u2500 Create table if it doesn't exist\n\u251c\u2500\u2500 Backfill models by running their queries and run standalone audits\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u2514\u2500\u2500 Fully refresh table\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u2514\u2500\u2500 Fully refresh table\n\u2514\u2500\u2500 Update the virtual layer for environment 'prod'\n    \u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n        \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n        \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n        \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n</code></pre> <p>The explanation has three top-level sections, corresponding to the three types of actions a plan takes:</p> <ul> <li>Validate SQL and create physical layer tables and views if they do not exist</li> <li>Backfill models by running their queries and run standalone audits</li> <li>Update the virtual layer for environment 'prod'</li> </ul> <p>Each section lists the affected models and provides more information about what will occur. For example, the first model in the first section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n</code></pre> <p>The first line shows the model name <code>vulcan_example.seed_model</code> and the physical layer table Vulcan will create to store its data: <code>db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172</code>. The second and third lines tell us that in this step Vulcan will dry-run the model query and create the physical layer table if it doesn't exist.</p> <p>The second section describes what will occur during the backfill step. The second model in this section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n</code></pre> <p>The first line shows the model name <code>vulcan_example.full_model</code> and the physical layer table Vulcan will insert the model's data into: <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>. The second and third lines tell us that the backfill action will fully refresh the model's physical table and run the <code>assert_positive_order_ids</code> audit.</p> <p>The final section describes Vulcan's action during the virtual layer update step. The first model in this section is:</p> <pre><code>\u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n    \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n</code></pre> <p>The virtual layer step will update the <code>vulcan_example.full_model</code> virtual layer view to <code>SELECT * FROM</code> the physical table <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>.</p> <p>The first Vulcan plan must execute every model to populate the production environment. Running <code>vulcan plan</code> will generate the plan and the following output:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb in 0.1 seconds.\n----------------------------------------------------------------------\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u2514\u2500\u2500 vulcan_example.seed_model\nModels needing backfill:\n\u251c\u2500\u2500 vulcan_example.full_model: [full refresh]\n\u251c\u2500\u2500 vulcan_example.incremental_model: [2020-01-01 - 2025-06-22]\n\u2514\u2500\u2500 vulcan_example.seed_model: [full refresh]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 3 of the output notes that <code>vulcan plan</code> successfully executed the project's test <code>tests/test_full_model.yaml</code> with duckdb.</p> <p>Line 6 describes what environments the plan will affect when applied - a new <code>prod</code> environment in this case.</p> <p>Lines 8-12 of the output show that Vulcan detected three new models relative to the current empty environment.</p> <p>Lines 13-16 list each model that will be executed by the plan, along with the date intervals or refresh types. For both <code>full_model</code> and <code>seed_model</code>, it shows <code>[full refresh]</code>, while for <code>incremental_model</code> it shows a specific date range <code>[2020-01-01 - 2025-06-22]</code>. The incremental model date range begins from 2020-01-01 because its definition specifies a model start date of <code>2020-01-01</code>.</p> Learn more about the project's models <p>A plan's actions are determined by the kinds of models the project uses. This example project uses three model kinds:</p> <ol> <li><code>SEED</code> models read data from CSV files stored in the Vulcan project directory.</li> <li><code>FULL</code> models fully refresh (rewrite) the data associated with the model every time the model is run.</li> <li><code>INCREMENTAL_BY_TIME_RANGE</code> models use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.</li> </ol> <p>We now briefly review each model in the project.</p> <p>The first model is a <code>SEED</code> model that imports <code>seed_data.csv</code>. This model consists of only a <code>MODEL</code> statement because <code>SEED</code> models do not query a database.</p> <p>In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the <code>grain</code> of the model to the columns that collectively form the model's unique identifier, <code>id</code> and <code>event_date</code>.</p> <pre><code>MODEL (\n  name vulcan_example.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INTEGER,\n    item_id INTEGER,\n    event_date DATE\n  ),\n  grain (id, event_date)\n);\n</code></pre> <p>The second model is an <code>INCREMENTAL_BY_TIME_RANGE</code> model that includes both a <code>MODEL</code> statement and a SQL query selecting from the first seed model.</p> <p>The <code>MODEL</code> statement's <code>kind</code> property includes the required specification of the data column containing each record's timestamp. It also includes the optional <code>start</code> property specifying the earliest date/time for which the model should process data and the <code>cron</code> property specifying that the model should run daily. It sets the model's grain to columns <code>id</code> and <code>event_date</code>.</p> <p>The SQL query includes a <code>WHERE</code> clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre> <p>The final model in the project is a <code>FULL</code> model. In addition to properties used in the other models, its <code>MODEL</code> statement includes the <code>audits</code> property. The project includes a custom <code>assert_positive_order_ids</code> audit in the project <code>audits</code> directory; it verifies that all <code>item_id</code> values are positive numbers. It will be run every time the model is executed.</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>Line 18 asks you whether to proceed with executing the model backfills described in lines 13-16. Enter <code>y</code> and press <code>Enter</code>, and Vulcan will execute the models and return this output:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example.seed_model          [insert seed file]                 0.01s\n[1/1] vulcan_example.incremental_model   [insert 2020-01-01 - 2025-06-22]   0.01s\n[1/1] vulcan_example.full_model          [full refresh, audits \u27141]          0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Vulcan performs three actions when applying the plan:</p> <ul> <li>Creating and storing new versions of the models</li> <li>Evaluating/running the models</li> <li>Virtually updating the plan's target environment</li> </ul> <p>Lines 2-4 show the progress and completion of the first step - updating the physical layer (creating new model versions).</p> <p>Lines 6-11 show the execution of each model with their specific operations and timing. Line 6 shows the seed model being inserted, line 8 shows the incremental model being inserted for the specified date range, and line 10 shows the full model being processed with its audit check passing.</p> <p>Lines 12-14 show the progress and completion of the second step - executing model batches.</p> <p>Lines 16-18 show the progress and completion of the final step - virtually updating the plan's target environment, which makes the data available for querying.</p> <p>Let's take a quick look at the project's DuckDB database file to see the objects Vulcan created. First, we open the built-in DuckDB CLI tool with the <code>duckdb db.db</code> command, then run our two queries.</p> <p>Our first query shows the three physical tables Vulcan created in the <code>vulcan__vulcan_example</code> schema (one table for each model):</p> <p></p> <p>Our second query shows that in the <code>vulcan</code> schema Vulcan created three virtual layer views that read from the three physical tables:</p> <p></p> <p>You've now created a new production environment with all of history backfilled!</p>"},{"location":"getting_started/cli/#3-update-a-model","title":"3. Update a model","text":"<p>Now that we have populated the <code>prod</code> environment, let's modify one of the SQL models.</p> <p>We modify the incremental SQL model by adding a new column to the query. Open the <code>models/incremental_model.sql</code> file and add <code>'z' AS new_column</code> below <code>item_id</code> as follows:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  'z' AS new_column, -- Added column\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre>"},{"location":"getting_started/cli/#4-work-with-a-development-environment","title":"4. Work with a development environment","text":""},{"location":"getting_started/cli/#41-create-a-dev-environment","title":"4.1 Create a dev environment","text":"<p>Now that you've modified a model, it's time to create a development environment so that you can validate the model change without affecting production.</p> <p>Run <code>vulcan plan dev</code> to create a development environment called <code>dev</code>:</p> <pre><code>$ vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nNew environment `dev` will be created from `prod`\n\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example__dev.incremental_model\n(Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.incremental_model: [2020-01-01 - 2025-04-17]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 6 of the output states that a new environment <code>dev</code> will be created from the existing <code>prod</code> environment.</p> <p>Lines 10-15 summarize the differences between the modified model and the <code>prod</code> environment, detecting that we directly modified <code>incremental_model</code> and that <code>full_model</code> was indirectly modified because it selects from the incremental model. Note that the model schemas are <code>vulcan_example__dev</code>, indicating that they are being created in the <code>dev</code> environment.</p> <p>On line 31, we see that Vulcan automatically classified the change as <code>Non-breaking</code> because it understood that the change was additive (added a column not used by <code>full_model</code>) and did not invalidate any data already in <code>prod</code>.</p> <p>Enter <code>y</code> at the prompt and press <code>Enter</code> to apply the plan and execute the backfill:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example__dev.incremental_model  [insert 2020-01-01 - 2025-04-17] 0.03s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Lines 3-5 show the progress and completion of updating the physical layer.</p> <p>Line 7 shows that Vulcan applied the change and evaluated <code>vulcan_example__dev.incremental_model</code> for the date range from 2020-01-01 to 2025-04-17.</p> <p>Lines 9-11 show the progress and completion of executing model batches.</p> <p>Lines 13-15 show the progress and completion of updating the virtual layer.</p> <p>Vulcan did not need to backfill anything for the <code>full_model</code> since the change was <code>Non-breaking</code>.</p>"},{"location":"getting_started/cli/#42-validate-updates-in-dev","title":"4.2 Validate updates in dev","text":"<p>You can now view this change by querying data from <code>incremental_model</code> with <code>vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"</code>.</p> <p>Note that the environment name <code>__dev</code> is appended to the schema namespace <code>vulcan_example</code> in the query:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre> <p>You can see that <code>new_column</code> was added to the dataset. The production table was not modified; you can validate this by querying the production table using <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>.</p> <p>Note that nothing has been appended to the schema namespace <code>vulcan_example</code> in this query because <code>prod</code> is the default environment.</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id   event_date\n0   1        2   2020-01-01\n1   2        1   2020-01-01\n2   3        3   2020-01-03\n3   4        1   2020-01-04\n4   5        1   2020-01-05\n5   6        1   2020-01-06\n6   7        1   2020-01-07\n</code></pre> <p>The production table does not have <code>new_column</code> because the changes to <code>dev</code> have not yet been applied to <code>prod</code>.</p>"},{"location":"getting_started/cli/#5-update-the-prod-environment","title":"5. Update the prod environment","text":""},{"location":"getting_started/cli/#51-apply-updates-to-prod","title":"5.1 Apply updates to prod","text":"<p>Now that we've tested the changes in dev, it's time to move them to production. Run <code>vulcan plan</code> to plan and apply your changes to the <code>prod</code> environment.</p> <p>Enter <code>y</code> and press <code>Enter</code> at the <code>Apply - Virtual Update [y/n]:</code> prompt to apply the plan and execute the backfill:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.full_model (Indirect Non-breaking)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Note that a backfill was not necessary and only a Virtual Update occurred, as indicated by the \"SKIP: No physical layer updates to perform\" and \"SKIP: No model batches to execute\" messages. This is because the changes were already calculated and executed in the <code>dev</code> environment, and Vulcan is smart enough to recognize that it only needs to update the virtual references to the existing tables rather than recomputing everything.</p>"},{"location":"getting_started/cli/#52-validate-updates-in-prod","title":"5.2 Validate updates in prod","text":"<p>Double-check that the data updated in <code>prod</code> by running <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre>"},{"location":"getting_started/cli/#6-next-steps","title":"6. Next steps","text":"<p>Congratulations, you've now conquered the basics of using Vulcan!</p> <p>From here, you can:</p> <ul> <li>Learn more about Vulcan CLI commands</li> <li>Set up a connection to a database or SQL engine</li> <li>Learn more about Vulcan concepts</li> <li>Join our Slack community</li> </ul>"},{"location":"getting_started/prerequisites/","title":"Prerequisites","text":""},{"location":"getting_started/prerequisites/#prerequisites","title":"Prerequisites","text":"<p>This page describes the system prerequisites needed to run Vulcan and provides instructions for meeting them.</p>"},{"location":"getting_started/prerequisites/#docker-prerequisites","title":"Docker Prerequisites","text":"<p>The recommended way to run Vulcan is using Docker. This provides a consistent, production-like environment.</p>"},{"location":"getting_started/prerequisites/#docker-desktop","title":"Docker Desktop","text":"<p>You'll need Docker Desktop installed and running on your machine:</p> <ul> <li>macOS: Download from Docker Desktop for Mac</li> <li>Windows: Download from Docker Desktop for Windows</li> <li>Linux: Install Docker Engine and Docker Compose following the official Docker installation guide</li> </ul>"},{"location":"getting_started/prerequisites/#verify-docker-installation","title":"Verify Docker Installation","text":"<p>Verify that Docker is installed and running:</p> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>You should see version numbers for both commands. If Docker Desktop is running, you should also be able to run:</p> <pre><code>docker ps\n</code></pre>"},{"location":"getting_started/prerequisites/#system-requirements","title":"System Requirements","text":"<ul> <li>RAM: At least 4GB of available RAM (8GB recommended)</li> <li>Disk Space: At least 5GB of free disk space</li> <li>CPU: Modern multi-core processor</li> </ul>"},{"location":"getting_started/prerequisites/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is included with Docker Desktop. If you're on Linux and need to install it separately, follow the Docker Compose installation guide.</p>"},{"location":"getting_started/prerequisites/#python-prerequisites","title":"Python Prerequisites","text":"<p>Python Library Installation</p> <p>Vulcan will be available as a Python library in the future. When available, you'll need Python 3.8 or higher.</p> <p>For now, use the Docker installation method above.</p>"},{"location":"guides/data_quality/","title":"Data Quality","text":""},{"location":"guides/data_quality/#data-quality","title":"Data Quality","text":"<p>This guide explains how to use Audits, Checks, and Tests together to ensure data quality in your Orders360 project. Learn when to use each tool and see complex examples where they work together.</p>"},{"location":"guides/data_quality/#the-three-layer-quality-strategy","title":"The Three-Layer Quality Strategy","text":"<pre><code>flowchart TB\n    subgraph \"Layer 1: Audits - Critical Blocking\"\n        AUDIT[Audits&lt;br/&gt;Block invalid data&lt;br/&gt;Run with model]\n        EXAMPLES1[\"\u2022 Primary keys unique&lt;br/&gt;\u2022 Revenue non-negative&lt;br/&gt;\u2022 Foreign keys valid\"]\n    end\n\n    subgraph \"Layer 2: Checks - Monitoring\"\n        CHECK[Checks&lt;br/&gt;Track quality trends&lt;br/&gt;Non-blocking]\n        EXAMPLES2[\"\u2022 Row count anomalies&lt;br/&gt;\u2022 Completeness trends&lt;br/&gt;\u2022 Cross-model validation\"]\n    end\n\n    subgraph \"Layer 3: Tests - Logic Validation\"\n        TEST[Tests&lt;br/&gt;Validate transformations&lt;br/&gt;Unit testing]\n        EXAMPLES3[\"\u2022 SQL logic correct&lt;br/&gt;\u2022 Expected outputs&lt;br/&gt;\u2022 Edge cases\"]\n    end\n\n    AUDIT --&gt; EXAMPLES1\n    CHECK --&gt; EXAMPLES2\n    TEST --&gt; EXAMPLES3\n\n    style AUDIT fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style TEST fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>When to Use Each:</p> Tool Purpose Blocks Pipeline? Best For Audits Critical validation \u2705 Yes (always) Business rules, data integrity Checks Quality monitoring \u274c No Trends, anomalies, monitoring Tests Logic validation \u274c No SQL correctness, edge cases <p>[Screenshot: Visual comparison of the three quality tools]</p>"},{"location":"guides/data_quality/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/data_quality/#audits-critical-blocking-validation","title":"Audits: Critical Blocking Validation","text":"<p>Use audits when: Data must be correct or the pipeline should stop.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Primary key validation\n    not_null(columns := (order_date)),\n    unique_values(columns := (order_date)),\n\n    -- Business rules\n    positive_values(column := total_revenue),\n    accepted_range(column := total_orders, min_v := 0, max_v := 10000)\n  )\n);\n</code></pre> <p>Characteristics: - \u2705 Always blocking - fails stop execution - \u2705 Run automatically with model execution - \u2705 Fast feedback during development - \u2705 Perfect for critical business rules</p> <p>[Screenshot: Audit failure blocking plan execution]</p>"},{"location":"guides/data_quality/#checks-quality-monitoring","title":"Checks: Quality Monitoring","text":"<p>Use checks when: You want to monitor trends and detect anomalies over time.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n</code></pre> <p>Characteristics: - \u2705 Non-blocking - warnings, not failures - \u2705 Historical tracking - see trends over time - \u2705 Anomaly detection - statistical analysis - \u2705 Perfect for monitoring and alerting</p> <p>[Screenshot: Check results showing trends over time]</p>"},{"location":"guides/data_quality/#tests-logic-validation","title":"Tests: Logic Validation","text":"<p>Use tests when: You need to validate SQL transformations and edge cases.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_aggregation\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n</code></pre> <p>Characteristics: - \u2705 Unit testing for SQL logic - \u2705 Validates expected outputs - \u2705 Tests edge cases - \u2705 Perfect for development</p> <p>[Screenshot: Test execution showing pass/fail results]</p>"},{"location":"guides/data_quality/#complex-example-orders360-daily-sales","title":"Complex Example: Orders360 Daily Sales","text":"<p>Let's see how all three tools work together for the <code>sales.daily_sales</code> model:</p>"},{"location":"guides/data_quality/#the-model","title":"The Model","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  assertions (\n    -- Audit: Critical validations\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    unique_values(columns := (order_date)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue),\n    accepted_range(column := total_revenue, min_v := 0, max_v := 1000000)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"guides/data_quality/#layer-1-audits-critical-blocking","title":"Layer 1: Audits (Critical Blocking)","text":"<p>Why: These rules must never fail. Invalid data should not flow downstream.</p> <pre><code>-- audits/revenue_consistency.sql\nAUDIT (name assert_revenue_consistency);\n-- Ensure revenue matches sum of individual orders\nSELECT \n  ds.order_date,\n  ds.total_revenue,\n  SUM(o.total_amount) as calculated_revenue\nFROM @this_model ds\nJOIN raw.raw_orders o ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 0.01;\n</code></pre> <p>Attach to model: </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- ... other audits ...\n    assert_revenue_consistency  -- Custom audit\n  )\n);\n</code></pre><p></p> <p>[Screenshot: Audit failure showing revenue mismatch]</p>"},{"location":"guides/data_quality/#layer-2-checks-monitoring","title":"Layer 2: Checks (Monitoring)","text":"<p>Why: Monitor trends and detect anomalies without blocking the pipeline.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    # Completeness: Ensure data exists\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n            severity: error\n\n      - missing_count(order_date) = 0:\n          name: no_missing_dates\n          attributes:\n            description: \"All dates must be present\"\n\n    # Validity: Check data ranges\n    validity:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            SELECT order_date, total_revenue\n            FROM sales.daily_sales\n            WHERE total_revenue &gt; 500000 OR total_revenue &lt; 0\n          samples limit: 10\n          attributes:\n            description: \"Revenue outside expected range\"\n            severity: warning\n\n    # Accuracy: Anomaly detection\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n            severity: warning\n\n      - anomaly detection for total_orders:\n          name: order_count_anomaly\n          attributes:\n            description: \"Detect unusual order volume\"\n\n    # Consistency: Cross-model validation\n    consistency:\n      - failed rows:\n          name: revenue_mismatch_with_raw\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue as daily_revenue,\n              SUM(o.total_amount) as raw_revenue\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 1.0\n          samples limit: 5\n          attributes:\n            description: \"Daily revenue should match sum of raw orders\"\n            severity: error\n\n    # Timeliness: Check data freshness\n    timeliness:\n      - change for row_count &gt;= -20%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if daily records drop more than 20%\"\n            severity: warning\n</code></pre> <p>[Screenshot: Check dashboard showing trends and anomalies]</p>"},{"location":"guides/data_quality/#layer-3-tests-logic-validation","title":"Layer 3: Tests (Logic Validation)","text":"<p>Why: Validate SQL logic and edge cases during development.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_single_order\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          customer_id: CUST-001\n          product_id: PROD-001\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n        last_order_id: ORD-001\n\n  - name: test_daily_sales_multiple_orders\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-15\n          total_amount: 50.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 3\n        total_revenue: 350.00\n        last_order_id: ORD-003\n\n  - name: test_daily_sales_empty_day\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders: []\n    outputs: []\n\n  - name: test_daily_sales_date_grouping\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15 10:00:00\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15 15:30:00\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-16 09:00:00\n          total_amount: 150.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 2\n        total_revenue: 300.00\n      - order_date: 2025-01-16\n        total_orders: 1\n        total_revenue: 150.00\n</code></pre> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/data_quality/#how-they-work-together","title":"How They Work Together","text":"<pre><code>flowchart TB\n    subgraph \"Development Workflow\"\n        DEV[Developer writes model]\n        TEST[Run Tests&lt;br/&gt;Validate logic]\n        PLAN[Run Plan&lt;br/&gt;Apply changes]\n    end\n\n    subgraph \"Execution Flow\"\n        EXEC[Model Executes]\n        AUDIT_RUN[Audits Run&lt;br/&gt;Block if fail]\n        CHECK_RUN[Checks Run&lt;br/&gt;Track trends]\n    end\n\n    subgraph \"Results\"\n        PASS[\u2705 Pass&lt;br/&gt;Data flows]\n        FAIL[\u274c Fail&lt;br/&gt;Pipeline stops]\n        TREND[\ud83d\udcca Trends&lt;br/&gt;Monitor quality]\n    end\n\n    DEV --&gt; TEST\n    TEST --&gt; PLAN\n    PLAN --&gt; EXEC\n    EXEC --&gt; AUDIT_RUN\n    EXEC --&gt; CHECK_RUN\n\n    AUDIT_RUN --&gt;|Pass| PASS\n    AUDIT_RUN --&gt;|Fail| FAIL\n    CHECK_RUN --&gt; TREND\n\n    style DEV fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style TEST fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style AUDIT_RUN fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CHECK_RUN fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PASS fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FAIL fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <p>Execution Order: 1. Tests run during development (validate logic) 2. Plan applies changes to environment 3. Model executes transformation 4. Audits run immediately (block if fail) 5. Checks run (track trends, don't block)</p> <p>[Screenshot: Complete workflow showing all three layers]</p>"},{"location":"guides/data_quality/#complex-scenario-revenue-validation","title":"Complex Scenario: Revenue Validation","text":"<p>Here's a complex example where audits and checks work together to validate revenue data:</p>"},{"location":"guides/data_quality/#the-problem","title":"The Problem","text":"<p>We need to ensure: 1. Critical: Revenue is always positive (audit - blocks) 2. Critical: Daily totals match raw order sums (audit - blocks) 3. Monitoring: Revenue trends are normal (check - warns) 4. Monitoring: Detect unusual spikes/drops (check - warns)</p>"},{"location":"guides/data_quality/#solution-combined-approach","title":"Solution: Combined Approach","text":"<p>Audits (Critical - Blocking): </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Basic validation\n    positive_values(column := total_revenue),\n    not_null(columns := (order_date, total_revenue)),\n\n    -- Complex validation: Revenue consistency\n    assert_revenue_matches_raw_orders\n  )\n);\n\n-- audits/revenue_matches_raw.sql\nAUDIT (name assert_revenue_matches_raw_orders);\nSELECT \n  ds.order_date,\n  ds.total_revenue as daily_total,\n  COALESCE(SUM(o.total_amount), 0) as raw_total,\n  ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) as difference\nFROM @this_model ds\nLEFT JOIN raw.raw_orders o \n  ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) &gt; 0.01;\n</code></pre><p></p> <p>Checks (Monitoring - Non-Blocking): </p><pre><code># checks/revenue_monitoring.yml\nchecks:\n  sales.daily_sales:\n    accuracy:\n      # Anomaly detection for revenue\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly_detection\n          attributes:\n            description: \"Detect statistically unusual revenue\"\n            severity: warning\n\n      # Trend monitoring\n      - change for total_revenue &gt;= 50%:\n          name: revenue_spike_alert\n          attributes:\n            description: \"Alert if revenue increases &gt;50% day-over-day\"\n            severity: warning\n\n      - change for total_revenue &lt;= -30%:\n          name: revenue_drop_alert\n          attributes:\n            description: \"Alert if revenue drops &gt;30% day-over-day\"\n            severity: error\n\n    consistency:\n      # Cross-model validation (non-blocking)\n      - failed rows:\n          name: revenue_vs_raw_check\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue,\n              SUM(o.total_amount) as raw_sum,\n              ABS(ds.total_revenue - SUM(o.total_amount)) as diff\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 10.0\n          samples limit: 5\n          attributes:\n            description: \"Monitor revenue consistency (wider tolerance than audit)\"\n            severity: warning\n</code></pre><p></p> <p>Why Both? - Audit: Stops pipeline if revenue is wrong (critical) - Check: Warns about trends and anomalies (monitoring) - Together: Critical issues blocked, trends monitored</p> <p>[Screenshot: Dashboard showing audit blocks vs check warnings]</p>"},{"location":"guides/data_quality/#running-quality-tools","title":"Running Quality Tools","text":""},{"location":"guides/data_quality/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\nvulcan test\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_single_order\n\n# Run tests matching pattern\nvulcan test tests/test_daily*\n</code></pre> <p>[Screenshot: Test execution output]</p>"},{"location":"guides/data_quality/#run-audits","title":"Run Audits","text":"<pre><code># Run all audits\nvulcan audit\n\n# Audits also run automatically with plan\nvulcan plan dev\n</code></pre> <p>[Screenshot: Audit execution output]</p>"},{"location":"guides/data_quality/#run-checks","title":"Run Checks","text":"<pre><code># Run all checks\nvulcan check\n\n# Run checks for specific model\nvulcan check --select sales.daily_sales\n\n# Checks also run automatically with plan/run\nvulcan plan dev\n</code></pre> <p>[Screenshot: Check execution output with trends]</p>"},{"location":"guides/data_quality/#best-practices","title":"Best Practices","text":""},{"location":"guides/data_quality/#do","title":"\u2705 DO:","text":"<ol> <li>Start with Audits - Add critical blocking validations first</li> <li>Add Checks Gradually - Monitor trends, then add anomaly detection</li> <li>Test During Development - Write tests before deploying</li> <li>Use Descriptive Names - Makes debugging easier</li> <li>Order Audits Efficiently - Fast checks first, slow checks last</li> </ol>"},{"location":"guides/data_quality/#dont","title":"\u274c DON'T:","text":"<ol> <li>Don't use Checks for Critical Rules - Use audits instead</li> <li>Don't Skip Audit Failures - Fix the root cause</li> <li>Don't Over-Audit - Focus on critical business rules</li> <li>Don't Ignore Check Trends - They indicate data quality issues</li> </ol>"},{"location":"guides/data_quality/#summary","title":"Summary","text":"<p>Three-Layer Strategy: - Audits = Critical blocking validation (must pass) - Checks = Quality monitoring (trends, anomalies) - Tests = Logic validation (development)</p> <p>Use Together: - Audits block invalid data - Checks monitor quality trends - Tests validate SQL logic</p> <p>Orders360 Example: - Audits ensure revenue is positive and matches raw data - Checks detect anomalies and trends - Tests validate aggregation logic</p>"},{"location":"guides/data_quality/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Built-in Audits</li> <li>Explore Check Dimensions</li> <li>Read about Testing</li> <li>See Orders360 Example for complete project</li> </ul>"},{"location":"guides/incremental_by_time/","title":"Incremental by Time","text":""},{"location":"guides/incremental_by_time/#incremental-by-time","title":"Incremental by Time","text":"<p>This guide explains how incremental by time models work in Vulcan using the Orders360 example project. You'll learn why they're efficient, how they process data, and how to create them.</p> <p>See the models guide for general model information or the model kinds page for all model types.</p>"},{"location":"guides/incremental_by_time/#why-use-incremental-models","title":"Why Use Incremental Models?","text":""},{"location":"guides/incremental_by_time/#the-problem-full-refreshes-are-expensive","title":"The Problem: Full Refreshes Are Expensive","text":"<p>Imagine you have a table with sales data from the last year (365 days). Every time you run a <code>FULL</code> model, it processes all 365 days:</p> <pre><code>flowchart LR\n    subgraph \"\ud83d\udd04 FULL Model - Every Run\"\n        FULL[FULL Model Run]\n        PROCESS[Process ALL 365 Days]\n        DAY1[Day 1 \u2705]\n        DAY2[Day 2 \u2705]\n        DAY3[Day 3 \u2705]\n        DOTS[...]\n        DAY365[Day 365 \u2705]\n    end\n\n    subgraph \"\ud83d\udcca Results\"\n        TIME1[\u23f1\ufe0f Time: 10 minutes]\n        COST1[\ud83d\udcb0 Cost: $10]\n        DATA1[\ud83d\udce6 All 365 days]\n    end\n\n    FULL --&gt; PROCESS\n    PROCESS --&gt; DAY1\n    PROCESS --&gt; DAY2\n    PROCESS --&gt; DAY3\n    PROCESS --&gt; DOTS\n    PROCESS --&gt; DAY365\n\n    DAY365 --&gt; TIME1\n    DAY365 --&gt; COST1\n    DAY365 --&gt; DATA1\n\n    style FULL fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style PROCESS fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style TIME1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing FULL model processing all 365 days]</p>"},{"location":"guides/incremental_by_time/#the-solution-only-process-whats-new","title":"The Solution: Only Process What's New","text":"<p>With incremental models, Vulcan only processes new or missing days:</p> <pre><code>flowchart LR\n    subgraph \"\u26a1 INCREMENTAL Model - Every Run\"\n        INCR[INCREMENTAL Model Run]\n        CHECK[Check State Database]\n        SKIP[Skip Days 1-364 \u2705]\n        PROCESS_NEW[Process Day 365 Only \ud83d\udd04]\n    end\n\n    subgraph \"\ud83d\udcca Results\"\n        TIME2[\u23f1\ufe0f Time: 30 seconds]\n        COST2[\ud83d\udcb0 Cost: $0.20]\n        DATA2[\ud83d\udce6 Only Day 365]\n    end\n\n    INCR --&gt; CHECK\n    CHECK --&gt; SKIP\n    CHECK --&gt; PROCESS_NEW\n\n    PROCESS_NEW --&gt; TIME2\n    PROCESS_NEW --&gt; COST2\n    PROCESS_NEW --&gt; DATA2\n\n    style INCR fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PROCESS_NEW fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TIME2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing incremental model processing only Day 365]</p> <p>Result: 50x faster and 50x cheaper! \ud83c\udf89</p>"},{"location":"guides/incremental_by_time/#how-incremental-models-work","title":"How Incremental Models Work","text":"<p>Incremental models use time intervals to track what's been processed. Think of it like a calendar where Vulcan checks off each day.</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udd04 Incremental Processing Flow\"\n        START[vulcan run]\n        CHECK[\ud83d\udd0d Check State Database&lt;br/&gt;What's already processed?]\n\n        subgraph \"\ud83d\udcca State Database\"\n            PROCESSED1[\u2705 Jan 1-7: Processed]\n            PROCESSED2[\u2705 Jan 8-14: Processed]\n            MISSING[\u274c Jan 15-21: Missing]\n        end\n\n        CALC[\ud83d\udcc5 Calculate Missing Intervals&lt;br/&gt;Jan 15-21 needs processing]\n        SET_MACROS[\ud83d\udd27 Set Macros&lt;br/&gt;@start_ds = '2025-01-15'&lt;br/&gt;@end_ds = '2025-01-21']\n        QUERY[\ud83d\udcbe Run Query&lt;br/&gt;WHERE order_date BETWEEN @start_ds AND @end_ds]\n        INSERT[\ud83d\udce5 Insert Results&lt;br/&gt;Into weekly_sales table]\n        UPDATE[\ud83d\udcbe Update State&lt;br/&gt;Mark Jan 15-21 as processed]\n    end\n\n    START --&gt; CHECK\n    CHECK --&gt; PROCESSED1\n    CHECK --&gt; PROCESSED2\n    CHECK --&gt; MISSING\n\n    MISSING --&gt; CALC\n    CALC --&gt; SET_MACROS\n    SET_MACROS --&gt; QUERY\n    QUERY --&gt; INSERT\n    INSERT --&gt; UPDATE\n\n    UPDATE --&gt; PROCESSED1\n    UPDATE --&gt; PROCESSED2\n\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESSED1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style PROCESSED2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style MISSING fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CALC fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style SET_MACROS fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style QUERY fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style INSERT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style UPDATE fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/incremental_by_time/#step-1-vulcan-checks-whats-already-done","title":"Step 1: Vulcan Checks What's Already Done","text":"<p>When you run <code>vulcan run</code>, Vulcan looks at your state database and asks:</p> <ul> <li>\"What dates have I already processed?\"</li> <li>\"What dates are missing?\"</li> </ul> <pre><code>State Database Check:\n\u2705 Jan 1-7:   Already processed\n\u2705 Jan 8-14:  Already processed  \n\u274c Jan 15-21: Missing - needs processing\n</code></pre> <p>[Screenshot: Visual diagram showing state database check with processed vs missing intervals]</p>"},{"location":"guides/incremental_by_time/#step-2-vulcan-processes-only-missing-intervals","title":"Step 2: Vulcan Processes Only Missing Intervals","text":"<p>Vulcan then processes only the missing dates:</p> <pre><code>Processing Jan 15-21:\n@start_ds = '2025-01-15'\n@end_ds   = '2025-01-21'\n\nQuery runs:\nSELECT ... FROM daily_sales\nWHERE order_date BETWEEN '2025-01-15' AND '2025-01-21'\n</code></pre> <p>[Screenshot: Visual showing how @start_ds and @end_ds are used in the query]</p>"},{"location":"guides/incremental_by_time/#step-3-results-are-inserted","title":"Step 3: Results Are Inserted","text":"<p>The processed data is inserted into your table, and Vulcan records that these dates are now complete:</p> <pre><code>\u2705 Jan 15-21: Now processed and recorded\n</code></pre> <p>[Screenshot: Visual showing data insertion and state update]</p>"},{"location":"guides/incremental_by_time/#understanding-time-intervals","title":"Understanding Time Intervals","text":"<p>Vulcan divides time into intervals based on your model's schedule.</p>"},{"location":"guides/incremental_by_time/#daily-intervals-example","title":"Daily Intervals Example","text":"<p>For a daily model (<code>cron '@daily'</code>), each day is one interval:</p> <pre><code>gantt\n    title Daily Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Jan 1 Complete     :done, interval1, 2025-01-01, 1d\n    Jan 2 Complete     :done, interval2, 2025-01-02, 1d\n    Jan 3 In Progress :active, interval3, 2025-01-03, 1d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 3, 2025 at 2pm\n\nIntervals:\n- Jan 1: \u2705 Complete (full day passed)\n- Jan 2: \u2705 Complete (full day passed)\n- Jan 3: \u23f3 In progress (day not finished yet)\n</code></pre> <p>[Screenshot: Calendar view showing daily intervals with Jan 1-2 complete, Jan 3 in progress]</p>"},{"location":"guides/incremental_by_time/#weekly-intervals-example","title":"Weekly Intervals Example","text":"<p>For a weekly model (<code>cron '@weekly'</code>), each week is one interval:</p> <pre><code>gantt\n    title Weekly Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Week 1 Jan 1-7     :done, week1, 2025-01-01, 7d\n    Week 2 Jan 8-14    :done, week2, 2025-01-08, 7d\n    Week 3 Jan 15-21   :active, week3, 2025-01-15, 7d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 15, 2025\n\nIntervals:\n- Week 1 (Jan 1-7):   \u2705 Complete\n- Week 2 (Jan 8-14):  \u2705 Complete\n- Week 3 (Jan 15-21): \u23f3 In progress\n</code></pre> <p>[Screenshot: Calendar view showing weekly intervals]</p>"},{"location":"guides/incremental_by_time/#how-vulcan-tracks-intervals","title":"How Vulcan Tracks Intervals","text":"<p>When you first run <code>vulcan plan</code> on an incremental model, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udcc5 First Plan - Jan 15, 2025\"\n        PLAN1[vulcan plan dev]\n        CALC1[\ud83d\udcca Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;3 weeks total]\n        PROCESS1[\ud83d\udd04 Process All Intervals&lt;br/&gt;Backfill everything]\n        RECORD1[\ud83d\udcbe Record in State DB&lt;br/&gt;Weeks 1-3 processed]\n\n        subgraph \"\ud83d\udcbe State Database After Plan\"\n            W1[\u2705 Week 1: Jan 1-7]\n            W2[\u2705 Week 2: Jan 8-14]\n            W3[\u2705 Week 3: Jan 15-21]\n        end\n    end\n\n    PLAN1 --&gt; CALC1\n    CALC1 --&gt; PROCESS1\n    PROCESS1 --&gt; RECORD1\n    RECORD1 --&gt; W1\n    RECORD1 --&gt; W2\n    RECORD1 --&gt; W3\n\n    style PLAN1 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CALC1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESS1 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RECORD1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates all intervals from the start date to now</li> <li>Processes all missing intervals (backfill)</li> <li>Records what was processed in the state database</li> </ol> <pre><code>First Plan (Jan 15, 2025):\n- Calculates: 3 weeks of intervals\n- Processes: All 3 weeks\n- Records: \"Weeks 1-3 processed\"\n\nState Database:\n\u2705 Week 1 (Jan 1-7)\n\u2705 Week 2 (Jan 8-14)\n\u2705 Week 3 (Jan 15-21)\n</code></pre> <p>[Screenshot: Visual showing first plan calculating and processing all intervals]</p> <p>When you run <code>vulcan run</code> later, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"\u26a1 Second Run - Jan 22, 2025\"\n        RUN2[vulcan run]\n        CALC2[\ud83d\udcca Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;4 weeks total]\n        CHECK[\ud83d\udd0d Check State DB&lt;br/&gt;What's already processed?]\n\n        subgraph \"\ud83d\udcbe Current State\"\n            W1_EXIST[\u2705 Week 1: Jan 1-7]\n            W2_EXIST[\u2705 Week 2: Jan 8-14]\n            W3_EXIST[\u2705 Week 3: Jan 15-21]\n            W4_MISS[\u274c Week 4: Jan 22-28]\n        end\n\n        PROCESS2[\ud83d\udd04 Process Only Week 4&lt;br/&gt;Skip Weeks 1-3]\n        RECORD2[\ud83d\udcbe Update State DB&lt;br/&gt;Week 4 now processed]\n\n        subgraph \"\ud83d\udcbe Updated State\"\n            W1_NEW[\u2705 Week 1: Jan 1-7]\n            W2_NEW[\u2705 Week 2: Jan 8-14]\n            W3_NEW[\u2705 Week 3: Jan 15-21]\n            W4_NEW[\u2705 Week 4: Jan 22-28]\n        end\n    end\n\n    RUN2 --&gt; CALC2\n    CALC2 --&gt; CHECK\n    CHECK --&gt; W1_EXIST\n    CHECK --&gt; W2_EXIST\n    CHECK --&gt; W3_EXIST\n    CHECK --&gt; W4_MISS\n\n    W4_MISS --&gt; PROCESS2\n    PROCESS2 --&gt; RECORD2\n\n    RECORD2 --&gt; W1_NEW\n    RECORD2 --&gt; W2_NEW\n    RECORD2 --&gt; W3_NEW\n    RECORD2 --&gt; W4_NEW\n\n    style RUN2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CALC2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style W4_MISS fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PROCESS2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RECORD2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates intervals from start to now</li> <li>Compares with what's already processed</li> <li>Processes only new intervals</li> </ol> <pre><code>Second Run (Jan 22, 2025):\n- Calculates: 4 weeks total\n- Already processed: Weeks 1-3\n- Missing: Week 4 (Jan 22-28)\n- Processes: Only Week 4\n\nState Database:\n\u2705 Week 1 (Jan 1-7)\n\u2705 Week 2 (Jan 8-14)\n\u2705 Week 3 (Jan 15-21)\n\u2705 Week 4 (Jan 22-28) \u2190 NEW\n</code></pre> <p>[Screenshot: Visual showing second run processing only new Week 4]</p>"},{"location":"guides/incremental_by_time/#creating-an-incremental-model","title":"Creating an Incremental Model","text":"<p>Let's create a weekly sales aggregation model for Orders360.</p>"},{"location":"guides/incremental_by_time/#step-1-create-the-model-file","title":"Step 1: Create the Model File","text":"<pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing new weekly_sales.sql file]</p>"},{"location":"guides/incremental_by_time/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit <code>models/sales/weekly_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,  -- \u23f0 This column contains the date\n    batch_size 1             -- Process 1 week at a time\n  ),\n  start '2025-01-01',       -- Start processing from this date\n  cron '@weekly',            -- Run weekly\n  grain [order_date],        -- One row per week\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- \ud83d\udd0d Filter by time range\nGROUP BY DATE_TRUNC('week', order_date)\nORDER BY order_date\n</code></pre> <p>[Screenshot: Code editor showing complete weekly_sales.sql model]</p>"},{"location":"guides/incremental_by_time/#key-components-explained","title":"Key Components Explained","text":""},{"location":"guides/incremental_by_time/#1-time-column-declaration","title":"1. Time Column Declaration","text":"<pre><code>kind INCREMENTAL_BY_TIME_RANGE (\n  time_column order_date  -- Tell Vulcan which column has dates\n)\n</code></pre> <p>What it does: Tells Vulcan which column contains the timestamp/date for each row.</p> <p>[Screenshot: Code highlighting time_column declaration]</p>"},{"location":"guides/incremental_by_time/#2-where-clause-with-macros","title":"2. WHERE Clause with Macros","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>What it does: Filters data to only the time range being processed.</p> <ul> <li><code>@start_ds</code> = Start date of the interval (e.g., '2025-01-15')</li> <li><code>@end_ds</code> = End date of the interval (e.g., '2025-01-21')</li> </ul> <p>Vulcan automatically replaces these with the correct dates!</p> <p>[Screenshot: Code highlighting WHERE clause with macros, showing how they're replaced]</p>"},{"location":"guides/incremental_by_time/#3-start-date","title":"3. Start Date","text":"<pre><code>start '2025-01-01'\n</code></pre> <p>What it does: Tells Vulcan when your data begins. Vulcan will backfill from this date.</p> <p>[Screenshot: Code highlighting start date]</p>"},{"location":"guides/incremental_by_time/#step-3-apply-the-model","title":"Step 3: Apply the Model","text":"<p>Run <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales model to be added]</p> <p>Vulcan will process each week incrementally:</p> <pre><code>[1/3] sales.weekly_sales  [insert 2025-01-01 - 2025-01-07]  1.2s\n[2/3] sales.weekly_sales  [insert 2025-01-08 - 2025-01-14]  1.1s\n[3/3] sales.weekly_sales  [insert 2025-01-15 - 2025-01-21]  1.3s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:03\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre> <p>[Screenshot: Backfill progress showing each week being processed]</p>"},{"location":"guides/incremental_by_time/#real-example-daily-sales-from-orders360","title":"Real Example: Daily Sales from Orders360","text":"<p>Here's the actual <code>daily_sales</code> model from Orders360 (currently FULL, but could be incremental):</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,  -- Could be INCREMENTAL_BY_TIME_RANGE\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing complete model]</p> <p>To make this incremental, you would:</p> <ol> <li>Change <code>kind FULL</code> to <code>kind INCREMENTAL_BY_TIME_RANGE</code></li> <li>Add <code>time_column order_date</code></li> <li>Add <code>WHERE order_date BETWEEN @start_ds AND @end_ds</code></li> </ol> <pre><code>MODEL (\n  name sales.daily_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  -- ... rest stays the same\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- ADD THIS\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: Comparison showing FULL vs INCREMENTAL changes]</p>"},{"location":"guides/incremental_by_time/#understanding-the-where-clause","title":"Understanding the WHERE Clause","text":"<p>You might wonder: \"Why do I need a WHERE clause if Vulcan adds one automatically?\"</p>"},{"location":"guides/incremental_by_time/#two-where-clauses-two-purposes","title":"Two WHERE Clauses, Two Purposes","text":"<p>Vulcan actually uses two WHERE clauses:</p>"},{"location":"guides/incremental_by_time/#1-your-models-where-clause","title":"1. Your Model's WHERE Clause","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data read into the model - Only reads necessary data from upstream tables - Saves processing time and resources - You control this in your SQL</p> <p>[Screenshot: Visual showing model WHERE clause filtering input data]</p>"},{"location":"guides/incremental_by_time/#2-vulcans-automatic-where-clause","title":"2. Vulcan's Automatic WHERE Clause","text":"<p>Vulcan automatically adds another filter on the output:</p> <pre><code>-- Vulcan adds this automatically:\nWHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data output by the model - Prevents data leakage (ensures no rows outside the time range) - Safety mechanism - Vulcan controls this automatically</p> <p>[Screenshot: Visual showing Vulcan's automatic WHERE clause filtering output]</p>"},{"location":"guides/incremental_by_time/#why-both-are-needed","title":"Why Both Are Needed","text":"<ul> <li>Your WHERE clause: Optimizes performance by reading less data</li> <li>Vulcan's WHERE clause: Ensures correctness by preventing data leakage</li> </ul> <p>Always include the WHERE clause in your model SQL!</p> <p>[Screenshot: Side-by-side comparison showing both WHERE clauses and their purposes]</p>"},{"location":"guides/incremental_by_time/#running-incremental-models","title":"Running Incremental Models","text":"<p>Vulcan has two commands for processing models:</p>"},{"location":"guides/incremental_by_time/#vulcan-plan-for-model-changes","title":"<code>vulcan plan</code> - For Model Changes","text":"<p>Use when you've changed a model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>What it does: - Detects model changes - Shows what will be affected - Backfills missing intervals - Applies changes to the environment</p> <p>[Screenshot: Plan command output showing model changes]</p>"},{"location":"guides/incremental_by_time/#vulcan-run-for-scheduled-execution","title":"<code>vulcan run</code> - For Scheduled Execution","text":"<p>Use when no models have changed:</p> <pre><code>vulcan run\n</code></pre> <p>What it does: - Checks each model's <code>cron</code> schedule - Processes only models that are due - Processes only missing intervals - Fast and efficient</p> <p>[Screenshot: Run command output showing scheduled execution]</p>"},{"location":"guides/incremental_by_time/#how-cron-schedules-work","title":"How Cron Schedules Work","text":"<p>Each model has a <code>cron</code> parameter that determines how often it should run:</p> <pre><code>flowchart LR\n    subgraph \"\u23f0 Cron Schedules\"\n        DAILY[@daily&lt;br/&gt;Every 24 hours]\n        WEEKLY[@weekly&lt;br/&gt;Every 7 days]\n        HOURLY[@hourly&lt;br/&gt;Every 1 hour]\n    end\n\n    subgraph \"\ud83d\udcca Example Models\"\n        M1[sales.daily_sales&lt;br/&gt;cron: @daily]\n        M2[sales.weekly_sales&lt;br/&gt;cron: @weekly]\n    end\n\n    DAILY --&gt; M1\n    WEEKLY --&gt; M2\n\n    style DAILY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style WEEKLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style HOURLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style M1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style M2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>cron '@daily'   -- Run once per day\ncron '@weekly'  -- Run once per week\ncron '@hourly'  -- Run once per hour\n</code></pre> <p>Example from Orders360:</p> <pre><code>-- Daily model\nMODEL (\n  name sales.daily_sales,\n  cron '@daily'  -- Runs every day\n);\n\n-- Weekly model\nMODEL (\n  name sales.weekly_sales,\n  cron '@weekly'  -- Runs once per week\n);\n</code></pre> <p>[Screenshot: Visual showing cron schedules for daily vs weekly models]</p> <p>When you run <code>vulcan run</code>:</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udd04 vulcan run Execution\"\n        RUN[vulcan run&lt;br/&gt;at 2pm on Jan 15]\n\n        subgraph \"\ud83d\udccb Model Evaluation\"\n            CHECK1[Check daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Last run: 24h ago]\n            CHECK2[Check weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Last run: 2 days ago]\n        end\n\n        subgraph \"\u2705 Decision\"\n            DUE1[\u2705 Due!&lt;br/&gt;Process daily_sales]\n            SKIP[\u23ed\ufe0f Not due&lt;br/&gt;Skip weekly_sales]\n        end\n\n        EXEC1[\ud83d\udd04 Execute daily_sales&lt;br/&gt;Process missing intervals]\n    end\n\n    RUN --&gt; CHECK1\n    RUN --&gt; CHECK2\n\n    CHECK1 --&gt; DUE1\n    CHECK2 --&gt; SKIP\n\n    DUE1 --&gt; EXEC1\n\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DUE1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Vulcan checks each model's <code>cron</code></li> <li>Determines if enough time has passed since last run</li> <li>Processes only models that are due</li> </ol> <pre><code>vulcan run at 2pm on Jan 15:\n\n\u2705 daily_sales (@daily):   Last run 24h ago \u2192 Due, process!\n\u23ed\ufe0f weekly_sales (@weekly): Last run 2 days ago \u2192 Not due, skip\n</code></pre> <p>[Screenshot: Visual showing cron evaluation logic]</p>"},{"location":"guides/incremental_by_time/#batch-processing","title":"Batch Processing","text":"<p>For large datasets, you can process intervals in batches using <code>batch_size</code>:</p> <pre><code>flowchart TB\n    subgraph \"Without batch_size Default\"\n        ALL[12 Weeks Missing]\n        SINGLE[\"Single Job&lt;br/&gt;Process all 12 weeks\"]\n        RESULT1[\"All done in 1 job&lt;br/&gt;30 minutes\"]\n    end\n\n    subgraph \"With batch_size = 4\"\n        ALL2[12 Weeks Missing]\n        BATCH1[\"Batch 1&lt;br/&gt;Weeks 1-4\"]\n        BATCH2[\"Batch 2&lt;br/&gt;Weeks 5-8\"]\n        BATCH3[\"Batch 3&lt;br/&gt;Weeks 9-12\"]\n        RESULT2[\"All done in 3 jobs&lt;br/&gt;10 min each\"]\n    end\n\n    ALL --&gt; SINGLE\n    SINGLE --&gt; RESULT1\n\n    ALL2 --&gt; BATCH1\n    ALL2 --&gt; BATCH2\n    ALL2 --&gt; BATCH3\n    BATCH1 --&gt; RESULT2\n    BATCH2 --&gt; RESULT2\n    BATCH3 --&gt; RESULT2\n\n    style ALL fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style SINGLE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style ALL2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style BATCH1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RESULT2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 4  -- Process 4 weeks at a time\n  )\n);\n</code></pre> <p>Without batch_size (default): - Processes all missing intervals in one job - Example: 12 weeks = 1 job</p> <p>With batch_size: - Divides intervals into batches - Example: 12 weeks \u00f7 4 = 3 jobs</p> <p>[Screenshot: Visual comparison showing batch processing vs single job]</p> <p>When to use batches: - \u2705 Large datasets that might timeout - \u2705 Need better progress tracking - \u2705 Want to parallelize processing</p> <p>When not to use batches: - \u2705 Small datasets (&lt; 1GB) - \u2705 Fast queries (&lt; 1 minute) - \u2705 Simple transformations</p>"},{"location":"guides/incremental_by_time/#forward-only-models","title":"Forward-Only Models","text":"<p>Sometimes you have models so large that rebuilding them is impossible. Forward-only models solve this.</p>"},{"location":"guides/incremental_by_time/#what-are-forward-only-models","title":"What Are Forward-Only Models?","text":"<p>Forward-only models never rebuild historical data. Changes are only applied going forward in time.</p> <pre><code>flowchart TB\n    subgraph \"Regular Model Change\"\n        REG_CHANGE[\"Breaking Change Detected\"]\n        REG_REBUILD[\"Rebuild Entire Table&lt;br/&gt;All dates: Jan 1 - Dec 31\"]\n        REG_RESULT[\"All data updated\"]\n    end\n\n    subgraph \"Forward-Only Model Change\"\n        FWD_CHANGE[\"Breaking Change Detected&lt;br/&gt;forward_only: true\"]\n        FWD_CHECK[\"Check Existing Data&lt;br/&gt;Jan 1 - Dec 15: Keep as-is\"]\n        FWD_APPLY[\"Apply Change Forward&lt;br/&gt;Dec 16 - Dec 31: New data\"]\n        FWD_RESULT[\"Historical preserved&lt;br/&gt;Future updated\"]\n    end\n\n    REG_CHANGE --&gt; REG_REBUILD\n    REG_REBUILD --&gt; REG_RESULT\n\n    FWD_CHANGE --&gt; FWD_CHECK\n    FWD_CHECK --&gt; FWD_APPLY\n    FWD_APPLY --&gt; FWD_RESULT\n\n    style REG_CHANGE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style REG_REBUILD fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style REG_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_CHANGE fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style FWD_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style FWD_APPLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Regular Model Change: </p><pre><code>Breaking change \u2192 Rebuild entire table (all dates)\n</code></pre><p></p> <p>Forward-Only Model Change: </p><pre><code>Breaking change \u2192 Only apply to new dates going forward\n</code></pre><p></p> <p>[Screenshot: Visual comparison showing regular rebuild vs forward-only]</p>"},{"location":"guides/incremental_by_time/#when-to-use-forward-only","title":"When to Use Forward-Only","text":"<p>\u2705 Use forward-only when: - Tables are too large to rebuild - Historical data can't be reprocessed - You only care about future data</p> <p>\u274c Don't use forward-only when: - You need to fix historical data - Schema changes affect past data - You want full data consistency</p>"},{"location":"guides/incremental_by_time/#making-a-model-forward-only","title":"Making a Model Forward-Only","text":"<p>Add <code>forward_only true</code> to your model:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    forward_only true  -- All changes are forward-only\n  )\n);\n</code></pre> <p>[Screenshot: Code showing forward_only configuration]</p>"},{"location":"guides/incremental_by_time/#forward-only-plans","title":"Forward-Only Plans","text":"<p>You can also make a specific plan forward-only:</p> <pre><code>vulcan plan dev --forward-only\n</code></pre> <p>This treats all changes in the plan as forward-only, even if models aren't configured that way.</p> <p>[Screenshot: Plan command with --forward-only flag]</p>"},{"location":"guides/incremental_by_time/#schema-changes-in-forward-only-models","title":"Schema Changes in Forward-Only Models","text":"<p>When you change a forward-only model, Vulcan checks for schema changes that could cause problems.</p>"},{"location":"guides/incremental_by_time/#types-of-schema-changes","title":"Types of Schema Changes","text":""},{"location":"guides/incremental_by_time/#destructive-changes","title":"Destructive Changes","text":"<p>Changes that remove or modify existing data:</p> <pre><code>flowchart LR\n    subgraph \"Destructive Changes\"\n        DROP[\"Dropping Column&lt;br/&gt;total_amount removed\"]\n        RENAME[\"Renaming Column&lt;br/&gt;order_id to id\"]\n        TYPE[\"Changing Type&lt;br/&gt;INT to STRING&lt;br/&gt;may cause loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE1[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER1[\"After:&lt;br/&gt;order_id only\"]\n    end\n\n    DROP --&gt; BEFORE1\n    BEFORE1 --&gt; AFTER1\n\n    style DROP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RENAME fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style TYPE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style BEFORE1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <ul> <li>\u274c Dropping a column</li> <li>\u274c Renaming a column  </li> <li>\u274c Changing data type (could cause data loss)</li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (destructive - drops total_amount)\nSELECT order_id FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing destructive change example]</p>"},{"location":"guides/incremental_by_time/#additive-changes","title":"Additive Changes","text":"<p>Changes that add new data without removing existing:</p> <pre><code>flowchart LR\n    subgraph \"Additive Changes\"\n        ADD[\"Adding Column&lt;br/&gt;customer_name added\"]\n        TYPE2[\"Compatible Type Change&lt;br/&gt;INT to STRING&lt;br/&gt;no data loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE2[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER2[\"After:&lt;br/&gt;order_id, total_amount,&lt;br/&gt;customer_name\"]\n    end\n\n    ADD --&gt; BEFORE2\n    BEFORE2 --&gt; AFTER2\n\n    style ADD fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TYPE2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style BEFORE2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ul> <li>\u2705 Adding a new column</li> <li>\u2705 Changing data type (compatible, e.g., INT \u2192 STRING)</li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (additive - adds customer_name)\nSELECT order_id, total_amount, customer_name FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing additive change example]</p>"},{"location":"guides/incremental_by_time/#controlling-schema-change-behavior","title":"Controlling Schema Change Behavior","text":"<p>You can control how Vulcan handles schema changes:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change error,  -- Block destructive changes\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>Options: - <code>error</code> - Stop and raise an error (default for destructive) - <code>warn</code> - Log a warning but continue - <code>allow</code> - Silently proceed (default for additive) - <code>ignore</code> - Skip the check entirely (dangerous!)</p> <p>[Screenshot: Code showing schema change configuration options]</p>"},{"location":"guides/incremental_by_time/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/incremental_by_time/#strict-schema-control","title":"Strict Schema Control","text":"<p>Prevent any schema changes:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change error,  -- Block destructive\n    on_additive_change error       -- Block even new columns\n  )\n);\n</code></pre> <p>[Screenshot: Strict schema control example]</p>"},{"location":"guides/incremental_by_time/#development-model","title":"Development Model","text":"<p>Allow all changes for rapid iteration:</p> <pre><code>MODEL (\n  name sales.dev_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change allow,  -- Allow dropping columns\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Development model example]</p>"},{"location":"guides/incremental_by_time/#production-safety","title":"Production Safety","text":"<p>Allow safe changes, warn about risky ones:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change warn,   -- Warn but allow\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Production safety example]</p>"},{"location":"guides/incremental_by_time/#important-notes","title":"Important Notes","text":""},{"location":"guides/incremental_by_time/#time-column-must-be-utc","title":"\u26a0\ufe0f Time Column Must Be UTC","text":"<p>Always use UTC timezone for your <code>time_column</code>:</p> <pre><code>-- \u2705 Good: UTC timezone\ntime_column order_date_utc\n\n-- \u274c Bad: Local timezone\ntime_column order_date_local\n</code></pre> <p>Why? Ensures correct interval calculations and proper interaction with Vulcan's scheduler.</p> <p>[Screenshot: Visual warning about UTC requirement]</p>"},{"location":"guides/incremental_by_time/#always-include-where-clause","title":"\u2705 Always Include WHERE Clause","text":"<p>Your model SQL must include a WHERE clause with <code>@start_ds</code> and <code>@end_ds</code>:</p> <pre><code>-- \u2705 Required\nWHERE order_date BETWEEN @start_ds AND @end_ds\n\n-- \u274c Missing WHERE clause\n-- WHERE clause is required!\n</code></pre> <p>[Screenshot: Code showing required WHERE clause]</p>"},{"location":"guides/incremental_by_time/#set-a-start-date","title":"\u2705 Set a Start Date","text":"<p>Always specify when your data begins:</p> <pre><code>start '2025-01-01'  -- Start processing from this date\n</code></pre> <p>[Screenshot: Code showing start date configuration]</p>"},{"location":"guides/incremental_by_time/#choose-appropriate-batch_size","title":"\u2705 Choose Appropriate batch_size","text":"<ul> <li>Start with <code>batch_size 1</code> for small datasets</li> <li>Increase for larger datasets that might timeout</li> <li>Monitor performance to find the sweet spot</li> </ul> <p>[Screenshot: Visual guide for choosing batch_size]</p>"},{"location":"guides/incremental_by_time/#summary","title":"Summary","text":"<p>Incremental by time models: - \u2705 Only process new or missing time intervals - \u2705 Much faster and cheaper than full refreshes - \u2705 Perfect for time-based data (orders, events, transactions) - \u2705 Require a time column and WHERE clause - \u2705 Use cron schedules to control execution frequency</p> <p>Key concepts: - Intervals: Time periods (days, weeks, hours) that Vulcan tracks - Backfill: Processing historical intervals when first creating a model - Cron: Schedule that determines how often a model runs - Forward-only: Models that never rebuild historical data</p>"},{"location":"guides/incremental_by_time/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Kinds for all model types</li> <li>Read the Models Guide for working with models</li> <li>Check the Plan Guide for applying changes</li> <li>See Run Guide for scheduled execution</li> <li>Explore Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/model_selection/","title":"Model Selection","text":""},{"location":"guides/model_selection/#model-selection","title":"Model Selection","text":"<p>This guide explains how to select specific models to include in a Vulcan plan using the Orders360 example project. This is useful when you only want to test or apply changes to a subset of your models.</p> <p>Note: The selector syntax described below is also used for the Vulcan <code>plan</code> <code>--allow-destructive-model</code> and <code>--allow-additive-model</code> selectors.</p>"},{"location":"guides/model_selection/#background","title":"Background","text":"<p>A Vulcan plan automatically detects changes between your local project and the deployed environment. When applied, it backfills directly modified models and their downstream dependencies.</p> <p>In large projects, a single model change can impact many downstream models, making plans take a long time. Model selection lets you filter which changes to include, so you can test specific models without processing everything.</p> <p>Key Concept: - Directly Modified: Models you changed in your code - Indirectly Modified: Downstream models affected by your changes</p> <p>[Screenshot: Visual showing directly vs indirectly modified models]</p>"},{"location":"guides/model_selection/#understanding-model-dependencies","title":"Understanding Model Dependencies","text":"<p>Before we dive into selection, let's understand how models relate to each other in Orders360:</p> <pre><code>flowchart TD\n    subgraph \"Orders360 Model DAG\"\n        RAW_CUSTOMERS[raw.raw_customers&lt;br/&gt;Seed Model]\n        RAW_ORDERS[raw.raw_orders&lt;br/&gt;Seed Model]\n        RAW_PRODUCTS[raw.raw_products&lt;br/&gt;Seed Model]\n\n        DAILY_SALES[sales.daily_sales&lt;br/&gt;Daily Aggregation]\n        WEEKLY_SALES[sales.weekly_sales&lt;br/&gt;Weekly Aggregation]\n    end\n\n    RAW_CUSTOMERS --&gt; DAILY_SALES\n    RAW_ORDERS --&gt; DAILY_SALES\n    RAW_PRODUCTS --&gt; DAILY_SALES\n\n    DAILY_SALES --&gt; WEEKLY_SALES\n\n    style RAW_CUSTOMERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_ORDERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_PRODUCTS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY_SALES fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style WEEKLY_SALES fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Dependency Flow: - <code>raw.raw_orders</code> \u2192 <code>sales.daily_sales</code> \u2192 <code>sales.weekly_sales</code> - Changing <code>raw.raw_orders</code> affects <code>daily_sales</code> (indirectly modified) - Changing <code>daily_sales</code> affects <code>weekly_sales</code> (indirectly modified)</p> <p>[Screenshot: Orders360 project structure showing model files]</p>"},{"location":"guides/model_selection/#syntax","title":"Syntax","text":"<p>Model selections use the <code>--select-model</code> argument in <code>vulcan plan</code>. You can select models in several ways:</p>"},{"location":"guides/model_selection/#basic-selection","title":"Basic Selection","text":"<p>Select a single model by name:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>Select multiple models:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\" --select-model \"raw.raw_orders\"\n</code></pre> <p>[Screenshot: Plan output showing multiple models selected]</p>"},{"location":"guides/model_selection/#wildcard-selection","title":"Wildcard Selection","text":"<p>Use <code>*</code> to match multiple models:</p> <pre><code># Select all models starting with \"raw.\"\nvulcan plan dev --select-model \"raw.*\"\n\n# Select all models ending with \"_sales\"\nvulcan plan dev --select-model \"sales.*_sales\"\n\n# Select all models containing \"daily\"\nvulcan plan dev --select-model \"*daily*\"\n</code></pre> <p>Examples: - <code>\"raw.*\"</code> matches <code>raw.raw_customers</code>, <code>raw.raw_orders</code>, <code>raw.raw_products</code> - <code>\"sales.*_sales\"</code> matches <code>sales.daily_sales</code>, <code>sales.weekly_sales</code> - <code>\"*.daily_sales\"</code> matches <code>sales.daily_sales</code></p> <p>[Screenshot: Plan output showing wildcard selection results]</p>"},{"location":"guides/model_selection/#tag-selection","title":"Tag Selection","text":"<p>Select models by tags using <code>tag:tag_name</code>:</p> <pre><code># Select all models with \"seed\" tag\nvulcan plan dev --select-model \"tag:seed\"\n\n# Select all models with tags starting with \"reporting\"\nvulcan plan dev --select-model \"tag:reporting*\"\n</code></pre> <p>Example: If <code>raw.raw_orders</code> and <code>raw.raw_customers</code> have the <code>seed</code> tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n# Selects: raw.raw_orders, raw.raw_customers\n</code></pre> <p>[Screenshot: Plan output showing tag-based selection]</p>"},{"location":"guides/model_selection/#upstreamdownstream-selection","title":"Upstream/Downstream Selection","text":"<p>Use <code>+</code> to include upstream or downstream models:</p> <ul> <li><code>+model_name</code> = Include upstream models (dependencies)</li> <li><code>model_name+</code> = Include downstream models (dependents)</li> </ul> <pre><code>flowchart LR\n    subgraph \"Model Dependencies\"\n        RAW[raw.raw_orders]\n        DAILY[sales.daily_sales]\n        WEEKLY[sales.weekly_sales]\n    end\n\n    RAW --&gt;|upstream| DAILY\n    DAILY --&gt;|downstream| WEEKLY\n\n    style RAW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000\n    style WEEKLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Examples:</p> <pre><code># Select daily_sales only\nvulcan plan dev --select-model \"sales.daily_sales\"\n# Result: daily_sales (directly modified)\n\n# Select daily_sales + upstream (raw.raw_orders)\nvulcan plan dev --select-model \"+sales.daily_sales\"\n# Result: raw.raw_orders, daily_sales\n\n# Select daily_sales + downstream (weekly_sales)\nvulcan plan dev --select-model \"sales.daily_sales+\"\n# Result: daily_sales, weekly_sales\n\n# Select daily_sales + both upstream and downstream\nvulcan plan dev --select-model \"+sales.daily_sales+\"\n# Result: raw.raw_orders, daily_sales, weekly_sales\n</code></pre> <p>[Screenshot: Plan outputs showing different selection results]</p>"},{"location":"guides/model_selection/#git-based-selection","title":"Git-Based Selection","text":"<p>Select models changed in a git branch:</p> <pre><code># Select models changed in feature branch\nvulcan plan dev --select-model \"git:feature\"\n\n# Select changed models + downstream\nvulcan plan dev --select-model \"git:feature+\"\n\n# Select changed models + upstream\nvulcan plan dev --select-model \"+git:feature\"\n</code></pre> <p>What it includes: - Untracked files (new models) - Uncommitted changes - Committed changes different from target branch</p> <p>[Screenshot: Plan output showing git-based selection]</p>"},{"location":"guides/model_selection/#complex-selections","title":"Complex Selections","text":"<p>Combine conditions with logical operators:</p> <ul> <li><code>&amp;</code> (AND): Both conditions must be true</li> <li><code>|</code> (OR): Either condition must be true</li> <li><code>^</code> (NOT): Negates a condition</li> </ul> <pre><code># Models with finance tag that don't have deprecated tag\nvulcan plan dev --select-model \"(tag:finance &amp; ^tag:deprecated)\"\n\n# daily_sales + upstream OR weekly_sales + downstream\nvulcan plan dev --select-model \"(+sales.daily_sales | sales.weekly_sales+)\"\n\n# Changed models that also have finance tag\nvulcan plan dev --select-model \"(tag:finance &amp; git:main)\"\n\n# Models in sales schema without test tag\nvulcan plan dev --select-model \"^(tag:test) &amp; sales.*\"\n</code></pre> <p>[Screenshot: Plan output showing complex selection results]</p>"},{"location":"guides/model_selection/#examples-with-orders360","title":"Examples with Orders360","text":"<p>Let's see how model selection works with the Orders360 project. We'll modify <code>raw.raw_orders</code> and <code>sales.daily_sales</code> to demonstrate different selection scenarios.</p>"},{"location":"guides/model_selection/#example-setup","title":"Example Setup","text":"<p>We've modified two models: - <code>raw.raw_orders</code> (directly modified) - <code>sales.daily_sales</code> (directly modified)</p> <p>The dependency chain: </p><pre><code>raw.raw_orders \u2192 sales.daily_sales \u2192 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Orders360 project showing modified files]</p>"},{"location":"guides/model_selection/#no-selection-default","title":"No Selection (Default)","text":"<p>Without selection, Vulcan includes all directly modified models and their downstream dependencies:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing all modified models]</p> <p>What Happened: - Both directly modified models are included - <code>weekly_sales</code> is indirectly modified (depends on <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-single-model","title":"Select Single Model","text":"<p>Select only <code>sales.daily_sales</code>:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>What Happened: - <code>raw.raw_orders</code> is excluded (not selected) - <code>daily_sales</code> is included (directly modified) - <code>weekly_sales</code> is included (indirectly modified, downstream of <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-with-upstream-indicator","title":"Select with Upstream Indicator","text":"<p>Select <code>daily_sales</code> and include its upstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"+sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 raw.raw_orders\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream selection]</p> <p>What Happened: - <code>raw.raw_orders</code> is included (upstream of <code>daily_sales</code>) - <code>daily_sales</code> is included (selected) - <code>weekly_sales</code> is included (downstream of <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-with-downstream-indicator","title":"Select with Downstream Indicator","text":"<p>Select <code>daily_sales</code> and include its downstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales+\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 sales.weekly_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    (none)\n</code></pre><p></p> <p>[Screenshot: Plan output showing downstream selection]</p> <p>What Happened: - <code>daily_sales</code> is included (selected) - <code>weekly_sales</code> is included (downstream, now directly modified) - <code>raw.raw_orders</code> is excluded (not selected)</p>"},{"location":"guides/model_selection/#select-with-wildcard","title":"Select with Wildcard","text":"<p>Select all models matching a pattern:</p> <pre><code>vulcan plan dev --select-model \"sales.*_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing wildcard selection]</p> <p>What Happened: - <code>sales.daily_sales</code> matches the pattern (selected) - <code>sales.weekly_sales</code> matches the pattern but is indirectly modified - <code>raw.raw_orders</code> doesn't match (excluded)</p>"},{"location":"guides/model_selection/#select-with-tags","title":"Select with Tags","text":"<p>If models have tags, select by tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sales.daily_sales\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing tag-based selection]</p> <p>What Happened: - <code>raw.raw_orders</code> has <code>seed</code> tag (selected) - Downstream models are indirectly modified</p>"},{"location":"guides/model_selection/#select-with-git-changes","title":"Select with Git Changes","text":"<p>Select models changed in a git branch:</p> <pre><code>vulcan plan dev --select-model \"git:feature\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales  # Changed in feature branch\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing git-based selection]</p> <p>What Happened: - Only models changed in <code>feature</code> branch are selected - Downstream models are included automatically</p>"},{"location":"guides/model_selection/#backfill-selection","title":"Backfill Selection","text":"<p>By default, Vulcan backfills all models in a plan. You can limit which models are backfilled using <code>--backfill-model</code>.</p> <p>Important: <code>--backfill-model</code> only works in development environments (not <code>prod</code>).</p>"},{"location":"guides/model_selection/#how-backfill-selection-works","title":"How Backfill Selection Works","text":"<pre><code>flowchart TB\n    subgraph \"Backfill Selection Flow\"\n        PLAN[vulcan plan dev]\n        SELECT[--select-model&lt;br/&gt;Which models in plan?]\n        BACKFILL[--backfill-model&lt;br/&gt;Which models to backfill?]\n\n        subgraph \"Plan Includes\"\n            IN_PLAN[Models in Plan&lt;br/&gt;daily_sales, weekly_sales]\n        end\n\n        subgraph \"Backfill Includes\"\n            BACKFILL_LIST[Models to Backfill&lt;br/&gt;Only daily_sales]\n        end\n\n        RESULT[Result:&lt;br/&gt;Plan shows all models&lt;br/&gt;Only selected models backfilled]\n    end\n\n    PLAN --&gt; SELECT\n    PLAN --&gt; BACKFILL\n    SELECT --&gt; IN_PLAN\n    BACKFILL --&gt; BACKFILL_LIST\n    IN_PLAN --&gt; RESULT\n    BACKFILL_LIST --&gt; RESULT\n\n    style PLAN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style SELECT fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BACKFILL fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Key Points: - <code>--select-model</code> determines which models appear in the plan - <code>--backfill-model</code> determines which models are actually backfilled - Upstream models are always backfilled (required for downstream models)</p> <p>[Screenshot: Visual diagram explaining backfill selection]</p>"},{"location":"guides/model_selection/#backfill-examples","title":"Backfill Examples","text":""},{"location":"guides/model_selection/#no-backfill-selection-default","title":"No Backfill Selection (Default)","text":"<p>All models in the plan are backfilled:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing all models needing backfill]</p>"},{"location":"guides/model_selection/#backfill-specific-model","title":"Backfill Specific Model","text":"<p>Only backfill <code>daily_sales</code>:</p> <pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u2514\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales needs backfill]</p> <p>What Happened: - <code>weekly_sales</code> is excluded from backfill - Only <code>daily_sales</code> will be processed</p>"},{"location":"guides/model_selection/#backfill-with-upstream","title":"Backfill with Upstream","text":"<p>When you backfill a model, its upstream dependencies are automatically included:</p> <pre><code>vulcan plan dev --backfill-model \"sales.weekly_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 raw__dev.raw_orders: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream models included in backfill]</p> <p>What Happened: - <code>weekly_sales</code> is selected for backfill - <code>raw.raw_orders</code> is automatically included (upstream dependency) - <code>daily_sales</code> is excluded (not upstream of <code>weekly_sales</code>)</p>"},{"location":"guides/model_selection/#visual-selection-guide","title":"Visual Selection Guide","text":"<p>Here's a quick reference for common selection patterns:</p> <pre><code>flowchart LR\n    subgraph \"Selection Patterns\"\n        PAT1[\"sales.daily_sales&lt;br/&gt;Select only this model\"]\n        PAT2[\"+sales.daily_sales&lt;br/&gt;Select + upstream\"]\n        PAT3[\"sales.daily_sales+&lt;br/&gt;Select + downstream\"]\n        PAT4[\"+sales.daily_sales+&lt;br/&gt;Select + both\"]\n        PAT5[\"sales.*_sales&lt;br/&gt;Wildcard match\"]\n        PAT6[\"tag:seed&lt;br/&gt;Tag selection\"]\n    end\n\n    subgraph \"Results\"\n        RES1[daily_sales only]\n        RES2[raw_orders + daily_sales]\n        RES3[daily_sales + weekly_sales]\n        RES4[All connected]\n        RES5[All matching]\n        RES6[All tagged]\n    end\n\n    PAT1 --&gt; RES1\n    PAT2 --&gt; RES2\n    PAT3 --&gt; RES3\n    PAT4 --&gt; RES4\n    PAT5 --&gt; RES5\n    PAT6 --&gt; RES6\n\n    style PAT1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT4 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT5 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT6 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual cheat sheet for selection patterns]</p>"},{"location":"guides/model_selection/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Start Small: Select only the models you're testing    </p><pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre><p></p> </li> <li> <p>Use Wildcards: When selecting multiple related models    </p><pre><code>vulcan plan dev --select-model \"sales.*\"\n</code></pre><p></p> </li> <li> <p>Include Dependencies: Use <code>+</code> when you need upstream/downstream models    </p><pre><code>vulcan plan dev --select-model \"+sales.daily_sales+\"\n</code></pre><p></p> </li> <li> <p>Limit Backfill: Use <code>--backfill-model</code> to save time in development    </p><pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre><p></p> </li> <li> <p>Use Tags: Organize models with tags for easier selection    </p><pre><code>vulcan plan dev --select-model \"tag:reporting\"\n</code></pre><p></p> </li> </ol>"},{"location":"guides/model_selection/#summary","title":"Summary","text":"<p>Model Selection: - \u2705 Filter which models appear in a plan - \u2705 Use wildcards, tags, and git changes - \u2705 Include upstream/downstream with <code>+</code> - \u2705 Combine with logical operators</p> <p>Backfill Selection: - \u2705 Limit which models are actually backfilled - \u2705 Upstream models are always included - \u2705 Only works in development environments - \u2705 Saves time when testing specific models</p>"},{"location":"guides/model_selection/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plans for understanding plan behavior</li> <li>Read the Plan Guide for applying changes</li> <li>Check Model Configuration for model properties</li> <li>Explore Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/models/","title":"Models","text":""},{"location":"guides/models/#models","title":"Models","text":"<p>This guide walks you through working with models in Vulcan using the Orders360 example project. You'll learn how to add, edit, evaluate, and manage models with practical examples.</p>"},{"location":"guides/models/#prerequisites","title":"Prerequisites","text":"<p>Before adding a model, ensure that you have:</p> <ul> <li>Created your project </li> <li>Applied your first plan</li> <li>Working in a dev environment for testing changes</li> </ul>"},{"location":"guides/models/#understanding-models","title":"Understanding Models","text":"<p>Models in Vulcan consist of two core components:</p> <ol> <li>DDL (Data Definition Language): The <code>MODEL</code> block that defines structure, metadata, and behavior</li> <li>DML (Data Manipulation Language): The <code>SELECT</code> query that contains transformation logic</li> </ol>"},{"location":"guides/models/#example-daily-sales-model","title":"Example: Daily Sales Model","text":"<p>Here's a real example from Orders360:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file in editor showing the complete model definition]</p>"},{"location":"guides/models/#adding-a-model","title":"Adding a Model","text":"<p>To add a new model to your Orders360 project:</p>"},{"location":"guides/models/#step-1-create-model-file","title":"Step 1: Create Model File","text":"<p>Create a new file in your <code>models</code> directory. For example, let's add a weekly sales aggregation:</p> <pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing models/sales directory structure]</p>"},{"location":"guides/models/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit the file and add your model definition:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@weekly',\n  grain [order_date],\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(total_amount) AS total_revenue,\n  AVG(total_amount) AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY DATE_TRUNC('week', order_date)\n</code></pre> <p>[Screenshot: weekly_sales.sql file in editor with model definition]</p>"},{"location":"guides/models/#step-3-check-model-status","title":"Step 3: Check Model Status","text":"<p>Verify your model is detected:</p> <pre><code>vulcan info\n</code></pre> <p>Expected Output: </p><pre><code>Connection: \u2705 Connected\nModels: 5\n  - raw.raw_customers\n  - raw.raw_orders\n  - raw.raw_products\n  - sales.daily_sales\n  - sales.weekly_sales  \u2190 NEW MODEL\n...\n</code></pre><p></p> <p>[Screenshot: <code>vulcan info</code> output showing the new weekly_sales model]</p>"},{"location":"guides/models/#step-4-apply-the-model","title":"Step 4: Apply the Model","text":"<p>Use <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing new weekly_sales model to be added]</p> <p>Type <code>y</code> to apply and backfill the model.</p>"},{"location":"guides/models/#editing-an-existing-model","title":"Editing an Existing Model","text":"<p>To edit an existing model, modify the model file and use Vulcan's tools to preview and apply changes.</p>"},{"location":"guides/models/#step-1-edit-the-model-file","title":"Step 1: Edit the Model File","text":"<p>Let's modify <code>sales.daily_sales</code> to add a new column. Open <code>models/sales/daily_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day',\n    avg_order_value = 'Average order value for the day'  -- NEW COLUMN DESCRIPTION\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,  -- NEW COLUMN\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing the added avg_order_value column]</p>"},{"location":"guides/models/#step-2-evaluate-the-model-optional","title":"Step 2: Evaluate the Model (Optional)","text":"<p>Preview the model output without materializing it:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate command output showing the new avg_order_value column]</p> <p>What Happened? - The <code>evaluate</code> command runs the model query without creating tables - Shows you the output with the new column - Useful for testing changes before applying them</p>"},{"location":"guides/models/#step-3-preview-changes-with-plan","title":"Step 3: Preview Changes with Plan","text":"<p>See what will change and how it affects downstream models:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Non-breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -22,6 +22,7 @@\n      SELECT\n        CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n        COUNT(order_id)::INTEGER AS total_orders,\n        SUM(total_amount)::FLOAT AS total_revenue,\n    +   AVG(total_amount)::FLOAT AS avg_order_value,\n        MAX(order_id)::VARCHAR AS last_order_id\n      FROM raw.raw_orders\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing non-breaking change with diff highlighting the new column]</p> <p>Understanding the Output: - Non-breaking: Vulcan detected this as non-breaking (adding a column) - Diff: Shows exactly what changed (green <code>+</code> indicates added line) - No downstream impact: <code>sales.weekly_sales</code> is not listed because it doesn't use this column yet</p>"},{"location":"guides/models/#step-4-apply-the-changes","title":"Step 4: Apply the Changes","text":"<p>Type <code>y</code> to apply the plan:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:05\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Plan application showing daily_sales being backfilled]</p>"},{"location":"guides/models/#making-a-breaking-change","title":"Making a Breaking Change","text":"<p>Breaking changes affect downstream models. Let's see how Vulcan handles this.</p>"},{"location":"guides/models/#step-1-add-a-filter-to-daily-sales","title":"Step 1: Add a Filter to Daily Sales","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a WHERE clause:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE total_amount &gt; 10  -- NEW FILTER: Only orders &gt; $10\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql showing the WHERE clause filter]</p>"},{"location":"guides/models/#step-2-create-plan","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,6 +26,7 @@\n      FROM raw.raw_orders\n    + WHERE total_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales.weekly_sales (Indirect Breaking)\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing breaking change with downstream impact on weekly_sales]</p> <p>Understanding Breaking Changes: - Breaking: Adding a WHERE clause filters data, making existing data invalid - Indirectly Modified: <code>sales.weekly_sales</code> depends on <code>daily_sales</code>, so it's affected - Cascading backfill: Both models need to be reprocessed</p>"},{"location":"guides/models/#evaluating-a-model","title":"Evaluating a Model","text":"<p>The <code>evaluate</code> command lets you test models without materializing data. Perfect for iteration and debugging.</p>"},{"location":"guides/models/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing single day results]</p>"},{"location":"guides/models/#evaluate-multiple-days","title":"Evaluate Multiple Days","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-10 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-10 00:00:00           38         1120.25           29.48        ORD-00110\n2025-01-11 00:00:00           45         1350.75           30.02        ORD-00111\n2025-01-12 00:00:00           41         1225.50           29.89        ORD-00112\n2025-01-13 00:00:00           39         1180.00           30.26        ORD-00113\n2025-01-14 00:00:00           44         1320.50           30.01        ORD-00114\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing multiple days of data]</p>"},{"location":"guides/models/#evaluate-with-filters","title":"Evaluate with Filters","text":"<p>Test your model logic with different conditions:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15 --where \"total_amount &gt; 50\"\n</code></pre> <p>[Screenshot: Evaluate command with WHERE clause filter]</p> <p>Use Cases for Evaluate: - \u2705 Test model logic before applying changes - \u2705 Debug query issues - \u2705 Verify data transformations - \u2705 Check data quality - \u2705 Iterate quickly without materialization costs</p>"},{"location":"guides/models/#reverting-a-change","title":"Reverting a Change","text":"<p>Vulcan makes it easy to revert model changes using Virtual Updates.</p>"},{"location":"guides/models/#step-1-revert-the-change","title":"Step 1: Revert the Change","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to remove the WHERE clause we added:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\n-- WHERE total_amount &gt; 10  -- REMOVED FILTER\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql with WHERE clause removed/commented out]</p>"},{"location":"guides/models/#step-2-apply-reverted-plan","title":"Step 2: Apply Reverted Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,7 +26,6 @@\n      FROM raw.raw_orders\n    - WHERE total_amount &gt; 10\n      GROUP BY order_date\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan showing reverted change with diff]</p> <p>Virtual Update: - No backfill required - just updates references - Fast operation - completes in seconds - Previous data remains available</p>"},{"location":"guides/models/#validating-models","title":"Validating Models","text":"<p>Vulcan provides multiple ways to validate your models.</p>"},{"location":"guides/models/#automatic-validation","title":"Automatic Validation","text":"<p>Vulcan automatically validates models when you run <code>plan</code>:</p> <ol> <li>Unit Tests: Run automatically to validate logic</li> <li>Audits: Execute when data is loaded to tables</li> <li>Assertions: Check data quality constraints</li> </ol> <p>Example Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n</code></pre><p></p> <p>[Screenshot: Plan output showing tests passed]</p>"},{"location":"guides/models/#manual-validation-options","title":"Manual Validation Options","text":"<ol> <li> <p>Evaluate: Test model output without materialization    </p><pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre><p></p> </li> <li> <p>Unit Tests: Write tests in <code>tests/</code> directory    </p><pre><code>vulcan test\n</code></pre><p></p> </li> <li> <p>Plan Preview: See changes before applying    </p><pre><code>vulcan plan dev\n</code></pre><p></p> </li> </ol> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/models/#deleting-a-model","title":"Deleting a Model","text":"<p>To remove a model from your project:</p>"},{"location":"guides/models/#step-1-delete-model-file","title":"Step 1: Delete Model File","text":"<pre><code>rm models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing weekly_sales.sql deleted]</p>"},{"location":"guides/models/#step-2-delete-associated-tests-if-any","title":"Step 2: Delete Associated Tests (if any)","text":"<pre><code>rm tests/test_weekly_sales.yaml\n</code></pre>"},{"location":"guides/models/#step-3-apply-deletion-plan","title":"Step 3: Apply Deletion Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 1 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales as removed]</p> <p>Type <code>y</code> to apply the deletion.</p> <p>Expected Output: </p><pre><code>Virtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\nVirtual Update executed successfully\n</code></pre><p></p> <p>[Screenshot: Virtual update completing successfully]</p>"},{"location":"guides/models/#step-4-apply-to-production","title":"Step 4: Apply to Production","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Production plan showing model removal]</p>"},{"location":"guides/models/#model-examples-from-orders360","title":"Model Examples from Orders360","text":""},{"location":"guides/models/#seed-model-raw-orders","title":"Seed Model: Raw Orders","text":"<pre><code>MODEL (\n  name raw.raw_orders,\n  kind SEED (\n    path '../../seeds/raw_orders.csv'\n  ),\n  description 'Seed model loading raw order data from CSV file',\n  columns (\n    order_id VARCHAR,\n    order_date DATE,\n    customer_id VARCHAR,\n    product_id VARCHAR,\n    total_amount FLOAT\n  ),\n  column_descriptions (\n    order_id = 'Unique identifier for each order',\n    order_date = 'Date when the order was placed',\n    customer_id = 'Reference to customer who placed the order',\n    product_id = 'Reference to product that was ordered',\n    total_amount = 'Total order amount in dollars'\n  ),\n  assertions (\n    unique_values(columns := (order_id)),\n    not_null(columns := (order_id, order_date, customer_id, product_id)),\n    positive_values(column := total_amount)\n  ),\n  grain order_id\n);\n</code></pre> <p>[Screenshot: raw_orders.sql seed model file]</p>"},{"location":"guides/models/#transformation-model-daily-sales","title":"Transformation Model: Daily Sales","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql transformation model file]</p>"},{"location":"guides/models/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive names: <code>sales.daily_sales</code> is clearer than <code>sales.ds</code></li> <li>Add column descriptions: Document what each column represents</li> <li>Use assertions: Validate data quality at the model level</li> <li>Test before applying: Use <code>evaluate</code> to preview changes</li> <li>Review plans carefully: Check diffs and downstream impacts</li> <li>Use dev environments: Test changes before production</li> </ol>"},{"location":"guides/models/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Kinds for different model types</li> <li>Explore Model Properties for advanced configuration</li> <li>Read about Plan Guide for applying model changes</li> <li>Check Testing Guide for model validation strategies</li> <li>See Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/plan/","title":"Plan","text":""},{"location":"guides/plan/#plan","title":"Plan","text":"<p>This guide walks you through Vulcan's plan functionality with practical scenarios using the Orders360 example project. You'll learn how plans work, how to interpret them, and how to apply changes to your data warehouse.</p>"},{"location":"guides/plan/#plan-architecture","title":"Plan Architecture","text":"<p>The following diagram illustrates how Vulcan's plan system works, showing the relationship between local project state, plans, model variants, physical tables, and environments:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcc1 Local Project\"\n        LP[\ud83d\udcdd Local Project Files&lt;br/&gt;Your SQL/Python Models]\n        M1[\ud83d\udcca Model: daily_sales v1]\n        M2[\ud83d\udcca Model: weekly_sales v1]\n    end\n\n    subgraph \"\ud83d\udd0d Plan Creation\"\n        PC[\u26a1 vulcan plan]\n        COMP[\ud83d\udd0e Compare Local vs Environment]\n        CAT[\ud83c\udff7\ufe0f Categorize Changes&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking]\n        PLAN[\ud83d\udccb Plan Generated&lt;br/&gt;Ready for Review]\n    end\n\n    subgraph \"\ud83d\udd00 Model Variants &amp; Snapshots\"\n        MV1[\ud83d\udd37 Model Variant 1&lt;br/&gt;daily_sales__hash1]\n        MV2[\ud83d\udd37 Model Variant 2&lt;br/&gt;daily_sales__hash2]\n        MV3[\ud83d\udd37 Model Variant 3&lt;br/&gt;weekly_sales__hash1]\n        SNAP1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd11 Fingerprint: hash1]\n        SNAP2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd11 Fingerprint: hash2]\n        SNAP3[\ud83d\udcf8 Snapshot 3&lt;br/&gt;\ud83d\udd11 Fingerprint: hash3]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Layer\"\n        PT1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;db.vulcan__sales.daily_sales__hash1]\n        PT2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;db.vulcan__sales.daily_sales__hash2]\n        PT3[\ud83d\uddc4\ufe0f Physical Table 3&lt;br/&gt;db.vulcan__sales.weekly_sales__hash1]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer\"\n        VL1[\ud83d\udd0d View: sales.daily_sales]\n        VL2[\ud83d\udd0d View: sales.weekly_sales]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;References Variant 1 &amp; 3]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;References Variant 2 &amp; 3]\n    end\n\n    subgraph \"\u2699\ufe0f Backfill Process\"\n        BF[\ud83d\udd04 Backfill Execution]\n        INC[\ud83d\udcc8 Incremental Backfill]\n        FULL[\ud83d\udd04 Full Refresh]\n    end\n\n    LP --&gt;|\"\ud83d\udce4\"| M1\n    LP --&gt;|\"\ud83d\udce4\"| M2\n    M1 --&gt;|\"\u27a1\ufe0f\"| PC\n    M2 --&gt;|\"\u27a1\ufe0f\"| PC\n    PC --&gt;|\"\ud83d\udd0d\"| COMP\n    COMP --&gt;|\"\ud83c\udff7\ufe0f\"| CAT\n    CAT --&gt;|\"\u2705\"| PLAN\n    PLAN --&gt;|\"\u2728\"| MV1\n    PLAN --&gt;|\"\u2728\"| MV2\n    PLAN --&gt;|\"\u2728\"| MV3\n\n    MV1 --&gt;|\"\ud83d\udd17\"| SNAP1\n    MV2 --&gt;|\"\ud83d\udd17\"| SNAP2\n    MV3 --&gt;|\"\ud83d\udd17\"| SNAP3\n\n    SNAP1 --&gt;|\"\ud83d\udcbe\"| PT1\n    SNAP2 --&gt;|\"\ud83d\udcbe\"| PT2\n    SNAP3 --&gt;|\"\ud83d\udcbe\"| PT3\n\n    PT1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    PT2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    PT3 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n\n    PROD --&gt;|\"\ud83d\udd17\"| MV1\n    PROD --&gt;|\"\ud83d\udd17\"| MV3\n    DEV --&gt;|\"\ud83d\udd17\"| MV2\n    DEV --&gt;|\"\ud83d\udd17\"| MV3\n\n    PLAN --&gt;|\"\u2699\ufe0f\"| BF\n    BF --&gt;|\"\ud83d\udcc8\"| INC\n    BF --&gt;|\"\ud83d\udd04\"| FULL\n    INC --&gt;|\"\ud83d\udcbe\"| PT1\n    INC --&gt;|\"\ud83d\udcbe\"| PT2\n    FULL --&gt;|\"\ud83d\udcbe\"| PT3\n\n    style LP fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style PC fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style PLAN fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style PROD fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style DEV fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000\n    style BF fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style MV1 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style MV2 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style MV3 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style PT1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px\n    style PT2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px\n    style PT3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px</code></pre>"},{"location":"guides/plan/#key-concepts-illustrated","title":"Key Concepts Illustrated","text":"<ol> <li>Local Project: Your model files define the desired state</li> <li>Plan Creation: Vulcan compares local state to environment state and generates a plan</li> <li>Model Variants: Each model change creates a new variant with a unique fingerprint</li> <li>Physical Tables: Each variant gets its own physical table in the warehouse</li> <li>Virtual Layer: Views point to the appropriate physical tables based on environment</li> <li>Environments: Collections of references to model variants (not the data itself)</li> <li>Backfill: Process of populating physical tables with data</li> </ol>"},{"location":"guides/plan/#what-is-a-plan","title":"What is a Plan?","text":"<p>A plan is Vulcan's way of comparing your local project state with a target environment and determining what changes need to be applied. Before any model changes take effect, Vulcan creates a plan that shows:</p> <ul> <li>Added models - New models to be created</li> <li>Removed models - Models to be deleted</li> <li>Modified models - Changes to existing models (with diffs)</li> <li>Indirectly affected models - Downstream models that depend on changed models</li> <li>Backfill requirements - Date ranges that need data reprocessing</li> </ul> <p>Plans allow you to review and verify all changes before they're applied to your data warehouse.</p>"},{"location":"guides/plan/#prerequisites","title":"Prerequisites","text":"<p>Before following this guide, ensure you have:</p> <ol> <li>Orders360 example project set up (see Examples Overview)</li> <li>Docker environment running (see Docker Quickstart)</li> <li>Vulcan CLI accessible via <code>vulcan</code> command or <code>vulcan.bat</code> (Windows)</li> </ol>"},{"location":"guides/plan/#scenario-1-first-plan-initializing-production","title":"Scenario 1: First Plan - Initializing Production","text":"<p>When you first create a project, you need to initialize the production environment. This scenario shows how Vulcan detects all models and creates the initial plan.</p>"},{"location":"guides/plan/#step-1-check-project-status","title":"Step 1: Check Project Status","text":"<p>First, verify your project setup:</p> <pre><code>vulcan info\n</code></pre> <p>Expected Output: </p><pre><code>Connection: \u2705 Connected\nModels: 4\nMacros: 0\nTests: 2\n...\n</code></pre><p></p> <p>[Screenshot: <code>vulcan info</code> output showing project status]</p>"},{"location":"guides/plan/#step-2-create-your-first-plan","title":"Step 2: Create Your First Plan","text":"<p>Run the plan command to see what Vulcan will create:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 raw.raw_customers\n    \u251c\u2500\u2500 raw.raw_orders\n    \u251c\u2500\u2500 raw.raw_products\n    \u2514\u2500\u2500 sales.daily_sales\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 raw.raw_customers: 2025-01-01 - 2025-01-15\n\u251c\u2500\u2500 raw.raw_orders: 2025-01-01 - 2025-01-15\n\u251c\u2500\u2500 raw.raw_products: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: First plan output showing all models to be added]</p>"},{"location":"guides/plan/#understanding-the-output","title":"Understanding the Output","text":"<ul> <li><code>prod</code> environment will be initialized: This is your first plan, so Vulcan is creating the production environment</li> <li>Added models: All 4 models are new and will be created</li> <li>Backfill dates: Each model needs data from its start date (2025-01-01) to the current date</li> </ul>"},{"location":"guides/plan/#step-3-apply-the-plan","title":"Step 3: Apply the Plan","text":"<p>Type <code>y</code> and press Enter to apply the plan:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/4] raw.raw_customers          [full refresh]                   2.3s\n[2/4] raw.raw_orders             [full refresh]                   1.8s\n[3/4] raw.raw_products           [full refresh]                   0.5s\n[4/4] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 4.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 4/4 \u2022 0:00:09\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Plan application progress showing all models being backfilled]</p>"},{"location":"guides/plan/#what-happened","title":"What Happened?","text":"<ol> <li>Seed models (<code>raw.*</code>) were fully refreshed - all data was reloaded</li> <li>Daily sales model was incrementally backfilled - data was inserted for each day from start date to today</li> <li>Physical tables were created in your warehouse</li> <li>Virtual layer was updated to point to the new tables</li> </ol>"},{"location":"guides/plan/#scenario-2-adding-a-new-model","title":"Scenario 2: Adding a New Model","text":"<p>After your initial setup, you'll add new models. This scenario shows how Vulcan detects new models and determines their dependencies.</p>"},{"location":"guides/plan/#step-1-add-a-new-model","title":"Step 1: Add a New Model","text":"<p>Create a new model file <code>models/sales/weekly_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@weekly',\n  grain [order_date]\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY DATE_TRUNC('week', order_date)\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan-for-new-model","title":"Step 2: Create Plan for New Model","text":"<p>Run the plan command:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing new weekly_sales model]</p>"},{"location":"guides/plan/#step-3-review-dependencies","title":"Step 3: Review Dependencies","text":"<p>Notice that: - Only the new model appears in the plan - The upstream model (<code>sales.daily_sales</code>) is not affected because adding a downstream model doesn't change upstream data - Backfill is needed from the model's start date</p>"},{"location":"guides/plan/#step-4-apply-the-plan","title":"Step 4: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.1s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:03\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Weekly sales model being backfilled]</p>"},{"location":"guides/plan/#scenario-3-modifying-a-model-non-breaking-change","title":"Scenario 3: Modifying a Model - Non-Breaking Change","text":"<p>Non-breaking changes don't affect existing data validity. Adding a new column is a common non-breaking change.</p>"},{"location":"guides/plan/#step-1-modify-the-model","title":"Step 1: Modify the Model","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a new column:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,  -- NEW COLUMN\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Non-breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -5,6 +5,7 @@\n      SELECT\n        order_date,\n        COUNT(DISTINCT order_id) AS total_orders,\n        SUM(order_amount) AS total_revenue,\n    +   AVG(order_amount) AS avg_order_value,\n        MAX(order_id) AS last_order_id\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing non-breaking change with diff]</p>"},{"location":"guides/plan/#understanding-non-breaking-changes","title":"Understanding Non-Breaking Changes","text":"<ul> <li>Directly Modified: The model you changed</li> <li>Non-breaking: Vulcan automatically detected this as non-breaking because:</li> <li>You added a new column</li> <li>Existing columns weren't modified</li> <li>Downstream models aren't affected (they don't use this column yet)</li> <li>Backfill required: The modified model needs to be backfilled to populate the new column</li> </ul>"},{"location":"guides/plan/#step-3-check-downstream-models","title":"Step 3: Check Downstream Models","text":"<p>Notice that <code>sales.weekly_sales</code> (which depends on <code>daily_sales</code>) is not listed. This is because: - The change is non-breaking - Downstream models don't need to be reprocessed - They'll automatically see the new column once <code>daily_sales</code> is backfilled</p>"},{"location":"guides/plan/#step-4-apply-the-plan_1","title":"Step 4: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:05\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Daily sales model being backfilled with new column]</p>"},{"location":"guides/plan/#scenario-4-modifying-a-model-breaking-change","title":"Scenario 4: Modifying a Model - Breaking Change","text":"<p>Breaking changes invalidate existing data and require downstream models to be reprocessed. Adding a WHERE clause is a common breaking change.</p>"},{"location":"guides/plan/#step-1-modify-the-model-with-a-filter","title":"Step 1: Modify the Model with a Filter","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a WHERE clause:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\n  AND order_amount &gt; 10  -- NEW FILTER: Only orders &gt; $10\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan_1","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -8,6 +8,7 @@\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n    +   AND order_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales.weekly_sales (Indirect Breaking)\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing breaking change with downstream impact]</p>"},{"location":"guides/plan/#understanding-breaking-changes","title":"Understanding Breaking Changes","text":"<ul> <li>Directly Modified: <code>sales.daily_sales</code> - the model you changed</li> <li>Breaking: Vulcan detected this as breaking because:</li> <li>You added a WHERE clause that filters data</li> <li>Existing data may now be invalid (rows that should be filtered out)</li> <li>Indirectly Modified: <code>sales.weekly_sales</code> - downstream model affected</li> <li>Cascading backfill: Both models need to be reprocessed</li> </ul>"},{"location":"guides/plan/#step-3-apply-the-plan_1","title":"Step 3: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.1s\n[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.8s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:09\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Both models being backfilled due to breaking change]</p>"},{"location":"guides/plan/#what-happened_1","title":"What Happened?","text":"<ol> <li>Daily sales was backfilled first (upstream)</li> <li>Weekly sales was backfilled second (downstream), using the updated daily sales data</li> <li>Both models now reflect the filtered data (only orders &gt; $10)</li> </ol>"},{"location":"guides/plan/#scenario-5-creating-a-development-environment","title":"Scenario 5: Creating a Development Environment","text":"<p>Development environments let you test changes without affecting production. This scenario shows how to create and use a dev environment.</p>"},{"location":"guides/plan/#step-1-revert-your-changes","title":"Step 1: Revert Your Changes","text":"<p>First, revert the breaking change from Scenario 4 to restore production:</p> <pre><code># Revert daily_sales.sql to remove the WHERE clause filter\n# (Edit the file to remove: AND order_amount &gt; 10)\n</code></pre>"},{"location":"guides/plan/#step-2-apply-reverted-plan-to-production","title":"Step 2: Apply Reverted Plan to Production","text":"<pre><code>vulcan plan\n# Type 'y' to apply\n</code></pre>"},{"location":"guides/plan/#step-3-create-development-environment","title":"Step 3: Create Development Environment","text":"<p>Now create a dev environment with your breaking change:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales__dev.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales__dev.weekly_sales\n\nDirectly Modified: sales__dev.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -8,6 +8,7 @@\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n    +   AND order_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales__dev.weekly_sales (Indirect Breaking)\n\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre><p></p> <p>[Screenshot: Creating dev environment with date prompt]</p>"},{"location":"guides/plan/#step-4-specify-date-range","title":"Step 4: Specify Date Range","text":"<p>For faster development, backfill only recent data:</p> <pre><code>Enter the backfill start date: 2025-01-10\n</code></pre> <p>Expected Output: </p><pre><code>Enter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2025-01-15 00:00:00':\n</code></pre><p></p> <p>Press Enter to use the default end date (today).</p> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sales__dev.daily_sales: 2025-01-10 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-10 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Dev environment plan with limited date range]</p>"},{"location":"guides/plan/#step-5-apply-dev-plan","title":"Step 5: Apply Dev Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales__dev.daily_sales     [insert 2025-01-10 - 2025-01-15] 2.1s\n[2/2] sales__dev.weekly_sales    [insert 2025-01-13 - 2025-01-13] 1.5s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:04\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Dev environment models being backfilled]</p>"},{"location":"guides/plan/#understanding-dev-environments","title":"Understanding Dev Environments","text":"<ul> <li>Isolated namespace: Models are prefixed with <code>__dev</code> (e.g., <code>sales__dev.daily_sales</code>)</li> <li>Separate tables: Dev environment has its own physical tables</li> <li>Limited backfill: Only recent data was processed (2025-01-10 to 2025-01-15)</li> <li>Production unaffected: Production data remains unchanged</li> </ul>"},{"location":"guides/plan/#step-6-query-dev-environment","title":"Step 6: Query Dev Environment","text":"<p>You can query the dev environment to verify changes:</p> <pre><code>vulcan fetchdf \"SELECT * FROM sales__dev.daily_sales LIMIT 5\"\n</code></pre> <p>Compare with production:</p> <pre><code>vulcan fetchdf \"SELECT * FROM sales.daily_sales LIMIT 5\"\n</code></pre> <p>[Screenshot: Comparing dev vs prod query results]</p>"},{"location":"guides/plan/#scenario-6-forward-only-plans","title":"Scenario 6: Forward-Only Plans","text":"<p>Forward-only plans reuse existing tables instead of creating new ones, avoiding backfill costs. This is useful for expensive models.</p>"},{"location":"guides/plan/#step-1-modify-model-for-forward-only","title":"Step 1: Modify Model for Forward-Only","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a comment (minimal change):</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\n-- Updated: Added comment for documentation\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-forward-only-plan","title":"Step 2: Create Forward-Only Plan","text":"<pre><code>vulcan plan --forward-only\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Forward-only)\n\u2514\u2500\u2500 Diff:\n    @@ -1,6 +1,7 @@\n    MODEL (\n      name sales.daily_sales,\n    ...\n    +-- Updated: Added comment for documentation\n      SELECT\n        order_date,\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-15 - 2025-01-15 (preview)\n\nApply - Virtual Update [y/n]:\n</code></pre><p></p> <p>[Screenshot: Forward-only plan showing preview backfill]</p>"},{"location":"guides/plan/#understanding-forward-only-plans","title":"Understanding Forward-Only Plans","text":"<ul> <li>Forward-only category: Automatically assigned</li> <li>Preview backfill: Only processes the latest interval for preview</li> <li>Virtual Update: No new physical table created</li> <li>Reuses existing table: Production will use the same physical table</li> </ul>"},{"location":"guides/plan/#step-3-apply-forward-only-plan","title":"Step 3: Apply Forward-Only Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Virtual Update [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-15 - 2025-01-15] 0.8s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:01\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Forward-only plan applied with minimal backfill]</p>"},{"location":"guides/plan/#benefits-of-forward-only","title":"Benefits of Forward-Only","text":"<ul> <li>Fast: Only processes latest interval</li> <li>Cost-effective: No full backfill required</li> <li>Safe for production: Reuses existing tables</li> </ul>"},{"location":"guides/plan/#scenario-7-restatement-plans","title":"Scenario 7: Restatement Plans","text":"<p>Restatement plans reprocess existing data without changing model definitions. Useful for fixing data issues or reprocessing after upstream corrections.</p>"},{"location":"guides/plan/#step-1-check-current-state","title":"Step 1: Check Current State","text":"<p>First, verify what data exists:</p> <pre><code>vulcan fetchdf \"SELECT MIN(order_date), MAX(order_date), COUNT(*) FROM sales.daily_sales\"\n</code></pre> <p>[Screenshot: Current data range in daily_sales]</p>"},{"location":"guides/plan/#step-2-create-restatement-plan","title":"Step 2: Create Restatement Plan","text":"<p>Restate the <code>daily_sales</code> model for a specific date range:</p> <pre><code>vulcan plan --restate-model \"sales.daily_sales\" --start \"2025-01-10\" --end \"2025-01-12\"\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nRestatement plan for `prod` environment\n\nModels to restate:\n\u2514\u2500\u2500 sales.daily_sales\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-10 - 2025-01-12\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-06 - 2025-01-13\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Restatement plan showing date range]</p>"},{"location":"guides/plan/#understanding-restatement","title":"Understanding Restatement","text":"<ul> <li>No model changes: Model definition unchanged</li> <li>Cascading restatement: Downstream models (<code>weekly_sales</code>) also need restatement</li> <li>Date range: Only specified dates will be reprocessed</li> </ul>"},{"location":"guides/plan/#step-3-apply-restatement-plan","title":"Step 3: Apply Restatement Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales.daily_sales          [insert 2025-01-10 - 2025-01-12] 1.2s\n[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 2.3s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:04\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Restatement plan execution]</p>"},{"location":"guides/plan/#use-cases-for-restatement","title":"Use Cases for Restatement","text":"<ul> <li>Upstream data correction: Raw data was fixed, need to reprocess</li> <li>Bug fixes: Found an issue in data processing logic (after fixing the model)</li> <li>Data refresh: Need to refresh specific date ranges</li> </ul>"},{"location":"guides/plan/#scenario-8-plan-with-explain-flag","title":"Scenario 8: Plan with Explain Flag","text":"<p>The <code>--explain</code> flag provides detailed information about what a plan will do.</p>"},{"location":"guides/plan/#step-1-create-plan-with-explain","title":"Step 1: Create Plan with Explain","text":"<pre><code>vulcan plan --explain\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nExplained plan\n\u251c\u2500\u2500 Validate SQL and create physical layer tables and views if they do not exist\n\u2502   \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n\u2502       \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502       \u2514\u2500\u2500 Create table if it doesn't exist\n\u251c\u2500\u2500 Backfill models by running their queries and run standalone audits\n\u2502   \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n\u2502   \u2502   \u2514\u2500\u2500 Fully refresh table\n\u2502   \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n\u2502       \u251c\u2500\u2500 Insert 2025-01-01 - 2025-01-15\n\u2502       \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n\u2514\u2500\u2500 Update the virtual layer for environment 'prod'\n    \u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables\n        \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n        \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n</code></pre><p></p> <p>[Screenshot: Explained plan output showing detailed actions]</p>"},{"location":"guides/plan/#understanding-explained-plans","title":"Understanding Explained Plans","text":"<p>The explain output shows three main phases:</p> <ol> <li> <p>Validation &amp; Table Creation:     - Dry runs each model query    - Creates physical tables if needed    - Shows mapping: <code>model_name -&gt; physical_table_name</code></p> </li> <li> <p>Backfill:    - Shows which models will be backfilled    - Indicates backfill type (full refresh vs incremental)    - Lists audits that will run</p> </li> <li> <p>Virtual Layer Update:    - Shows how views will be created/updated    - Maps virtual layer names to physical tables</p> </li> </ol>"},{"location":"guides/plan/#common-plan-scenarios-summary","title":"Common Plan Scenarios Summary","text":"Scenario Command When to Use First Plan <code>vulcan plan</code> Initializing production environment Add Model <code>vulcan plan</code> Adding new models to existing project Non-Breaking Change <code>vulcan plan</code> Adding columns, comments, or non-functional changes Breaking Change <code>vulcan plan</code> Modifying logic that affects downstream models Dev Environment <code>vulcan plan dev</code> Testing changes without affecting production Forward-Only <code>vulcan plan --forward-only</code> Expensive models, avoiding backfill costs Restatement <code>vulcan plan --restate-model \"model\"</code> Reprocessing existing data Explain <code>vulcan plan --explain</code> Understanding detailed plan actions"},{"location":"guides/plan/#best-practices","title":"Best Practices","text":"<ol> <li>Always review plans before applying them</li> <li>Use dev environments for testing breaking changes</li> <li>Use forward-only for expensive models when appropriate</li> <li>Check downstream impact - breaking changes cascade</li> <li>Use <code>--explain</code> when unsure about plan actions</li> <li>Restate carefully - it reprocesses existing data</li> </ol>"},{"location":"guides/plan/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plans Concepts for deeper understanding</li> <li>Explore Environments for managing multiple environments</li> <li>Read about Model Kinds to understand different model types</li> <li>Check Run Guide for scheduled execution after applying plans</li> <li>Set up Notifications to monitor plan execution</li> </ul>"},{"location":"guides/run_and_scheduling/","title":"Run and Scheduling","text":""},{"location":"guides/run_and_scheduling/#run-and-scheduling","title":"Run and Scheduling","text":"<p>This guide covers Vulcan's run functionality and scheduling strategies. Learn how <code>vulcan run</code> processes new data intervals and how to automate it for production.</p>"},{"location":"guides/run_and_scheduling/#run-and-scheduler-architecture","title":"Run and Scheduler Architecture","text":"<p>The following diagram illustrates how Vulcan's run system works with cron-based scheduling:</p> <pre><code>graph TB\n    subgraph \"\u23f0 Scheduler Triggers\"\n        CRON[\ud83d\udd04 Cron Job / CI/CD&lt;br/&gt;Runs periodically]\n        MANUAL[\ud83d\udc64 Manual Execution&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\ud83d\udd0d Run Process\"\n        START[\u26a1 vulcan run&lt;br/&gt;Command starts]\n        CHECK[\ud83d\udd0e Check for missing intervals&lt;br/&gt;Compare with state]\n        CRON_CHECK[\ud83d\udcc5 Check cron schedules&lt;br/&gt;Which models are due?]\n        FILTER[\ud83d\udd3d Filter models&lt;br/&gt;Only process due intervals]\n    end\n\n    subgraph \"\ud83d\udcca Model Execution\"\n        M1[\ud83d\udcc8 sales.daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Due: \u2705]\n        M2[\ud83d\udcca sales.weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Due: \u274c]\n        M3[\ud83d\udcc9 sales.monthly_sales&lt;br/&gt;cron: @monthly&lt;br/&gt;Due: \u274c]\n    end\n\n    subgraph \"\ud83d\udcbe State Management\"\n        STATE[\ud83d\uddc4\ufe0f State Database&lt;br/&gt;Tracks processed intervals]\n        UPDATE[\ud83d\udcdd Update State&lt;br/&gt;Mark intervals as processed]\n    end\n\n    subgraph \"\u2699\ufe0f Execution Flow\"\n        EXEC1[\ud83d\udd04 Execute daily_sales&lt;br/&gt;Process missing intervals]\n        EXEC2[\u23ed\ufe0f Skip weekly_sales&lt;br/&gt;Not due yet]\n        EXEC3[\u23ed\ufe0f Skip monthly_sales&lt;br/&gt;Not due yet]\n    end\n\n    subgraph \"\u2705 Results\"\n        SUCCESS[\u2705 Run Complete&lt;br/&gt;Intervals processed]\n        LOG[\ud83d\udccb Log Results&lt;br/&gt;Execution summary]\n    end\n\n    CRON --&gt;|\"\u23f0 Scheduled\"| START\n    MANUAL --&gt;|\"\ud83d\udc64 Triggered\"| START\n    START --&gt;|\"\ud83d\udd0d\"| CHECK\n    CHECK --&gt;|\"\ud83d\udcca\"| CRON_CHECK\n    CRON_CHECK --&gt;|\"\ud83d\udcc5\"| FILTER\n    FILTER --&gt;|\"\u2705 Due\"| M1\n    FILTER --&gt;|\"\u274c Not due\"| M2\n    FILTER --&gt;|\"\u274c Not due\"| M3\n\n    M1 --&gt;|\"\ud83d\udd04\"| EXEC1\n    M2 --&gt;|\"\u23ed\ufe0f\"| EXEC2\n    M3 --&gt;|\"\u23ed\ufe0f\"| EXEC3\n\n    EXEC1 --&gt;|\"\ud83d\udcbe\"| STATE\n    EXEC2 -.-&gt;|\"\u23ed\ufe0f\"| STATE\n    EXEC3 -.-&gt;|\"\u23ed\ufe0f\"| STATE\n\n    STATE --&gt;|\"\ud83d\udcdd\"| UPDATE\n    UPDATE --&gt;|\"\u2705\"| SUCCESS\n    SUCCESS --&gt;|\"\ud83d\udccb\"| LOG\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style M1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style M2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style M3 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#key-concepts-illustrated","title":"Key Concepts Illustrated","text":"<ol> <li>Scheduler Triggers: Run can be triggered by cron jobs, CI/CD pipelines, or manually</li> <li>Interval Detection: Vulcan checks for missing intervals by comparing current state with model schedules</li> <li>Cron-Based Filtering: Only models whose cron schedules indicate they're due are executed</li> <li>State Tracking: Processed intervals are tracked in the state database</li> <li>Efficient Execution: Models not due are skipped, saving computational resources</li> </ol>"},{"location":"guides/run_and_scheduling/#cron-schedule-flow","title":"Cron Schedule Flow","text":"<p>The following diagram shows how different cron schedules determine model execution:</p> <pre><code>gantt\n    title Model Execution Timeline (Example: Hourly, Daily, Weekly)\n    dateFormat YYYY-MM-DD HH:mm\n    axisFormat %H:%M\n\n    section Hourly Model\n    Run every hour    :active, hourly1, 2025-01-20 00:00, 1h\n    Run every hour    :active, hourly2, 2025-01-20 01:00, 1h\n    Run every hour    :active, hourly3, 2025-01-20 02:00, 1h\n    Run every hour    :active, hourly4, 2025-01-20 03:00, 1h\n\n    section Daily Model\n    Run once daily    :active, daily1, 2025-01-20 00:00, 24h\n\n    section Weekly Model\n    Run once weekly   :active, weekly1, 2025-01-20 00:00, 168h</code></pre> <p>Visual Explanation:  - Hourly models run every hour when <code>vulcan run</code> executes - Daily models run once per day (at the scheduled time) - Weekly models run once per week (at the scheduled time)</p>"},{"location":"guides/run_and_scheduling/#understanding-run-vs-plan","title":"Understanding Run vs Plan","text":"Aspect <code>vulcan plan</code> <code>vulcan run</code> Purpose Apply model changes to environment Execute existing models on schedule When to Use When models are modified/added/removed When no changes, just process new data Change Detection Compares local files vs environment No file comparison needed Backfill Backfills based on changes Processes missing intervals only Cron Schedule Not used (processes all affected dates) Uses model's cron to determine what runs User Interaction Prompts for change categorization Runs automatically Output Shows diffs and change summary Shows execution progress <p>Key Insight: Use <code>plan</code> when you've changed code. Use <code>run</code> for regular scheduled execution.</p>"},{"location":"guides/run_and_scheduling/#how-run-works","title":"How Run Works","text":"<p>The <code>vulcan run</code> command processes missing data intervals for models that haven't changed:</p> <pre><code>flowchart TD\n    START[\u26a1 vulcan run&lt;br/&gt;Command starts] --&gt; CHECK{\ud83d\udd0d Check model&lt;br/&gt;definitions}\n\n    CHECK --&gt;|\"\u274c Changed\"| ERROR[\ud83d\udeab Error: Use 'vulcan plan'&lt;br/&gt;to apply changes first]\n    CHECK --&gt;|\"\u2705 No changes\"| STATE[\ud83d\udcca Query state database&lt;br/&gt;Get processed intervals]\n\n    STATE --&gt; CRON[\ud83d\udcc5 Check cron schedules&lt;br/&gt;Which models are due?]\n\n    CRON --&gt; FILTER{\ud83d\udd3d Filter models&lt;br/&gt;by cron schedule}\n\n    FILTER --&gt;|\"\u2705 Due\"| EXEC1[\ud83d\udd04 Execute Model 1&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"\u2705 Due\"| EXEC2[\ud83d\udd04 Execute Model 2&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"\u274c Not due\"| SKIP1[\u23ed\ufe0f Skip Model 3&lt;br/&gt;Not due yet]\n    FILTER --&gt;|\"\u274c Not due\"| SKIP2[\u23ed\ufe0f Skip Model 4&lt;br/&gt;Not due yet]\n\n    EXEC1 --&gt; UPDATE[\ud83d\udcbe Update state database&lt;br/&gt;Mark intervals as processed]\n    EXEC2 --&gt; UPDATE\n    SKIP1 -.-&gt;|\"\u23ed\ufe0f\"| UPDATE\n    SKIP2 -.-&gt;|\"\u23ed\ufe0f\"| UPDATE\n\n    UPDATE --&gt; SUCCESS[\u2705 Run complete&lt;br/&gt;Summary output]\n\n    ERROR --&gt; END[\u274c Exit with error]\n    SUCCESS --&gt; END\n\n    style START fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style ERROR fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CRON fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style SKIP1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style SKIP2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre> <p>Process Steps:</p> <ol> <li>No Model Changes: Assumes no model definitions have changed</li> <li>Cron-Based Execution: Each model's <code>cron</code> parameter determines if it should run</li> <li>Missing Intervals: Only processes intervals that haven't been processed yet</li> <li>Automatic: No prompts or user interaction required</li> </ol> <p>Interactive Diagrams</p> <p>All diagrams in this guide are interactive! Double-click any diagram to zoom in and explore details. Use drag to pan, arrow keys to navigate, or the zoom controls.</p>"},{"location":"guides/run_and_scheduling/#scenario-1-first-run-processing-new-data","title":"Scenario 1: First Run - Processing New Data","text":"<p>After applying your first plan, use <code>run</code> to process new data as it arrives.</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nModels to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-16 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:02\n\n[1/1] sales.daily_sales          [insert 2025-01-16 - 2025-01-16]   2.1s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: First run output showing new interval processing]</p> <p>What Happened? - <code>sales.daily_sales</code> has <code>cron: '@daily'</code>, so it runs daily - Yesterday's plan processed up to 2025-01-15 - Today (2025-01-16) is a new interval that needs processing - <code>run</code> automatically processes this missing interval</p>"},{"location":"guides/run_and_scheduling/#scenario-2-cron-based-execution","title":"Scenario 2: Cron-Based Execution","text":"<p>Different models can have different <code>cron</code> schedules. <code>run</code> respects each model's schedule.</p>"},{"location":"guides/run_and_scheduling/#daily-model-execution","title":"Daily Model Execution","text":"<pre><code>vulcan run\n</code></pre> <p>Expected Output (Day 2): </p><pre><code>Models to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-17 (1 interval)\n</code></pre><p></p> <p>[Screenshot: Daily run showing only daily model executed]</p>"},{"location":"guides/run_and_scheduling/#weekly-model-execution","title":"Weekly Model Execution","text":"<p>After 7 days, both daily and weekly models run:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>Models to execute:\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-18 - 2025-01-24 (7 intervals)\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-20 - 2025-01-20 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:08\n\n[1/2] sales.daily_sales          [insert 2025-01-18 - 2025-01-24]   5.2s\n[2/2] sales.weekly_sales         [insert 2025-01-20 - 2025-01-20]   2.8s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: Weekly run showing both daily and weekly models]</p> <p>Understanding Cron Schedules: - Daily model (<code>@daily</code>): Processes missing daily intervals - Weekly model (<code>@weekly</code>): Only processes when 7 days have elapsed - Efficient: Each model only processes what's due based on its schedule</p>"},{"location":"guides/run_and_scheduling/#scenario-3-run-with-no-missing-intervals","title":"Scenario 3: Run with No Missing Intervals","text":"<p>When all intervals are up to date, <code>run</code> skips execution:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nNo models to execute. All intervals are up to date.\n\n\u2714 Run completed successfully\n</code></pre><p></p> <p>[Screenshot: Run output showing no models to execute]</p> <p>This is normal when running frequently - nothing to process means everything is up to date.</p>"},{"location":"guides/run_and_scheduling/#scenario-4-run-after-model-changes-error-case","title":"Scenario 4: Run After Model Changes (Error Case)","text":"<p>If models have changed, Vulcan detects this and requires a plan first:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nError: Model definitions have changed. Use 'vulcan plan' to apply changes first.\n\nChanged models:\n\u2514\u2500\u2500 sales.daily_sales\n\nPlease run 'vulcan plan' to apply these changes before using 'vulcan run'.\n</code></pre><p></p> <p>[Screenshot: Error message when trying to run with model changes]</p> <p>Workflow: Always <code>plan</code> first to apply changes, then <code>run</code> for scheduled execution.</p>"},{"location":"guides/run_and_scheduling/#scheduling-for-production","title":"Scheduling for Production","text":"<p>The <code>vulcan run</code> command doesn't run continuously - it executes once and exits. For production, you need to schedule it to run periodically.</p>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-architecture","title":"Built-in Scheduler Architecture","text":"<pre><code>graph TB\n    subgraph \"\ud83d\udd04 Automation Layer - Triggers\"\n        CRON[\u23f0 Cron Job&lt;br/&gt;Schedule: Every hour&lt;br/&gt;Example: 0 * * * *]\n        CI[\ud83d\ude80 CI/CD Pipeline&lt;br/&gt;GitHub Actions / GitLab CI&lt;br/&gt;Scheduled workflows]\n        K8S[\u2638\ufe0f Kubernetes CronJob&lt;br/&gt;Container orchestration&lt;br/&gt;K8s native scheduling]\n        MANUAL[\ud83d\udc64 Manual Trigger&lt;br/&gt;Developer runs manually&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\u26a1 Vulcan Run Command\"\n        RUN[vulcan run&lt;br/&gt;Command starts]\n        VALIDATE[\u2705 Validate Models&lt;br/&gt;Check for changes&lt;br/&gt;Error if modified]\n        QUERY[\ud83d\udd0d Query State Database&lt;br/&gt;Get execution history&lt;br/&gt;Read processed intervals]\n    end\n\n    subgraph \"\ud83d\udcbe State Database\"\n        STATE[\ud83d\uddc4\ufe0f State Storage&lt;br/&gt;PostgreSQL / SQL Engine&lt;br/&gt;Transaction-safe storage]\n\n        subgraph \"\ud83d\udcca State Tables\"\n            INTERVALS[\ud83d\udccb Processed Intervals&lt;br/&gt;model_name, start_ds, end_ds&lt;br/&gt;status: completed]\n            CRON_STATE[\u23f0 Cron Execution State&lt;br/&gt;model_name, last_run_time&lt;br/&gt;next_run_time]\n            MODEL_STATE[\ud83d\udd37 Model State&lt;br/&gt;model_name, fingerprint&lt;br/&gt;environment, version]\n        end\n    end\n\n    subgraph \"\ud83d\udcc5 Cron Evaluation Engine\"\n        CRON_CHECK[\ud83d\udcc5 Evaluate Cron Schedules&lt;br/&gt;Compare current time&lt;br/&gt;with last execution]\n        CALC[\ud83e\uddee Calculate Missing Intervals&lt;br/&gt;Determine what's due&lt;br/&gt;Based on cron + state]\n        FILTER[\ud83d\udd3d Filter Models&lt;br/&gt;Only select due models&lt;br/&gt;Skip not-due models]\n    end\n\n    subgraph \"\ud83d\udcca Model Execution Queue\"\n        QUEUE[\ud83d\udccb Execution Queue&lt;br/&gt;Ordered by dependencies&lt;br/&gt;Upstream first]\n        EXEC1[\ud83d\udd04 Execute Hourly Model&lt;br/&gt;@hourly - Due \u2705&lt;br/&gt;Process missing intervals]\n        EXEC2[\ud83d\udd04 Execute Daily Model&lt;br/&gt;@daily - Due \u2705&lt;br/&gt;Process missing intervals]\n        SKIP[\u23ed\ufe0f Skip Weekly Model&lt;br/&gt;@weekly - Not due \u274c&lt;br/&gt;Wait for next week]\n    end\n\n    subgraph \"\ud83d\udcbe Update State\"\n        UPDATE[\ud83d\udcdd Update State Database&lt;br/&gt;Mark intervals processed&lt;br/&gt;Update cron state]\n        COMMIT[\u2705 Commit Transaction&lt;br/&gt;Ensure consistency&lt;br/&gt;Rollback on error]\n    end\n\n    subgraph \"\ud83d\udcca Results &amp; Logging\"\n        LOG[\ud83d\udccb Log Execution&lt;br/&gt;Summary output&lt;br/&gt;Success/failure status]\n        NOTIFY[\ud83d\udd14 Notifications&lt;br/&gt;Optional: Slack/Email&lt;br/&gt;On success/failure]\n    end\n\n    CRON --&gt;|\"\u23f0 Scheduled trigger\"| RUN\n    CI --&gt;|\"\ud83d\ude80 Pipeline trigger\"| RUN\n    K8S --&gt;|\"\u2638\ufe0f K8s trigger\"| RUN\n    MANUAL --&gt;|\"\ud83d\udc64 Manual trigger\"| RUN\n\n    RUN --&gt;|\"1\ufe0f\u20e3 Validate\"| VALIDATE\n    VALIDATE --&gt;|\"2\ufe0f\u20e3 Query state\"| QUERY\n    QUERY --&gt;|\"\ud83d\udcca Read\"| STATE\n\n    STATE --&gt;|\"\ud83d\udccb Intervals\"| INTERVALS\n    STATE --&gt;|\"\u23f0 Cron state\"| CRON_STATE\n    STATE --&gt;|\"\ud83d\udd37 Model state\"| MODEL_STATE\n\n    INTERVALS --&gt;|\"\ud83d\udd0e Compare\"| CRON_CHECK\n    CRON_STATE --&gt;|\"\ud83d\udcc5 Check schedule\"| CRON_CHECK\n    MODEL_STATE --&gt;|\"\ud83d\udd37 Get models\"| CRON_CHECK\n\n    CRON_CHECK --&gt;|\"\ud83d\udcc5 Evaluate\"| CALC\n    CALC --&gt;|\"\ud83e\uddee Calculate\"| FILTER\n\n    FILTER --&gt;|\"\u2705 Due models\"| QUEUE\n    FILTER -.-&gt;|\"\u274c Skip\"| SKIP\n\n    QUEUE --&gt;|\"\ud83d\udd04 Execute\"| EXEC1\n    QUEUE --&gt;|\"\ud83d\udd04 Execute\"| EXEC2\n\n    EXEC1 --&gt;|\"\ud83d\udcbe Update\"| UPDATE\n    EXEC2 --&gt;|\"\ud83d\udcbe Update\"| UPDATE\n    SKIP -.-&gt;|\"\u23ed\ufe0f No update\"| UPDATE\n\n    UPDATE --&gt;|\"\ud83d\udcbe Commit\"| COMMIT\n    COMMIT --&gt;|\"\u2705 Success\"| LOG\n    LOG --&gt;|\"\ud83d\udd14 Optional\"| NOTIFY\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CI fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style K8S fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style MANUAL fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style VALIDATE fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style STATE fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style INTERVALS fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style MODEL_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CALC fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style QUEUE fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style EXEC2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style COMMIT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style LOG fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NOTIFY fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-components","title":"Built-in Scheduler Components","text":"<p>The built-in scheduler consists of several key components working together:</p> <ol> <li>Automation Layer: External triggers (cron, CI/CD, Kubernetes) that periodically execute <code>vulcan run</code></li> <li>State Database: Stores execution history, processed intervals, and cron state</li> <li>Cron Evaluation Engine: Determines which models are due based on their schedules</li> <li>Execution Queue: Orders models by dependencies and executes them</li> <li>State Updates: Records what was processed for future runs</li> </ol> <p>Key Features: - \u2705 Stores state in your SQL engine (or separate state database) - \u2705 Automatically detects missing intervals - \u2705 Respects each model's <code>cron</code> schedule - \u2705 Processes only what's due - \u2705 Transaction-safe state updates - \u2705 Dependency-aware execution order</p>"},{"location":"guides/run_and_scheduling/#setting-up-automation","title":"Setting Up Automation","text":"<p>Run <code>vulcan run</code> periodically using one of these methods:</p>"},{"location":"guides/run_and_scheduling/#option-1-linuxmac-cron-job","title":"Option 1: Linux/Mac Cron Job","text":"<pre><code># Edit crontab\ncrontab -e\n\n# Run every hour\n0 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n\n# Run every 15 minutes\n*/15 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n</code></pre>"},{"location":"guides/run_and_scheduling/#option-2-cicd-pipeline","title":"Option 2: CI/CD Pipeline","text":"<p>GitHub Actions Example: </p><pre><code>name: Vulcan Run\non:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  workflow_dispatch:\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Vulcan\n        run: |\n          docker run --network=vulcan --rm \\\n            -v $PWD:/workspace \\\n            tmdcio/vulcan:latest vulcan run\n</code></pre><p></p> <p>GitLab CI Example: </p><pre><code>vulcan_run:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  script:\n    - docker run --network=vulcan --rm \\\n        -v $PWD:/workspace \\\n        tmdcio/vulcan:latest vulcan run\n</code></pre><p></p>"},{"location":"guides/run_and_scheduling/#option-3-kubernetes-cronjob","title":"Option 3: Kubernetes CronJob","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: vulcan-run\nspec:\n  schedule: \"0 * * * *\"  # Every hour\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: vulcan\n            image: tmdcio/vulcan:latest\n            command: [\"vulcan\", \"run\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"guides/run_and_scheduling/#determining-run-frequency","title":"Determining Run Frequency","text":"<p>Set your automation frequency based on your most frequent model's <code>cron</code>:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Cron Schedules\"\n        H[\u23f0 Hourly Model&lt;br/&gt;cron: @hourly]\n        D[\ud83d\udcc5 Daily Model&lt;br/&gt;cron: @daily]\n        W[\ud83d\udcc6 Weekly Model&lt;br/&gt;cron: @weekly]\n    end\n\n    subgraph \"\ud83d\udd04 Automation Frequency\"\n        AUTO_H[\u23f0 Run every hour&lt;br/&gt;vulcan run]\n        AUTO_D[\ud83d\udcc5 Run daily&lt;br/&gt;vulcan run]\n        AUTO_W[\ud83d\udcc6 Run weekly&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\u2705 Execution Result\"\n        RESULT1[\u2705 Hourly: Runs every time&lt;br/&gt;\u2705 Daily: Runs when due&lt;br/&gt;\u2705 Weekly: Runs when due]\n        RESULT2[\u23ed\ufe0f Hourly: Skipped&lt;br/&gt;\u2705 Daily: Runs when due&lt;br/&gt;\u2705 Weekly: Runs when due]\n        RESULT3[\u23ed\ufe0f Hourly: Skipped&lt;br/&gt;\u23ed\ufe0f Daily: Skipped&lt;br/&gt;\u2705 Weekly: Runs when due]\n    end\n\n    H --&gt;|\"Requires\"| AUTO_H\n    D --&gt;|\"Can use\"| AUTO_H\n    W --&gt;|\"Can use\"| AUTO_H\n\n    AUTO_H --&gt;|\"Hour 1\"| RESULT1\n    AUTO_H --&gt;|\"Hour 2-23\"| RESULT2\n    AUTO_H --&gt;|\"Week 1\"| RESULT3\n\n    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style W fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style AUTO_H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RESULT2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT3 fill:#ffe082,stroke:#f9a825,stroke-width:2px,color:#000</code></pre> <p>Rule: Schedule <code>vulcan run</code> based on your fastest model's cron.</p> <ul> <li>Hourly models \u2192 Run automation every hour</li> <li>Daily models \u2192 Run automation daily  </li> <li>Weekly models \u2192 Run automation weekly</li> </ul> <p>Example: If your fastest model runs <code>@hourly</code>, schedule <code>vulcan run</code> to execute hourly. Models with slower schedules (daily, weekly) will only process when their intervals are due.</p>"},{"location":"guides/run_and_scheduling/#advanced-run-options","title":"Advanced Run Options","text":""},{"location":"guides/run_and_scheduling/#run-specific-models","title":"Run Specific Models","text":"<pre><code>vulcan run --select-model \"sales.daily_sales\"\n</code></pre> <p>Processes only the specified model and its upstream dependencies.</p>"},{"location":"guides/run_and_scheduling/#ignore-cron-schedules","title":"Ignore Cron Schedules","text":"<pre><code>vulcan run --ignore-cron\n</code></pre> <p>Processes all missing intervals regardless of cron schedules. Use sparingly - typically for catching up after downtime.</p>"},{"location":"guides/run_and_scheduling/#custom-execution-time","title":"Custom Execution Time","text":"<pre><code>vulcan run --execution-time \"2025-01-20 10:00:00\"\n</code></pre> <p>Simulates running at a specific time. Useful for testing cron schedules.</p>"},{"location":"guides/run_and_scheduling/#run-in-different-environments","title":"Run in Different Environments","text":"<pre><code>vulcan run dev\n</code></pre> <p>Runs models in the <code>dev</code> environment, maintaining separate execution state from production.</p>"},{"location":"guides/run_and_scheduling/#state-database-considerations","title":"State Database Considerations","text":"<p>By default, Vulcan stores scheduler state in your SQL engine. For production:</p> <p>Recommended: Use a separate PostgreSQL database for state storage when: - Your SQL engine is BigQuery (not optimized for frequent transactions) - You observe performance degradation - You need better isolation</p> <p>See Connections Guide for configuring a separate state database.</p>"},{"location":"guides/run_and_scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>run</code> for scheduled execution - Don't use <code>plan</code> for regular data processing</li> <li>Set up automation - Schedule <code>vulcan run</code> based on your most frequent model's cron</li> <li>Monitor execution - Check logs to ensure intervals are processing correctly</li> <li>Use <code>--ignore-cron</code> sparingly - Only when catching up on missed intervals</li> <li>Separate state database - Consider PostgreSQL for state storage in production</li> <li>Handle errors gracefully - Set up notifications for run failures</li> </ol>"},{"location":"guides/run_and_scheduling/#quick-reference","title":"Quick Reference","text":"Scenario Command When to Use Regular Run <code>vulcan run</code> Scheduled execution (cron jobs, CI/CD) Dev Environment <code>vulcan run dev</code> Running models in dev environment Select Models <code>vulcan run --select-model \"model\"</code> Running specific models only Ignore Cron <code>vulcan run --ignore-cron</code> Catch up on all missing intervals Custom Time <code>vulcan run --execution-time \"...\"</code> Testing/simulating runs"},{"location":"guides/run_and_scheduling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plan Guide for applying model changes</li> <li>Check Run Command for complete CLI reference</li> <li>Set up Notifications to monitor run execution</li> <li>Explore Environments for managing multiple environments</li> <li>Configure Connections for state database setup</li> </ul>"},{"location":"guides/transpiling_semantics/","title":"Transpiling Semantics","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantics","title":"Transpiling Semantics","text":"<p>The <code>vulcan transpile</code> command converts semantic queries into executable SQL, allowing you to preview, debug, and validate semantic logic before execution.</p>"},{"location":"guides/transpiling_semantics/#what-is-transpilation","title":"What is Transpilation?","text":"<p>Transpilation transforms semantic layer queries into database-specific SQL:</p> <ul> <li>Semantic SQL \u2192 Native SQL: Converts semantic SQL queries with <code>MEASURE()</code> functions into standard SQL</li> <li>REST API Payload \u2192 Native SQL: Converts JSON query payloads into executable SQL statements</li> <li>Validation: Catches errors before query execution</li> <li>Debugging: Inspect the generated SQL to understand query behavior</li> </ul>"},{"location":"guides/transpiling_semantics/#basic-structure","title":"Basic Structure","text":""},{"location":"guides/transpiling_semantics/#semantic-sql-query-structure","title":"Semantic SQL Query Structure","text":"<p>Semantic SQL queries follow standard SQL syntax with semantic layer extensions:</p> <pre><code>SELECT \n  alias.dimension_name,           # Dimensions: attributes for grouping and filtering\n  MEASURE(alias.measure_name)  # Measures: aggregated calculations (required wrapper)\nFROM alias                        # Semantic model alias (business-friendly name)\nCROSS JOIN other_alias            # Optional: join multiple models\nWHERE \n  alias.dimension_name = 'value'  # Optional: filter on dimensions\n  AND segment_name = true         # Optional: use segments (only = true supported)\nGROUP BY alias.dimension_name     # Required: all non-aggregated columns\nORDER BY MEASURE(alias.measure_name)    # Optional: sort results\nLIMIT 100                         # Optional: limit result set\nOFFSET 0                          # Optional: pagination offset\n</code></pre> <p>Key Components: - <code>alias.dimension_name</code> \u2014 Reference dimensions using semantic model alias - <code>MEASURE(measure_name)</code> \u2014 Required wrapper for measures to apply aggregation - <code>FROM alias</code> \u2014 Use semantic model alias, not physical model name - <code>CROSS JOIN</code> \u2014 Join syntax (join conditions automatically inferred) - <code>segment_name = true</code> \u2014 Segments only support <code>= true</code>, not <code>= false</code></p>"},{"location":"guides/transpiling_semantics/#rest-api-payload-structure","title":"REST API Payload Structure","text":"<p>REST API queries use JSON payloads with semantic query definitions:</p> <pre><code>{\n  \"query\": {\n    \"measures\": [\"alias.measure_name\"],              # Required: array of measure names\n    \"dimensions\": [\"alias.dimension_name\"],         # Optional: array of dimension names\n    \"segments\": [\"segment_name\"],                    # Optional: array of segment names\n    \"timeDimensions\": [{                             # Optional: array of time dimension objects\n      \"dimension\": \"alias.time_dimension\",           # Required: time dimension member\n      \"dateRange\": [\"2024-01-01\", \"2024-12-31\"],    # Optional: date range array or string\n      \"granularity\": \"month\"                         # Optional: hour, day, week, month, quarter, year\n    }],\n    \"filters\": [{                                    # Optional: array of filter objects\n      \"member\": \"alias.dimension_name\",              # Required: fully qualified member name\n      \"operator\": \"equals\",                          # Required: filter operator\n      \"values\": [\"value1\", \"value2\"]                 # Optional: array of filter values\n    }],\n    \"order\": {                                       # Optional: sort order object\n      \"alias.measure_name\": \"desc\",                  # Member name: \"asc\" or \"desc\"\n      \"alias.dimension_name\": \"asc\"\n    },\n    \"limit\": 100,                                    # Optional: maximum rows to return\n    \"offset\": 0,                                     # Optional: rows to skip\n    \"timezone\": \"UTC\",                               # Optional: timezone for date parsing\n    \"renewQuery\": false                              # Optional: bypass cache if true\n  },\n  \"ttl_minutes\": 60                                  # Optional: cache duration in minutes\n}\n</code></pre> <p>Key Components: - <code>measures</code> \u2014 Array of fully qualified measure names: <code>\"alias.measure_name\"</code> - <code>dimensions</code> \u2014 Array of fully qualified dimension names: <code>\"alias.dimension_name\"</code> - <code>segments</code> \u2014 Array of segment names (no alias prefix needed) - <code>timeDimensions</code> \u2014 Array of objects with <code>dimension</code>, <code>dateRange</code>, and <code>granularity</code> - <code>filters</code> \u2014 Array of filter objects with <code>member</code>, <code>operator</code>, and <code>values</code> - <code>order</code> \u2014 Object mapping member names to sort direction (<code>\"asc\"</code> or <code>\"desc\"</code>)</p>"},{"location":"guides/transpiling_semantics/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql-queries","title":"Transpiling Semantic SQL Queries","text":"<p>Convert semantic SQL queries to native SQL:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Output: Generated SQL that can be executed directly against your database.</p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads","title":"Transpiling REST API Payloads","text":"<p>Convert JSON query payloads to native SQL:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Output: Generated SQL from the REST-style query definition.</p>"},{"location":"guides/transpiling_semantics/#command-syntax","title":"Command Syntax","text":""},{"location":"guides/transpiling_semantics/#basic-format","title":"Basic Format","text":"<pre><code>vulcan transpile --format &lt;format&gt; \"&lt;query&gt;\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>--format</code> (required) \u2014 Output format: <code>sql</code> or <code>json</code></li> <li><code>\"&lt;query&gt;\"</code> (required) \u2014 The semantic query to transpile</li> <li>For SQL format: Semantic SQL query string</li> <li>For JSON format: JSON query payload string</li> </ul>"},{"location":"guides/transpiling_semantics/#advanced-options","title":"Advanced Options","text":"<pre><code>vulcan transpile --format sql \"&lt;query&gt;\" [--disable-post-processing]\n</code></pre> <p>Options:</p> <ul> <li><code>--disable-post-processing</code> \u2014 Enable pushdown mode for CTE support and advanced SQL features</li> <li>Default: Post-processing enabled (CTEs not supported)</li> <li>With flag: Pushdown enabled (CTEs supported, no pre-aggregations)</li> </ul>"},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql","title":"Transpiling Semantic SQL","text":""},{"location":"guides/transpiling_semantics/#basic-query","title":"Basic Query","text":"<p>Transpile a simple semantic SQL query:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions and grouping:</p> <pre><code>vulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".plan_type, sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\nGROUP BY \"users\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters","title":"Query with Filters","text":"<p>Transpile queries with WHERE conditions:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-grouping","title":"Query with Time Grouping","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format sql \"SELECT DATE_TRUNC('month', subscriptions.start_date) as month, MEASURE(total_arr) FROM subscriptions GROUP BY month\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS month,\n       sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-joins","title":"Query with Joins","text":"<p>Transpile queries joining multiple models:</p> <pre><code>vulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users GROUP BY users.industry\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".industry, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".user_id = \"users\".user_id\nGROUP BY \"users\".industry\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads_1","title":"Transpiling REST API Payloads","text":""},{"location":"guides/transpiling_semantics/#minimal-query","title":"Minimal Query","text":"<p>Transpile a basic REST API query:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions_1","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"dimensions\": [\"subscriptions.plan_type\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"subscriptions\".plan_type, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY \"subscriptions\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-dimensions","title":"Query with Time Dimensions","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"orders.total_revenue\"], \"timeDimensions\": [{\"dimension\": \"orders.order_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"orders\".order_date) AS orders_order_date_month,\n       sum(\"orders\".amount) AS total_revenue\nFROM analytics.orders AS \"orders\"\nWHERE \"orders\".order_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"orders\".order_date &lt;= '2024-12-31T23:59:59.999'\nGROUP BY DATE_TRUNC('month', \"orders\".order_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters_1","title":"Query with Filters","text":"<p>Transpile queries with filters:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-segments","title":"Query with Segments","text":"<p>Transpile queries using segments:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"segments\": [\"subscriptions.active_subscriptions\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".end_date IS NULL\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#complex-query","title":"Complex Query","text":"<p>Transpile complex queries with multiple components:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\", \"subscriptions.total_seats\"], \"dimensions\": [\"subscriptions.plan_type\", \"users.industry\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}], \"timeDimensions\": [{\"dimension\": \"subscriptions.start_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}], \"order\": {\"subscriptions.total_arr\": \"desc\"}, \"limit\": 100}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS subscriptions_start_date_month,\n       \"subscriptions\".plan_type,\n       \"users\".industry,\n       sum(\"subscriptions\".arr) AS total_arr,\n       sum(\"subscriptions\".seats) AS total_seats\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".start_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"subscriptions\".start_date &lt;= '2024-12-31T23:59:59.999'\n  AND \"subscriptions\".user_id = \"users\".user_id\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date),\n         \"subscriptions\".plan_type,\n         \"users\".industry\nORDER BY sum(\"subscriptions\".arr) DESC\nLIMIT 100\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#use-cases","title":"Use Cases","text":""},{"location":"guides/transpiling_semantics/#query-validation","title":"Query Validation","text":"<p>Validate semantic queries before execution:</p> <pre><code># Check if query syntax is correct\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>If the query is invalid, you'll get an error message indicating the issue.</p>"},{"location":"guides/transpiling_semantics/#debugging-query-behavior","title":"Debugging Query Behavior","text":"<p>Inspect generated SQL to understand how semantic queries are translated:</p> <pre><code># See how measures are aggregated\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#performance-analysis","title":"Performance Analysis","text":"<p>Review generated SQL to identify optimization opportunities:</p> <pre><code># Check join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#documentation","title":"Documentation","text":"<p>Generate SQL examples for documentation or training:</p> <pre><code># Create SQL reference from semantic queries\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"guides/transpiling_semantics/#error-unknown-member-x","title":"Error: \"Unknown member: X\"","text":"<p>Cause: Member doesn't exist in semantic model or is misspelled.</p> <p>Solution: - Verify member exists in your semantic model - Check spelling and casing (case-sensitive) - Use fully qualified format: <code>alias.member_name</code></p>"},{"location":"guides/transpiling_semantics/#error-measure-not-found-x","title":"Error: \"Measure not found: X\"","text":"<p>Cause: Measure referenced without proper qualification or doesn't exist.</p> <p>Solution: - Use <code>MEASURE(measure_name)</code> wrapper for SQL format - Use fully qualified format: <code>alias.measure_name</code> for JSON format - Verify measure is defined in semantic model</p>"},{"location":"guides/transpiling_semantics/#error-model-not-found-x","title":"Error: \"Model not found: X\"","text":"<p>Cause: Alias doesn't match any semantic model.</p> <p>Solution: - Check semantic model aliases in your <code>semantics/</code> directory - Verify alias spelling and casing - Ensure semantic models are properly defined</p>"},{"location":"guides/transpiling_semantics/#error-invalid-json-format","title":"Error: \"Invalid JSON format\"","text":"<p>Cause: JSON payload is malformed.</p> <p>Solution: - Validate JSON syntax - Ensure proper quoting of strings - Check array and object structure</p>"},{"location":"guides/transpiling_semantics/#error-projection-references-non-aggregate-values","title":"Error: \"Projection references non-aggregate values\"","text":"<p>Cause: Non-aggregated columns not in GROUP BY, or measures missing MEASURE() wrapper.</p> <p>Solution: - Add all non-aggregated columns to GROUP BY - Use MEASURE() wrapper for all measures in SQL format</p>"},{"location":"guides/transpiling_semantics/#best-practices","title":"Best Practices","text":""},{"location":"guides/transpiling_semantics/#validate-before-execution","title":"Validate Before Execution","text":"<p>Always transpile queries before running them in production:</p> <pre><code># \u2705 Good: Validate first\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n# Review output, then execute\n\n# \u274c Bad: Execute without validation\n# Direct execution without checking generated SQL\n</code></pre>"},{"location":"guides/transpiling_semantics/#use-transpilation-for-debugging","title":"Use Transpilation for Debugging","text":"<p>When queries return unexpected results, transpile to inspect generated SQL:</p> <pre><code># Debug query behavior\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Compare generated SQL with expected behavior\n</code></pre>"},{"location":"guides/transpiling_semantics/#document-query-patterns","title":"Document Query Patterns","text":"<p>Use transpilation output to document common query patterns:</p> <pre><code># Generate SQL examples for documentation\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#test-both-formats","title":"Test Both Formats","text":"<p>When building applications, test both SQL and JSON formats:</p> <pre><code># Test SQL format\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n\n# Test equivalent JSON format\nvulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre>"},{"location":"guides/transpiling_semantics/#choose-appropriate-mode","title":"Choose Appropriate Mode","text":"<p>Select post-processing or pushdown mode based on needs:</p> <ul> <li>Post-processing (default): Use for queries that benefit from pre-aggregations and caching</li> <li>Pushdown (<code>--disable-post-processing</code>): Use when you need CTEs or complex SQL structures</li> </ul>"},{"location":"guides/transpiling_semantics/#integration-with-development-workflow","title":"Integration with Development Workflow","text":""},{"location":"guides/transpiling_semantics/#pre-commit-validation","title":"Pre-commit Validation","text":"<p>Add transpilation checks to your development workflow:</p> <pre><code># Validate semantic queries in CI/CD\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#query-testing","title":"Query Testing","text":"<p>Use transpilation to generate test SQL:</p> <pre><code># Generate SQL for testing\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Use output in test assertions\n</code></pre>"},{"location":"guides/transpiling_semantics/#performance-tuning","title":"Performance Tuning","text":"<p>Analyze generated SQL for optimization:</p> <pre><code># Review join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Semantic Models that define the queryable members</li> <li>Explore Business Metrics for time-series analysis</li> <li>See the Semantics Overview for the complete picture</li> </ul>"},{"location":"guides/get-started/docker/","title":"Get Started","text":""},{"location":"guides/get-started/docker/#get-started","title":"Get Started","text":"<p>Welcome to the Vulcan quickstart, which will get you up and running with an example project.</p> <p>The example project runs locally on your machine with a Postgres SQL engine, and Vulcan will generate all the necessary project files - no configuration necessary!</p> <p>All you need to do is download Vulcan on your machine - get started by ensuring your system meets the basic prerequisites for using Vulcan.</p>"},{"location":"guides/get-started/docker/#prerequisites","title":"Prerequisites","text":"Mac/LinuxWindows <ol> <li>Docker Desktop is installed and running, at least with 4GB RAM, Verify that Docker is installed and running:  <pre><code>docker --version\ndocker compose version\n</code></pre></li> <li> <p>If not Download from Docker Desktop for Mac</p> </li> <li> <p>Install Docker Engine and Docker Compose from Docker for Linux</p> </li> </ol> <ol> <li> <p>Docker Desktop for Windows installed and running, at least 4GB RAM, Verify that Docker is installed and running:  </p><pre><code>docker --version\ndocker compose version\n</code></pre><p></p> </li> <li> <p>If not Download from Docker Desktop for Windows</p> </li> </ol>"},{"location":"guides/get-started/docker/#vulcan-setup-locally","title":"Vulcan Setup Locally","text":"Mac/LinuxWindows <p> Download for Mac/Linux</p> <p>Contains: Docker Compose files, Makefile, and README</p> <ol> <li> <p>Extract the zip file and navigate to the directory: </p><pre><code>cd vulcan-project\n</code></pre><p></p> </li> <li> <p>Run setup: </p><pre><code>make setup\n</code></pre>    This creates:<p></p> <ul> <li>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, &amp; execution history, and Vulcan uses it to persist the semantic model, plans, and track materialization state</li> <li>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates, and  Vulcan uses it to store query results and artifacts, enabling efficient data retrieval and caching</li> <li>minio-init: Initializes MinIO buckets and policies, and these services are essential for Vulcan's operation and must be running before you can use Vulcan</li> </ul> </li> <li> <p>Access Vulcan: </p><pre><code>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan\"\n</code></pre> Note: This alias is temporary and will be lost when you close your shell session. To make it permanent, add it to your shell configuration file (~/.bashrc or ~/.zshrc).<p></p> </li> <li> <p>Start API Services: </p><pre><code>make vulcan-up\n</code></pre>    This starts vulcan-api for querying your semantic model by REST API(available at <code>http://localhost:8000</code>) &amp;     vulcan-transpiler for transpiling semantic queries to SQL<p></p> </li> </ol> <p> Download for Windows</p> <p>Contains: Docker Compose files, Windows batch scripts, and README</p> <ol> <li> <p>Extract the zip file and navigate to the directory: </p><pre><code>cd vulcan-project\n</code></pre><p></p> </li> <li> <p>Run setup: </p><pre><code>setup.bat\n</code></pre>    This creates:<p></p> <ul> <li>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, &amp; execution history, and Vulcan uses it to persist the semantic model, plans, and track materialization state</li> <li>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates, and  Vulcan uses it to store query results and artifacts, enabling efficient data retrieval and caching</li> <li>minio-init: Initializes MinIO buckets and policies, and these services are essential for Vulcan's operation and must be running before you can use Vulcan</li> </ul> </li> <li> <p>Access Vulcan: </p><pre><code>vulcan.bat\n</code></pre><p></p> </li> <li> <p>Start API Services: </p><pre><code>start-vulcan-api.bat\n</code></pre>    This starts:<p></p> <p>vulcan-api for querying your semantic model by REST API(available at <code>http://localhost:8000</code>) &amp;</p> <p>vulcan-transpiler for transpiling semantic queries to SQL</p> </li> </ol>"},{"location":"guides/get-started/docker/#create-first-project","title":"Create First Project","text":"Mac/LinuxWindows <ol> <li> <p>Initialize project: Click here </p><pre><code>vulcan init\n</code></pre>    Choose <code>DEFAULT</code> project type and <code>Postgres</code> as SQL engine. <p></p> <p>It creates 7 <code>directories</code> containing SQL/PYTHON models,seed data files, audit files, test files, macro files, checks files, and semantics files</p> </li> <li> <p>Check connection and number of models, macros, and other project components: Click here </p><pre><code>vulcan info\n</code></pre>   It verifies that the setup is correct before running plans<p></p> </li> <li> <p>Run the plan: Click here </p><pre><code>vulcan plan\n</code></pre>   This will:<p></p> <ol> <li>Validate and creates the necessary database objects (tables, views, etc.) based on your models</li> <li>Backfills historical data according to your model's <code>start</code> date and <code>cron</code> schedule</li> <li>Prompt you to apply the plan</li> </ol> <p>Enter <code>y</code> when prompted to apply the plan and backfill your models.</p> </li> <li> <p>Query the models: Click here </p><pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre>    Executes a SQL query and returns results as a pandas DataFrame<p></p> </li> <li> <p>Query the Semantic: Click here </p><pre><code> vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre>   Returns the generated SQL that can be executed against your warehouse<p></p> </li> </ol> <ol> <li> <p>Initialize project: Click here </p><pre><code>vulcan init\n</code></pre>    Choose <code>DEFAULT</code> project type and <code>Postgres</code> as SQL engine. <p></p> <p>It creates 7 <code>directories</code> containing SQL/PYTHON models,seed data files, audit files, test files, macro files, checks files, and semantics files</p> </li> <li> <p>Check connection and number of models, macros, and other project components: Click here </p><pre><code>vulcan info\n</code></pre>    It verifies that the setup is correct before running plans<p></p> </li> <li> <p>Run the plan: Click here </p><pre><code>vulcan plan\n</code></pre>   This will:<p></p> <ol> <li>Validate and creates the necessary database objects (tables, views, etc.) based on your models</li> <li>Backfills historical data according to your model's <code>start</code> date and <code>cron</code> schedule</li> <li>Prompt you to apply the plan</li> </ol> <p>Enter <code>y</code> when prompted to apply the plan and backfill your models.</p> </li> <li> <p>Query the models: Click here </p><pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre>    Executes a SQL query and returns results as a pandas DataFrame<p></p> </li> <li> <p>Query the Semantic: Click here </p><pre><code> vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre>   Returns the generated SQL that can be executed against your warehouse<p></p> </li> </ol>"},{"location":"guides/get-started/docker/#stopping-services","title":"Stopping Services","text":"Mac/LinuxWindows <p>To stop all services:   </p><pre><code>make all-down       # Stop all services\nmake all-clean      # Stop and remove volumes (this will delete all data)\nmake vulcan-down     # Stop only Vulcan services\n</code></pre> To stop individual services: <pre><code>make vulcan-down     # Stop Vulcan services\nmake infra-down      # Stop infrastructure services\nmake warehouse-down  # Stop warehouse services\n</code></pre><p></p> <p>To stop services, you can use batch scripts:</p> <pre><code>stop-all.bat           # Stop all services\nclean.bat              # Stop and remove volumes (this will delete all data)\nvulcan-down.bat        # Stop only Vulcan API services\n</code></pre>"},{"location":"guides/get-started/docker/#troubleshooting","title":"Troubleshooting","text":"Troubleshooting <p>Services won't start: Ensure Docker Desktop is running with at least 4GB RAM allocated.</p> <p>Network errors: Ensure the <code>vulcan</code> network exists:</p> Mac/LinuxWindows <p></p><pre><code>docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p></p><pre><code> docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p>Port conflicts: If ports 5431, 5433, 9000, 9001, or 8000 are already in use, you can modify the port mappings in the Docker Compose files.</p> <p>Can't connect to services: Make sure all services are running: </p><pre><code>docker compose -f docker/docker-compose.infra.yml ps\ndocker compose -f docker/docker-compose.warehouse.yml ps\n</code></pre><p></p> <p>MinIO console: You can access the MinIO console at <code>http://localhost:9001</code> with: - Username: <code>admin</code> - Password: <code>password</code></p>"},{"location":"guides/get-started/docker/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Vulcan CLI commands</li> <li>Explore Vulcan concepts</li> <li>Set up connections to different warehouses</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/","title":"Vulcan Docker Quickstart","text":""},{"location":"guides/get-started/zip-mac/vulcan-project/#vulcan-docker-quickstart","title":"Vulcan Docker Quickstart","text":"<p>This package contains all the Docker Compose files and configuration needed to get started with Vulcan using Docker.</p>"},{"location":"guides/get-started/zip-mac/vulcan-project/#contents","title":"Contents","text":"<ul> <li><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</li> <li><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</li> <li><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</li> <li><code>Makefile</code> - Convenient commands for managing services</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop installed and running</li> <li>Docker Compose (included with Docker Desktop)</li> <li>At least 4GB of available RAM</li> <li>A terminal/command line interface</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Start all infrastructure: </p><pre><code>make setup\n</code></pre>    This will:    - Create the Docker network    - Start statestore (PostgreSQL) on port 5431    - Start MinIO object storage on ports 9000 and 9001    - Start warehouse database (PostgreSQL) on port 5433<p></p> </li> <li> <p>Access Vulcan: </p><pre><code>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan\"\n</code></pre><p></p> </li> <li> <p>Initialize your project: </p><pre><code>vulcan init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code>gateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\nmodel_defaults:\n  dialect: postgres\n  start: 2025-01-01\n  cron: '@daily'\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-mac/vulcan-project/#available-make-commands","title":"Available Make Commands","text":"<ul> <li><code>make setup</code> - Run all setup steps</li> <li> <p><code>make vulcan-up</code> - Start Vulcan API services</p> </li> <li> <p><code>make network</code> - Create the Docker network</p> </li> <li><code>make infra</code> - Start infrastructure services</li> <li> <p><code>make warehouse</code> - Start warehouse database</p> </li> <li> <p><code>make all-down</code> - Stop all services</p> </li> <li><code>make all-clean</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li>Statestore: 5431</li> <li>Warehouse: 5433</li> <li>MinIO API: 9000</li> <li>MinIO Console: 9001 (admin/password)</li> <li>Vulcan API: 8000</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/","title":"Vulcan Docker Quickstart - Windows","text":""},{"location":"guides/get-started/zip-window/vulcan-project/#vulcan-docker-quickstart-windows","title":"Vulcan Docker Quickstart - Windows","text":"<p>This package contains all the Docker Compose files and Windows batch scripts needed to get started with Vulcan using Docker on Windows.</p>"},{"location":"guides/get-started/zip-window/vulcan-project/#contents","title":"Contents","text":"<ul> <li><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</li> <li><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</li> <li><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</li> <li><code>setup.bat</code> - Setup script to start all infrastructure</li> <li><code>vulcan.bat</code> - Wrapper script to run Vulcan CLI commands</li> <li><code>start-vulcan-api.bat</code> - Start Vulcan API services</li> <li><code>stop-all.bat</code> - Stop all services</li> <li><code>clean.bat</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop for Windows installed and running</li> <li>At least 4GB of available RAM</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Run the setup script: </p><pre><code>setup.bat\n</code></pre>    This will:    - Create the Docker network    - Start statestore (PostgreSQL) on port 5431    - Start MinIO object storage on ports 9000 and 9001    - Start warehouse database (PostgreSQL) on port 5433<p></p> </li> <li> <p>Access Vulcan: </p><pre><code>vulcan.bat\n</code></pre><p></p> </li> <li> <p>Initialize your project: </p><pre><code>vulcan.bat init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code>gateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\nmodel_defaults:\n  dialect: postgres\n  start: 2025-01-01\n  cron: '@daily'\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan.bat plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-window/vulcan-project/#available-scripts","title":"Available Scripts","text":"<ul> <li><code>setup.bat</code> - Create network and start all infrastructure</li> <li><code>start-vulcan-api.bat</code> - Start Vulcan API services</li> <li><code>stop-all.bat</code> - Stop all services</li> <li><code>clean.bat</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li>Statestore: 5431</li> <li>Warehouse: 5433</li> <li>MinIO API: 9000</li> <li>MinIO Console: 9001 (admin/password)</li> <li>Vulcan API: 8000 ```</li> </ul>"},{"location":"guides-old/configuration/","title":"Configuration","text":""},{"location":"guides-old/configuration/#configuration","title":"Configuration","text":"<p>Vulcan's behavior is determined by three things: a project's files (e.g., models), user actions (e.g., creating a <code>plan</code>), and how Vulcan is configured.</p> <p>This page describes how Vulcan configuration works and discusses the aspects of Vulcan behavior that can be modified via configuration.</p> <p>The configuration reference page contains concise lists of all configuration parameters and their default values.</p>"},{"location":"guides-old/configuration/#configuration-files","title":"Configuration files","text":"<p>NOTE: Vulcan project configurations have the following two requirements:</p> <ol> <li>A <code>config.yaml</code> or <code>config.py</code> file must be present in the project's folder.</li> <li>That configuration file must contain a default SQL dialect for the project's models in the <code>model_defaults</code> <code>dialect</code> key.</li> </ol> <p>Vulcan configuration parameters can be set as environment variables, in a configuration file in the <code>~/.vulcan</code> folder, and in the configuration file within a project folder.</p> <p>The sources have the following order of precedence:</p> <ol> <li>Environment variable (e.g., <code>VULCAN__MODEL_DEFAULTS__DIALECT</code>). [HIGHEST PRECEDENCE]</li> <li><code>config.yaml</code> or <code>config.py</code> in the <code>~/.vulcan</code> folder.</li> <li><code>config.yaml</code> or <code>config.py</code> in a project folder. [LOWEST PRECEDENCE]</li> </ol> <p>Note</p> <p>To relocate the <code>.vulcan</code> folder, set the <code>VULCAN_HOME</code> environment variable to your preferred directory path.</p>"},{"location":"guides-old/configuration/#file-type","title":"File type","text":"<p>You can specify a Vulcan configuration in either YAML or Python.</p> <p>YAML configuration is simpler, and we recommend it for most projects. Python configuration is more complex, but it enables functionality that YAML does not support.</p> <p>Because Python configuration files are evaluated by Python when Vulcan reads them, they support dynamic parameters based on the computational environment in which Vulcan is running.</p> <p>For example, Python configuration files enable use of third-party secrets managers for storing passwords and other sensitive information. They also support user-specific parameters such as automatically setting project defaults based on which user account is running Vulcan.</p>"},{"location":"guides-old/configuration/#yaml","title":"YAML","text":"<p>YAML configuration files consist of configuration keys and values. Strings are not quoted, and some keys are \"dictionaries\" that contain one or more sub-keys.</p> <p>For example, the <code>default_gateway</code> key specifies the default gateway Vulcan should use when executing commands. It takes a single, unquoted gateway name as its value:</p> <pre><code>default_gateway: local\n</code></pre> <p>In contrast, the <code>gateways</code> key takes dictionaries as values, and each gateway dictionary contains one or more connection dictionaries. This example specifies the <code>my_gateway</code> gateway with a Snowflake <code>connection</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>Gateway dictionaries can contain multiple connection dictionaries if different Vulcan components should use different connections (e.g., Vulcan <code>test</code>s should run in a different database than Vulcan <code>plan</code>s). See the gateways section for more information on gateway configuration.</p>"},{"location":"guides-old/configuration/#python","title":"Python","text":"<p>Python configuration files consist of statements that import Vulcan configuration classes and a configuration specification using those classes.</p> <p>At minimum, a Python configuration file must:</p> <ol> <li>Create an object of the Vulcan <code>Config</code> class named <code>config</code></li> <li>Specify that object's <code>model_defaults</code> argument with a <code>ModelDefaultsConfig()</code> object specifying the default SQL dialect for the project's models</li> </ol> <p>For example, this minimal configuration specifies a default SQL dialect of <code>duckdb</code> and uses the default values for all other configuration parameters:</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n)\n</code></pre> <p>Python configuration files may optionally define additional configuration objects and switch between the configurations when issuing <code>vulcan</code> commands. For example, if a configuration file contained a second configuration object <code>my_second_config</code>, you could create a plan using that config with <code>vulcan --config my_second_config plan</code>.</p> <p>Different <code>Config</code> arguments accept different object types. Some, such as <code>model_defaults</code>, take Vulcan configuration objects. Others, such as <code>default_gateway</code>, take strings or other Python object types like dictionaries.</p> <p>Vulcan's Python configuration components are documented in the <code>vulcan.core.config</code> module's API documentation.</p> <p>The <code>config</code> sub-module API documentation describes the individual classes used for the relevant <code>Config</code> arguments:</p> <ul> <li>Model defaults configuration: <code>ModelDefaultsConfig()</code></li> <li>Gateway configuration: <code>GatewayConfig()</code><ul> <li>Connection configuration (separate classes for each supported database/engine)</li> <li>Scheduler configuration (separate classes for each supported scheduler)</li> </ul> </li> <li>Plan change categorization configuration: <code>CategorizerConfig()</code></li> <li>User configuration: <code>User()</code></li> <li>Notification configuration (separate classes for each notification target)</li> </ul> <p>See the notifications guide for more information about user and notification specification.</p>"},{"location":"guides-old/configuration/#environment-variables","title":"Environment variables","text":"<p>All software runs within a system environment that stores information as \"environment variables.\"</p> <p>Vulcan can access environment variables during configuration, which enables approaches like storing passwords/secrets outside the configuration file and changing configuration parameters dynamically based on which user is running Vulcan.</p> <p>You can specify environment variables in the configuration file or by storing them in a <code>.env</code> file.</p>"},{"location":"guides-old/configuration/#env-files","title":".env files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory. This provides a convenient way to manage environment variables without having to set them in your shell.</p> <p>Create a <code>.env</code> file in your project root with key-value pairs:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override specific Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>See the overrides section for a detailed explanation of how these are defined.</p> <p>The rest of the <code>.env</code> file variables can be used in your configuration files with <code>{{ env_var('VARIABLE_NAME') }}</code> syntax in YAML or accessed via <code>os.environ['VARIABLE_NAME']</code> in Python.</p>"},{"location":"guides-old/configuration/#custom-dot-env-file-location-and-name","title":"Custom dot env file location and name","text":"<p>By default, Vulcan loads <code>.env</code> files from each project directory. However, you can specify a custom path using the <code>--dotenv</code> CLI flag directly when running a command:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag is a global option and must be placed before the subcommand (e.g. <code>plan</code>, <code>run</code>), not after.</p> <p>Alternatively, you can export the <code>VULCAN_DOTENV_PATH</code> environment variable once, to persist a custom path across all subsequent commands in your shell session:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\nvulcan run\n</code></pre> <p>Important considerations: - Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information - Vulcan will only load the <code>.env</code> file if it exists in the project directory (unless a custom path is specified) - When using a custom path, that specific file takes precedence over any <code>.env</code> file in the project directory.</p>"},{"location":"guides-old/configuration/#configuration-file","title":"Configuration file","text":"<p>This section demonstrates using environment variables in YAML and Python configuration files.</p> <p>The examples specify a Snowflake connection whose password is stored in an environment variable <code>SNOWFLAKE_PW</code>.</p> YAMLPython <p>Specify environment variables in a YAML configuration with the syntax <code>{{ env_var('&lt;ENVIRONMENT VARIABLE NAME&gt;') }}</code>. Note that the environment variable name is contained in single quotes.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: {{ env_var('SNOWFLAKE_PW') }}\n      account: &lt;account&gt;\n</code></pre> <p>Python accesses environment variables via the <code>os</code> library's <code>environ</code> dictionary.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>import os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=os.environ['SNOWFLAKE_PW'],\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#default-target-environment","title":"Default target environment","text":"<p>The Vulcan <code>plan</code> command acts on the <code>prod</code> environment by default (i.e., <code>vulcan plan</code> is equivalent to <code>vulcan plan prod</code>).</p> <p>In some organizations, users never run plans directly against <code>prod</code> - they do all Vulcan work in a development environment unique to them. In a standard Vulcan configuration, this means they need to include their development environment name every time they issue the <code>plan</code> command (e.g., <code>vulcan plan dev_tony</code>).</p> <p>If your organization works like this, it may be convenient to change the <code>plan</code> command's default environment from <code>prod</code> to each user's development environment. That way people can issue <code>vulcan plan</code> without typing the environment name every time.</p> <p>The Vulcan configuration <code>user()</code> function returns the name of the user currently logged in and running Vulcan. It retrieves the username from system environment variables like <code>USER</code> on MacOS/Linux or <code>USERNAME</code> on Windows.</p> <p>Call <code>user()</code> inside Jinja curly braces with the syntax <code>{{ user() }}</code>, which allows you to combine the user name with a prefix or suffix.</p> <p>The example configuration below constructs the environment name by appending the username to the end of the string <code>dev_</code>. If the user running Vulcan is <code>tony</code>, the default target environment when they run Vulcan will be <code>dev_tony</code>. In other words, <code>vulcan plan</code> will be equivalent to <code>vulcan plan dev_tony</code>.</p> YAMLPython <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <pre><code>default_target_environment: dev_{{ user() }}\n</code></pre> <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <p>Retrieve the username with the <code>getpass.getuser()</code> function, and combine it with <code>dev_</code> in a Python f-string.</p> <pre><code>import getpass\nimport os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(),\n        ),\n    },\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre>"},{"location":"guides-old/configuration/#overrides","title":"Overrides","text":"<p>Environment variables have the highest precedence among configuration methods, as noted above. They will automatically override configuration file specifications if they follow a specific naming structure.</p> <p>The structure is based on the names of the configuration fields, with double underscores <code>__</code> between the field names. The environment variable name must begin with <code>VULCAN__</code>, followed by the YAML field names starting at the root and moving downward in the hierarchy.</p> <p>For example, we can override the password specified in a Snowflake connection. This is the YAML specification contained in our configuration file, which specifies a password <code>dummy_pw</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: dummy_pw\n      account: &lt;account&gt;\n</code></pre> <p>We can override the <code>dummy_pw</code> value with the true password <code>real_pw</code> by creating the environment variable. This example demonstrates creating the variable with the bash <code>export</code> function:</p> <pre><code>$ export VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre> <p>After the initial string <code>VULCAN__</code>, the environment variable name components move down the key hierarchy in the YAML specification: <code>GATEWAYS</code> \u2192 <code>MY_GATEWAY</code> \u2192 <code>CONNECTION</code> \u2192 <code>PASSWORD</code>.</p>"},{"location":"guides-old/configuration/#configuration-types","title":"Configuration types","text":"<p>A Vulcan project configuration is hierarchical and consists of root level parameters within which other parameters are defined.</p> <p>Conceptually, we can group the root level parameters into the following types. Each type links to its table of parameters in the Vulcan configuration reference page:</p> <ol> <li>Project - configuration options for Vulcan project directories.</li> <li>Environment - configuration options for Vulcan environment creation/promotion, physical table schemas, and view schemas.</li> <li>Gateways - configuration options for how Vulcan should connect to the data warehouse, state backend, and scheduler.</li> <li>Gateway/connection defaults - configuration options for what should happen when gateways or connections are not all explicitly specified.</li> <li>Model defaults - configuration options for what should happen when model-specific configurations are not explicitly specified in a model's file.</li> <li>Debug mode - configuration option for Vulcan to print and log actions and full backtraces.</li> </ol>"},{"location":"guides-old/configuration/#configuration-details","title":"Configuration details","text":"<p>The rest of this page provides additional detail for some of the configuration options and provides brief examples. Comprehensive lists of configuration options are at the configuration reference page.</p>"},{"location":"guides-old/configuration/#cache-directory","title":"Cache directory","text":"<p>By default, the Vulcan cache is stored in a <code>.cache</code> directory within your project folder. You can customize the cache location using the <code>cache_dir</code> configuration option:</p> YAMLPython <pre><code># Relative path to project directory\ncache_dir: my_custom_cache\n\n# Absolute path\ncache_dir: /tmp/vulcan_cache\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    cache_dir=\"/tmp/vulcan_cache\",\n)\n</code></pre> <p>The cache directory is automatically created if it doesn't exist. You can clear the cache using the <code>vulcan clean</code> command.</p>"},{"location":"guides-old/configuration/#tableview-storage-locations","title":"Table/view storage locations","text":"<p>Vulcan creates schemas, physical tables, and views in the data warehouse/engine. You can override where Vulcan creates physical tables and views with the <code>physical_schema_mapping</code>, <code>environment_suffix_target</code>, and <code>environment_catalog_mapping</code> configuration options.</p> <p>You can also override what the physical tables are called by using the <code>physical_table_naming_convention</code> option.</p> <p>These options are in the environments section of the configuration reference page.</p>"},{"location":"guides-old/configuration/#physical-table-schemas","title":"Physical table schemas","text":"<p>By default, Vulcan creates physical schemas for a model with a naming convention of <code>vulcan__[model schema]</code>.</p> <p>This can be overridden on a per-schema basis using the <code>physical_schema_mapping</code> option, which removes the <code>vulcan__</code> prefix and uses the regex pattern you provide to map the schemas defined in your model to their corresponding physical schemas.</p> <p>This example configuration overrides the default physical schemas for the <code>my_schema</code> model schema and any model schemas starting with <code>dev</code>:</p> YAMLPython <pre><code>physical_schema_mapping:\n  '^my_schema$': my_new_schema,\n  '^dev.*': development\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_schema_mapping={\n        \"^my_schema$\": \"my_new_schema\",\n        '^dev.*': \"development\"\n    },\n)\n</code></pre> <p>This config causes the following mapping behaviour:</p> Model name Default physical location Resolved physical location <code>my_schema.my_table</code> <code>vulcan__my_schema.table_&lt;fingerprint&gt;</code> <code>my_new_schema.table_&lt;fingerprint&gt;</code> <code>dev_schema.my_table</code> <code>vulcan__dev_schema.table_&lt;fingerprint&gt;</code> <code>development.table_&lt;fingerprint&gt;</code> <code>other.my_table</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <p>This only applies to the physical tables that Vulcan creates - the views are still created in <code>my_schema</code> (prod) or <code>my_schema__&lt;env&gt;</code>.</p>"},{"location":"guides-old/configuration/#disable-environment-specific-schemas","title":"Disable environment-specific schemas","text":"<p>Vulcan stores <code>prod</code> environment views in the schema in a model's name - for example, the <code>prod</code> views for a model <code>my_schema.users</code> will be located in <code>my_schema</code>.</p> <p>By default, for non-prod environments Vulcan creates a new schema that appends the environment name to the model name's schema. For example, by default the view for a model <code>my_schema.users</code> in a Vulcan environment named <code>dev</code> will be located in the schema <code>my_schema__dev</code> as <code>my_schema__dev.users</code>.</p>"},{"location":"guides-old/configuration/#show-at-the-table-level-instead","title":"Show at the table level instead","text":"<p>This behavior can be changed to append a suffix at the end of a table/view name instead. Appending the suffix to a table/view name means that non-prod environment views will be created in the same schema as the <code>prod</code> environment. The prod and non-prod views are differentiated by non-prod view names ending with <code>__&lt;env&gt;</code>.</p> <p>For example, if you created a <code>dev</code> environment for a project containing a model named <code>my_schema.users</code>, the model view would be created as <code>my_schema.users__dev</code> instead of the default behavior of <code>my_schema__dev.users</code>.</p> <p>Config example:</p> YAMLPython <pre><code>environment_suffix_target: table\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.TABLE,\n)\n</code></pre> <p>Default behavior</p> <p>The default behavior of appending the suffix to schemas is recommended because it leaves production with a single clean interface for accessing the views. However, if you are deploying Vulcan in an environment with tight restrictions on schema creation then this can be a useful way of reducing the number of schemas Vulcan uses.</p>"},{"location":"guides-old/configuration/#show-at-the-catalog-level-instead","title":"Show at the catalog level instead","text":"<p>If neither the schema (default) nor the table level are sufficient for your use case, you can indicate the environment at the catalog level instead.</p> <p>This can be useful if you have downstream BI reporting tools and you would like to point them at a development environment to test something out without renaming all the table / schema references within the report query.</p> <p>In order to achieve this, you can configure environment_suffix_target like so:</p> YAMLPython <pre><code>environment_suffix_target: catalog\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.CATALOG,\n)\n</code></pre> <p>Given the example of a model called <code>my_schema.users</code> with a default catalog of <code>warehouse</code> this will cause the following behavior:</p> <ul> <li>For the <code>prod</code> environment, the default catalog as configured in the gateway will be used. So the view will be created at <code>warehouse.my_schema.users</code></li> <li>For any other environment, eg <code>dev</code>, the environment name will be appended to the default catalog. So the view will be created at <code>warehouse__dev.my_schema.users</code></li> <li>If a model is fully qualified with a catalog already, eg <code>finance_mart.my_schema.users</code>, then the environment catalog will be based off the model catalog and not the default catalog. In this example, the view will be created at <code>finance_mart__dev.my_schema.users</code></li> </ul> <p>Caveats</p> <ul> <li>Using <code>environment_suffix_target: catalog</code> only works on engines that support querying across different catalogs. If your engine does not support cross-catalog queries then you will need to use <code>environment_suffix_target: schema</code> or <code>environment_suffix_target: table</code> instead.</li> <li>Automatic catalog creation is not supported on all engines even if they support cross-catalog queries. For engines where it is not supported, the catalogs must be managed externally from Vulcan and exist prior to invoking Vulcan.</li> </ul>"},{"location":"guides-old/configuration/#physical-table-naming-convention","title":"Physical table naming convention","text":"<p>Out of the box, Vulcan has the following defaults set:</p> <ul> <li><code>environment_suffix_target: schema</code></li> <li><code>physical_table_naming_convention: schema_and_table</code></li> <li>no <code>physical_schema_mapping</code> overrides, so a <code>vulcan__&lt;model schema&gt;</code> physical schema will be created for each model schema</li> </ul> <p>This means that given a catalog of <code>warehouse</code> and a model named <code>finance_mart.transaction_events_over_threshold</code>, Vulcan will create physical tables using the following convention:</p> <pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;schema&gt;__&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.finance_mart__transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre> <p>This deliberately contains some redundancy with the model schema as it's repeated at the physical layer in both the physical schema name as well as the physical table name.</p> <p>This default exists to make the physical table names portable between different configurations. If you were to define a <code>physical_schema_mapping</code> that maps all models to the same physical schema, since the model schema is included in the table name as well, there are no naming conflicts.</p>"},{"location":"guides-old/configuration/#table-only","title":"Table only","text":"<p>Some engines have object name length limitations which cause them to silently truncate table and view names that exceed this limit. This behaviour breaks Vulcan, so we raise a runtime error if we detect the engine would silently truncate the name of the table we are trying to create.</p> <p>Having redundancy in the physical table names does reduce the number of characters that can be utilised in model names. To increase the number of characters available to model names, you can use <code>physical_table_naming_convention</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: table_only\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.TABLE_ONLY,\n)\n</code></pre> <p>This will cause Vulcan to omit the model schema from the table name and generate physical names that look like (using the above example): </p><pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre><p></p> <p>Notice that the model schema name is no longer part of the physical table name. This allows for slightly longer model names on engines with low identifier length limits, which may be useful for your project.</p> <p>In this configuration, it is your responsibility to ensure that any schema overrides in <code>physical_schema_mapping</code> result in each model schema getting mapped to a unique physical schema.</p> <p>For example, the following configuration will cause data corruption:</p> <pre><code>physical_table_naming_convention: table_only\nphysical_schema_mapping:\n  '.*': vulcan\n</code></pre> <p>This is because every model schema is mapped to the same physical schema but the model schema name is omitted from the physical table name.</p>"},{"location":"guides-old/configuration/#md5-hash","title":"MD5 hash","text":"<p>If you still need more characters, you can set <code>physical_table_naming_convention: hash_md5</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: hash_md5\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.HASH_MD5,\n)\n</code></pre> <p>This will cause Vulcan generate physical names that are always 45-50 characters in length and look something like:</p> <pre><code># vulcan_md5__&lt;hash of what we would have generated using 'schema_and_table'&gt;\n\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00\n\n# or, for a dev preview\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00__dev\n</code></pre> <p>This has a downside that now it's much more difficult to determine which table corresponds to which model by just looking at the database with a SQL client. However, the table names have a predictable length so there are no longer any surprises with identfiers exceeding the max length at the physical layer.</p>"},{"location":"guides-old/configuration/#virtual-data-environment-modes","title":"Virtual Data Environment Modes","text":"<p>By default, Virtual Data Environments (VDE) are applied across both development and production environments. This allows Vulcan to reuse physical tables when appropriate, even when promoting from development to production.</p> <p>However, users may prefer their production environment to be non-virtual. The non-exhaustive list of reasons may include:</p> <ul> <li>Integration with third-party tools and platforms, such as data catalogs, may not work well with the virtual view layer that Vulcan imposes by default</li> <li>A desire to rely on time travel features provided by cloud data warehouses such as BigQuery, Snowflake, and Databricks</li> </ul> <p>To mitigate this, Vulcan offers an alternative 'dev-only' mode for using VDE. It can be enabled in the project configuration like so:</p> YAMLPython <pre><code>virtual_environment_mode: dev_only\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    virtual_environment_mode=\"dev_only\",\n)\n</code></pre> <p>'dev-only' mode means that VDE is applied only in development environments. While in production, model tables and views are updated directly and bypass the virtual layer. This also means that physical tables in production will be created using the original, unversioned model names. Users will still benefit from VDE and data reuse across development environments.</p> <p>Please note the following tradeoffs when enabling this mode:</p> <ul> <li>All data inserted in development environments is used only for preview and will not be reused in production</li> <li>Reverting a model to a previous version will be applied going forward and may require an explicit data restatement</li> </ul> <p>Warning</p> <p>Switching the mode for an existing project will result in a complete rebuild of all models in the project. Refer to the Table Migration Guide to migrate existing tables without rebuilding them from scratch.</p>"},{"location":"guides-old/configuration/#environment-view-catalogs","title":"Environment view catalogs","text":"<p>By default, Vulcan creates an environment view in the same catalog as the physical table the view points to. The physical table's catalog is determined by either the catalog specified in the model name or the default catalog defined in the connection.</p> <p>It can be desirable to create <code>prod</code> and non-prod virtual layer objects in separate catalogs instead. For example, there might be a \"prod\" catalog that contains all <code>prod</code> environment views and a separate \"dev\" catalog that contains all <code>dev</code> environment views.</p> <p>Separate prod and non-prod catalogs can also be useful if you have a CI/CD pipeline that creates environments, like the Vulcan Github Actions CI/CD Bot. You might want to store the CI/CD environment objects in a dedicated catalog since there can be many of them.</p> <p>Virtual layer only</p> <p>Note that the following setting only affects the virtual layer. If you need full segregation by catalog between environments in the physical layer as well, see the Isolated Systems Guide.</p> <p>To configure separate catalogs, provide a mapping from regex patterns to catalog names. Vulcan will compare the name of an environment to the regex patterns; when it finds a match it will store the environment's objects in the corresponding catalog.</p> <p>Vulcan evaluates the regex patterns in the order defined in the configuration; it uses the catalog for the first matching pattern. If no match is found, the catalog defined in the model or the default catalog defined on the connection will be used.</p> <p>Config example:</p> YAMLPython <pre><code>environment_catalog_mapping:\n  '^prod$': prod\n  '^dev.*': dev\n  '^analytics_repo.*': cicd\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_catalog_mapping={\n        '^prod$': 'prod',\n        '^dev.*': 'dev',\n        '^analytics_repo.*': 'cicd',\n    },\n)\n</code></pre> <p>With the example configuration above, Vulcan would evaluate environment names as follows:</p> <ul> <li>If the environment name is <code>prod</code>, the catalog will be <code>prod</code>.</li> <li>If the environment name starts with <code>dev</code>, the catalog will be <code>dev</code>.</li> <li>If the environment name starts with <code>analytics_repo</code>, the catalog will be <code>cicd</code>.</li> </ul> <p>Warning</p> <p>This feature is mutually exclusive with <code>environment_suffix_target: catalog</code> in order to prevent ambiguous mappings from being defined. Attempting to specify both <code>environment_catalog_mapping</code> and <code>environment_suffix_target: catalog</code> will raise an error on project load</p> <p>Note: This feature is only available for engines that support querying across catalogs. At the time of writing, the following engines are NOT supported:</p> <ul> <li>MySQL</li> <li>Postgres</li> <li>GCP Postgres</li> </ul>"},{"location":"guides-old/configuration/#regex-tips","title":"Regex Tips","text":"<ul> <li>If you are less familiar with regex, you can use a tool like regex101 to help you build your regex patterns.<ul> <li>LLMs, like ChatGPT, can help with generating regex patterns. Make sure to validate the suggestion in regex101.</li> </ul> </li> <li>If you are wanting to do an exact word match then surround it with <code>^</code> and <code>$</code> like in the example above.</li> <li>If you want a catch-all at the end of your mapping, to avoid ever using the model catalog or default catalog, then use <code>.*</code> as the pattern. This will match any environment name that hasn't already been matched.</li> </ul>"},{"location":"guides-old/configuration/#auto-categorize-model-changes","title":"Auto-categorize model changes","text":"<p>Vulcan compares the current state of project files to an environment when <code>vulcan plan</code> is run. It detects changes to models, which can be classified as breaking or non-breaking.</p> <p>Vulcan can  attempt to automatically categorize the changes it detects. The <code>plan.auto_categorize_changes</code> option determines whether Vulcan should attempt automatic change categorization. This option is in the plan section of the configuration reference page.</p> <p>Supported values:</p> <ul> <li><code>full</code>: Never prompt the user for input, instead fall back to the most conservative category (breaking) if the category can't be determined automatically.</li> <li><code>semi</code>: Prompt the user for input only if the change category can't be determined automatically.</li> <li><code>off</code>: Always prompt the user for input; automatic categorization will not be attempted.</li> </ul> <p>Example showing default values:</p> YAMLPython <pre><code>plan:\n  auto_categorize_changes:\n    external: full\n    python: off\n    sql: full\n    seed: full\n</code></pre> <p>The Python <code>auto_categorize_changes</code> argument takes <code>CategorizerConfig</code> object. That object's arguments take an <code>AutoCategorizationMode</code> enumeration with values of <code>AutoCategorizationMode.FULL</code>, <code>AutoCategorizationMode.SEMI</code>, or <code>AutoCategorizationMode.OFF</code>.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    AutoCategorizationMode,\n    CategorizerConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        auto_categorize_changes=CategorizerConfig(\n            external=AutoCategorizationMode.FULL,\n            python=AutoCategorizationMode.OFF,\n            sql=AutoCategorizationMode.FULL,\n            seed=AutoCategorizationMode.FULL,\n        )\n    ),\n)\n</code></pre>"},{"location":"guides-old/configuration/#always-comparing-against-production","title":"Always comparing against production","text":"<p>By default, Vulcan compares the current state of project files to the target <code>&lt;env&gt;</code> environment when <code>vulcan plan &lt;env&gt;</code> is run. However, a common expectation is that local changes should always be compared to the production environment.</p> <p>The <code>always_recreate_environment</code> boolean plan option can alter this behavior. When enabled, Vulcan will always attempt to compare against the production environment by recreating the target environment; If <code>prod</code> does not exist, Vulcan will fall back to comparing against the target environment.</p> <p>NOTE:: Upon succesfull plan application, changes are still promoted to the target <code>&lt;env&gt;</code> environment.</p> YAMLPython <pre><code>plan:\n    always_recreate_environment: True\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        always_recreate_environment=True,\n    ),\n)\n</code></pre>"},{"location":"guides-old/configuration/#change-categorization-example","title":"Change Categorization Example","text":"<p>Consider this scenario with <code>always_recreate_environment</code> enabled:</p> <ol> <li> <p>Initial state in <code>prod</code>: </p><pre><code>MODEL (name vulcan_example.test_model, kind FULL);\nSELECT 1 AS col\n</code></pre><p></p> </li> <li> <p>First (breaking) change in <code>dev</code>: </p><pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL);\nSELECT 2 AS col\n</code></pre><p></p> </li> </ol> Output plan example #1 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n+++\n\n\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n</code></pre> <ol> <li>Second (metadata) change in <code>dev</code>: <pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL, owner 'John Doe');\nSELECT 5 AS col\n</code></pre></li> </ol> Output plan example #2 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n\n+++\n\n@@ -1,8 +1,9 @@\n\nMODEL (\nname vulcan_example.test_model,\n+  owner \"John Doe\",\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n\nDirectly Modified: vulcan_example__dev.test_model (Breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.test_model: [full refresh]\n</code></pre> <p>Even though the second change should have been a metadata change (thus not requiring a backfill), it will still be classified as a breaking change because the comparison is against production instead of the previous development state. This is intentional and may cause additional backfills as more changes are accumulated.</p>"},{"location":"guides-old/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> configuration defines how Vulcan should connect to the data warehouse, state backend, and scheduler. These options are in the gateway section of the configuration reference page.</p> <p>Each gateway key represents a unique gateway name and configures its connections. Gateway names are case-insensitive - Vulcan automatically normalizes gateway names to lowercase during configuration validation. This means you can use any case in your configuration files (e.g., <code>MyGateway</code>, <code>mygateway</code>, <code>MYGATEWAY</code>) and they will all work correctly.</p> <p>For example, this configures the <code>my_gateway</code> gateway:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      ...\n    state_connection:\n      ...\n    test_connection:\n      ...\n    scheduler:\n      ...\n</code></pre> <p>The Python <code>gateways</code> argument takes a dictionary of gateway names and <code>GatewayConfig</code> objects. A <code>GatewayConfig</code>'s connection-related arguments take an engine-specific connection config object, and the <code>scheduler</code> argument takes a scheduler config object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    ...\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=...,\n            state_connection=...,\n            test_connection=...,\n            scheduler=...,\n        ),\n    }\n)\n</code></pre> <p>Gateways do not need to specify all four components in the example above. The gateway defaults options control what happens if they are not all specified - find more information on gateway defaults below.</p>"},{"location":"guides-old/configuration/#connections","title":"Connections","text":"<p>The <code>connection</code> configuration controls the data warehouse connection. These options are in the connection section of the configuration reference page.</p> <p>The allowed keys include:</p> <ul> <li>The optional <code>concurrent_tasks</code> key specifies the maximum number of concurrent tasks Vulcan will run. Default value is 4 for engines that support concurrent tasks.</li> <li>Most keys are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</li> </ul> <p>Example snowflake connection configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>A Snowflake connection is specified with a <code>SnowflakeConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#engine-connection-configuration","title":"Engine connection configuration","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Athena</li> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>Fabric</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides-old/configuration/#state-connection","title":"State connection","text":"<p>Configuration for the state backend connection if different from the data warehouse connection.</p> <p>The data warehouse connection is used to store Vulcan state if the <code>state_connection</code> key is not specified.</p> <p>Unlike data transformations, storing state information requires database transactions. Data warehouses aren\u2019t optimized for executing transactions, and storing state information in them can slow down your project or produce corrupted data due to simultaneous writes to the same table. Therefore, production Vulcan deployments should use a dedicated state connection.</p> <p>Note</p> <p>Using the same connection for data warehouse and state is not recommended for production deployments of Vulcan.</p> <p>The easiest and most reliable way to manage your state connection is for Tobiko Cloud to do it for you. If you'd rather handle it yourself, we list recommended and unsupported state engines below.</p> <p>Recommended state engines for production deployments:</p> <ul> <li>Postgres</li> <li>GCP Postgres</li> </ul> <p>Other state engines with fast and reliable database transactions (less tested than the recommended engines):</p> <ul> <li>DuckDB<ul> <li>With the caveat that it's a single user database so will not scale to production usage</li> </ul> </li> <li>MySQL</li> <li>MSSQL</li> </ul> <p>Unsupported state engines, even for development:</p> <ul> <li>ClickHouse</li> <li>Spark</li> <li>Trino</li> </ul> <p>This example gateway configuration uses Snowflake for the data warehouse connection and Postgres for the state backend connection:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      # snowflake credentials here\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n    state_connection:\n      # postgres credentials here\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n</code></pre> <p>A Postgres connection is specified with a <code>PostgresConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            # snowflake credentials here\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n            # postgres credentials here\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#state-schema-name","title":"State schema name","text":"<p>By default, the schema name used to store state tables is <code>vulcan</code>. This can be changed by providing the <code>state_schema</code> config key in the gateway configuration.</p> <p>Example configuration to store state information in a postgres database's <code>custom_name</code> schema:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    state_connection:\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n    state_schema: custom_name\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n            state_schema=\"custom_name\",\n        ),\n    }\n)\n</code></pre> <p>This would create all state tables in the schema <code>custom_name</code>.</p>"},{"location":"guides-old/configuration/#test-connection","title":"Test connection","text":"<p>Configuration for a connection used to run unit tests. An in-memory DuckDB database is used if the <code>test_connection</code> key is not specified.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    test_connection:\n      type: duckdb\n</code></pre> <p>A DuckDB connection is specified with a <code>DuckDBConnectionConfig</code> object. A <code>DuckDBConnectionConfig</code> with no arguments specified uses an in-memory DuckDB database.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            test_connection=DuckDBConnectionConfig(),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans. By default, the scheduler type is set to <code>builtin</code>, which uses the existing SQL engine to store metadata.</p> <p>These options are in the scheduler section of the configuration reference page.</p>"},{"location":"guides-old/configuration/#builtin","title":"Builtin","text":"<p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    scheduler:\n      type: builtin\n</code></pre> <p>A built-in scheduler is specified with a <code>BuiltInSchedulerConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    BuiltInSchedulerConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            scheduler=BuiltInSchedulerConfig(),\n        ),\n    }\n)\n</code></pre> <p>No additional configuration options are supported by this scheduler type.</p>"},{"location":"guides-old/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. These options are in the gateway/connection defaults section of the configuration reference page.</p> <p>The gateway specified in <code>default_gateway</code> is used when a <code>vulcan</code> command does not explicitly specify a gateway. All Vulcan CLI commands accept a gateway option after <code>vulcan</code> and before the command name; for example, <code>vulcan --gateway my_gateway plan</code>. If the option is not specified in a command call, the <code>default_gateway</code> is used.</p> <p>The three default connection types are used when some gateways in the <code>gateways</code> configuration dictionaries do not specify every connection type.</p>"},{"location":"guides-old/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> <p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    &lt;gateway specification&gt;\ndefault_gateway: my_gateway\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            &lt;gateway specification&gt;\n        ),\n    },\n    default_gateway=\"my_gateway\",\n)\n</code></pre>"},{"location":"guides-old/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> <p>Example configuration specifying a Postgres default connection, in-memory DuckDB default test connection, and builtin default scheduler:</p> YAMLPython <pre><code>default_connection:\n  type: postgres\n  host: &lt;host&gt;\n  port: &lt;port&gt;\n  user: &lt;username&gt;\n  password: &lt;password&gt;\n  database: &lt;database&gt;\ndefault_test_connection:\n  type: duckdb\ndefault_scheduler:\n  type: builtin\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PostgresConnectionConfig,\n    DuckDBConnectionConfig,\n    BuiltInSchedulerConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    default_connection=PostgresConnectionConfig(\n        host=&lt;host&gt;,\n        port=&lt;port&gt;,\n        user=&lt;username&gt;,\n        password=&lt;password&gt;,\n        database=&lt;database&gt;,\n    ),\n    default_test_connection=DuckDBConnectionConfig(),\n    default_scheduler=BuiltInSchedulerConfig(),\n)\n</code></pre>"},{"location":"guides-old/configuration/#models","title":"Models","text":""},{"location":"guides-old/configuration/#model-defaults","title":"Model defaults","text":"<p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed. Other values are set automatically unless explicitly overridden in the model definition.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference page.</p> <p>Example configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless overridden with the <code>kind</code> key. For more information on model kinds, refer to model concepts page.</p>"},{"location":"guides-old/configuration/#identifier-resolution","title":"Identifier resolution","text":"<p>When a SQL engine receives a query such as <code>SELECT id FROM \"some_table\"</code>, it eventually needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This process is usually referred to as identifier (or name) resolution.</p> <p>Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.</p> <p>Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it normalizes and quotes all identifiers in those queries, respecting each dialect's resolution rules.</p> <p>The \"normalization strategy\", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.</p> <p>See here to learn more about the supported normalization strategies.</p>"},{"location":"guides-old/configuration/#gateway-specific-model-defaults","title":"Gateway-specific model defaults","text":"<p>You can also define gateway specific <code>model_defaults</code> in the <code>gateways</code> section, which override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This allows you to tailor the behavior of models for each gateway without affecting the global <code>model_defaults</code>.</p> <p>For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.</p> <p>Gateway-specific <code>model_defaults</code> allow you to change how Vulcan performs identifier normalization by engine to align the different engines' behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway configuration overrides that global default dialect with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>That value tells Vulcan that the <code>redshift</code> gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"guides-old/configuration/#model-kinds","title":"Model Kinds","text":"<p>Model kinds are required in each model file's <code>MODEL</code> DDL statement. They may optionally be used to specify a default kind in the model defaults configuration key.</p> <p>All model kind specification keys are listed in the models configuration reference page.</p> <p>The <code>VIEW</code>, <code>FULL</code>, and <code>EMBEDDED</code> model kinds are specified by name only, while other models kinds require additional parameters and are provided with an array of parameters:</p> YAML <p><code>FULL</code> model only requires a name:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  kind FULL\n);\n</code></pre> <p><code>INCREMENTAL_BY_TIME_RANGE</code> requires an array specifying the model's <code>time_column</code> (which should be in the UTC time zone):</p> <pre><code>MODEL(\n  name docs_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column model_time_column\n  )\n);\n</code></pre> <p>Python model kinds are specified with model kind objects. Python model kind objects have the same arguments as their SQL counterparts, listed in the models configuration reference page.</p> <p>This example demonstrates how to specify an incremental by time range model kind in Python:</p> Python <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan.core.model.kind import ModelKindName\n\n@model(\n    \"docs_example.incremental_model\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"ds\"\n    )\n)\n</code></pre> <p>Learn more about specifying Python models at the Python models concepts page.</p>"},{"location":"guides-old/configuration/#model-naming","title":"Model Naming","text":"<p>The <code>model_naming</code> configuration controls if model names are inferred based on the project's directory structure. If <code>model_naming</code> is not defined or <code>infer_names</code> is set to false, the model names must be provided explicitly.</p> <p>With <code>infer_names</code> set to true, model names are inferred based on their path. For example, a model located at <code>models/catalog/schema/model.sql</code> would be named <code>catalog.schema.model</code>. However, if a name is provided in the model definition, it will take precedence over the inferred name.</p> <p>Example enabling name inference:</p> YAMLPython <pre><code>model_naming:\n  infer_names: true\n</code></pre> <pre><code>from vulcan.core.config import Config, NameInferenceConfig\n\nconfig = Config(\n    model_naming=NameInferenceConfig(\n        infer_names=True\n    )\n)\n</code></pre>"},{"location":"guides-old/configuration/#before_all-and-after_all-statements","title":"Before_all and after_all Statements","text":"<p>The <code>before_all</code> and <code>after_all</code> statements are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands.</p> <p>These statements can be defined in the configuration file under the <code>before_all</code> and <code>after_all</code> keys, either as a list of SQL statements or by using Vulcan macros:</p> YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\nafter_all:\n  - \"@grant_select_privileges()\"\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all = [\n        \"CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\"\n    ],\n    after_all = [\n        \"@grant_select_privileges()\",\n        \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n    ],\n)\n</code></pre>"},{"location":"guides-old/configuration/#examples","title":"Examples","text":"<p>These statements allow for actions to be executed before all individual model statements or after all have run, respectively. They can also simplify tasks such as granting privileges.</p>"},{"location":"guides-old/configuration/#example-granting-select-privileges","title":"Example: Granting Select Privileges","text":"<p>For example, rather than using an <code>on_virtual_update</code> statement in each model to grant privileges on the views of the virtual layer, a single macro can be defined and used at the end of the plan:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    if evaluator.views:\n        return [\n            f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE admin_role;\"\n            for view_name in evaluator.views\n        ]\n</code></pre> <p>By including the comment <code>/* sqlglot.meta replace=false */</code>, you further ensure that the evaluator does not replace the view name with the physical table name during rendering.</p>"},{"location":"guides-old/configuration/#example-granting-schema-privileges","title":"Example: Granting Schema Privileges","text":"<p>Similarly, you can define a macro to grant schema usage privileges and, as demonstrated in the configuration above, using <code>this_env</code> macro conditionally execute it only in the production environment.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    if evaluator.this_env == \"prod\" and evaluator.schemas:\n        return [\n            f\"GRANT USAGE ON SCHEMA {schema} TO admin_role;\"\n            for schema in evaluator.schemas\n        ]\n</code></pre> <p>As demonstrated in these examples, the <code>schemas</code>  and <code>views</code> are available within the macro evaluator for macros invoked within the <code>before_all</code> and <code>after_all</code> statements. Additionally, the macro <code>this_env</code> provides access to the current environment name, which can be helpful for more advanced use cases that require fine-grained control over their behaviour.</p>"},{"location":"guides-old/configuration/#linting","title":"Linting","text":"<p>Vulcan provides a linter that checks for potential issues in your models' code. Enable it and specify which linting rules to apply in the configuration file's <code>linter</code> key.</p> <p>Learn more about linting configuration in the linting guide.</p>"},{"location":"guides-old/configuration/#debug-mode","title":"Debug mode","text":"<p>To enable debug mode set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</p> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"guides-old/configuration/#python-library-dependencies","title":"Python library dependencies","text":"<p>Vulcan enables you to write Python models and macros which depend on third-party libraries. To ensure each run / evaluation uses the same version, you can specify versions in a <code>vulcan-requirements.lock</code> file in the root of your project.</p> <p>The vulcan.lock must be of the format <code>dep==version</code>. Only <code>==</code> is supported.</p> <p>For example:</p> <pre><code>numpy==2.1.2\npandas==2.2.3\n</code></pre> <p>This feature is only available in Tobiko Cloud.</p>"},{"location":"guides-old/configuration/#excluding-dependencies","title":"Excluding dependencies","text":"<p>You can exclude dependencies by prefixing the dependency with a <code>^</code>. For example:</p> <pre><code>^numpy\npandas==2.2.3\n</code></pre>"},{"location":"guides-old/connections/","title":"Connections","text":""},{"location":"guides-old/connections/#connections","title":"Connections","text":""},{"location":"guides-old/connections/#overview","title":"Overview","text":"<p>In order to deploy models and to apply changes to them, you must configure a connection to your Data Warehouse and, optionally, connection to the database where the Vulcan state is stored. This can be done in either the <code>config.yaml</code> file in your project folder, or the one in <code>~/.vulcan</code>.</p> <p>Each connection is configured as part of a gateway which has a unique name associated with it. The gateway name can be used to select a specific combination of connection settings  when using the CLI. For example:</p> <pre><code>gateways:\n  local_db:\n    connection:\n      type: duckdb\n</code></pre> <p>Now the defined connection can be selected in the <code>vulcan plan</code> CLI command as follows:</p> <pre><code>vulcan --gateway local_db plan\n</code></pre>"},{"location":"guides-old/connections/#state-connection","title":"State connection","text":"<p>By default, the data warehouse connection is also used to store the Vulcan state.</p> <p>The state connection can be changed by providing different connection settings in the <code>state_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    state_connection:\n      type: duckdb\n      database: state.db\n</code></pre> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p>"},{"location":"guides-old/connections/#default-connection","title":"Default connection","text":"<p>Additionally, you can set a default connection by defining its configuration in the <code>default_connection</code> key:</p> <pre><code>default_connection:\n  type: duckdb\n  database: local.db\n</code></pre> <p>This connection configuration will be used if one is not provided in the target gateway.</p>"},{"location":"guides-old/connections/#test-connection","title":"Test connection","text":"<p>By default, when running tests, Vulcan uses an in-memory DuckDB database connection. You can override this behavior by providing connection settings in the <code>test_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    test_connection:\n      type: duckdb\n      database: test.db\n</code></pre>"},{"location":"guides-old/connections/#default-test-connection","title":"Default test connection","text":"<p>To configure a default test connection for all gateways use the <code>default_test_connection</code> key:</p> <pre><code>default_test_connection:\n  type: duckdb\n  database: test.db\n</code></pre>"},{"location":"guides-old/connections/#default-gateway","title":"Default gateway","text":"<p>To change the default gateway used by the CLI when no gateway name is provided, set the desired name in the <code>default_gateway</code> key:</p> <pre><code>default_gateway: local_db\n</code></pre>"},{"location":"guides-old/connections/#supported-engines","title":"Supported engines","text":"<ul> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides-old/customizing_vulcan/","title":"Customizing Vulcan","text":""},{"location":"guides-old/customizing_vulcan/#customizing-vulcan","title":"Customizing Vulcan","text":"<p>Vulcan supports the workflows used by the vast majority of data engineering teams. However, your company may have bespoke processes or tools that require special integration with Vulcan.</p> <p>Fortunately, Vulcan is an open-source Python library, so you can view its underlying code and customize it for your needs.</p> <p>Customization generally involves subclassing Vulcan classes to extend or modify their functionality.</p> <p>Caution</p> <p>Customize Vulcan with extreme caution. Errors may cause Vulcan to produce unexpected results.</p>"},{"location":"guides-old/customizing_vulcan/#custom-loader","title":"Custom loader","text":"<p>Loading is the process of reading project files and converting their contents into Vulcan's internal Python objects.</p> <p>The loading stage is a convenient place to customize Vulcan behavior because you can access a project's objects after they've been ingested from file but before Vulcan uses them.</p> <p>Vulcan's <code>VulcanLoader</code> class handles the loading process - customize it by subclassing it and overriding its methods.</p> <p>Python configuration only</p> <p>Custom loaders require using the Python configuration format (YAML is not supported).</p>"},{"location":"guides-old/customizing_vulcan/#modify-every-model","title":"Modify every model","text":"<p>One reason to customize the loading process is to do something to every model. For example, you might want to add a post-statement to every model.</p> <p>The loading process parses all model SQL statements, so new or modified SQL must be parsed by SQLGlot before being passed to a model object.</p> <p>This custom loader example adds a post-statement to every model:</p> config.py<pre><code>from vulcan.core.loader import VulcanLoader\nfrom vulcan.utils import UniqueKeyDict\nfrom vulcan.core.dialect import parse_one\nfrom vulcan.core.config import Config\n\n# New `CustomLoader` class subclasses `VulcanLoader`\nclass CustomLoader(VulcanLoader):\n    # Override VulcanLoader's `_load_models` method to access every model\n    def _load_models(\n        self,\n        macros: \"MacroRegistry\",\n        jinja_macros: \"JinjaMacroRegistry\",\n        gateway: str | None,\n        audits: UniqueKeyDict[str, \"ModelAudit\"],\n        signals: UniqueKeyDict[str, \"signal\"],\n    ) -&gt; UniqueKeyDict[str, \"Model\"]:\n        # Call VulcanLoader's normal `_load_models` method to ingest models from file and parse model SQL\n        models = super()._load_models(macros, jinja_macros, gateway, audits, signals)\n\n        new_models = {}\n        # Loop through the existing model names/objects\n        for model_name, model in models.items():\n            # Create list of existing and new post-statements\n            new_post_statements = [\n                # Existing post-statements from model object\n                *model.post_statements,\n                # New post-statement is raw SQL, so we parse it with SQLGlot's `parse_one` function.\n                # Make sure to specify the SQL dialect if different from the project default.\n                parse_one(f\"VACUUM @this_model\"),\n            ]\n            # Create a copy of the model with the `post_statements_` field updated\n            new_models[model_name] = model.copy(update={\"post_statements_\": new_post_statements})\n\n        return new_models\n\n# Pass the CustomLoader class to the Vulcan configuration object\nconfig = Config(\n    # &lt; your configuration parameters here &gt;,\n    loader=CustomLoader,\n)\n</code></pre>"},{"location":"guides-old/isolated_systems/","title":"Isolated systems","text":""},{"location":"guides-old/isolated_systems/#isolated-systems","title":"Isolated systems","text":"<p>Vulcan is optimized for use in systems where developers have access to production data.</p> <p>Writing code against partial or unrepresentative data can cause problems because you don't become aware of changes in production data until errors have already occurred.</p> <p>Other data products, such as machine learning models, may depend on the distribution of values in the training data - building them on unrepresentative data may lead to different behavior in production than in development.</p> <p>However, some companies store production and non-production data in different data warehouses that can't talk to one another (\"isolated systems\"). This is usually due to information security concerns, as the non-production warehouse may be accessible to more users and/or have looser security restrictions.</p> <p>This guide explains how to use Vulcan with isolated systems and how isolating systems affects Vulcan's behavior.</p>"},{"location":"guides-old/isolated_systems/#terminology","title":"Terminology","text":"<p>Isolated systems are sometimes referred to as \"isolated environments,\" but we avoid that term because \"environments\" has a specific meaning in Vulcan.</p> <p>Instead, we will refer to them as isolated systems - the \"production system\" and \"non-production system.\"</p> <p>When we refer to \"environments,\" we are always talking about Vulcan environments - the isolated namespaces created and managed by Vulcan.</p>"},{"location":"guides-old/isolated_systems/#configuring-vulcan","title":"Configuring Vulcan","text":""},{"location":"guides-old/isolated_systems/#separate-state-data","title":"Separate state data","text":"<p>Vulcan maintains a record of every model version so it can identify changes when models are updated. Those records are called \"state\" data, as in \"the state of the model at that point in time.\"</p> <p>State data can be stored alongside other data in the primary data warehouse or in a separate database. We recommend using a separate transactional database for projects running on cloud SQL engines.</p> <p>Isolated systems must use a separate state database for each system. The state of models and other objects in the non-production system is not accurate for the production system, and sharing state data will prevent the project from running correctly.</p>"},{"location":"guides-old/isolated_systems/#multiple-gateways","title":"Multiple gateways","text":"<p>Vulcan database connections are configured with gateways that contain connections and other configuration parameters.</p> <p>A gateway must contain a connection to a SQL engine and may optionally contain a different connection to the database where Vulcan should store its state data.</p> <p>Isolated systems should configure two separate gateways: one for the production system and one for the non-production system.</p> <p>For example, this configuration creates gateways named <code>nonproduction</code> and <code>production</code>. You may omit the <code>state_connection</code> keys if state data will be stored in the gateway's primary connection.</p> <pre><code>gateways:\n  nonproduction:\n    connection:\n      ...[your non-production connection parameters]...\n    state_connection:\n      ...[your non-production state connection parameters]...\n  production:\n    connection:\n      ...[your production connection parameters]...\n    state_connection:\n      ...[your production state connection parameters]...\n</code></pre> <p>Vulcan will use the first gateway in the configuration as the default when executing a command. For example, with the configuration above Vulcan would use the <code>nonproduction</code> gateway when executing the command <code>vulcan plan</code>.</p> <p>Commands can override the default gateway with the <code>--gateway</code> option, such as <code>vulcan --gateway production plan</code>.</p>"},{"location":"guides-old/isolated_systems/#gateway-specific-schemas","title":"Gateway-specific schemas","text":"<p>We recommend using identical schema and model names in both systems, but in some scenarios that is not possible.</p> <p>Schema and model names may be parameterized by gateway using the predefined <code>@gateway</code> macro variable.</p> <p>This example demonstrates conditioning the model schema name on the current gateway with the Vulcan <code>@IF</code> macro operator. If the gateway is named <code>production</code>, <code>my_model</code>'s schema is <code>prod_schema</code>; otherwise, it is <code>dev_schema</code>.</p> <pre><code>MODEL (\n  name @IF(@gateway = 'production', prod_schema, dev_schema).my_model\n)\n</code></pre> <p>To embed the gateway name directly in the schema name, use the curly brace <code>@{gateway}</code> syntax:</p> <pre><code>MODEL (\n  name @{gateway}_schema.my_model\n)\n</code></pre> <p>Learn more about the curly brace <code>@{}</code> syntax here.</p>"},{"location":"guides-old/isolated_systems/#workflow","title":"Workflow","text":""},{"location":"guides-old/isolated_systems/#linking-systems","title":"Linking systems","text":"<p>The point of isolating systems is to prevent sharing of data by limiting network communications between the systems. Given this, how can a Vulcan project be shared between them at all?</p> <p>The Vulcan project files provide the link between the systems. The files should be stored in a mutually accessible location, such as a git repository.</p> <p></p>"},{"location":"guides-old/isolated_systems/#workflow-with-one-system","title":"Workflow with one system","text":"<p>This section describes workflows for updating Vulcan projects with one system.</p> <p>We assume that a version of the Vulcan project is currently running in production and serves as the starting point for code modifications.</p>"},{"location":"guides-old/isolated_systems/#basic-workflow","title":"Basic workflow","text":"<p>Use this workflow if your data system does not use CI/CD to implement changes:</p> <ul> <li>Make a change to a model</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in a local environment</li> <li>Run <code>vulcan plan</code> to apply the changes to the <code>prod</code> environment</li> </ul>"},{"location":"guides-old/isolated_systems/#cicd-workflow","title":"CI/CD workflow","text":"<p>Use this workflow with the Vulcan Github CI/CD bot:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Push the branch to the project repo and make a pull request. The bot will create a development environment for you to preview the changes if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>Learn more about synchronized and desynchronized deployments in the CI/CD bot documentation.</p>"},{"location":"guides-old/isolated_systems/#reusing-computations","title":"Reusing computations","text":"<p>Local environment previews are computed on the same data used by the <code>prod</code> environment in these workflows, so applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p>"},{"location":"guides-old/isolated_systems/#workflow-with-isolated-systems","title":"Workflow with isolated systems","text":"<p>This section describes the workflow with isolated systems.</p> <p>This workflow combines the basic and CI/CD workflows above, where the basic workflow is used in the non-production system and the CI/CD workflow is used in the production system:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in the nonproduction system. You may need to include the nonproduction <code>--gateway</code> option, depending on your project configuration.</li> <li>Push the branch to the project repo and make a pull request. The bot will create an environment to preview the changes in the production system if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>The breaking/non-breaking change classifications in the non-production system will not be available to the production system because the systems do not share Vulcan state data. Therefore, the classifications must occur again in the production system.</p>"},{"location":"guides-old/isolated_systems/#reusing-computations_1","title":"Reusing computations","text":"<p>In isolated systems, Vulcan's virtual data environments operate normally within each system, but not across systems.</p> <p>In the non-production system, computations will be reused across preview environments. However, the system's data are not representative of the production data and will not be reused by the production system.</p> <p>In the production system, the CI/CD bot will execute the necessary computations when a pull request is submitted if it is configured for synchronized deployment. Merging to main and applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p> <p>This approach enables true blue-green deployment. Deploying to production occurs with no system downtime because virtual updates only require swapping views. If issues are identified after changes have been pushed to production, reverting is quick and painless because it just swaps the views back.</p>"},{"location":"guides-old/migrations/","title":"Migrations","text":""},{"location":"guides-old/migrations/#migrations","title":"Migrations","text":"<p>New versions of Vulcan may be incompatible with the project's stored metadata format. Migrations provide a way to upgrade the project metadata format to operate with the new Vulcan version.</p>"},{"location":"guides-old/migrations/#detecting-incompatibility","title":"Detecting incompatibility","text":"<p>When issuing a Vulcan command, Vulcan will automatically check for incompatibilities between the installed version of Vulcan and the project's metadata format, prompting what action is required. Vulcan commands will not execute until the action is complete.</p>"},{"location":"guides-old/migrations/#installed-version-is-newer-than-metadata-format","title":"Installed version is newer than metadata format","text":"<p>In this scenario, the project's metadata format needs to be migrated.</p> <pre><code>&gt; vulcan plan my_dev\nError: Vulcan (local) is using version '2' which is ahead of '1' (remote). Please run a migration ('vulcan migrate' command).\n</code></pre>"},{"location":"guides-old/migrations/#installed-version-is-older-than-metadata-format","title":"Installed version is older than metadata format","text":"<p>Here, the installed version of Vulcan needs to be upgraded.</p> <pre><code>&gt; vulcan plan my_dev\nVulcanError: Vulcan (local) is using version '1' which is behind '2' (remote). Please upgrade Vulcan.\n</code></pre>"},{"location":"guides-old/migrations/#how-to-migrate","title":"How to migrate","text":""},{"location":"guides-old/migrations/#built-in-scheduler-migrations","title":"Built-in Scheduler Migrations","text":"<p>The project metadata can be migrated to the latest metadata format using Vulcan's migrate command.</p> <pre><code>&gt; vulcan migrate\n</code></pre> <p>Migration should be issued manually by a single user and the migration will affect all users of the project.  Migrations should ideally run when no one will be running plan/apply.  Migrations should not be run in parallel.  Due to these constraints, it is better for a person responsible for managing Vulcan to manually issue migrations.  Therefore, it is not recommended to issue migrations from CI/CD pipelines.</p>"},{"location":"guides-old/notifications/","title":"Notifications","text":""},{"location":"guides-old/notifications/#notifications","title":"Notifications","text":"<p>Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.</p>"},{"location":"guides-old/notifications/#notification-targets","title":"Notification targets","text":"<p>Notifications are configured with <code>notification targets</code>. Targets are specified in a project's configuration file (<code>config.yml</code> or <code>config.py</code>), and multiple targets can be specified for a project.</p> <p>A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each event type (e.g., notifications for <code>run</code> will be sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification target, corresponding to the two Slack notification methods and email notification. They are specified in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"guides-old/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter may be useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"guides-old/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"guides-old/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"guides-old/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n      - apply_failure\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#api-configuration","title":"API Configuration","text":"<p>If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n      - apply_end\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides-old/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, new notification targets can be configured to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>). See the definitions of these classes on Github here.</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides-old/projects/","title":"Projects","text":""},{"location":"guides-old/projects/#projects","title":"Projects","text":""},{"location":"guides-old/projects/#creating-a-project","title":"Creating a project","text":"<p>Before getting started, ensure that you meet the prerequisites for using Vulcan.</p> <p>To create a project from the command line, follow these steps:</p> <ol> <li> <p>Create a directory for your project:</p> <pre><code>mkdir my-project\n</code></pre> </li> <li> <p>Change directories into your new project:</p> <pre><code>cd my-project\n</code></pre> <p>From here, you can create your project structure from scratch, or Vulcan can scaffold one for you. For the purposes of this guide, we'll show you how to scaffold your project so that you can get up and running quickly.</p> </li> <li> <p>To scaffold a project, it is recommended that you use a python virtual environment by running the following commands:</p> <pre><code>python -m venv .venv\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>pip install vulcan\n</code></pre> <p>Note: When using a python virtual environment, you must ensure that it is activated first. You should see <code>(.venv)</code> in your command line; if you don't, run <code>source .venv/bin/activate</code> from your project directory to activate your environment.</p> </li> <li> <p>Once you have activated your environment, run the following command and Vulcan will build out your project:</p> <pre><code>vulcan init [SQL_DIALECT]\n</code></pre> <p>In the command above, you can use any SQL dialect supported by sqlglot, for example \"duckdb\".</p> <p>The following directories and files will be created that you can use to organize your Vulcan project:</p> <ul> <li>config.py (database configuration file)</li> <li>./models (SQL and Python models)</li> <li>./audits (shared audits)</li> <li>./tests (unit tests)</li> <li>./macros</li> </ul> </li> </ol>"},{"location":"guides-old/projects/#editing-an-existing-project","title":"Editing an existing project","text":"<p>To edit an existing project, open the project file you wish to edit in your preferred editor.</p> <p>If using CLI, you can open a file in your project for editing by using the <code>vulcan</code> command with the <code>-p</code> variable, and pointing to your project's path as follows:</p> <pre><code>vulcan -p &lt;your-project-path&gt;\n</code></pre> <p>For more details, refer to CLI</p>"},{"location":"guides-old/table_migration/","title":"Table migration","text":""},{"location":"guides-old/table_migration/#table-migration","title":"Table migration","text":"<p>Vulcan projects can read directly from tables not managed by Vulcan, but in some scenarios it may be useful to migrate an existing table into a Vulcan project.</p> <p>This guide describes two methods for migrating existing tables into a Vulcan project.</p>"},{"location":"guides-old/table_migration/#do-you-need-to-migrate","title":"Do you need to migrate?","text":"<p>Vulcan does not assume it manages all data sources: SQL models can read from any data source accessible by the SQL engine, treating them as external models that include column-level lineage or as generic sources. This approach is preferred to migrating existing tables into a Vulcan project.</p> <p>You should only migrate a table if both of the following are true:</p> <ol> <li>The table is ingesting from an upstream source that will continue generating new data</li> <li>The table is either too large to be rebuilt or cannot be rebuilt because the necessary historical data is unavailable</li> </ol> <p>If the table's upstream source will not generate more data, there is no ongoing activity for Vulcan to manage. A Vulcan model or any other downstream consumer can select directly from the table under its current name.</p> <p>If the table's upstream source is generating new data, we assume that the table is already being loaded incrementally, as there is no need for migration if the table can be fully rebuilt.</p> <p>We describe two migration methods below. The stage and union method is preferred and should be used if feasible.</p>"},{"location":"guides-old/table_migration/#migration-methods","title":"Migration methods","text":"<p>This section describes two methods for migrating tables into Vulcan.</p> <p>The method descriptions contain renaming steps that are only necessary if downstream consumers must select from the original table name (e.g., step 2 in the first example). If that is not the case, the original table can retain its name.</p> <p>The table and model names in the examples below are arbitrary - you may name them whatever is appropriate for your project.</p>"},{"location":"guides-old/table_migration/#stage-and-union","title":"Stage and union","text":"<p>The stage and union method works by treating new and historical data as separate sources.</p> <p>It requires creating an incremental staging model to ingest new records and a <code>VIEW</code> model that unions those records with the existing table's static historical records.</p>"},{"location":"guides-old/table_migration/#example","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the stage and union method consists of five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_historical</code><ul> <li>Optionally, enable column-level lineage for the table by making it an <code>EXTERNAL</code> model and adding it to the project's <code>external_models.yaml</code> file</li> </ul> </li> <li>Create a new incremental staging model named <code>my_schema.existing_table_staging</code> (see below for code)</li> <li>Create a new <code>VIEW</code> model named <code>my_schema.existing_table</code> (see below for code)</li> <li>Run <code>vulcan plan</code> to create and backfill the models</li> </ol> <p>The staging model would contain code similar to the following for an <code>INCREMENTAL_BY_TIME_RANGE</code> model. An <code>INCREMENTAL_BY_UNIQUE_KEY</code> model would have a different <code>kind</code> specification in the <code>MODEL</code> DDL and might not include the query's <code>WHERE</code> clause.</p> <pre><code>MODEL(\n  name my_schema.existing_table_staging,\n  kind INCREMENTAL_BY_TIME_RANGE ( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column\n  )\n);\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre> <p>The primary model would contain code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind VIEW\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_staging -- New data\nUNION\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_historical; -- Historical data\n</code></pre> <p>Changes to columns in the source data or staging model may require modifying the code selecting from the historical data so the two tables can be safely unioned.</p>"},{"location":"guides-old/table_migration/#snapshot-replacement","title":"Snapshot replacement","text":"<p>The snapshot replacement method works by renaming an existing table to a name that Vulcan recognizes as an existing Vulcan model.</p>"},{"location":"guides-old/table_migration/#background","title":"Background","text":"<p>This section briefly describes how Vulcan's virtual data environments, forward-only models, and start times work. This information is not necessary for migrating tables but is necessary for understanding why each step in the migration process is required.</p>"},{"location":"guides-old/table_migration/#virtual-data-environments","title":"Virtual data environments","text":"<p>Conceptually, Vulcan divides the database into a \"physical layer\" where data is stored and a \"virtual layer\" where data is accessed by end users. The physical layer stores materialized objects like tables, and the virtual layer contains views that point to the physical layer objects.</p> <p>Each time a Vulcan <code>plan</code> adds or modifies a model, Vulcan creates a physical layer \"snapshot\" object to which the virtual layer view points. The snapshot replacement method simply renames the migrating table to the name of the appropriate snapshot table.</p>"},{"location":"guides-old/table_migration/#forward-only-models","title":"Forward-only models","text":"<p>Sometimes a model's data may be so large that it is not feasible to rebuild either its own or its downstream models' physical tables. In those situations a  \"forward only\" model can be used. The name reflects that the change is only applied \"going forward\" in time.</p> <p>Historical data already in the migrated table should not be overwritten, so we specify that the new model is forward-only in step 3a below.</p>"},{"location":"guides-old/table_migration/#start-time","title":"Start time","text":"<p>Vulcan incremental by time models track the time periods whose data a model has loaded with the interval approach.</p> <p>The interval approach requires specifying the earliest time interval Vulcan should track - when time \"starts\" for the model. For migrated tables, Vulcan should never load data for the time intervals the table ingested before migration, so interval tracking should start immediately after the time of the last ingested record.</p> <p>In the example below, we set the model's start time in its <code>MODEL</code> DDL (step 3b) and pass it as an option to the <code>vulcan plan</code> command (step 3c). The same value must be used in both the <code>MODEL</code> DDL and the plan command. In this example, the existing table's data ingestion stopped on 2023-12-31, so the model and plan start date is the next day 2024-01-01.</p>"},{"location":"guides-old/table_migration/#example_1","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the snapshot replacement method involves five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_temp</code></li> <li> <p>Create and initialize an empty incremental model named <code>my_schema.existing_table</code>:</p> <p>a. Make the model forward only by setting the <code>MODEL</code> DDL <code>kind</code>'s <code>forward_only</code> key to <code>true</code></p> <p>b. Specify the start of the first time interval Vulcan should track in the <code>MODEL</code> DDL <code>start</code> key (example uses \"2024-01-01\")</p> <p>c. Create the model in the Vulcan project without backfilling any data by running <code>vulcan plan [environment name] --empty-backfill --start 2024-01-01</code>, replacing \"[environment name]\" with an environment name other than <code>prod</code> and using the same start date from the <code>MODEL</code> DDL in step 3b.</p> </li> <li> <p>Determine the name of the model's snapshot physical table by running <code>vulcan table_name --env [environment name] --prod my_schema.existing_table</code>. For example, it might return <code>vulcan__my_schema.existing_table_123456</code>.</p> </li> <li>Rename the original table <code>my_schema.existing_table_temp</code> to <code>vulcan__my_schema.existing_table_123456</code></li> </ol> <p>The model would have code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind INCREMENTAL_BY_TIME_RANGE( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column,\n    forward_only true -- Forward-only model\n  ),\n  -- Start of first time interval Vulcan should track, immediately\n  --  after the last data point the table ingested. Must match\n  --  the value passed to the `vulcan plan --start` option.\n  start \"2024-01-01\"\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre>"},{"location":"references/configuration/","title":"Vulcan configuration","text":""},{"location":"references/configuration/#vulcan-configuration","title":"Vulcan configuration","text":"<p>This page lists Vulcan configuration options and their parameters. </p> <p>Configuration options for model definitions are listed in the model configuration reference page.</p>"},{"location":"references/configuration/#root-configurations","title":"Root configurations","text":"<p>A Vulcan project configuration consists of root level parameters within which other parameters are defined.</p> <p>Two important root level parameters are <code>gateways</code> and gateway/connection defaults, which have their own sections below.</p> <p>This section describes the other root level configuration parameters.</p>"},{"location":"references/configuration/#projects","title":"Projects","text":"<p>Configuration options for Vulcan project directories.</p> Option Description Type Required <code>ignore_patterns</code> Files that match glob patterns specified in this list are ignored when scanning the project folder (Default: <code>[]</code>) list[string] N <code>project</code> The project name of this config. Used for multi-repo setups. string N <code>cache_dir</code> The directory to store the Vulcan cache. Can be an absolute path or relative to the project directory. (Default: <code>.cache</code>) string N <code>log_limit</code> The default number of historical log files to keep (Default: <code>20</code>) int N"},{"location":"references/configuration/#database-physical-layer","title":"Database (Physical Layer)","text":"<p>Configuration options for how Vulcan manages database objects in the physical layer.</p> Option Description Type Required <code>snapshot_ttl</code> The period of time that a model snapshot not a part of any environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>physical_schema_override</code> (Deprecated) Use <code>physical_schema_mapping</code> instead. A mapping from model schema names to names of schemas in which physical tables for the corresponding models will be placed. dict[string, string] N <code>physical_schema_mapping</code> A mapping from regular expressions to names of schemas in which physical tables for the corresponding models will be placed. (Default physical schema name: <code>vulcan__[model schema]</code>) dict[string, string] N <code>physical_table_naming_convention</code> Sets which parts of the model name are included in the physical table names. Options are <code>schema_and_table</code>, <code>table_only</code> or <code>hash_md5</code> - additional details. (Default: <code>schema_and_table</code>) string N"},{"location":"references/configuration/#environments-virtual-layer","title":"Environments (Virtual Layer)","text":"<p>Configuration options for how Vulcan manages environment creation and promotion in the virtual layer.</p> Option Description Type Required <code>environment_ttl</code> The period of time that a development environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>pinned_environments</code> The list of development environments that are exempt from deletion due to expiration list[string] N <code>default_target_environment</code> The name of the environment that will be the default target for the <code>vulcan plan</code> and <code>vulcan run</code> commands. (Default: <code>prod</code>) string N <code>environment_suffix_target</code> Whether Vulcan views should append their environment name to the <code>schema</code>, <code>table</code> or <code>catalog</code> - additional details. (Default: <code>schema</code>) string N <code>gateway_managed_virtual_layer</code> Whether Vulcan views of the virtual layer will be created by the default gateway or model specified gateways. (Default: False) boolean N <code>environment_catalog_mapping</code> A mapping from regular expressions to catalog names. The catalog name is used to determine the target catalog for a given environment. dict[string, string] N <code>virtual_environment_mode</code> Determines the Virtual Data Environment (VDE) mode. If set to <code>full</code>, VDE is used in both production and development environments. The <code>dev_only</code> option enables VDE only in development environments, while in production, no virtual layer is used and models are materialized directly using their original names (i.e., no versioned physical tables). (Default: <code>full</code>) string N"},{"location":"references/configuration/#models","title":"Models","text":"Option Description Type Required <code>time_column_format</code> The default format to use for all model time columns. This time format uses python format codes (Default: <code>%Y-%m-%d</code>) string N <code>infer_python_dependencies</code> Whether Vulcan will statically analyze Python code to automatically infer Python package requirements. (Default: True) boolean N <code>model_defaults</code> Default properties to set on each model. At a minimum, <code>dialect</code> must be set. dict[string, any] Y <p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key.</p> <p>See all the keys allowed in <code>model_defaults</code> at the model configuration reference page.</p>"},{"location":"references/configuration/#variables","title":"Variables","text":"<p>The <code>variables</code> key can be used to provide values for user-defined variables, accessed using the <code>@VAR</code> macro function in SQL model definitions, <code>context.var</code> method in Python model definitions, and <code>evaluator.var</code> method in Python macro functions.</p> <p>The <code>variables</code> key consists of a mapping of variable names to their values - see an example on the Vulcan macros concepts page. Note that keys are case insensitive.</p> <p>Global variable values may be any of the data types in the table below or lists or dictionaries containing those types.</p> Option Description Type Required <code>variables</code> Mapping of variable names to values dict[string, int | float | bool | string | list | dict] N"},{"location":"references/configuration/#before_all-after_all","title":"Before_all / after_all","text":"<p>The <code>before_all</code> and <code>after_all</code> keys can be used to specify lists of SQL statements and/or Vulcan macros that are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands. For more information and examples, see the configuration guide.</p> Option Description Type Required <code>before_all</code> List of SQL statements to be executed at the start of the <code>plan</code> and <code>run</code> commands. list[string] N <code>after_all</code> List of SQL statements to be executed at the end of the <code>plan</code> and <code>run</code> commands. list[string] N"},{"location":"references/configuration/#plan","title":"Plan","text":"<p>Configuration for the <code>vulcan plan</code> command.</p> Option Description Type Required <code>auto_categorize_changes</code> Indicates whether Vulcan should attempt to automatically categorize model changes during plan creation per each model source type (additional details) dict[string, string] N <code>include_unmodified</code> Indicates whether to create views for all models in the target development environment or only for modified ones (Default: False) boolean N <code>auto_apply</code> Indicates whether to automatically apply a new plan after creation (Default: False) boolean N <code>forward_only</code> Indicates whether the plan should be forward-only (Default: False) boolean N <code>enable_preview</code> Indicates whether to enable data preview for forward-only models when targeting a development environment (Default: True, except for dbt projects where the target engine does not support cloning) Boolean N <code>no_diff</code> Don't show diffs for changed models (Default: False) boolean N <code>no_prompts</code> Disables interactive prompts in CLI (Default: True) boolean N <code>always_recreate_environment</code> Always recreates the target environment from the environment specified in <code>create_from</code> (by default <code>prod</code>) (Default: False) boolean N"},{"location":"references/configuration/#run","title":"Run","text":"<p>Configuration for the <code>vulcan run</code> command. Please note that this is only applicable when configured with the builtin scheduler.</p> Option Description Type Required <code>environment_check_interval</code> The number of seconds to wait between attempts to check the target environment for readiness (Default: 30 seconds) int N <code>environment_check_max_wait</code> The maximum number of seconds to wait for the target environment to be ready (Default: 6 hours) int N"},{"location":"references/configuration/#format","title":"Format","text":"<p>Formatting settings for the <code>vulcan format</code> command and UI.</p> Option Description Type Required <code>normalize</code> Whether to normalize SQL (Default: False) boolean N <code>pad</code> The number of spaces to use for padding (Default: 2) int N <code>indent</code> The number of spaces to use for indentation (Default: 2) int N <code>normalize_functions</code> Whether to normalize function names. Supported values are: 'upper' and 'lower' (Default: None) string N <code>leading_comma</code> Whether to use leading commas (Default: False) boolean N <code>max_text_width</code> The maximum text width in a segment before creating new lines (Default: 80) int N <code>append_newline</code> Whether to append a newline to the end of the file (Default: False) boolean N <code>no_rewrite_casts</code> Preserve the existing casts, without rewriting them to use the :: syntax. (Default: False) boolean N"},{"location":"references/configuration/#janitor","title":"Janitor","text":"<p>Configuration for the <code>vulcan janitor</code> command.</p> Option Description Type Required <code>warn_on_delete_failure</code> Whether to warn instead of erroring if the janitor fails to delete the expired environment schema / views (Default: False) boolean N <code>expired_snapshots_batch_size</code> Maximum number of expired snapshots to clean in a single batch (Default: 200) int N"},{"location":"references/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> dictionary defines how Vulcan should connect to the data warehouse, state backend, test backend, and scheduler.</p> <p>It takes one or more named <code>gateway</code> configuration keys, each of which can define its own connections. Gateway names are case-insensitive - Vulcan normalizes all gateway names to lowercase during configuration validation, allowing you to use any case when referencing gateways. A named gateway does not need to specify all four components and will use defaults if any are omitted - more information is provided about gateway defaults below.</p> <p>For example, a project might configure the <code>gate1</code> and <code>gate2</code> gateways:</p> <pre><code>gateways:\n  gate1:\n    connection:\n      ...\n    state_connection: # defaults to `connection` if omitted\n      ...\n    test_connection: # defaults to `connection` if omitted\n      ...\n    scheduler: # defaults to `builtin` if omitted\n      ...\n  gate2:\n    connection:\n      ...\n</code></pre> <p>Find additional information about gateways in the configuration guide gateways section.</p>"},{"location":"references/configuration/#gateway","title":"Gateway","text":"<p>Configuration for each named gateway.</p>"},{"location":"references/configuration/#connections","title":"Connections","text":"<p>A named gateway key may define any or all of a data warehouse connection, state backend connection, state schema name, test backend connection, and scheduler.</p> <p>Some connections use default values if not specified:</p> <ul> <li>The <code>connection</code> key may be omitted if a <code>default_connection</code> is specified.</li> <li>The state connection defaults to <code>connection</code> if omitted.</li> <li>The test connection defaults to <code>connection</code> if omitted.</li> </ul> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p> Option Description Type Required <code>connection</code> The data warehouse connection for core Vulcan functions. connection configuration N (if <code>default_connection</code> specified) <code>state_connection</code> The data warehouse connection where Vulcan will store internal information about the project. (Default: <code>connection</code> if using builtin scheduler, otherwise scheduler database) connection configuration N <code>state_schema</code> The name of the schema where state information should be stored. (Default: <code>vulcan</code>) string N <code>test_connection</code> The data warehouse connection Vulcan will use to execute tests. (Default: <code>connection</code>) connection configuration N <code>scheduler</code> The scheduler Vulcan will use to execute tests. (Default: <code>builtin</code>) scheduler configuration N <code>variables</code> The gateway-specific variables which override the root-level variables by key. dict[string, int | float | bool | string | list | dict] N"},{"location":"references/configuration/#connection","title":"Connection","text":"<p>Configuration for a data warehouse connection.</p> <p>Most parameters are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</p>"},{"location":"references/configuration/#general","title":"General","text":"Option Description Type Required <code>type</code> The engine type name, listed in engine-specific configuration pages below. str Y <code>concurrent_tasks</code> The maximum number of concurrent tasks that will be run by Vulcan. (Default: 4 for engines that support concurrent tasks.) int N <code>register_comments</code> Whether Vulcan should register model comments with the SQL engine (if the engine supports it). (Default: <code>true</code>.) bool N <code>pre_ping</code> Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive. This can only be enabled for engines with transaction support. bool N <code>pretty_sql</code> If SQL should be formatted before being executed, not recommended in a production setting. (Default: <code>false</code>.) bool N"},{"location":"references/configuration/#engine-specific","title":"Engine-specific","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Postgres</li> <li>Snowflake</li> </ul>"},{"location":"references/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans.</p> <p>By default, the scheduler type is set to <code>builtin</code> and uses the gateway's connection to store metadata.</p> <p>Below is the list of configuration options specific to each corresponding scheduler type. Find additional details in the configuration overview scheduler section.</p>"},{"location":"references/configuration/#builtin","title":"Builtin","text":"<p>Type: <code>builtin</code></p> <p>No configuration options are supported by this scheduler type.</p>"},{"location":"references/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. Find additional details in the configuration overview page gateway/connection defaults section.</p>"},{"location":"references/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> Option Description Type Required <code>default_gateway</code> The name of a gateway to use if one is not provided explicitly (Default: the gateway defined first in the <code>gateways</code> option). Gateway names are case-insensitive. string N"},{"location":"references/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> Option Description Type Required <code>default_connection</code> The default connection to use if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_test_connection</code> The default connection to use when running tests if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_scheduler</code> The default scheduler configuration to use if one is not specified in a gateway (Default: built-in scheduler) scheduler N"},{"location":"references/configuration/#debug-mode","title":"Debug mode","text":"<p>Enable debug mode in one of two ways:</p> <ul> <li>Pass the <code>--debug</code> flag between the CLI command and the subcommand. For example, <code>vulcan --debug plan</code>.</li> <li>Set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</li> </ul> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ vulcan --debug plan\n</code></pre> <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; vulcan --debug plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; vulcan --debug plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"references/configuration/#parallel-loading","title":"Parallel loading","text":"<p>Vulcan by default uses all of your cores when loading models and snapshots. It takes advantage of <code>fork</code> which is not available on Windows. The default is to use the same number of workers as cores on your machine if fork is available.</p> <p>You can override this setting by setting the environment variable <code>MAX_FORK_WORKERS</code>. A value of 1 will disable forking and load things sequentially.</p>"},{"location":"references/environments/","title":"Environments","text":""},{"location":"references/environments/#environments","title":"Environments","text":"<p>Environments are isolated namespaces that allow you to test and preview your changes.</p> <p>Vulcan differentiates between production and development environments. Currently, only the environment with the name <code>prod</code> is treated by Vulcan as the production one. Environments with other names are considered to be development ones.</p> <p>Models in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name <code>db.model_a</code> in the target environment <code>my_dev</code>, the <code>db__my_dev.model_a</code> table name should be used in a query. Models in the production environment are referred to by their original names.</p>"},{"location":"references/environments/#why-use-environments","title":"Why use environments","text":"<p>Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.</p> <p>Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment do not impact other environments. At the same time, any computation that was done in this environment can be safely reused in other environments.</p>"},{"location":"references/environments/#how-to-use-environments","title":"How to use environments","text":"<p>When running the plan command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is <code>prod</code>, which refers to the production environment. Environment with names other than <code>prod</code> are considered to be development environments.</p> <p>By default, the <code>vulcan plan</code> command targets the production (<code>prod</code>) environment.</p>"},{"location":"references/environments/#example","title":"Example","text":"<p>A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name <code>my_dev</code>, run:</p> <p></p><pre><code>vulcan plan my_dev\n</code></pre> A new environment is created automatically the first time a plan is applied to it.<p></p>"},{"location":"references/environments/#how-environments-work","title":"How environments work","text":"<p>Whenever a model definition changes, a new model snapshot is created with a unique fingerprint. This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model snapshots.</p> <p>Refer to plans for additional details.</p>"},{"location":"references/environments/#date-range","title":"Date range","text":"<p>A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the plan creation.</p>"},{"location":"references/glossary/","title":"Glossary","text":""},{"location":"references/glossary/#glossary","title":"Glossary","text":""},{"location":"references/glossary/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.</p>"},{"location":"references/glossary/#backfill","title":"Backfill","text":"<p>Load or refresh model data, triggered by a vulcan plan command.</p>"},{"location":"references/glossary/#catalog","title":"Catalog","text":"<p>A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.</p>"},{"location":"references/glossary/#cicd","title":"CI/CD","text":"<p>An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with tests and audits.</p>"},{"location":"references/glossary/#cte","title":"CTE","text":"<p>A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to tests.</p>"},{"location":"references/glossary/#dag","title":"DAG","text":"<p>Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.</p>"},{"location":"references/glossary/#data-modeling","title":"Data modeling","text":"<p>Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.</p>"},{"location":"references/glossary/#data-pipeline","title":"Data pipeline","text":"<p>The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.</p>"},{"location":"references/glossary/#data-transformation","title":"Data transformation","text":"<p>Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.</p>"},{"location":"references/glossary/#data-warehouse","title":"Data warehouse","text":"<p>The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.</p>"},{"location":"references/glossary/#direct-modification","title":"Direct Modification","text":"<p>A change to a model's definition from the user instead of being inherited from an upstream dependency like Indirect Modification.</p>"},{"location":"references/glossary/#elt","title":"ELT","text":"<p>Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.</p>"},{"location":"references/glossary/#etl","title":"ETL","text":"<p>Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.</p>"},{"location":"references/glossary/#full-refresh","title":"Full refresh","text":"<p>In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.</p>"},{"location":"references/glossary/#idempotency","title":"Idempotency","text":"<p>The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.</p>"},{"location":"references/glossary/#incremental-loads","title":"Incremental Loads","text":"<p>Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See Model Kinds for more information.</p>"},{"location":"references/glossary/#indirect-modification","title":"Indirect Modification","text":"<p>A change to model's upstream dependency and not to the model itself like a Direct Modification.</p>"},{"location":"references/glossary/#integration","title":"Integration","text":"<p>Combining data from various sources (such as from a data warehouse) into one unified view.</p>"},{"location":"references/glossary/#lineage","title":"Lineage","text":"<p>The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.</p>"},{"location":"references/glossary/#physical-layer","title":"Physical Layer","text":"<p>The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the Vulcan virtual layer's views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.</p>"},{"location":"references/glossary/#plan-summaries","title":"Plan Summaries","text":"<p>An upcoming feature that allows users to see a summary of changes applied to a given environment.</p>"},{"location":"references/glossary/#semantic-understanding","title":"Semantic Understanding","text":"<p>Vulcan, by leveraging SQLGlot, understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.</p>"},{"location":"references/glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.</p>"},{"location":"references/glossary/#table","title":"Table","text":"<p>A table is the visual representation of data stored in rows and columns.</p>"},{"location":"references/glossary/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.</p>"},{"location":"references/glossary/#view","title":"View","text":"<p>A view is the result of a SQL query on a database.</p>"},{"location":"references/glossary/#virtual-environments","title":"Virtual Environments","text":"<p>Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See plan application for more information.</p>"},{"location":"references/glossary/#virtual-layer","title":"Virtual Layer","text":"<p>The virtual layer is Vulcan's abstraction layer over the physical layer and physical data storage. While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.</p>"},{"location":"references/glossary/#virtual-update","title":"Virtual Update","text":"<p>Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See Virtual Update for more information.</p>"},{"location":"references/glossary/#virtual-preview","title":"Virtual Preview","text":"<p>Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.</p>"},{"location":"references/model_configuration/","title":"Model configuration","text":""},{"location":"references/model_configuration/#model-configuration","title":"Model configuration","text":"<p>This page lists Vulcan model configuration options and their parameters.</p> <p>Learn more about specifying Vulcan model properties in the model concepts overview page.</p>"},{"location":"references/model_configuration/#general-model-properties","title":"General model properties","text":"<p>Configuration options for Vulcan model properties. Supported by all model kinds other than <code>SEED</code> models.</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. <code>str</code> N <code>project</code> The name of the project the model belongs to - used in multi-repo deployments <code>str</code> N <code>kind</code> The model kind (Additional Details). (Default: <code>VIEW</code>) <code>str</code> | <code>dict</code> N <code>audits</code> Vulcan audits that should run against the model's output <code>array[str]</code> N <code>dialect</code> The SQL dialect in which the model's query is written. All SQL dialects supported by the SQLGlot library are allowed. <code>str</code> N <code>owner</code> The owner of a model; may be used for notification purposes <code>str</code> N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name <code>str</code> N <code>tags</code> Arbitrary strings used to organize or classify a model <code>array[str]</code> N <code>cron</code> The cron expression specifying how often the model should be refreshed. (Default: <code>@daily</code>) <code>str</code> N <code>interval_unit</code> The temporal granularity of the model's data intervals. Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code>. (Default: inferred from <code>cron</code>) <code>str</code> N <code>start</code> The date/time that determines the earliest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. (Default: <code>yesterday</code>) <code>str</code> | <code>int</code> N <code>end</code> The date/time that determines the latest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. <code>str</code> | <code>int</code> N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). <code>str</code> N <code>column_descriptions</code> A key-value mapping of column names to column comments that will be registered in the SQL engine's table COMMENT field (if supported by the engine). Specified as key-value pairs (<code>column_name = 'column comment'</code>). If present, inline column comments will not be registered in the SQL engine. <code>dict</code> N <code>grains</code> The column(s) whose combination uniquely identifies each row in the model <code>str</code> | <code>array[str]</code> N <code>profiles</code> The column(s) to profile for data quality checks using Soda profiling <code>str</code> | <code>array[str]</code> N <code>references</code> The model column(s) used to join to other models' grains <code>str</code> | <code>array[str]</code> N <code>depends_on</code> Models on which this model depends, in addition to the ones inferred from the model's query. (Default: dependencies inferred from model code) <code>array[str]</code> N <code>table_format</code> The table format that should be used to manage the physical files (eg <code>iceberg</code>, <code>hive</code>, <code>delta</code>); only applicable to engines such as Spark and Athena <code>str</code> N <code>storage_format</code> The storage format that should be used to store physical files (eg <code>parquet</code>, <code>orc</code>); only applicable to engines such as Spark and Athena <code>str</code> N <code>partitioned_by</code> The column(s) and/or column expressions used define a model's partitioning key. Required for the <code>INCREMENTAL_BY_PARTITION</code> model kind. Optional for all other model kinds; used to partition the model's physical table in engines that support partitioning. <code>str</code> | <code>array[str]</code> N <code>clustered_by</code> The column(s) and/or column expressions used to cluster the model's physical table; only applicable to engines that support clustering <code>str</code> N <code>columns</code> The column names and data types returned by the model. Disables automatic inference of column names and types from the SQL query. <code>array[str]</code> N <code>physical_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model table / view in the physical layer. Specified as key-value pairs (<code>key = value</code>). The view/table type (e.g. <code>TEMPORARY</code>, <code>TRANSIENT</code>) can be added with the <code>creatable_type</code> key. <code>dict</code> N <code>virtual_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model view in the virtual layer. Specified as key-value pairs (<code>key = value</code>). The view type (e.g. <code>SECURE</code>) can be added with the <code>creatable_type</code> key. <code>dict</code> N <code>session_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the engine session. Specified as key-value pairs (<code>key = value</code>). <code>dict</code> N <code>allow_partials</code> Whether this model can process partial (incomplete) data intervals <code>bool</code> N <code>enabled</code> Whether the model is enabled. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to ignore this model when loading the project. <code>bool</code> N <code>optimize_query</code> Whether the model's query should be optimized. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to disable query canonicalization &amp; simplification. This should be turned off only if the optimized query leads to errors such as surpassing text limit. <code>bool</code> N <code>ignored_rules</code> A list of linter rule names (or \"ALL\") to be ignored/excluded for this model <code>str</code> | <code>array[str]</code> N <code>formatting</code> Whether the model will be formatted. All models are formatted by default. Setting this to <code>false</code> causes Vulcan to ignore this model during <code>vulcan format</code>. <code>bool</code> N"},{"location":"references/model_configuration/#model-defaults","title":"Model defaults","text":"<p>The Vulcan project-level configuration must contain the <code>model_defaults</code> key and must specify a value for its <code>dialect</code> key. Other values are set automatically unless explicitly overridden in the model definition. Learn more about project-level configuration in the configuration guide.</p> <p>In <code>physical_properties</code>, <code>virtual_properties</code>, and <code>session_properties</code>, when both project-level and model-specific properties are defined, they are merged, with model-level properties taking precedence. To unset a project-wide property for a specific model, set it to <code>None</code> in the <code>MODEL</code>'s DDL properties or within the <code>@model</code> decorator for Python models.</p> <p>For example, with the following <code>model_defaults</code> configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    partition_expiration_days: 7\n    require_partition_filter: True\n    project_level_property: \"value\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"partition_expiration_days\": 7,\n      \"require_partition_filter\": True,\n      \"project_level_property\": \"value\"\n    },\n  ),\n)\n</code></pre> <p>To override <code>partition_expiration_days</code>, add a new <code>creatable_type</code> property and unset <code>project_level_property</code>, you can define the model as follows:</p> SQLPython <pre><code>MODEL (\n  ...,\n  physical_properties (\n    partition_expiration_days = 14,\n    creatable_type = TRANSIENT,\n    project_level_property = None,\n  )\n);\n</code></pre> <pre><code>@model(\n  ...,\n  physical_properties={\n    \"partition_expiration_days\": 14,\n    \"creatable_type\": \"TRANSIENT\",\n    \"project_level_property\": None\n  },\n)\n</code></pre> <p>You can also use the <code>@model_kind_name</code> variable to fine-tune control over <code>physical_properties</code> in <code>model_defaults</code>. This holds the current model's kind name and is useful for conditionally assigning a property. For example, to disable <code>creatable_type</code> for your project's <code>VIEW</code> kind models:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    creatable_type: \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"creatable_type\": \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\",\n    },\n  ),\n)\n</code></pre> <p>You can aso define <code>pre_statements</code>, <code>post_statements</code> and <code>on_virtual_update</code> statements at the project level that will be applied to all models. These default statements are merged with any model-specific statements, with default statements executing first, followed by model-specific statements.</p> YAMLPython <pre><code>model_defaults:\n  dialect: duckdb\n  pre_statements:\n    - \"SET timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"duckdb\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre> <p>The Vulcan project-level <code>model_defaults</code> key supports the following options, described in the general model properties table above:</p> <ul> <li>kind</li> <li>dialect</li> <li>cron</li> <li>owner</li> <li>start</li> <li>table_format</li> <li>storage_format</li> <li>physical_properties</li> <li>virtual_properties</li> <li>session_properties (on per key basis)</li> <li>on_destructive_change (described below)</li> <li>on_additive_change (described below)</li> <li>audits (described here)</li> <li>optimize_query</li> <li>allow_partials</li> <li>enabled</li> <li>interval_unit</li> <li>pre_statements (described here)</li> <li>post_statements (described here)</li> <li>on_virtual_update (described here)</li> </ul>"},{"location":"references/model_configuration/#model-naming","title":"Model Naming","text":"<p>Configuration option for name inference. Learn more in the model naming guide.</p> Option Description Type Required <code>infer_names</code> Whether to automatically infer model names based on the directory structure (Default: <code>False</code>) <code>bool</code> N"},{"location":"references/model_configuration/#model-kind-properties","title":"Model kind properties","text":"<p>Configuration options for kind-specific Vulcan model properties, in addition to the general model properties listed above.</p> <p>Learn more about model kinds at the model kind concepts page. Learn more about specifying model kind in Python models at the Python models concepts page.</p>"},{"location":"references/model_configuration/#view-models","title":"<code>VIEW</code> models","text":"<p>Configuration options for models of the <code>VIEW</code> kind (in addition to general model properties).</p> Option Description Type Required <code>materialized</code> Whether views should be materialized (for engines supporting materialized views). (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.VIEW</p>"},{"location":"references/model_configuration/#full-models","title":"<code>FULL</code> models","text":"<p>The <code>FULL</code> model kind does not support any configuration options other than the general model properties listed above.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.FULL</p>"},{"location":"references/model_configuration/#incremental-models","title":"Incremental models","text":"<p>Configuration options for all incremental models (in addition to general model properties).</p> Option Description Type Required <code>forward_only</code> Whether the model's changes should always be classified as forward-only. (Default: <code>False</code>) <code>bool</code> N <code>on_destructive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes a destructive modification to the model schema. Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>error</code>) <code>str</code> N <code>on_additive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes an additive modification to the model schema (like adding new columns). Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>allow</code>) <code>str</code> N <code>disable_restatement</code> Whether restatements should be disabled for the model. (Default: <code>False</code>) <code>bool</code> N"},{"location":"references/model_configuration/#incremental-by-time-range","title":"Incremental by time range","text":"<p>Configuration options for <code>INCREMENTAL_BY_TIME_RANGE</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>time_column</code> The model column containing each row's timestamp. Should be UTC time zone. <code>str</code> Y <code>format</code> Argument to <code>time_column</code>. Format of the time column's data. (Default: <code>%Y-%m-%d</code>) <code>str</code> N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) <code>int</code> N <code>batch_concurrency</code> The maximum number of batches that can run concurrently for this model. (Default: the number of concurrent tasks set in the connection settings) <code>int</code> N <code>lookback</code> The number of <code>interval_unit</code>s prior to the current interval that should be processed - learn more. (Default: <code>0</code>) <code>int</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_TIME_RANGE</p>"},{"location":"references/model_configuration/#incremental-by-unique-key","title":"Incremental by unique key","text":"<p>Configuration options for <code>INCREMENTAL_BY_UNIQUE_KEY</code> models (in addition to general model properties and incremental model properties). Batch concurrency cannot be set for incremental by unique key models because they cannot safely be run in parallel.</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key <code>str</code> | <code>array[str]</code> Y <code>when_matched</code> SQL logic used to update columns when a match occurs - only available on engines that support <code>MERGE</code>. (Default: update all columns) <code>str</code> N <code>merge_filter</code> A single or a conjunction of predicates used to filter data in the ON clause of a MERGE operation - only available on engines that support <code>MERGE</code> <code>str</code> N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) <code>int</code> N <code>lookback</code> The number of time unit intervals prior to the current interval that should be processed. (Default: <code>0</code>) <code>int</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</p>"},{"location":"references/model_configuration/#incremental-by-partition","title":"Incremental by partition","text":"<p>The <code>INCREMENTAL_BY_PARTITION</code> models kind does not support any configuration options other than the general model properties and incremental model properties.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_PARTITION</p>"},{"location":"references/model_configuration/#scd-type-2-models","title":"SCD Type 2 models","text":"<p>Configuration options for <code>SCD_TYPE_2</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key <code>array[str]</code> Y <code>valid_from_name</code> The model column containing each row's valid from date. (Default: <code>valid_from</code>) <code>str</code> N <code>valid_to_name</code> The model column containing each row's valid to date. (Default: <code>valid_to</code>) <code>str</code> N <code>invalidate_hard_deletes</code> If set to true, when a record is missing from the source table it will be marked as invalid - see here for more information. (Default: <code>True</code>) <code>bool</code> N"},{"location":"references/model_configuration/#scd-type-2-by-time","title":"SCD Type 2 By Time","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_TIME</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>updated_at_name</code> The model column containing each row's updated at date. (Default: <code>updated_at</code>) <code>str</code> N <code>updated_at_as_valid_from</code> By default, for new rows the <code>valid_from</code> column is set to 1970-01-01 00:00:00. This sets <code>valid_from</code> to the value of <code>updated_at</code> when the row is inserted. (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_TIME</p>"},{"location":"references/model_configuration/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_COLUMN</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>columns</code> Columns whose changed data values indicate a data update (instead of an <code>updated_at</code> column). <code>*</code> to represent that all columns should be checked. <code>str</code> | <code>array[str]</code> Y <code>execution_time_as_valid_from</code> By default, for new rows <code>valid_from</code> is set to 1970-01-01 00:00:00. This changes the behavior to set it to the execution_time of when the pipeline ran. (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_COLUMN</p>"},{"location":"references/model_configuration/#seed-models","title":"<code>SEED</code> models","text":"<p>Configuration options for <code>SEED</code> models. <code>SEED</code> models do not support all the general properties supported by other models; they only support the properties listed in this table.</p> <p>Top-level options inside the MODEL DDL:</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. <code>str</code> N <code>kind</code> The model kind. Must be <code>SEED</code>. <code>str</code> Y <code>columns</code> The column names and data types in the CSV file. Disables automatic inference of column names and types by the pandas CSV reader. NOTE: order of columns overrides the order specified in the CSV header row (if present). <code>array[str]</code> N <code>audits</code> Vulcan audits that should run against the model's output <code>array[str]</code> N <code>owner</code> The owner of a model; may be used for notification purposes <code>str</code> N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name <code>str</code> N <code>tags</code> Arbitrary strings used to organize or classify a model <code>array[str]</code> N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). <code>str</code> N <p>Options specified within the top-level <code>kind</code> property:</p> Option Description Type Required <code>path</code> Path to seed CSV file. <code>str</code> Y <code>batch_size</code> The maximum number of CSV rows ingested in each batch. All rows ingested in one batch if not specified. <code>int</code> N <code>csv_settings</code> Pandas CSV reader settings (overrides default values). Specified as key-value pairs (<code>key = value</code>). <code>dict</code> N <p> Options specified within the <code>kind</code> property's <code>csv_settings</code> property (overrides default Pandas CSV reader settings):</p> Option Description Type Required <code>delimiter</code> Character or regex pattern to treat as the delimiter. More information at the Pandas. <code>str</code> N <code>quotechar</code> Character used to denote the start and end of a quoted item. More information at the Pandas. <code>str</code> N <code>doublequote</code> When quotechar is specified, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. More information at the Pandas. <code>bool</code> N <code>escapechar</code> Character used to escape other characters. More information at the Pandas. <code>str</code> N <code>skipinitialspace</code> Skip spaces after delimiter. More information at the Pandas. <code>bool</code> N <code>lineterminator</code> Character used to denote a line break. More information at the Pandas. <code>str</code> N <code>encoding</code> Encoding to use for UTF when reading/writing (ex. 'utf-8'). More information at the Pandas. <code>str</code> N <code>na_values</code> An array of values that should be recognized as NA/NaN. In order to specify such an array per column, a mapping in the form of <code>(col1 = (v1, v2, ...), col2 = ...)</code> can be passed instead. These values can be integers, strings, booleans or NULL, and they are converted to their corresponding Python values. More information at the Pandas. <code>array[value]</code> | <code>array[array[key = value]]</code> N <code>keep_default_na</code> Whether or not to include the default NaN values when parsing the data. More information at the Pandas. <code>bool</code> N"},{"location":"references/overview/","title":"Overview","text":""},{"location":"references/overview/#overview","title":"Overview","text":"<p>This page provides a conceptual overview of what Vulcan does and how its components fit together.</p>"},{"location":"references/overview/#what-vulcan-is","title":"What Vulcan is","text":"<p>Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of execution engines.</p> <p>It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.</p> <p>You can use Vulcan with the CLI.</p>"},{"location":"references/overview/#how-vulcan-works","title":"How Vulcan works","text":""},{"location":"references/overview/#create-models","title":"Create models","text":"<p>You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.</p>"},{"location":"references/overview/#make-a-plan","title":"Make a plan","text":"<p>Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.</p> <p>Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change before you expend the time and resources to actually perform the computations.</p> <p>Vulcan automatically identifies all affected models and the computations a change entails by creating a \"Vulcan plan.\" When you execute the <code>plan</code> command, Vulcan generates the plan for the environment specified in the command (e.g., dev, test, prod).</p> <p>The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.</p> <p>Learn more about plans.</p>"},{"location":"references/overview/#apply-the-plan","title":"Apply the plan","text":"<p>After using <code>plan</code> to understand the impacts of a change in an environment, Vulcan offers to execute the computations by <code>apply</code>ing the plan. However, you must provide additional information that determines the scope of what computations are executed.</p> <p>The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.</p> <p>\"Backfilling\" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.</p> <p>Most business data is temporal \u2014 each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.</p> <p>The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.</p>"},{"location":"references/overview/#build-a-virtual-environment","title":"Build a Virtual Environment","text":"<p>Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.</p> <p>One challenge with using multiple data environments is that backfill and other computations must happen twice \u2014 once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.</p> <p>Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.</p> <p>Vulcan uses its knowledge of equivalent outputs to create a Virtual Environment. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but without computation or data movement.</p> <p>Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.</p>"},{"location":"references/overview/#test-your-code-and-data","title":"Test your code and data","text":"<p>Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.</p>"},{"location":"references/overview/#tests","title":"Tests","text":"<p>Vulcan \"tests\" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model code \u2014 you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.</p> <p>Vulcan automatically runs tests when you apply a <code>plan</code>, or you can run them on demand with the <code>test</code> command.</p>"},{"location":"references/overview/#audits","title":"Audits","text":"<p>In contrast to tests, Vulcan \"audits\" validate the results of model code applied to your actual data.</p> <p>You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure <code>your_field</code> has no <code>NULL</code> values would include <code>WHERE your_field IS NULL</code>. If any NULLs are detected, the query will return at least one row and the audit will fail.</p> <p>Audits are flexible \u2014 they can be tied to a specific model's contents, or you can use macros to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.</p> <p>You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.</p> <p>Vulcan automatically runs audits when you apply a <code>plan</code> to an environment, or you can run them on demand with the <code>audit</code> command.</p>"},{"location":"references/overview/#infrastructure-and-orchestration","title":"Infrastructure and orchestration","text":"<p>Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use \u2014 its only requirement is access to the target SQL/analytics engine.</p> <p>Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a <code>vulcan</code> schema in your data warehouse for its internal metadata.</p>"},{"location":"references/plans/","title":"Plans","text":""},{"location":"references/plans/#plans","title":"Plans","text":"<p>A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target environment. In order for any model changes to take effect in a target environment, a plan needs to be created and applied.</p>"},{"location":"references/plans/#plan-architecture-overview","title":"Plan Architecture Overview","text":"<p>The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:</p> <pre><code>flowchart TD\n    subgraph \"1\ufe0f\u20e3 Local Development\"\n        A[\ud83d\udc68\u200d\ud83d\udcbb Developer modifies model files&lt;br/&gt;\ud83d\udcdd Edit SQL/Python models]\n        B[\ud83d\udcc1 Local project state&lt;br/&gt;\u2728 Your changes ready]\n    end\n\n    subgraph \"2\ufe0f\u20e3 Plan Creation\"\n        C[\u26a1 vulcan plan&lt;br/&gt;\ud83d\ude80 Command execution]\n        D[\ud83d\udd0e Compare local vs environment&lt;br/&gt;\ud83d\udcca State comparison]\n        E{\ud83d\udd0d Changes detected?}\n        F[\ud83d\udccb Generate plan summary&lt;br/&gt;\u2728 Plan ready for review]\n        G[\ud83c\udff7\ufe0f Change categorization&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking / \ud83d\udfe1 Forward-only]\n    end\n\n    subgraph \"3\ufe0f\u20e3 Plan Review\"\n        H[\ud83d\udc40 Review plan output&lt;br/&gt;\ud83d\udcca Check changes &amp; impacts]\n        I{\u2705 Apply plan?}\n        J[\u274c Cancel&lt;br/&gt;\ud83d\udeab No changes applied]\n    end\n\n    subgraph \"4\ufe0f\u20e3 Plan Application\"\n        K[\ud83d\udd37 Create model variants&lt;br/&gt;\ud83d\udd11 With unique fingerprints]\n        L[\ud83d\uddc4\ufe0f Create physical tables&lt;br/&gt;\ud83d\udcbe In data warehouse]\n        M[\ud83d\udd04 Backfill data&lt;br/&gt;\ud83d\udcc8 Process historical data]\n        N[\ud83d\udc41\ufe0f Update virtual layer&lt;br/&gt;\ud83d\udd0d Create/update views]\n        O[\ud83c\udf0d Update environment references&lt;br/&gt;\ud83d\udd17 Point to new variants]\n    end\n\n    subgraph \"5\ufe0f\u20e3 Result\"\n        P[\u2705 Environment updated&lt;br/&gt;\ud83c\udf89 Changes deployed]\n        Q[\ud83d\udd0d Models accessible via views&lt;br/&gt;\ud83d\udcca Ready for queries]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\u27a1\ufe0f\"| C\n    C --&gt;|\"\ud83d\udd0d\"| D\n    D --&gt;|\"\ud83d\udd0e\"| E\n    E --&gt;|\"\u2705 Yes\"| F\n    E --&gt;|\"\u274c No\"| P\n    F --&gt;|\"\ud83c\udff7\ufe0f\"| G\n    G --&gt;|\"\ud83d\udccb\"| H\n    H --&gt;|\"\ud83d\udc40\"| I\n    I --&gt;|\"\u2705 Yes\"| K\n    I --&gt;|\"\u274c No\"| J\n    K --&gt;|\"\ud83d\udd37\"| L\n    L --&gt;|\"\ud83d\udcbe\"| M\n    M --&gt;|\"\ud83d\udd04\"| N\n    N --&gt;|\"\ud83d\udc41\ufe0f\"| O\n    O --&gt;|\"\ud83d\udd17\"| P\n    P --&gt;|\"\u2728\"| Q\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre>"},{"location":"references/plans/#plan-components","title":"Plan Components","text":"<pre><code>graph LR\n    subgraph \"\ud83d\udccb Plan Contents\"\n        PC1[\u2795 Added Models&lt;br/&gt;\u2728 New models to create]\n        PC2[\u2796 Removed Models&lt;br/&gt;\ud83d\uddd1\ufe0f Models to delete]\n        PC3[\u270f\ufe0f Modified Models&lt;br/&gt;\ud83d\udcdd With diffs]\n        PC4[\ud83d\udd17 Indirectly Affected&lt;br/&gt;\ud83d\udcca Downstream models]\n        PC5[\ud83d\udcc5 Backfill Requirements&lt;br/&gt;\ud83d\udcc6 Date ranges]\n    end\n\n    subgraph \"\ud83c\udff7\ufe0f Change Types\"\n        CT1[\ud83d\udd34 Breaking Change&lt;br/&gt;\u26a0\ufe0f Requires downstream backfill&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        CT2[\ud83d\udfe2 Non-Breaking Change&lt;br/&gt;\u2705 Only direct model backfill&lt;br/&gt;\ud83c\udfaf Isolated impact]\n        CT3[\ud83d\udfe1 Forward-Only&lt;br/&gt;\u267b\ufe0f Reuses existing tables&lt;br/&gt;\ud83d\udcb0 Cost-effective]\n    end\n\n    PC3 --&gt;|\"\ud83d\udd34\"| CT1\n    PC3 --&gt;|\"\ud83d\udfe2\"| CT2\n    PC3 --&gt;|\"\ud83d\udfe1\"| CT3\n    PC4 --&gt;|\"\ud83d\udd34\"| CT1\n\n    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000</code></pre> <p>During plan creation:</p> <ul> <li>The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.</li> <li>Users may be prompted to categorize changes to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through configuration.</li> <li>Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.</li> </ul> <p>The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:</p> <ul> <li>A list of added models</li> <li>A list of removed models</li> <li>A list of directly modified models and a text diff of changes that have been made</li> <li>A list of indirectly modified models</li> <li>Missing data intervals for affected models</li> <li>A date range that will be affected by the plan application</li> </ul> <p>To create a new plan, run the following command: </p><pre><code>vulcan plan [environment name]\n</code></pre><p></p> <p>If no environment name is specified, the plan is generated for the <code>prod</code> environment.</p>"},{"location":"references/plans/#change-categories","title":"Change categories","text":"<p>Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.</p> <p>If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.</p>"},{"location":"references/plans/#change-propagation-flow","title":"Change Propagation Flow","text":"<p>The following diagram illustrates how changes propagate through the dependency graph:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Dependencies\"\n        A[\ud83d\udce5 raw.raw_orders&lt;br/&gt;\u2b06\ufe0f Upstream]\n        B[\ud83d\udcca sales.daily_sales&lt;br/&gt;\ud83d\udd04 Midstream]\n        C[\ud83d\udcc8 sales.weekly_sales&lt;br/&gt;\u2b07\ufe0f Downstream]\n        D[\ud83d\udcc9 analytics.revenue_report&lt;br/&gt;\u2b07\ufe0f Downstream]\n    end\n\n    subgraph \"\ud83d\udfe2 Scenario 1: Non-Breaking Change\"\n        NB1[\u2795 Add column to daily_sales&lt;br/&gt;\u2728 New column added]\n        NB2[\u2705 Only daily_sales backfilled&lt;br/&gt;\ud83d\udd04 Single model update]\n        NB3[\u23ed\ufe0f weekly_sales NOT affected&lt;br/&gt;\u2705 No cascade]\n        NB4[\u23ed\ufe0f revenue_report NOT affected&lt;br/&gt;\u2705 No cascade]\n    end\n\n    subgraph \"\ud83d\udd34 Scenario 2: Breaking Change\"\n        BC1[\ud83d\udd0d Add WHERE clause to daily_sales&lt;br/&gt;\u26a0\ufe0f Filter logic changed]\n        BC2[\ud83d\udd04 daily_sales backfilled&lt;br/&gt;\ud83d\udcca Data reprocessed]\n        BC3[\ud83d\udd04 weekly_sales backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        BC4[\ud83d\udd04 revenue_report backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\ud83d\udce4\"| C\n    B --&gt;|\"\ud83d\udce4\"| D\n\n    NB1 --&gt;|\"\u270f\ufe0f\"| B\n    B --&gt;|\"\u2705\"| NB2\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| C\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| D\n\n    BC1 --&gt;|\"\u26a0\ufe0f\"| B\n    B --&gt;|\"\ud83d\udd04\"| BC2\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC3\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC4\n    BC3 --&gt;|\"\ud83d\udd04\"| C\n    BC4 --&gt;|\"\ud83d\udd04\"| D\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000</code></pre>"},{"location":"references/plans/#breaking-change","title":"Breaking change","text":"<p>If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.</p> <p>In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to backfilling).</p> <p>Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's <code>WHERE</code> clause is a breaking change because downstream models contain rows that would now be filtered out.</p>"},{"location":"references/plans/#non-breaking-change","title":"Non-breaking change","text":"<p>A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.</p> <p>This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.</p> <p>If any downstream models contain a <code>select *</code> from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.</p>"},{"location":"references/plans/#summary","title":"Summary","text":"Change Category Change Type Behaviour Breaking Direct or Indirect Backfill Non-breaking Direct Backfill Non-breaking Indirect No Backfill"},{"location":"references/plans/#forward-only-change","title":"Forward-only change","text":"<p>In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.</p> <p>A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the <code>prod</code> environment). This means that no backfill will take place.</p> <p>While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.</p> <p>In either case the data produced this way in the development environment can only be used for preview and will not be reused once the change is deployed to production. See Forward-only Plans for more details.</p> <p>This category is assigned by Vulcan automatically either when a user opts into using a forward-only plan or when a model is explicitly configured to be forward-only.</p>"},{"location":"references/plans/#plan-application","title":"Plan application","text":"<p>Once a plan has been created and reviewed, it is then applied to the target environment in order for its changes to take effect.</p> <p>Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a snapshot with a unique fingerprint is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for forward-only plans.</p> <p>When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.</p>"},{"location":"references/plans/#model-versioning-architecture","title":"Model Versioning Architecture","text":"<p>The following diagram shows how model variants, physical tables, and environments relate:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcdd Model Definitions\"\n        M1[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 1&lt;br/&gt;\u2728 Original]\n        M2[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 2 - Modified&lt;br/&gt;\u270f\ufe0f Updated]\n    end\n\n    subgraph \"\ud83d\udd37 Model Variants &amp; Snapshots\"\n        V1[\ud83d\udd37 Variant 1&lt;br/&gt;\ud83d\udd11 Fingerprint: abc123&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        V2[\ud83d\udd37 Variant 2&lt;br/&gt;\ud83d\udd11 Fingerprint: def456&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        S1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd10 Immutable state]\n        S2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd10 Immutable state]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Tables\"\n        T1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__abc123&lt;br/&gt;\ud83d\udcbe Actual data storage]\n        T2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__def456&lt;br/&gt;\ud83d\udcbe Actual data storage]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer Views\"\n        VL1[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 1&lt;br/&gt;\ud83d\udd17 Reference mapping]\n        VL2[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 2&lt;br/&gt;\ud83d\udd17 Reference mapping]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;\u2705 References Variant 1&lt;br/&gt;\ud83c\udf10 Live production data]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;\ud83d\udd2c References Variant 2&lt;br/&gt;\ud83e\uddea Testing environment]\n    end\n\n    M1 --&gt;|\"\u2728\"| V1\n    M2 --&gt;|\"\u270f\ufe0f\"| V2\n    V1 --&gt;|\"\ud83d\udcf8\"| S1\n    V2 --&gt;|\"\ud83d\udcf8\"| S2\n    S1 --&gt;|\"\ud83d\udcbe\"| T1\n    S2 --&gt;|\"\ud83d\udcbe\"| T2\n    T1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    T2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n    PROD --&gt;|\"\ud83d\udd17\"| V1\n    DEV --&gt;|\"\ud83d\udd17\"| V2\n\n    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000</code></pre> <p></p> <p>Each model variant gets its own physical table while environments only contain references to these tables.</p> <p>This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.</p> <p>Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to Virtual Update) as long as its physical table hasn't been garbage collected by the janitor process.</p> <p>Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.</p>"},{"location":"references/plans/#backfilling","title":"Backfilling","text":"<p>Despite all the benefits, the approach described above is not without trade-offs.</p> <p>When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.</p> <p>We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:</p> <ul> <li>When a VIEW model is created</li> <li>When a FULL model is built</li> <li>When an INCREMENTAL model is built for the first time</li> <li>When an INCREMENTAL model has recent data appended to it</li> <li>When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)</li> </ul> <p>Note for incremental models: despite the fact that backfilling can happen incrementally (see <code>batch_size</code> parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a forward-only plan instead.</p>"},{"location":"references/plans/#virtual-update","title":"Virtual Update","text":"<p>A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.</p> <p>With this approach, the process of promoting a change to production is reduced to reference swapping.</p> <p>If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.</p>"},{"location":"references/plans/#start-and-end-dates","title":"Start and end dates","text":"<p>The <code>plan</code> command provides two temporal options: <code>--start</code> and <code>--end</code>. These options are only applicable to plans for non-prod environments.</p> <p>For context, every model has a start date. The start can be specified in the model definition, in the project configuration's <code>model_defaults</code>, or by Vulcan's default value of yesterday.</p> <p>Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the <code>plan</code> command's <code>--start</code> and <code>--end</code> options are not supported for regular plans against prod. The options are supported for restatement plans against prod to allow re-processing a subset of existing data.</p> <p>Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the <code>--start</code> and <code>--end</code> options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.</p>"},{"location":"references/plans/#model-kind-limitations","title":"Model kind limitations","text":"<p>Some model kinds do not support backfilling a limited date range.</p> <p>For context, Vulcan strives to make models idempotent, meaning that if we ran them multiple times we would get the same correct result every time.</p> <p>However, some model kinds are inherently non-idempotent:</p> <ul> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>SCD_TYPE_2_BY_TIME</li> <li>SCD_TYPE_2_BY_COLUMN</li> <li>Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)</li> </ul> <p>Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:</p> <ul> <li>If the <code>--start</code> option date is the same as or before the model's start date, the model is fully refreshed for all of time</li> <li>If the <code>--start</code> option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production</li> </ul>"},{"location":"references/plans/#example","title":"Example","text":"<p>Consider a Vulcan project with a default start date of 2024-09-20.</p> <p>It contains the following <code>INCREMENTAL_BY_UNIQUE_KEY</code> model that specifies an explicit start date of 2024-09-23:</p> <pre><code>MODEL (\n  name vulcan_example.start_end_model,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key item_id\n  ),\n  start '2024-09-23'\n);\n\nSELECT\n  item_id,\n  num_orders\nFROM\n  vulcan_example.full_model\n</code></pre> <p>When we run the project's first plan, we see that Vulcan correctly detected a different start date for our <code>start_end_model</code> than the other models (which have the project default start of 2024-09-20):</p> <pre><code>\u276f vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u251c\u2500\u2500 vulcan_example.seed_model\n    \u2514\u2500\u2500 vulcan_example.start_end_model\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example.full_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.incremental_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.seed_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example.start_end_model: 2024-09-23 - 2024-09-26\nApply - Backfill Tables [y/n]:\n</code></pre> <p>After executing that plan, we add columns to both the <code>incremental_model</code> and <code>start_end_model</code> queries.</p> <p>We then execute <code>vulcan plan dev</code> to create the new <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre> <p>Note two things about the output:</p> <ol> <li>As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for <code>incremental_model</code> and 2024-09-23 for <code>start_end_model</code></li> <li>Vulcan prompted us for a backfill start date because we didn't pass the <code>--start</code> option to the <code>vulcan plan dev</code> command</li> </ol> <p>Let's cancel that plan and start a new one, passing a start date of 2024-09-24.</p> <p>The <code>start_end_model</code> is of kind <code>INCREMENTAL_BY_UNIQUE_KEY</code>, which is non-idempotent and cannot be backfilled for a limited time range.</p> <p>Because the command's <code>--start</code> of 2024-09-24 is after <code>start_end_model</code>'s start date 2024-09-23, <code>start_end_model</code> is marked as preview:</p> <pre><code>\u276f vulcan plan dev --start 2024-09-24\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':\n</code></pre>"},{"location":"references/plans/#minimum-intervals","title":"Minimum intervals","text":"<p>When you run a plan with a fixed <code>--start</code> or <code>--end</code> date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.</p> <p>For example, if you have a model like so:</p> <pre><code>MODEL(\n    name vulcan_example.monthly_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column month\n    ),\n    cron '@monthly'\n);\n\nSELECT SUM(a) AS sum_a, MONTH(day) AS month\nFROM vulcan_example.upstream_model\nWHERE day BETWEEN @start_ds AND @end_ds\n</code></pre> <p>make a change to it and run the following:</p> <pre><code>$ vulcan plan dev --start '1 day ago' \n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\nSKIP: No model batches to execute\n</code></pre> <p>No data will be backfilled because <code>1 day ago</code> does not contain a complete month. However, you can use the <code>--min-intervals</code> option to override this behaviour like so:</p> <pre><code>$ vulcan plan dev --start '1 day ago' --min-intervals 1\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\n[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   \nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00                                                             \n\n\u2714 Model batches executed\n</code></pre> <p>This will ensure that regardless of the plan <code>--start</code> date, all added or modified models will have at least <code>--min-intervals</code> intervals considered for backfill.</p> <p>Info</p> <p>If you are running plans manually you can just adjust the <code>--start</code> date to be wide enough to cover the models in question.</p> <p>The <code>--min-intervals</code> option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) \"2 weeks worth of data\" in the target environment.</p>"},{"location":"references/plans/#data-preview-for-forward-only-changes","title":"Data preview for forward-only changes","text":"<p>As mentioned earlier, the data output produced by forward-only changes in a development environment can only be used for preview and will not be reused in production.</p> <p>The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.</p> <p>Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with <code>(preview)</code> in the plan summary: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28\n\u251c\u2500\u2500 sushi__dev.top_waiters: 2023-12-22 - 2023-12-28\n\u2514\u2500\u2500 sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)\n</code></pre><p></p>"},{"location":"references/plans/#forward-only-plans","title":"Forward-only plans","text":"<p>Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.</p> <p>The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and restatements.</p> <p>Note that once a forward-only change is applied to <code>prod</code>, all development environments that referred to the previous versions of the updated models will be impacted.</p> <p>A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, <code>vulcan plan [environment name]</code> evaluates code in non-<code>prod</code> environments while targeting shallow (a.k.a. \"zero-copy\") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.</p> <p>This means that only a limited preview of changes is available in the development environment before the change is promoted to <code>prod</code>. The date range of the preview is provided as part of plan creation command.</p> <p>Engines for which table cloning is supported include:</p> <ul> <li><code>BigQuery</code></li> <li><code>Databricks</code></li> <li><code>Snowflake</code></li> </ul> <p>Note that all changes made as part of a forward-only plan automatically get a forward-only category assigned to them. These types of changes can't be mixed together with breaking and non-breaking changes within the same plan.</p> <p>To create a forward-only plan, add the <code>--forward-only</code> option to the <code>plan</code> command: </p><pre><code>vulcan plan [environment name] --forward-only\n</code></pre><p></p> <p>Note</p> <p>The <code>--forward-only</code> flag is not required when applying changes to models that have been explicitly configured as forward-only.</p> <p>Use it only if you need to provide a time range for the preview window or the effective date.</p>"},{"location":"references/plans/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to forward-only models - learn more here.</p> <p>Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy: </p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models.  Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. </p> <p>For example, to allow destructive changes to all models in the <code>analytics</code> schema: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"analytics.*\"\n</code></pre><p></p> <p>Or to allow destructive changes to multiple specific models: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"sales.revenue_model\" --allow-destructive-model \"marketing.campaign_model\"\n</code></pre><p></p> <p>Learn more about model selectors here.</p>"},{"location":"references/plans/#effective-date","title":"Effective date","text":"<p>Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:</p> <pre><code>vulcan plan --forward-only --effective-from 2023-01-01\n</code></pre> <p>This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.</p>"},{"location":"references/plans/#restatement-plans","title":"Restatement plans","text":"<p>Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.</p> <p>For example, these scenarios all require re-evaluating model data that already exists:</p> <ul> <li>Correcting an upstream data issue by reprocessing some of a model's existing data</li> <li>Retroactively applying a forward-only plan change to some historical data</li> <li>Fully refreshing a model</li> </ul> <p>In Vulcan, reprocessing existing data is called a \"restatement.\"</p> <p>Restate one or more models' data with the <code>plan</code> command's <code>--restate-model</code> selector. The selector lets you specify which models to restate by name, wildcard, or tag (syntax below).</p> <p>No changes allowed</p> <p>Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.</p> <p>You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running <code>vulcan plan</code> without the <code>--restate-model</code> option.</p> <p>Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.</p> <p>You may restate external models. An external model is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.</p> <p>The plan's <code>--start</code> and <code>--end</code> date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more below.</p> <p>Just catching up</p> <p>Restatement plans \"catch models up\" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.</p> <p>If you pass an <code>--end</code> date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.</p> <p>To prevent models from ever being restated, set the disable_restatement attribute to <code>true</code>.</p> <p> These examples demonstrate how to select which models to restate based on model names or model tags.</p> Names OnlyUpstreamWildcardsUpstream + WildcardsSpecific Date Range <pre><code>vulcan plan --restate-model \"db.model_a\" --restate-model \"tag:expensive\"\n</code></pre> <pre><code># All selected models (including upstream models) will also include their downstream models\nvulcan plan --restate-model \"+db.model_a\" --restate-model \"+tag:expensive\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db*\" --restate-model \"tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"+db*\" --restate-model \"+tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre>"},{"location":"references/plans/#restating-production-vs-development","title":"Restating production vs development","text":"<p>Restatement plans behave differently depending on if you're targeting the <code>prod</code> environment or a development environment.</p> <p>If you target a development environment by including an environment name like <code>dev</code>:</p> <pre><code>vulcan plan dev --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the requested intervals for the specified model in the <code>dev</code> environment. In other environments, the model will be unaffected.</p> <p>However, if you target the <code>prod</code> environment by omitting an environment name:</p> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the intervals in the <code>prod</code> table and clear the model's time intervals from state in every other environment.</p> <p>The next time you do a run in <code>dev</code>, the intervals already reprocessed in <code>prod</code> are reprocessed in <code>dev</code> as well. This is to prevent old data from getting promoted to <code>prod</code> in the future.</p> <p>This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:</p> <ul> <li>Table <code>A</code> exists in <code>prod</code></li> <li>A virtual environment <code>dev</code> is created with new tables <code>B</code> and <code>C</code> downstream of <code>A</code><ul> <li>the DAG in <code>prod</code> looks like <code>A</code></li> <li>the DAG in <code>dev</code> looks like <code>A &lt;- B &lt;- C</code></li> </ul> </li> <li>A restatement plan is executed against table <code>A</code> in <code>prod</code></li> <li>Vulcan will clear the affected intervals for <code>B</code> and <code>C</code> in <code>dev</code> even though those tables do not exist in <code>prod</code></li> </ul> <p>Bringing development environments up to date</p> <p>A restatement plan against <code>prod</code> clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.</p> <p>Execute <code>vulcan run &lt;environment name&gt;</code> to trigger reprocessing in the development environment.</p> <p>This is necessary because a <code>prod</code> restatement plan only does work in the <code>prod</code> environment for speed and efficiency.</p>"},{"location":"references/state/","title":"State","text":""},{"location":"references/state/#state","title":"State","text":"<p>Vulcan stores information about your project in a state database that is usually separate from your main warehouse.</p> <p>The Vulcan state database contains:</p> <ul> <li>Information about every Model Version in your project (query, loaded intervals, dependencies)</li> <li>A list of every Virtual Data Environment in the project</li> <li>Which model versions are promoted into each Virtual Data Environment</li> <li>Information about any auto restatements present in your project</li> <li>Other metadata about your project such as current Vulcan / SQLGlot version</li> </ul> <p>The state database is how Vulcan \"remembers\" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for incremental models so you dont need to add branching logic into the model query to handle this.</p> <p>State database performance</p> <p>The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.</p> <p>For the best experience, we recommend databases designed for OLTP workloads such as PostgreSQL.</p> <p>Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and will lead to poor performance and consistency.</p> <p>For more information on engines suitable for the Vulcan state database, see the configuration guide.</p>"},{"location":"references/state/#exporting-importing-state","title":"Exporting / Importing State","text":"<p>Vulcan supports exporting the state database to a <code>.json</code> file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.</p>"},{"location":"references/state/#exporting-state","title":"Exporting state","text":"<p>Vulcan can export the state database to a file like so:</p> <pre><code>$ vulcan state export -o state.json\nExporting state to 'state.json' from the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\nContinue? [y/n]: y\n\n    Exporting versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Exporting snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nExporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState exported successfully to 'state.json'\n</code></pre> <p>This will produce a file <code>state.json</code> in the current directory containing the Vulcan state.</p> <p>The state file is a simple <code>json</code> file that looks like:</p> <pre><code>{\n    /* State export metadata */\n    \"metadata\": {\n        \"timestamp\": \"2025-03-16 23:09:00+00:00\", /* UTC timestamp of when the file was produced */\n        \"file_version\": 1, /* state export file format version */\n        \"importable\": true /* whether or not this file can be imported with `vulcan state import` */\n    },\n    /* Library versions used to produce this state export file */\n    \"versions\": {\n        \"schema_version\": 76 /* vulcan state database schema version */,\n        \"sqlglot_version\": \"26.10.1\" /* version of SQLGlot used to produce the state file */,\n        \"vulcan_version\": \"0.165.1\" /* version of Vulcan used to produce the state file */,\n    },\n    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */\n    \"snapshots\": [\n        { \"name\": \"...\" }\n    ],\n    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */\n    \"environments\": {\n        \"prod\": {\n            /* information about the environment itself */\n            \"environment\": {\n                \"...\"\n            },\n            /* information about any before_all / after_all statements for this environment */\n            \"statements\": [\n                \"...\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"references/state/#specific-environments","title":"Specific environments","text":"<p>You can export a specific environment like so:</p> <pre><code>$ vulcan state export --environment my_dev -o my_dev_state.json\n</code></pre> <p>Note that every snapshot that is part of the environment will be exported, not just the differences from <code>prod</code>. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.</p>"},{"location":"references/state/#local-state","title":"Local state","text":"<p>You can export local state like so:</p> <pre><code>$ vulcan state export --local -o local_state.json\n</code></pre> <p>This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.</p> <p>Therefore, a local state export will only have <code>snapshots</code> populated. <code>environments</code> will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as not importable so it cannot be used with a subsequent <code>vulcan state import</code> command.</p>"},{"location":"references/state/#importing-state","title":"Importing state","text":"<p>Back up your state database first!</p> <p>Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.</p> <p>Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means a import error has the potential to leave the state database in an inconsistent state.</p> <p>Vulcan can import a state file into the state database like so:</p> <pre><code>$ vulcan state import -i state.json --replace\nLoading state from 'state.json' into the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\n[WARNING] This destructive operation will delete all existing state against the 'dev' gateway\nand replace it with what\\'s in the 'state.json' file.\n\nAre you sure? [y/n]: y\n\nState File Information:\n\u251c\u2500\u2500 Creation Timestamp: 2025-03-31 02:15:00+00:00\n\u251c\u2500\u2500 File Version: 1\n\u251c\u2500\u2500 Vulcan version: 0.170.1.dev0\n\u251c\u2500\u2500 Vulcan migration version: 76\n\u2514\u2500\u2500 SQLGlot version: 26.12.0\n\n    Importing versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Importing snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nImporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState imported successfully from 'state.json'\n</code></pre> <p>Note that the state database structure needs to be present and up to date, so run <code>vulcan migrate</code> before running <code>vulcan state import</code> if you get a version mismatch error.</p> <p>If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the <code>--replace</code> parameter:</p> <pre><code>$ vulcan state import -i state.json\n...\n\n[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.\nMatching snapshots or environments will be replaced.\nNon-matching snapshots or environments will be ignored.\n\nAre you sure? [y/n]: y\n\n...\nState imported successfully from 'state.json'\n</code></pre>"},{"location":"references/state/#specific-gateways","title":"Specific gateways","text":"<p>If your project has multiple gateways with different state connections per gateway, you can target the state_connection of a specific gateway like so:</p> <pre><code># state export\n$ vulcan --gateway &lt;gateway&gt; state export -o state.json\n\n# state import\n$ vulcan --gateway &lt;gateway&gt; state import -i state.json\n</code></pre>"},{"location":"references/state/#version-compatibility","title":"Version Compatibility","text":"<p>When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.</p> <p>If you attempt to import state with an incompatible version, you will get the following error:</p> <pre><code>$ vulcan state import -i state.json\n...SNIP...\n\nState import failed!\nError: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.\nPlease upgrade/downgrade your Vulcan version to match the state file before performing the import.\n</code></pre>"},{"location":"references/state/#upgrading-a-state-file","title":"Upgrading a state file","text":"<p>You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:</p> <ul> <li>Loading it into a local database using the older Vulcan version</li> <li>Installing the newer Vulcan version</li> <li>Running <code>vulcan migrate</code> to upgrade the state within the local database</li> <li>Running <code>vulcan state export</code> to export it back out again. The new export is now compatible with the newer version of Vulcan.</li> </ul> <p>Below is an example of how to upgrade a state file created with Vulcan <code>0.164.1</code> to be compatible with Vulcan <code>0.165.1</code>.</p> <p>First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:</p> <pre><code>$ python -m venv migration-env\n\n$ . ./migration-env/bin/activate\n\n(migration-env)$\n</code></pre> <p>Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg <code>the state file was created with '0.164.1'</code> means you need to install Vulcan <code>0.164.1</code>:</p> <pre><code>(migration-env)$ pip install \"vulcan==0.164.1\"\n</code></pre> <p>Add a gateway to your <code>config.yaml</code> like so:</p> <pre><code>gateways:\n  migration:\n    connection:\n      type: duckdb\n      database: ./state-migration.duckdb\n</code></pre> <p>The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the <code>model_defaults</code> from your project in order to migrate state correctly which is why we have not used an isolated directory.</p> <p>Warning</p> <p>From here on, be sure to specify <code>--gateway migration</code> to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway</p> <p>You can now import your state export using the same version of Vulcan it was created with:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n\n(migration-env)$ vulcan --gateway migration state import -i state.json\n...\nState imported successfully from 'state.json'\n</code></pre> <p>Now we have the state imported, we can upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, eg <code>You are running '0.165.1'</code></p> <p>To upgrade Vulcan, simply install the new version:</p> <pre><code>(migration-env)$ pip install --upgrade \"vulcan==0.165.1\"\n</code></pre> <p>Migrate the state to the new version:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n</code></pre> <p>And finally, create a new state file which is now compatible with the new Vulcan version:</p> <pre><code> (migration-env)$ vulcan --gateway migration state export -o state-migrated.json\n</code></pre> <p>The <code>state-migrated.json</code> file is now compatible with the newer version of Vulcan. You can then transfer it to the place you originally needed it and import it in:</p> <pre><code>$ vulcan state import -i state-migrated.json\n...\nState imported successfully from 'state-migrated.json'\n</code></pre>"},{"location":"references/integrations/engines/athena/","title":"Athena","text":""},{"location":"references/integrations/engines/athena/#athena","title":"Athena","text":""},{"location":"references/integrations/engines/athena/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[athena]\"\n</code></pre>"},{"location":"references/integrations/engines/athena/#connection-options","title":"Connection options","text":""},{"location":"references/integrations/engines/athena/#pyathena-connection-options","title":"PyAthena connection options","text":"<p>Vulcan leverages the PyAthena DBAPI driver to connect to Athena. Therefore, the connection options relate to the PyAthena connection options. Note that PyAthena uses boto3 under the hood so you can also use boto3 environment variables for configuration.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>athena</code> string Y <code>aws_access_key_id</code> The access key for your AWS user string N <code>aws_secret_access_key</code> The secret key for your AWS user string N <code>role_arn</code> The ARN of a role to assume once authenticated string N <code>role_session_name</code> The session name to use when assuming <code>role_arn</code> string N <code>region_name</code> The AWS region to use string N <code>work_group</code> The Athena workgroup to send queries to string N <code>s3_staging_dir</code> The S3 location for Athena to write query results. Only required if not using <code>work_group</code> OR the configured <code>work_group</code> doesnt have a results location set string N <code>schema_name</code> The default schema to place objects in if a schema isnt specified. Defaults to <code>default</code> string N <code>catalog_name</code> The default catalog to place schemas in. Defaults to <code>AwsDataCatalog</code> string N"},{"location":"references/integrations/engines/athena/#vulcan-connection-options","title":"Vulcan connection options","text":"<p>These options are specific to Vulcan itself and are not passed to PyAthena</p> Option Description Type Required <code>s3_warehouse_location</code> Set the base path in S3 where Vulcan will instruct Athena to place table data. Only required if you arent specifying the location in the model itself. See S3 Locations below. string N"},{"location":"references/integrations/engines/athena/#model-properties","title":"Model properties","text":"<p>The Athena adapter utilises the following model top-level properties:</p> Name Description Type Required <code>table_format</code> Sets the table_type Athena uses when creating the table. Valid values are <code>hive</code> or <code>iceberg</code>. string N <code>storage_format</code> Configures the file format to be used by the <code>table_format</code>. For Hive tables, this sets the STORED AS option. For Iceberg tables, this sets format property. string N <p>The Athena adapter recognises the following model physical_properties:</p> Name Description Type Default <code>s3_base_location</code> <code>s3://</code> base URI of where the snapshot tables for this model should be written. Overrides <code>s3_warehouse_location</code> if one is configured. string"},{"location":"references/integrations/engines/athena/#s3-locations","title":"S3 Locations","text":"<p>When creating tables, Athena needs to know where in S3 the table data is located. You cannot issue a <code>CREATE TABLE</code> statement without specifying a <code>LOCATION</code> for the table data.</p> <p>In addition, unlike other engines such as Trino, Athena will not infer a table location if you set a schema location via <code>CREATE SCHEMA &lt;schema&gt; LOCATION 's3://schema/location'</code>.</p> <p>Therefore, in order for Vulcan to issue correct <code>CREATE TABLE</code> statements to Athena, you need to configure where the tables should be stored. There are two options for this:</p> <ul> <li>Project-wide: set <code>s3_warehouse_location</code> in the connection config. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_warehouse_location&gt;/&lt;schema_name&gt;/&lt;snapshot_table_name&gt;</code> when it creates a snapshot of your model.</li> <li>Per-model: set <code>s3_base_location</code> in the model <code>physical_properties</code>. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_base_location&gt;/&lt;snapshot_table_name&gt;</code> every time it creates a snapshot of your model. This takes precedence over any <code>s3_warehouse_location</code> set in the connection config.</li> </ul>"},{"location":"references/integrations/engines/athena/#limitations","title":"Limitations","text":"<p>Athena was initially designed to read data stored in S3 and to do so without changing that data. This means that it does not have good support for mutating tables. In particular, it will not delete data from Hive tables.</p> <p>Consequently, forward only changes that mutate the schemas of existing tables have a high chance of failure because Athena supports very limited schema modifications on Hive tables.</p> <p>However, Athena does support Apache Iceberg tables which allow a full range of operations. These can be used for more complex model types such as <code>INCREMENTAL_BY_UNIQUE_KEY</code> and <code>SCD_TYPE_2</code>.</p> <p>To use an Iceberg table for a model, set <code>table_format iceberg</code> in the model properties.</p> <p>In general, Iceberg tables offer the most flexibility and you'll run into the least Vulcan limitations when using them. However, we create Hive tables by default because Athena creates Hive tables by default, so Iceberg tables are opt-in rather than opt-out.</p>"},{"location":"references/integrations/engines/azuresql/","title":"Azure SQL","text":""},{"location":"references/integrations/engines/azuresql/#azure-sql","title":"Azure SQL","text":"<p>Azure SQL is \"a family of managed, secure, and intelligent products that use the SQL Server database engine in the Azure cloud.\"</p>"},{"location":"references/integrations/engines/azuresql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>azuresql</code></p>"},{"location":"references/integrations/engines/azuresql/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/azuresql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[azuresql]\"\n</code></pre>"},{"location":"references/integrations/engines/azuresql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[azuresql-odbc]\"\n</code></pre>"},{"location":"references/integrations/engines/azuresql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>azuresql</code> string Y <code>host</code> The hostname of the Azure SQL server string Y <code>user</code> The username / client ID to use for authentication with the Azure SQL server string N <code>password</code> The password / client secret to use for authentication with the Azure SQL server string N <code>port</code> The port number of the Azure SQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"references/integrations/engines/bigquery/","title":"BigQuery","text":""},{"location":"references/integrations/engines/bigquery/#bigquery","title":"BigQuery","text":""},{"location":"references/integrations/engines/bigquery/#introduction","title":"Introduction","text":"<p>This guide provides step-by-step instructions on how to connect Vulcan to the BigQuery SQL engine.</p> <p>It will walk you through the steps of installing Vulcan and BigQuery connection libraries locally, configuring the connection in Vulcan, and running the quickstart project.</p>"},{"location":"references/integrations/engines/bigquery/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes the following about the BigQuery project being used with Vulcan:</p> <ul> <li>The project already exists</li> <li>Project CLI/API access is enabled</li> <li>Project billing is configured (i.e. it's not a sandbox project)</li> <li>Vulcan can authenticate using an account with permissions to execute commands against the project</li> </ul>"},{"location":"references/integrations/engines/bigquery/#installation","title":"Installation","text":"<p>Follow the quickstart guide to set up Vulcan, then install the necessary BigQuery libraries.</p> <p>Instead of installing just Vulcan core, we will also include the BigQuery engine libraries:</p> <pre><code>&gt; pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#install-google-cloud-sdk","title":"Install Google Cloud SDK","text":"<p>Vulcan connects to BigQuery via the Python <code>google-cloud-bigquery</code> library, which uses the Google Cloud SDK <code>gcloud</code> tool for authenticating with BigQuery.</p> <p>Follow these steps to install and configure the Google Cloud SDK on your computer:</p> <ul> <li>Download the appropriate installer for your system from the Google Cloud installation guide</li> <li> <p>Unpack the downloaded file with the <code>tar</code> command:</p> <pre><code>&gt; tar -xzvf google-cloud-cli-{SYSTEM_SPECIFIC_INFO}.tar.gz\n</code></pre> </li> <li> <p>Run the installation script:</p> <pre><code>&gt; ./google-cloud-sdk/install.sh\n</code></pre> </li> <li> <p>Reload your shell profile (e.g., for zsh):</p> <pre><code>&gt; source $HOME/.zshrc\n</code></pre> </li> <li> <p>Run <code>gcloud init</code> to setup authentication</p> </li> </ul>"},{"location":"references/integrations/engines/bigquery/#configuration","title":"Configuration","text":""},{"location":"references/integrations/engines/bigquery/#configure-vulcan-for-bigquery","title":"Configure Vulcan for BigQuery","text":"<p>Add the following gateway specification to your Vulcan project's <code>config.yaml</code> file:</p> <pre><code>bigquery:\n  connection:\n    type: bigquery\n    project: &lt;your_project_id&gt;\n\ndefault_gateway: bigquery\n</code></pre> <p>This creates a gateway named <code>bigquery</code> and makes it your project's default gateway.</p> <p>It uses the <code>oauth</code> authentication method, which does not specify a username or other information directly in the connection configuration. Other authentication methods are described below.</p> <p>In BigQuery, navigate to the dashboard and select the BigQuery project your Vulcan project will use. From the Google Cloud dashboard, use the arrow to open the pop-up menu:</p> <p></p> <p>Now we can identify the project ID needed in the <code>config.yaml</code> gateway specification above. Select the project that you want to work with, the project ID that you need to add to your yaml file is the ID label from the pop-up menu.</p> <p></p> <p>For this guide, the Docs-Demo is the one we will use, thus the project ID for this example is <code>healthy-life-440919-s0</code>.</p>"},{"location":"references/integrations/engines/bigquery/#usage","title":"Usage","text":""},{"location":"references/integrations/engines/bigquery/#test-the-connection","title":"Test the connection","text":"<p>Run the following command to verify that Vulcan can connect to BigQuery:</p> <pre><code>&gt; vulcan info\n</code></pre> <p>The output will look something like this:</p> <p></p> <ul> <li> <p>Set quota project (optional)</p> <p>You may see warnings like this when you run <code>vulcan info</code>:</p> <p></p> <p>You can avoid these warnings about quota projects by running:</p> <pre><code>&gt; gcloud auth application-default set-quota-project &lt;your_project_id&gt;\n&gt; gcloud config set project &lt;your_project_id&gt;\n</code></pre> </li> </ul>"},{"location":"references/integrations/engines/bigquery/#create-and-run-a-plan","title":"Create and run a plan","text":"<p>We've verified our connection, so we're ready to create and execute a plan in BigQuery:</p> <pre><code>&gt; vulcan plan\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#view-results-in-bigquery-console","title":"View results in BigQuery Console","text":"<p>Let's confirm that our project models are as expected.</p> <p>First, navigate to the BigQuery Studio Console:</p> <p></p> <p>Then use the left sidebar to find your project and the newly created models:</p> <p></p> <p>We have confirmed that our Vulcan project is running properly in BigQuery!</p>"},{"location":"references/integrations/engines/bigquery/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>bigquery</code></p>"},{"location":"references/integrations/engines/bigquery/#installation_1","title":"Installation","text":"<pre><code>pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>bigquery</code> string Y <code>method</code> Connection methods - see allowed values below. Default: <code>oauth</code>. string N <code>project</code> The ID of the GCP project string N <code>location</code> The location of for the datasets (can be regional or multi-regional) string N <code>execution_project</code> The name of the GCP project to bill for the execution of the models. If not set, the project associated with the model will be used. string N <code>quota_project</code> The name of the GCP project used for the quota. If not set, the <code>quota_project_id</code> set within the credentials of the account is used to authenticate to BigQuery. string N <code>keyfile</code> Path to the keyfile to be used with service-account method string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>token</code> OAuth 2.0 access token string N <code>refresh_token</code> OAuth 2.0 refresh token string N <code>client_id</code> OAuth 2.0 client ID string N <code>client_secret</code> OAuth 2.0 client secret string N <code>token_uri</code> OAuth 2.0 authorization server's token endpoint URI string N <code>scopes</code> The scopes used to obtain authorization list N <code>impersonated_service_account</code> If set, Vulcan will attempt to impersonate this service account string N <code>job_creation_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to be created. int N <code>job_execution_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to complete. int N <code>job_retries</code> The number of times to retry the underlying job if it fails. (Default: <code>1</code>) int N <code>priority</code> The priority of the underlying job. (Default: <code>INTERACTIVE</code>) string N <code>maximum_bytes_billed</code> The maximum number of bytes to be billed for the underlying job. int N"},{"location":"references/integrations/engines/bigquery/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>oauth (default)<ul> <li>Related Credential Configuration:<ul> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>oauth-secrets<ul> <li>Related Credential Configuration:<ul> <li><code>token</code> (Optional): Can be None if refresh information is provided.</li> <li><code>refresh_token</code> (Optional): If specified, credentials can be refreshed.</li> <li><code>client_id</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>client_secret</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>token_uri</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>scopes</code> (Optional): OAuth 2.0 credentials can not request additional scopes after authorization. The scopes must be derivable from the refresh token if refresh information is provided (e.g. The refresh token scopes are a superset of this or contain a wild card scope like 'https://www.googleapis.com/auth/any-api')</li> </ul> </li> </ul> </li> <li>service-account<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>service-account-json<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile_json</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> </ul> <p>If the <code>impersonated_service_account</code> argument is set, Vulcan will:</p> <ol> <li>Authenticate user account credentials with one of the methods above</li> <li>Attempt to impersonate the service account with those credentials</li> </ol> <p>The user account must have sufficient permissions to impersonate the service account.</p>"},{"location":"references/integrations/engines/bigquery/#permissions-required","title":"Permissions Required","text":"<p>With any of the above connection methods, ensure these BigQuery permissions are enabled to allow Vulcan to work correctly.</p> <ul> <li><code>BigQuery Data Owner</code></li> <li><code>BigQuery User</code></li> </ul>"},{"location":"references/integrations/engines/clickhouse/","title":"ClickHouse","text":""},{"location":"references/integrations/engines/clickhouse/#clickhouse","title":"ClickHouse","text":"<p>This page describes Vulcan support for the ClickHouse engine, including configuration options specific to ClickHouse.</p> <p>Note</p> <p>ClickHouse may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/clickhouse/#background","title":"Background","text":"<p>ClickHouse is a distributed, column-oriented SQL engine designed to rapidly execute analytical workloads.</p> <p>It provides users fine-grained control of its behavior, but that control comes at the cost of complex configuration.</p> <p>This section provides background information about ClickHouse, providing context for how to use Vulcan with the ClickHouse engine.</p>"},{"location":"references/integrations/engines/clickhouse/#object-naming","title":"Object naming","text":"<p>Most SQL engines use a three-level hierarchical naming scheme: tables/views are nested within schemas, and schemas are nested within catalogs. For example, the full name of a table might be <code>my_catalog.my_schema.my_table</code>.</p> <p>ClickHouse instead uses a two-level hierarchical naming scheme that has no counterpart to catalog. In addition, it calls the second level in the hierarchy \"databases.\" Vulcan and its documentation refer to this second level as \"schemas.\"</p> <p>Vulcan fully supports ClickHouse's two-level naming scheme without user action.</p>"},{"location":"references/integrations/engines/clickhouse/#table-engines","title":"Table engines","text":"<p>Every ClickHouse table is created with a \"table engine\" that controls how the table's data is stored and queried. ClickHouse's (and Vulcan's) default table engine is <code>MergeTree</code>.</p> <p>The <code>MergeTree</code> engine family requires that every table be created with an <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY</code> clause into every <code>MergeTree</code> family table's <code>CREATE</code> statement, or you can specify the columns/expressions by which the table should be ordered.</p>"},{"location":"references/integrations/engines/clickhouse/#clickhouse-modes-of-operation","title":"ClickHouse modes of operation","text":"<p>Conceptually, it may be helpful to view ClickHouse as having three modes of operation: single server, cluster, and ClickHouse Cloud. Vulcan supports all three modes.</p>"},{"location":"references/integrations/engines/clickhouse/#single-server-mode","title":"Single server mode","text":"<p>Single server mode is similar to other SQL engines: aside from choosing each table's engine, you do not need to worry about how computations are executed. You issue standard SQL commands/queries, and ClickHouse executes them.</p>"},{"location":"references/integrations/engines/clickhouse/#cluster-mode","title":"Cluster mode","text":"<p>Cluster mode allows you to scale your ClickHouse engine to any number of networked servers. This enables massive workloads, but requires that you specify how computations are executed by the networked servers.</p> <p>ClickHouse coordinates the computations on the networked servers with ClickHouse Keeper (it also supports Apache ZooKeeper).</p> <p>You specify named virtual clusters of servers in the Keeper configuration, and those clusters provide namespaces for data objects and computations. For example, you might include all networked servers in the cluster you name <code>MyCluster</code>.</p> <p>In general, you must be connected to a ClickHouse server to execute commands. By default, each command you execute runs in single-server mode on the server you are connected to.</p> <p>To associate an object with a cluster, DDL commands that create or modify it must include the text <code>ON CLUSTER [your cluster name]</code>.</p> <p>If you provide a cluster name in your Vulcan connection configuration, Vulcan will automatically inject the <code>ON CLUSTER</code> statement into the DDL commands for all objects created while executing the project. We provide more information about clusters in Vulcan below.</p>"},{"location":"references/integrations/engines/clickhouse/#clickhouse-cloud-mode","title":"ClickHouse Cloud mode","text":"<p>ClickHouse Cloud is a managed ClickHouse platform. It allows you to scale ClickHouse without administering a cluster yourself or modifying your SQL commands to run on the cluster.</p> <p>ClickHouse Cloud automates ClickHouse's cluster controls, which sometimes constrains ClickHouse's flexibility or how you execute SQL commands. For example, creating a table with a <code>SELECT</code> command must occur in two steps on ClickHouse Cloud. Vulcan handles this limitation for you.</p> <p>Aside from those constraints, ClickHouse Cloud mode is similar to single server mode - you run standard SQL commands/queries, and ClickHouse Cloud executes them.</p>"},{"location":"references/integrations/engines/clickhouse/#permissions","title":"Permissions","text":"<p>In the default Vulcan configuration, users must have sufficient permissions to create new ClickHouse databases.</p> <p>Alternatively, you can configure specific databases where Vulcan should create table and view objects.</p>"},{"location":"references/integrations/engines/clickhouse/#environment-views","title":"Environment views","text":"<p>Use the <code>environment_suffix_target</code> key in your project configuration to specify that environment views should be created within the model's database instead of in a new database:</p> <pre><code>environment_suffix_target: table\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#physical-tables","title":"Physical tables","text":"<p>Use the <code>physical_schema_mapping</code> key in your project configuration to specify the databases where physical tables should be created.</p> <p>The key accepts a dictionary of regular expressions that map model database names to the corresponding databases where physical tables should be created.</p> <p>Vulcan will compare a model's database name to each regular expression and use the first match to determine which database a physical table should be created in.</p> <p>For example, this configuration places every model's physical table in the <code>model_physical_tables</code> database because the regular expression <code>.*</code> matches any database name:</p> <pre><code>physical_schema_mapping:\n  '.*': model_physical_tables\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#cluster-specification","title":"Cluster specification","text":"<p>A ClickHouse cluster allows multiple networked ClickHouse servers to operate on the same data object. Every cluster must be named in the ClickHouse configuration files, and that name is passed to a table's DDL statements in the <code>ON CLUSTER</code> clause.</p> <p>For example, we could create a table <code>my_schema.my_table</code> on cluster <code>TheCluster</code> like this: <code>CREATE TABLE my_schema.my_table ON CLUSTER TheCluster (col1 Int8)</code>.</p> <p>To create Vulcan objects on a cluster, provide the cluster name to the <code>cluster</code> key in the Vulcan connection definition (see all connection parameters below).</p> <p>Vulcan will automatically inject the <code>ON CLUSTER</code> clause and cluster name you provide into all project DDL statements.</p>"},{"location":"references/integrations/engines/clickhouse/#model-definition","title":"Model definition","text":"<p>This section describes how you control a table's engine and other ClickHouse-specific functionality in Vulcan models.</p>"},{"location":"references/integrations/engines/clickhouse/#table-engine","title":"Table engine","text":"<p>Vulcan uses the <code>MergeTree</code> table engine with an empty <code>ORDER BY</code> clause by default.</p> <p>Specify a different table engine by passing the table engine definition to the model DDL's <code>storage_format</code> parameter. For example, you could specify the <code>Log</code> table engine like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    storage_format Log,\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>You may also specify more complex table engine definitions. For example:</p> <pre><code>MODEL (\n    name my_schema.my_rep_table,\n    kind full,\n    storage_format ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name', '{replica}', ver),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#order-by","title":"ORDER BY","text":"<p><code>MergeTree</code> family engines require that a table's <code>CREATE</code> statement include the <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY ()</code> when creating a table with an engine in the <code>MergeTree</code> family. This creates the table without any ordering.</p> <p>You may specify columns/expressions to <code>ORDER BY</code> by passing them to the model <code>physical_properties</code> dictionary's <code>order_by</code> key.</p> <p>For example, you could order by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2)\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>order_by</code> key name and value <code>(col1, col2)</code>.</p> <p>Complex <code>ORDER BY</code> expressions may need to be passed in single quotes, with interior single quotes escaped by the <code>\\</code> character.</p>"},{"location":"references/integrations/engines/clickhouse/#primary-key","title":"PRIMARY KEY","text":"<p>Table engines may also accept a <code>PRIMARY KEY</code> specification. Similar to <code>ORDER BY</code>, specify a primary key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>primary_key</code> key name and value <code>col1</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#ttl","title":"TTL","text":"<p>ClickHouse tables accept a TTL expression that triggers actions like deleting rows after a certain amount of time has passed.</p> <p>Similar to <code>ORDER_BY</code> and <code>PRIMARY_KEY</code>, specify a TTL key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1,\n        ttl = timestamp + INTERVAL 1 WEEK\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>ttl</code> key name and value <code>timestamp + INTERVAL 1 WEEK</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#partitioning","title":"Partitioning","text":"<p>Some ClickHouse table engines support partitioning. Specify the partitioning columns/expressions in the model DDL's <code>partitioned_by</code> key.</p> <p>For example, you could partition by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    partitioned_by (col1, col2),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Learn more below about how Vulcan uses partitioned tables to improve performance.</p>"},{"location":"references/integrations/engines/clickhouse/#settings","title":"Settings","text":"<p>ClickHouse supports an immense number of settings, many of which can be altered in multiple places: ClickHouse configuration files, Python client connection arguments, DDL statements, SQL queries, and others.</p> <p>This section discusses how to control ClickHouse settings in Vulcan.</p>"},{"location":"references/integrations/engines/clickhouse/#connection-settings","title":"Connection settings","text":"<p>Vulcan connects to Python with the <code>clickhouse-connect</code> library. Its connection method accepts a dictionary of arbitrary settings that are passed to ClickHouse.</p> <p>Specify these settings in the <code>connection_settings</code> key. This example demonstrates how to set the <code>distributed_ddl_task_timeout</code> setting to <code>300</code>:</p> <pre><code>clickhouse_gateway:\n  connection:\n    type: clickhouse\n    host: localhost\n    port: 8123\n    username: user\n    password: pw\n    connection_settings:\n      distributed_ddl_task_timeout: 300\n  state_connection:\n    type: duckdb\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#ddl-settings","title":"DDL settings","text":"<p>ClickHouse settings may also be specified in DDL commands like <code>CREATE</code>.</p> <p>Specify these settings in a model DDL's <code>physical_properties</code> key (where the <code>order_by</code> and <code>primary_key</code> values are specified, if present).</p> <p>This example demonstrates how to set the <code>index_granularity</code> setting to <code>128</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        index_granularity = 128\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>index_granularity</code> key name and value <code>128</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#query-settings","title":"Query settings","text":"<p>ClickHouse settings may be specified directly in a model's query with the <code>SETTINGS</code> keyword.</p> <p>This example demonstrates setting the <code>join_use_nulls</code> setting to <code>1</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n);\n\nselect\n    *\nfrom other_schema.other_table\nSETTINGS join_use_nulls = 1;\n</code></pre> <p>Multiple settings may be specified in a query with repeated use of the <code>SETTINGS</code> keyword: <code>SELECT * FROM other_table SETTINGS first_setting = 1 SETTINGS second_setting = 2;</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#usage-by-vulcan","title":"Usage by Vulcan","text":"<p>The ClickHouse setting <code>join_use_nulls</code> affects the behavior of Vulcan SCD models and table diffs. This section describes how Vulcan uses query settings to control that behavior.</p> <p>Background</p> <p>In general, table <code>JOIN</code>s can return empty cells for rows not present in both tables.</p> <p>For example, consider <code>LEFT JOIN</code>ing two tables <code>left</code> and <code>right</code>, where the column <code>right_column</code> is only present in the <code>right</code> table. Any rows only present in the <code>left</code> table will have no value for <code>right_column</code> in the joined table.</p> <p>In other SQL engines, those empty cells are filled with <code>NULL</code>s.</p> <p>In contrast, ClickHouse fills the empty cells with data type-specific default values (e.g., 0 for integer column types). It will instead fill the cells with <code>NULL</code>s if you set the <code>join_use_nulls</code> setting to <code>1</code>.</p> <p>Vulcan</p> <p>Vulcan automatically generates SQL queries for both SCD Type 2 models and table diff comparisons. These queries include table <code>JOIN</code>s and calculations based on the presence of <code>NULL</code> values.</p> <p>Because those queries expect <code>NULL</code> values in empty cells, Vulcan automatically adds <code>SETTINGS join_use_nulls = 1</code> to the generated SCD and table diff SQL code.</p> <p>The SCD model definition query is embedded as a CTE in the full Vulcan-generated query. If run alone, the model definition query would use the ClickHouse server's current <code>join_use_nulls</code> value.</p> <p>If that value is not <code>1</code>, the Vulcan setting on the outer query would override the server value and produce incorrect results.</p> <p>Therefore, Vulcan uses the following procedure to ensure the model definition query runs with the correct <code>join_use_nulls</code> value:</p> <ul> <li>If the model query sets <code>join_use_nulls</code> itself, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>1</code>, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>0</code>, add <code>SETTINGS join_use_nulls = 0</code> to the CTE model query<ul> <li>All other CTEs and the outer query will still execute with a <code>join_use_nulls</code> value of <code>1</code></li> </ul> </li> </ul>"},{"location":"references/integrations/engines/clickhouse/#performance-considerations","title":"Performance considerations","text":"<p>ClickHouse is optimized for writing/reading records, so deleting/replacing records can be extremely slow.</p> <p>This section describes why Vulcan needs to delete/replace records and how the ClickHouse engine adapter works around the limitations.</p>"},{"location":"references/integrations/engines/clickhouse/#why-delete-or-replace","title":"Why delete or replace?","text":"<p>Vulcan \"materializes\" model kinds in a number of ways, such as:</p> <ul> <li>Replacing an entire table (<code>FULL</code> models)</li> <li>Replacing records in a specific time range (<code>INCREMENTAL_BY_TIME_RANGE</code> models)</li> <li>Replacing records with specific key values (<code>INCREMENTAL_BY_UNIQUE_KEY</code> models)</li> <li>Replacing records in specific partitions (<code>INCREMENTAL_BY_PARTITION</code> models)</li> </ul> <p>Different SQL engines provide different methods for performing record replacement.</p> <p>Some engines natively support updating or inserting (\"upserting\") records. For example, in some engines you can <code>merge</code> a new table into an existing table based on a key. Records in the new table whose keys are already in the existing table will update/replace the existing records. Records in the new table without keys in the existing table will be inserted into the existing table.</p> <p>Other engines do not natively support upserts, so Vulcan replaces records in two steps: delete the records to update/replace from the existing table, then insert the new records.</p> <p>ClickHouse does not support upserts, and it performs the two step delete/insert operation so slowly as to be unusable. Therefore, Vulcan uses a different method for replacing records.</p>"},{"location":"references/integrations/engines/clickhouse/#temp-table-swap","title":"Temp table swap","text":"<p>Vulcan uses what we call the \"temp table swap\" method of replacing records in ClickHouse.</p> <p>Because ClickHouse is optimized for writing and reading records, it is often faster to copy most of a table than to delete a small portion of its records. That is the approach used by the temp table swap method (with optional performance improvements for partitioned tables).</p> <p>The temp table swap has four steps:</p> <ol> <li>Make an empty temp copy of the existing table that has the same structure (columns, data types, table engine, etc.)</li> <li>Insert new records into the temp table</li> <li>Insert the existing records that should be kept into the temp table</li> <li>Swap the table names, such that the temp table now has the existing table's name</li> </ol> <p>Figure 1 illustrates these four steps: </p> <p> Figure 1: steps to execute a temp table swap </p> <p>The weakness of this method is that it requires copying all existing rows to keep (step three), which can be problematic for large tables.</p> <p>To address this weakness, Vulcan instead uses partition swapping if a table is partitioned.</p>"},{"location":"references/integrations/engines/clickhouse/#partition-swap","title":"Partition swap","text":"<p>ClickHouse supports partitioned tables, which store groups of records in separate files, or \"partitions.\"</p> <p>A table is partitioned based on a table column or SQL expression - the \"partitioning key.\" All records with the same value for the partitioning key are stored together in a partition.</p> <p>For example, consider a table containing each record's creation date in a datetime column. If we partition the table by month, all the records whose timestamp was in January will be stored in one partition, records from February in another partition, and so on.</p> <p>Table partitioning provides a major benefit for improving swap performance: records can be inserted, updated, or deleted in individual partitions.</p> <p>Vulcan leverages this to avoid copying large numbers of existing records into a temp table. Instead, it only copies the records that are in partitions affected by a load's newly ingested records.</p> <p>Vulcan automatically uses partition swapping for any incremental model that specifies the <code>partitioned_by</code> key.</p>"},{"location":"references/integrations/engines/clickhouse/#choosing-a-partitioning-key","title":"Choosing a partitioning key","text":"<p>The first step of partitioning a table is choosing its partitioning key (columns or expression). The primary consideration for a key is the total number of partitions it will generate, which affects table performance.</p> <p>Too many partitions can drastically decrease performance because the overhead of handling partition files swamps the benefits of copying fewer records. Too few partitions decreases swap performance because many existing records must still be copied in each incremental load.</p> <p>How many partitions is too many?</p> <p>ClickHouse's documentation specifically warns against tables having too many partitions, suggesting a maximum of 1000.</p> <p>The total number of partitions in a table is determined by the actual data in the table, not by the partition column/expression alone.</p> <p>For example, consider a table partitioned by date. If we insert records created on <code>2024-10-23</code>, the table will have one partition. If we then insert records from <code>2024-10-24</code>, the table will have two partitions. One partition is created for each unique value of the key.</p> <p>For each partitioned table in your project, carefully consider the number of partitions created by the combination of your partitioning expression and the characteristics of your data.</p>"},{"location":"references/integrations/engines/clickhouse/#incremental-by-time-models","title":"Incremental by time models","text":"<p><code>INCREMENTAL_BY_TIME_RANGE</code> kind models must be partitioned by time. If the model's <code>time_column</code> is not present in any <code>partitioned_by</code> expression, Vulcan will automatically add it as the first partitioning expression.</p> <p>By default, <code>INCREMENTAL_BY_TIME_RANGE</code> models partition by week, so the maximum recommended 1000 partitions corresponds to about 19 years of data. Vulcan projects have widely varying time ranges and data sizes, so you should choose a model's partitioning key based on the data your system will process.</p> <p>If a model has many records in each partition, you may see additional performance benefits by including the time column in the model's <code>ORDER_BY</code> expression.</p> <p>Partitioning by time</p> <p><code>INCREMENTAL_BY_TIME_RANGE</code> models must be partitioned by time.</p> <p>Vulcan will automatically partition them by week unless the <code>partitioned_by</code> configuration key includes the time column or an expression based on it.</p> <p>Choose a model's time partitioning granularity based on the characteristics of the data it will process, making sure the total number of partitions is 1000 or fewer.</p>"},{"location":"references/integrations/engines/clickhouse/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>clickhouse</code></p> Option Description Type Required <code>type</code> Engine type name - must be <code>clickhouse</code> string Y <code>host</code> ClickHouse server hostname or IP address string Y <code>username</code> ClickHouse user name string Y <code>password</code> ClickHouse user password string N <code>port</code> The ClickHouse HTTP or HTTPS port (Default: <code>8123</code>) int N <code>cluster</code> ClickHouse cluster name string N <code>connect_timeout</code> Connection timeout in seconds (Default: <code>10</code>) int N <code>send_receive_timeout</code> Send/receive timeout in seconds (Default: <code>300</code>) int N <code>query_limit</code> Query result limit (Default: <code>0</code> - no limit) int N <code>use_compression</code> Whether to use compression (Default: <code>True</code>) bool N <code>compression_method</code> Compression method to use string N <code>http_proxy</code> HTTP proxy address (equivalent to setting the HTTP_PROXY environment variable) string N <code>verify</code> Verify server TLS/SSL certificate (Default: <code>True</code>) bool N <code>ca_cert</code> Ignored if verify is <code>False</code>. If verify is <code>True</code>, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Not necessary if the ClickHouse server certificate is a globally trusted root as verified by the operating system. string N <code>client_cert</code> File path to a TLS Client certificate in .pem format (for mutual TLS authentication). The file should contain a full certificate chain, including any intermediate certificates. string N <code>client_cert_key</code> File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file. string N <code>https_proxy</code> HTTPS proxy address (equivalent to setting the HTTPS_PROXY environment variable) string N <code>server_host_name</code> The ClickHouse server hostname as identified by the CN or SNI of its TLS certificate. Set this to avoid SSL errors when connecting through a proxy or tunnel with a different hostname. string N <code>tls_mode</code> Controls advanced TLS behavior. proxy and strict do not invoke ClickHouse mutual TLS connection, but do send client cert and key. mutual assumes ClickHouse mutual TLS auth with a client certificate. string N <code>connection_settings</code> Additional connection settings dict N <code>connection_pool_options</code> Additional options                                                                                                                                         for the HTTP connection pool dict N"},{"location":"references/integrations/engines/coming_soon/","title":"Engines Coming Soon","text":""},{"location":"references/integrations/engines/coming_soon/#engines-coming-soon","title":"Engines Coming Soon","text":"<p>The following execution engines are coming soon:</p> <ul> <li>Athena</li> <li>Azure SQL</li> <li>BigQuery</li> <li>ClickHouse</li> <li>Databricks</li> <li>DuckDB</li> <li>Fabric</li> <li>MotherDuck</li> <li>MSSQL</li> <li>MySQL</li> <li>GCP Postgres</li> <li>Redshift</li> <li>RisingWave</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"references/integrations/engines/databricks/","title":"Databricks","text":""},{"location":"references/integrations/engines/databricks/#databricks","title":"Databricks","text":"<p>This page provides information about how to use Vulcan with the Databricks SQL engine. It begins with a description of the three methods for connecting Vulcan to Databricks.</p> <p>After that is a Connection Quickstart that demonstrates how to connect to Databricks, or you can skip directly to information about using Databricks with the built-in.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connection-methods","title":"Databricks connection methods","text":"<p>Databricks provides multiple computing options and connection methods. This section describes the three methods for connecting with Vulcan.</p>"},{"location":"references/integrations/engines/databricks/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Vulcan connects to Databricks with the Databricks SQL Connector library by default.</p> <p>The SQL Connector is bundled with Vulcan and automatically installed when you include the <code>databricks</code> extra in the command <code>pip install \"vulcan[databricks]\"</code>.</p> <p>The SQL Connector has all the functionality needed for Vulcan to execute SQL models on Databricks and Python models that do not return PySpark DataFrames.</p> <p>If you have Python models returning PySpark DataFrames, check out the Databricks Connect section.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connect","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan must use the Databricks Connect library to connect to Databricks (instead of the Databricks SQL Connector library).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>Find more configuration details below.</p>"},{"location":"references/integrations/engines/databricks/#databricks-notebook-interface","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly in a Databricks Cluster interface, the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>Find more configuration details below.</p>"},{"location":"references/integrations/engines/databricks/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Databricks.</p> <p>It demonstrates connecting to a Databricks All-Purpose Compute instance with the <code>databricks-sql-connector</code> Python library bundled with Vulcan.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"references/integrations/engines/databricks/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Databricks account with access to an appropriate Databricks Workspace<ul> <li>The Workspace must support authenticating with personal access tokens (Databricks Community Edition workspaces do not)</li> <li>Your account must have Workspace Access and Create Compute permissions (these permissions are enabled by default)</li> </ul> </li> <li>Your Databricks compute resources have Unity Catalog activated</li> <li>Your computer has Vulcan installed with the Databricks extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[databricks]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init duckdb</code></li> </ul> </li> </ol> <p>Unity Catalog required</p> <p>Databricks compute resources used by Vulcan must have Unity Catalog activated.</p>"},{"location":"references/integrations/engines/databricks/#get-connection-info","title":"Get connection info","text":"<p>The first step to configuring a Databricks connection is gathering the necessary information from your Databricks compute instance.</p>"},{"location":"references/integrations/engines/databricks/#create-compute","title":"Create Compute","text":"<p>We must have something to connect to, so we first create and activate a Databricks compute instance. If you already have one running, skip to the next section.</p> <p>We begin in the default view for our Databricks Workspace. Access the Compute view by clicking the <code>Compute</code> entry in the left-hand menu:</p> <p></p> <p>In the Compute view, click the <code>Create compute</code> button:</p> <p></p> <p>Modify compute cluster options if desired and click the <code>Create compute</code> button:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#get-jdbcodbc-info","title":"Get JDBC/ODBC info","text":"<p>Scroll to the bottom of the view and click the open the <code>Advanced Options</code> view:</p> <p></p> <p>Click the <code>JDBC/ODBC</code> tab:</p> <p></p> <p>Open your project's <code>config.yaml</code> configuration file in a text editor and add a new gateway named <code>databricks</code> below the existing <code>local</code> gateway:</p> <p></p> <p>Copy the <code>server_hostname</code> and <code>http_path</code> connection values from the Databricks JDBC/ODBC tab to the <code>config.yaml</code> file:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#get-personal-access-token","title":"Get personal access token","text":"<p>The final piece of information we need for the <code>config.yaml</code> file is your personal access token.</p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre> <p> To create a personal access token, click on your profile logo and go to your profile's <code>Settings</code> page:</p> <p></p> <p>Go to the <code>Developer</code> view in the User menu. Depending on your account's role, your page may not display the Workspace Admin section of the page.</p> <p></p> <p>Click the <code>Manage</code> button in the Access Tokens section:</p> <p></p> <p>Click the <code>Generate new token</code> button:</p> <p></p> <p>Name your token in the <code>Comment</code> field, and click the <code>Generate</code> button:</p> <p></p> <p>Click the copy button and paste the token into the <code>access_token</code> key:</p> <p></p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre>"},{"location":"references/integrations/engines/databricks/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>databricks</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Databricks. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan --gateway databricks info</code>.</p> <p>We manually specify the <code>databricks</code> gateway because it is not our project's default gateway:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Databricks SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Databricks is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"references/integrations/engines/databricks/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>databricks</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>databricks_state.db</code> file:</p> <p></p> <p>Now we no longer see the warning when running <code>vulcan --gateway databricks info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>For convenience, we can omit the <code>--gateway</code> option from our CLI commands by specifying <code>databricks</code> as our project's <code>default_gateway</code>:</p> <p></p> <p>And run a <code>vulcan plan</code> in Databricks:</p> <p></p> <p>And confirm that our schemas and objects exist in the Databricks catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Databricks!</p> <p>Tip</p> <p>Vulcan connects to your Databricks Cluster's default catalog by default. Connect to a different catalog by specifying its name in the connection configuration's <code>catalog</code> parameter.</p>"},{"location":"references/integrations/engines/databricks/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>databricks</code></p>"},{"location":"references/integrations/engines/databricks/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[databricks]\"\n</code></pre>"},{"location":"references/integrations/engines/databricks/#connection-method-details","title":"Connection method details","text":"<p>Databricks provides multiple computing options and connection methods. The section above explains how to use them with Vulcan, and this section provides additional configuration details.</p>"},{"location":"references/integrations/engines/databricks/#databricks-sql-connector_1","title":"Databricks SQL Connector","text":"<p>Vulcan uses the Databricks SQL Connector to connect to Databricks by default. Learn more above.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connect_1","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan needs to use the Databricks Connect to connect to Databricks (instead of the Databricks SQL Connector).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>If Vulcan detects that you have Databricks Connect installed, then it will automatically configure the connection and use it for all Python models that return a Pandas or PySpark DataFrame.</p> <p>To have databricks-connect installed but ignored by Vulcan, set <code>disable_databricks_connect</code> to <code>true</code> in the connection configuration.</p> <p>Databricks Connect can execute SQL and DataFrame operations on different clusters by setting the Vulcan <code>databricks_connect_*</code> connection options. For example, these options could configure Vulcan to run SQL on a Databricks SQL Warehouse while still routing DataFrame operations to a normal Databricks Cluster.</p> <p>Note</p> <p>If using Databricks Connect, make sure to learn about the Databricks requirements and limitations.</p>"},{"location":"references/integrations/engines/databricks/#databricks-notebook-interface_1","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly on a Databricks Cluster (like in a Databricks Notebook), the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>The only relevant Vulcan configuration parameter is the optional <code>catalog</code> parameter.</p>"},{"location":"references/integrations/engines/databricks/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>databricks</code> string Y <code>server_hostname</code> Databricks instance host name string N <code>http_path</code> HTTP path, either to a DBSQL endpoint (such as <code>/sql/1.0/endpoints/1234567890abcdef</code>) or to an All-Purpose cluster (such as <code>/sql/protocolv1/o/1234567890123456/1234-123456-slid123</code>) string N <code>access_token</code> HTTP Bearer access token, such as Databricks Personal Access Token string N <code>catalog</code> The name of the catalog to use for the connection. Defaults to use Databricks cluster default. string N <code>auth_type</code> SQL Connector Only: Set to 'databricks-oauth' or 'azure-oauth' to trigger OAuth (or dont set at all to use <code>access_token</code>) string N <code>oauth_client_id</code> SQL Connector Only: Optional M2M OAuth Client ID to use when <code>auth_type</code> is set string N <code>oauth_client_secret</code> SQL Connector Only: Optional M2M OAuth Client Secret to use when <code>auth_type</code> is set string N <code>http_headers</code> SQL Connector Only: An optional dictionary of HTTP headers that will be set on every request dict N <code>session_configuration</code> SQL Connector Only: An optional dictionary of Spark session parameters. Execute the SQL command <code>SET -v</code> to get a full list of available commands. dict N <code>databricks_connect_server_hostname</code> Databricks Connect Only: Databricks Connect server hostname. Uses <code>server_hostname</code> if not set. string N <code>databricks_connect_access_token</code> Databricks Connect Only: Databricks Connect access token. Uses <code>access_token</code> if not set. string N <code>databricks_connect_cluster_id</code> Databricks Connect Only: Databricks Connect cluster ID. Uses <code>http_path</code> if not set. Cannot be a Databricks SQL Warehouse. string N <code>databricks_connect_use_serverless</code> Databricks Connect Only: Use a serverless cluster for Databricks Connect instead of <code>databricks_connect_cluster_id</code>. bool N <code>force_databricks_connect</code> When running locally, force the use of Databricks Connect for all model operations (so don't use SQL Connector for SQL models) bool N <code>disable_databricks_connect</code> When running locally, disable the use of Databricks Connect for all model operations (so use SQL Connector for all models) bool N <code>disable_spark_session</code> Do not use SparkSession if it is available (like when running in a notebook). bool N"},{"location":"references/integrations/engines/databricks/#model-table-properties-to-support-altering-tables","title":"Model table properties to support altering tables","text":"<p>If you are making a change to the structure of a table that is forward only, then you may need to add the following to your model's <code>physical_properties</code>:</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    ...\n    physical_properties (\n        'delta.columnMapping.mode' = 'name'\n    ),\n)\n</code></pre> <p>If you attempt to alter without having this property set, you will get an error similar to <code>databricks.sql.exc.ServerOperationError: [DELTA_UNSUPPORTED_DROP_COLUMN] DROP COLUMN is not supported for your Delta table.</code>. Databricks Documentation for more details.</p>"},{"location":"references/integrations/engines/duckdb/","title":"DuckDB","text":""},{"location":"references/integrations/engines/duckdb/#duckdb","title":"DuckDB","text":"<p>DuckDB state connection limitations</p> <p>DuckDB is a single user database. Using it for a state connection in your Vulcan project limits you to a single workstation. This means your project cannot be shared amongst your team members or your CI/CD infrastructure. This is usually fine for proof of concept or test projects but it will not scale to production usage.</p> <p>For production projects, use Tobiko Cloud or a more robust state database such as Postgres.</p>"},{"location":"references/integrations/engines/duckdb/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>duckdb</code></p>"},{"location":"references/integrations/engines/duckdb/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>duckdb</code> string Y <code>database</code> The optional database name. If not specified, the in-memory database is used. Cannot be defined if using <code>catalogs</code>. string N <code>catalogs</code> Mapping to define multiple catalogs. Can attach DuckDB catalogs or catalogs for other connections. First entry is the default catalog. Cannot be defined if using <code>database</code>. dict N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g., S3) using DuckDB secrets. Can be a list of secret configurations or a dictionary with custom secret names. list/dict N <code>filesystems</code> Configuration for registering <code>fsspec</code> filesystems to the DuckDB connection. dict N"},{"location":"references/integrations/engines/duckdb/#duckdb-catalogs-example","title":"DuckDB Catalogs Example","text":"<p>This example specifies two catalogs. The first catalog is named \"persistent\" and maps to the DuckDB file database <code>local.duckdb</code>. The second catalog is named \"ephemeral\" and maps to the DuckDB in-memory database.</p> <p><code>persistent</code> is the default catalog since it is the first entry in the dictionary. Vulcan will place models without an explicit catalog, such as <code>my_schema.my_model</code>, into the <code>persistent</code> catalog <code>local.duckdb</code> DuckDB file database.</p> <p>Vulcan will place models with the explicit catalog \"ephemeral\", such as <code>ephemeral.other_schema.other_model</code>, into the <code>ephemeral</code> catalog DuckDB in-memory database.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        persistent: 'local.duckdb'\n        ephemeral: ':memory:'\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"persistent\": \"local.duckdb\"\n                    \"ephemeral\": \":memory:\"\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#ducklake-catalog-example","title":"DuckLake Catalog Example","text":"YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: 'catalog.ducklake'\n          data_path: data/ducklake\n          encrypted: True\n          data_inlining_row_limit: 10\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"ducklake\": DuckDBAttachOptions(\n                        type=\"ducklake\",\n                        path=\"catalog.ducklake\",\n                        data_path=\"data/ducklake\",\n                        encrypted=True,\n                        data_inlining_row_limit=10,\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#other-connection-catalogs-example","title":"Other Connection Catalogs Example","text":"<p>Catalogs can also be defined to connect to anything that DuckDB can be attached to.</p> <p>Below are examples of connecting to a SQLite database and a PostgreSQL database. The SQLite database is read-write, while the PostgreSQL database is read-only.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        memory: ':memory:'\n        sqlite:\n          type: sqlite\n          path: 'test.db'\n        postgres:\n          type: postgres\n          path: 'dbname=postgres user=postgres host=127.0.0.1'\n          read_only: true\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"memory\": \":memory:\",\n                    \"sqlite\": DuckDBAttachOptions(\n                        type=\"sqlite\",\n                        path=\"test.db\"\n                    ),\n                    \"postgres\": DuckDBAttachOptions(\n                        type=\"postgres\",\n                        path=\"dbname=postgres user=postgres host=127.0.0.1\",\n                        read_only=True\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#catalogs-for-postgresql","title":"Catalogs for PostgreSQL","text":"<p>In PostgreSQL, the catalog name must match the actual catalog name it is associated with, as shown in the example above, where the database name (<code>dbname</code> in the path) is the same as the catalog name.</p>"},{"location":"references/integrations/engines/duckdb/#connectors-without-schemas","title":"Connectors without schemas","text":"<p>Some connections, like SQLite, do not support schema names and therefore objects will be attached under the default schema name of <code>main</code>.</p> <p>Example: mounting a SQLite database with the name <code>sqlite</code> that has a table <code>example_table</code> will be accessible as <code>sqlite.main.example_table</code>.</p>"},{"location":"references/integrations/engines/duckdb/#sensitive-fields-in-paths","title":"Sensitive fields in paths","text":"<p>If a connector, like Postgres, requires sensitive information in the path, it might support defining environment variables instead. See DuckDB Documentation for more information.</p>"},{"location":"references/integrations/engines/duckdb/#cloud-service-authentication","title":"Cloud service authentication","text":"<p>DuckDB can read data directly from cloud services via extensions (e.g., httpfs, azure).</p> <p>The <code>secrets</code> option allows you to configure DuckDB's Secrets Manager to authenticate with external services like S3. This is the recommended approach for cloud storage authentication in DuckDB v0.10.0 and newer, replacing the legacy authentication method via variables.</p>"},{"location":"references/integrations/engines/duckdb/#secrets-configuration","title":"Secrets Configuration","text":"<p>The <code>secrets</code> option supports two formats:</p> <ol> <li>List format (default secrets): A list of secret configurations where each secret uses DuckDB's default naming</li> <li>Dictionary format (named secrets): A dictionary where keys are custom secret names and values are the secret configurations</li> </ol> <p>This flexibility allows you to organize multiple secrets of the same type or reference specific secrets by name in your SQL queries.</p>"},{"location":"references/integrations/engines/duckdb/#list-format-example-default-secrets","title":"List Format Example (Default Secrets)","text":"<p>Using a list creates secrets with DuckDB's default naming:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        - type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets=[\n                    {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    }\n                ]\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#dictionary-format-example-named-secrets","title":"Dictionary Format Example (Named Secrets)","text":"<p>Using a dictionary allows you to assign custom names to your secrets for better organization and reference:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        my_s3_secret:\n          type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n        my_azure_secret:\n          type: azure\n          account_name: \"YOUR_AZURE_ACCOUNT\"\n          account_key: \"YOUR_AZURE_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets={\n                    \"my_s3_secret\": {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    },\n                    \"my_azure_secret\": {\n                        \"type\": \"azure\",\n                        \"account_name\": \"YOUR_AZURE_ACCOUNT\",\n                        \"account_key\": \"YOUR_AZURE_KEY\"\n                    }\n                }\n            )\n        ),\n    }\n)\n</code></pre> <p>After configuring the secrets, you can directly reference S3 paths in your catalogs or in SQL queries without additional authentication steps.</p> <p>Refer to the official DuckDB documentation for the full list of supported S3 secret parameters and for more information on the Secrets Manager configuration.</p> <p>Note: Loading credentials at runtime using <code>load_aws_credentials()</code> or similar deprecated functions may fail when using Vulcan.</p>"},{"location":"references/integrations/engines/duckdb/#file-system-configuration-example-for-microsoft-onelake","title":"File system configuration example for Microsoft Onelake","text":"<p>The <code>filesystems</code> accepts a list of file systems to register in the DuckDB connection. This is especially useful for Azure Storage Accounts, as it adds write support for DuckDB which is not natively supported by DuckDB (yet).</p> YAML <pre><code>gateways:\n  ducklake:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: myducklakecatalog.duckdb\n          data_path: abfs://MyFabricWorkspace/MyFabricLakehouse.Lakehouse/Files/DuckLake.Files\n    extensions:\n      - ducklake\n    filesystems:\n      - fs: abfs\n        account_name: onelake\n        account_host: onelake.blob.fabric.microsoft.com\n        client_id: {{ env_var('AZURE_CLIENT_ID') }}\n        client_secret: {{ env_var('AZURE_CLIENT_SECRET') }}\n        tenant_id: {{ env_var('AZURE_TENANT_ID') }}\n        # anon: False # To use azure.identity.DefaultAzureCredential authentication \n</code></pre> <p>Refer to the documentation for <code>fsspec</code> fsspec.filesystem and <code>adlfs</code> adlfs.AzureBlobFileSystem for a full list of storage options. </p>"},{"location":"references/integrations/engines/fabric/","title":"Fabric","text":""},{"location":"references/integrations/engines/fabric/#fabric","title":"Fabric","text":"<p>Info</p> <p>The Fabric engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"references/integrations/engines/fabric/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>fabric</code></p> <p>NOTE: Fabric Warehouse is not recommended to be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/fabric/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/fabric/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[fabric]\"\n</code></pre>"},{"location":"references/integrations/engines/fabric/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>fabric</code> string Y <code>host</code> The hostname of the Fabric Warehouse server string Y <code>user</code> The client id to use for authentication with the Fabric Warehouse server string N <code>password</code> The client secret to use for authentication with the Fabric Warehouse server string N <code>port</code> The port number of the Fabric Warehouse server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pyodbc string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>tenant_id</code> The Azure / Entra tenant UUID string Y <code>workspace_id</code> The Fabric workspace UUID. The preferred way to retrieve it is by running <code>notebookutils.runtime.context.get(\"currentWorkspaceId\")</code> in a python notebook. string Y <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"references/integrations/engines/gcp-postgres/","title":"GCP Postgres","text":""},{"location":"references/integrations/engines/gcp-postgres/#gcp-postgres","title":"GCP Postgres","text":""},{"location":"references/integrations/engines/gcp-postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>gcp_postgres</code></p>"},{"location":"references/integrations/engines/gcp-postgres/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[gcppostgres]\"\n</code></pre>"},{"location":"references/integrations/engines/gcp-postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>gcp_postgres</code> string Y <code>instance_connection_string</code> Connection name for the postgres instance string Y <code>user</code> The username (postgres or IAM) to use for authentication string Y <code>password</code> The password to use for authentication. Required when connecting as a Postgres user string N <code>enable_iam_auth</code> Enables IAM authentication. Required when connecting as an IAM user boolean N <code>keyfile</code> Path to the keyfile to be used with enable_iam_auth instead of ADC string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>db</code> The name of the database instance to connect to string Y <code>ip_type</code> The IP type to use for the connection. Must be one of <code>public</code>, <code>private</code>, or <code>psc</code>. Default: <code>public</code> string N <code>timeout</code> The connection timeout in seconds. Default: <code>30</code> integer N <code>scopes</code> The scopes to use for the connection. Default: <code>(https://www.googleapis.com/auth/sqlservice.admin,)</code> tuple[str] N <code>driver</code> The driver to use for the connection. Default: <code>pg8000</code>. Note: only <code>pg8000</code> is tested string N"},{"location":"references/integrations/engines/motherduck/","title":"MotherDuck","text":""},{"location":"references/integrations/engines/motherduck/#motherduck","title":"MotherDuck","text":"<p>This page provides information about how to use Vulcan with MotherDuck.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to MotherDuck, or you can skip directly to information about using MotherDuck with the built-in scheduler.</p>"},{"location":"references/integrations/engines/motherduck/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with MotherDuck.</p> <p>It demonstrates connecting to MotherDuck with the <code>duckdb</code> library bundled with Vulcan.</p> <p>MotherDuck provides a single way to authorize a connection. This quickstart demonstrates authenticating with a token.</p> <p>Tip</p> <p>This quick start assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not familiar, work through the Vulcan Quickstart before continuing.</p>"},{"location":"references/integrations/engines/motherduck/#prerequisites","title":"Prerequisites","text":"<p>Before working through this quickstart guide, ensure that:</p> <ol> <li>You have a motherduck account and an access token.</li> <li>Your computer has Vulcan installed with the DuckDB extra available.</li> <li>Install from command line with the command <code>pip install \u201cvulcan[duckdb]\u201d</code></li> <li>You have initialized a Vulcan example project on your computer</li> <li>Open a command line interface and navigate to the directory where the project files should go.</li> <li>Initialize the project with the command <code>vulcan init duckdb</code>, since <code>duckdb</code> is the dialect.</li> </ol>"},{"location":"references/integrations/engines/motherduck/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access your MotherDuck databases. Since permission is granted to specific databases for a specific user, you should create a service account for Vulcan that will contain the credentials for writing to MotherDuck.</p>"},{"location":"references/integrations/engines/motherduck/#configure-the-connection","title":"Configure the connection","text":"<p>We now have what is required to configure Vulcan\u2019s connection to MotherDuck.</p> <p>We start the configuration by adding a gateway named <code>motherduck</code> to our example project\u2019s config.yaml file and making it our <code>default gateway</code>, as well as adding our token, persistent, and ephemeral catalogs.</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n        catalogs:\n          persistent: \"md:\"\n          ephemeral: \":memory:\"\n      token: &lt;your_token&gt;\n\ndefault_gateway: motherduck\n</code></pre> <p>Catalogs can be defined to connect to anything that DuckDB can be attached to.</p> <p>Warning</p> <p>Best practice for storing secrets like tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>MOTHERDUCK_TOKEN</code> for the configuration's <code>token</code> parameter:</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n      token: {{ env_var('MOTHERDUCK_TOKEN') }}\n</code></pre>"},{"location":"references/integrations/engines/motherduck/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>motherduck</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to MotherDuck. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p>"},{"location":"references/integrations/engines/motherduck/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in MotherDuck:</p> <p></p> <p>And confirm that our schemas and objects exist in the MotherDuck catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on MotherDuck!</p>"},{"location":"references/integrations/engines/motherduck/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>motherduck</code></p>"},{"location":"references/integrations/engines/motherduck/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>motherduck</code> string Y <code>database</code> The database name. string Y <code>token</code> The optional MotherDuck token. If not specified, the user will be prompted to login with their web browser. string N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g. S3) using DuckDB secrets. dict N"},{"location":"references/integrations/engines/mssql/","title":"MSSQL","text":""},{"location":"references/integrations/engines/mssql/#mssql","title":"MSSQL","text":""},{"location":"references/integrations/engines/mssql/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/mssql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[mssql]\"\n</code></pre>"},{"location":"references/integrations/engines/mssql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[mssql-odbc]\"\n</code></pre>"},{"location":"references/integrations/engines/mssql/#incremental-by-unique-key-merge","title":"Incremental by unique key <code>MERGE</code>","text":"<p>Vulcan executes a <code>MERGE</code> statement to insert rows for incremental by unique key model kinds.</p> <p>By default, the <code>MERGE</code> statement updates all non-key columns of an existing row when a new row with the same key values is inserted. If all column values match between the two rows, those updates are unnecessary.</p> <p>Vulcan provides an optional performance optimization that skips unnecessary updates by comparing column values with the <code>EXISTS</code> and <code>EXCEPT</code> operators.</p> <p>Enable the optimization by setting the <code>mssql_merge_exists</code> key to <code>true</code> in the <code>physical_properties</code> section of the <code>MODEL</code> statement.</p> <p>For example:</p> <pre><code>MODEL (\n    name vulcan_example.unique_key,\n    kind INCREMENTAL_BY_UNIQUE_KEY (\n        unique_key id\n    ),\n    cron '@daily',\n    physical_properties (\n        mssql_merge_exists = true\n    )\n);\n</code></pre> <p>Not all column types supported</p> <p>The <code>mssql_merge_exists</code> optimization is not supported for all column types, including <code>GEOMETRY</code>, <code>XML</code>, <code>TEXT</code>, <code>NTEXT</code>, <code>IMAGE</code>, and most user-defined types.</p> <p>Learn more in the MSSQL <code>EXCEPT</code> statement documentation.</p>"},{"location":"references/integrations/engines/mssql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mssql</code></p>"},{"location":"references/integrations/engines/mssql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mssql</code> string Y <code>host</code> The hostname of the MSSQL server string Y <code>user</code> The username / client id to use for authentication with the MSSQL server string N <code>password</code> The password / client secret to use for authentication with the MSSQL server string N <code>port</code> The port number of the MSSQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection (e.g., ODBC Driver 18 for SQL Server). string N <code>odbc_properties</code> ODBC connection properties (e.g., authentication: ActiveDirectoryServicePrincipal). See more here. dict N"},{"location":"references/integrations/engines/mysql/","title":"MySQL","text":""},{"location":"references/integrations/engines/mysql/#mysql","title":"MySQL","text":""},{"location":"references/integrations/engines/mysql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mysql</code></p>"},{"location":"references/integrations/engines/mysql/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[mysql]\"\n</code></pre>"},{"location":"references/integrations/engines/mysql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mysql</code> string Y <code>host</code> The hostname of the MysQL server string Y <code>user</code> The username to use for authentication with the MySQL server string Y <code>password</code> The password to use for authentication with the MySQL server string Y <code>port</code> The port number of the MySQL server int N <code>charset</code> The character set used for the connection string N <code>ssl_disabled</code> Is SSL disabled bool N"},{"location":"references/integrations/engines/postgres/","title":"Postgres","text":""},{"location":"references/integrations/engines/postgres/#postgres","title":"Postgres","text":""},{"location":"references/integrations/engines/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"references/integrations/engines/postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"references/integrations/engines/redshift/","title":"Redshift","text":""},{"location":"references/integrations/engines/redshift/#redshift","title":"Redshift","text":""},{"location":"references/integrations/engines/redshift/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>redshift</code></p>"},{"location":"references/integrations/engines/redshift/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[redshift]\"\n</code></pre>"},{"location":"references/integrations/engines/redshift/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>redshift</code> string Y <code>user</code> The username to use for authentication with the Amazon Redshift cluster string N <code>password</code> The password to use for authentication with the Amazon Redshift cluster string N <code>database</code> The name of the database instance to connect to string N <code>host</code> The hostname of the Amazon Redshift cluster string N <code>port</code> The port number of the Amazon Redshift cluster int N <code>ssl</code> Is SSL enabled. SSL must be enabled when authenticating using IAM (Default: <code>True</code>) bool N <code>sslmode</code> The security of the connection to the Amazon Redshift cluster. <code>verify-ca</code> and <code>verify-full</code> are supported. string N <code>timeout</code> The number of seconds before the connection to the server will timeout. int N <code>tcp_keepalive</code> Is TCP keepalive used. (Default: <code>True</code>) bool N <code>application_name</code> The name of the application string N <code>preferred_role</code> The IAM role preferred for the current connection string N <code>principal_arn</code> The ARN of the IAM entity (user or role) for which you are generating a policy string N <code>credentials_provider</code> The class name of the IdP that will be used for authenticating with the Amazon Redshift cluster string N <code>region</code> The AWS region of the Amazon Redshift cluster string N <code>cluster_identifier</code> The cluster identifier of the Amazon Redshift cluster string N <code>iam</code> If IAM authentication is enabled. IAM must be True when authenticating using an IdP dict N <code>is_serverless</code> If the Amazon Redshift cluster is serverless (Default: <code>False</code>) bool N <code>serverless_acct_id</code> The account ID of the serverless cluster string N <code>serverless_work_group</code> The name of work group for serverless end point string N <code>enable_merge</code> Whether the incremental_by_unique_key model kind will use the native Redshift MERGE operation or Vulcan's logical merge. (Default: <code>False</code>) bool N"},{"location":"references/integrations/engines/redshift/#performance-considerations","title":"Performance Considerations","text":""},{"location":"references/integrations/engines/redshift/#timestamp-macro-variables-and-sort-keys","title":"Timestamp Macro Variables and Sort Keys","text":"<p>When working with Redshift tables that have a <code>TIMESTAMP</code> sort key, using the standard <code>@start_dt</code> and <code>@end_dt</code> macro variables may lead to performance issues. These macros render as <code>TIMESTAMP WITH TIME ZONE</code> values in SQL queries, which prevents Redshift from performing efficient pruning when filtering against <code>TIMESTAMP</code> (without timezone) sort keys.</p> <p>This can result in full table scans instead, causing significant performance degradation.</p> <p>Solution: Use the <code>_dtntz</code> (datetime no timezone) variants of macro variables:</p> <ul> <li><code>@start_dtntz</code> instead of <code>@start_dt</code></li> <li><code>@end_dtntz</code> instead of <code>@end_dt</code></li> </ul> <p>These variants render as <code>TIMESTAMP WITHOUT TIME ZONE</code>, allowing Redshift to properly utilize sort key optimizations.</p> <p>Example:</p> <pre><code>-- Inefficient: May cause full table scan\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dt\n  AND timestamp_column &lt; @end_dt\n\n-- Efficient: Uses sort key optimization\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dtntz\n  AND timestamp_column &lt; @end_dtntz\n\n-- Alternative: Cast to timestamp\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_ts::timestamp\n  AND timestamp_column &lt; @end_ts::timestamp\n</code></pre>"},{"location":"references/integrations/engines/risingwave/","title":"RisingWave","text":""},{"location":"references/integrations/engines/risingwave/#risingwave","title":"RisingWave","text":"<p>This page provides information about how to use Vulcan with the RisingWave streaming database engine.</p> <p>Info</p> <p>The RisingWave engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"references/integrations/engines/risingwave/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>risingwave</code></p>"},{"location":"references/integrations/engines/risingwave/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[risingwave]\"\n</code></pre>"},{"location":"references/integrations/engines/risingwave/#connection-options","title":"Connection options","text":"<p>RisingWave is based on Postgres and uses the same <code>psycopg2</code> connection library. Therefore, the connection parameters are very similar to Postgres.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>risingwave</code> string Y <code>host</code> The hostname of the RisingWave server string Y <code>user</code> The username to use for authentication with the RisingWave server string Y <code>password</code> The password to use for authentication with the RisingWave server string N <code>port</code> The port number of the RisingWave engine server int Y <code>database</code> The name of the database instance to connect to string Y <code>role</code> The role to use for authentication with the RisingWave server string N <code>sslmode</code> The security of the connection to the RisingWave server string N"},{"location":"references/integrations/engines/risingwave/#extra-features","title":"Extra Features","text":"<p>As a streaming database engine, RisingWave contains some extra features tailored specifically to streaming usecases.</p> <p>Primarily, these are:  - Sources which are used to stream records into RisingWave from streaming sources like Kafka  - Sinks which are used to write the results of data processed by RisingWave to an external target, such as an Apache Iceberg table in object storage.</p> <p>RisingWave exposes these features via normal SQL statements, namely <code>CREATE SOURCE</code> and <code>CREATE SINK</code>. To utilize these in Vulcan, you can use them in pre / post statements.</p> <p>Here is an example of creating a Sink from a Vulcan model using a post statement:</p> <pre><code>MODEL (\n    name vulcan_example.view_model,\n    kind VIEW (\n      materialized true\n    )\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id;\n\nCREATE\n  SINK IF NOT EXISTS kafka_sink\nFROM\n  @this_model\nWITH (\n  connector='kafka',\n  \"properties.bootstrap.server\"='localhost:9092',\n  topic='test1',\n)\nFORMAT PLAIN\nENCODE JSON (force_append_only=true);\n</code></pre> <p>@this_model</p> <p>The <code>@this_model</code> macro resolves to the physical table for the current version of the model. See here for more information.</p>"},{"location":"references/integrations/engines/snowflake/","title":"Snowflake","text":""},{"location":"references/integrations/engines/snowflake/#snowflake","title":"Snowflake","text":"<p>This page provides information about how to use Vulcan with the Snowflake SQL engine.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the built-in.</p>"},{"location":"references/integrations/engines/snowflake/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.</p> <p>It demonstrates connecting to Snowflake with the <code>snowflake-connector-python</code> library bundled with Vulcan.</p> <p>Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are described below.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"references/integrations/engines/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Snowflake account and know your username and password</li> <li>Your Snowflake account has at least one warehouse available for running computations</li> <li>Your computer has Vulcan installed with the Snowflake extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[snowflake]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init snowflake</code></li> </ul> </li> </ol>"},{"location":"references/integrations/engines/snowflake/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access different types of database objects.</p> <p>Vulcan's core functionality requires relatively broad permissions, including:</p> <ol> <li>Ability to create and delete schemas in a database</li> <li>Ability to create, modify, delete, and query tables and views in the schemas it creates</li> </ol> <p>If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.</p> <p>We now describe how to grant Vulcan appropriate permissions.</p>"},{"location":"references/integrations/engines/snowflake/#snowflake-roles","title":"Snowflake roles","text":"<p>Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a \"role\" that you then grant to the user.</p> <p>Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.</p> <p>The role must be granted <code>USAGE</code> on a warehouse so it can execute computations. We describe other permissions below.</p>"},{"location":"references/integrations/engines/snowflake/#database-permissions","title":"Database permissions","text":"<p>The top-level object container in Snowflake is a \"database\" (often called a \"catalog\" in other engines). Vulcan does not need permission to create databases; it may use an existing one.</p> <p>The simplest way to grant Vulcan sufficient permissions for a database is to give it <code>OWNERSHIP</code> of the database, which includes all the necessary permissions.</p> <p>Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.</p>"},{"location":"references/integrations/engines/snowflake/#granting-the-permissions","title":"Granting the permissions","text":"<p>This section provides example code for creating a <code>vulcan</code> role, granting it sufficient permissions, and granting it to a user.</p> <p>The code must be executed by a user with <code>USERADMIN</code> level permissions or higher. We provide two versions of the code, one that grants database <code>OWNERSHIP</code> to the role and another that does not.</p> <p>Both examples create a role named <code>vulcan</code>, grant it usage of the warehouse <code>compute_wh</code>, create a database named <code>demo_db</code>, and assign the role to the user <code>demo_user</code>. The step that creates the database can be omitted if the database already exists.</p> With database ownershipWithout database ownership <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\n\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\nGRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre> <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\n\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\nGRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database\n\nGRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates\nGRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas\nGRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas\nGRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas\nGRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#get-connection-info","title":"Get connection info","text":"<p>Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.</p>"},{"location":"references/integrations/engines/snowflake/#account-name","title":"Account name","text":"<p>Snowflake connection configurations require the <code>account</code> parameter that identifies the Snowflake account Vulcan should connect to.</p> <p>Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a <code>/</code>.</p> <p>This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:</p> <p></p> <p>In this example, our organization name is <code>idapznw</code>, and our account name is <code>wq29399</code>.</p> <p>We concatenate the two components, separated by a <code>-</code>, for the Vulcan <code>account</code> parameter: <code>idapznw-wq29399</code>.</p>"},{"location":"references/integrations/engines/snowflake/#warehouse-name","title":"Warehouse name","text":"<p>Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.</p> <p>Some Snowflake user accounts may have a default warehouse they automatically use when connecting.</p> <p>The connection configuration's <code>warehouse</code> parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.</p>"},{"location":"references/integrations/engines/snowflake/#database-name","title":"Database name","text":"<p>Snowflake user accounts may have a \"Default Namespace\" that includes a default database they automatically use when connecting.</p> <p>The connection configuration's <code>database</code> parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.</p>"},{"location":"references/integrations/engines/snowflake/#configure-the-connection","title":"Configure the connection","text":"<p>We now have the information we need to configure Vulcan's connection to Snowflake.</p> <p>We start the configuration by adding a gateway named <code>snowflake</code> to our example project's config.yaml file and making it our <code>default_gateway</code>:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>And we specify the <code>account</code>, <code>user</code>, <code>password</code>, <code>database</code>, and <code>warehouse</code> connection parameters using the information from above:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Warning</p> <p>Best practice for storing secrets like passwords is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>SNOWFLAKE_PASSWORD</code> for the configuration's <code>password</code> parameter:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      password: {{ env_var('SNOWFLAKE_PASSWORD') }}\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>snowflake</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Snowflake SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"references/integrations/engines/snowflake/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>snowflake</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>snowflake_state.db</code> file:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; your password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n    state_connection:\n      type: duckdb\n      database: snowflake_state.db\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Now we no longer see the warning when running <code>vulcan info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"references/integrations/engines/snowflake/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in Snowflake:</p> <p></p> <p>And confirm that our schemas and objects exist in the Snowflake catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Snowflake!</p>"},{"location":"references/integrations/engines/snowflake/#where-are-the-row-counts","title":"Where are the row counts?","text":"<p>Vulcan reports the number of rows processed by each model in its <code>plan</code> and <code>run</code> terminal output.</p> <p>However, due to limitations in the Snowflake Python connector, row counts cannot be determined for <code>CREATE TABLE AS</code> statements. Therefore, Vulcan does not report row counts for certain model kinds, such as <code>FULL</code> models.</p> <p>Learn more about the connector limitation on Github.</p>"},{"location":"references/integrations/engines/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"references/integrations/engines/snowflake/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[snowflake]\"\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account name string Y <code>user</code> The Snowflake username string N <code>password</code> The Snowflake password string N <code>authenticator</code> The Snowflake authenticator method string N <code>warehouse</code> The Snowflake warehouse name string N <code>database</code> The Snowflake database name string N <code>role</code> The Snowflake role name string N <code>token</code> The Snowflake OAuth 2.0 access token string N <code>private_key</code> The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). string N <code>private_key_path</code> The optional path to the private key to use for authentication. This would be used instead of <code>private_key</code>. string N <code>private_key_passphrase</code> The optional passphrase to use to decrypt <code>private_key</code> (if in PEM format) or <code>private_key_path</code>. Keys can be created without encryption so only provide this if needed. string N <code>session_parameters</code> The optional session parameters to set for the connection. dict N"},{"location":"references/integrations/engines/snowflake/#lowercase-object-names","title":"Lowercase object names","text":"<p>Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command <code>CREATE SCHEMA vulcan</code> will generate a schema named <code>VULCAN</code> in Snowflake.</p> <p>If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.</p> <p>For example, a connection to the database <code>\"my_db\"</code> would include:</p> <pre><code>connection:\n  type: snowflake\n  &lt;other connection options&gt;\n  database: '\"my_db\"' # outer single and inner double quotes\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-authorization-methods","title":"Snowflake authorization methods","text":"<p>The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.</p> <p>This section describes how to configure other authorization methods.</p>"},{"location":"references/integrations/engines/snowflake/#snowflake-sso-authorization","title":"Snowflake SSO Authorization","text":"<p>Vulcan supports Snowflake SSO authorization connections using the <code>externalbrowser</code> authenticator method. For example:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: ************\n      user: ************\n      authenticator: externalbrowser\n      warehouse: ************\n      database: ************\n      role: ************\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-oauth-authorization","title":"Snowflake OAuth Authorization","text":"<p>Vulcan supports Snowflake OAuth authorization connections using the <code>oauth</code> authenticator method. For example:</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      authenticator: oauth\n      token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                authenticator=\"oauth\",\n                token=\"eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-private-key-authorization","title":"Snowflake Private Key Authorization","text":"<p>Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).</p> <p>The <code>account</code> and <code>user</code> parameters are required for each of these methods.</p> <p>Private Key Path</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key_path: '/path/to/key.key'\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key_path=\"/path/to/key.key\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key PEM</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: |\n        -----BEGIN PRIVATE KEY-----\n        ...\n        -----END PRIVATE KEY-----\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"\"\"\n                -----BEGIN PRIVATE KEY-----\n                ...\n                -----END PRIVATE KEY-----\"\"\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#private-key-base64","title":"Private Key Base64","text":"<p>Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Bytes</p> YAMLPython <p>Base64 encode the bytes and follow Private Key Base64 instructions.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    GatewayConfig,\n    ModelDefaultsConfig,\n    SnowflakeConnectionConfig,\n)\n\nfrom cryptography.hazmat.primitives import serialization\n\nkey = \"\"\"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\"\"\".encode()\n\np_key= serialization.load_pem_private_key(key, password=None)\n\npkb = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=pkb,\n            ),\n        ),\n    }\n)\n</code></pre> <p>The authenticator method is assumed to be <code>snowflake_jwt</code> when <code>private_key</code> is provided, but it can also be explicitly provided in the connection configuration.</p>"},{"location":"references/integrations/engines/snowflake/#configuring-virtual-warehouses","title":"Configuring Virtual Warehouses","text":"<p>The Snowflake Virtual Warehouse a model should use can be specified in the <code>session_properties</code> attribute of the model definition:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  session_properties (\n    'warehouse' = TEST_WAREHOUSE,\n  ),\n);\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#custom-view-and-table-types","title":"Custom View and Table types","text":"<p>Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the <code>physical_properties</code> and <code>virtual_properties</code> attributes respectively. For example:</p>"},{"location":"references/integrations/engines/snowflake/#secure-views","title":"Secure Views","text":"<p>A table can be exposed through a <code>SECURE</code> view in the virtual layer by specifying the <code>creatable_type</code> property and setting it to <code>SECURE</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  virtual_properties (\n      creatable_type = SECURE\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#transient-tables","title":"Transient Tables","text":"<p>A model can use a <code>TRANSIENT</code> table in the physical layer by specifying the <code>creatable_type</code> property and setting it to <code>TRANSIENT</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  physical_properties (\n      creatable_type = TRANSIENT\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#iceberg-tables","title":"Iceberg Tables","text":"<p>In order for Snowflake to be able to create an Iceberg table, there must be an External Volume configured to store the Iceberg table data on.</p> <p>Once that is configured, you can create a model backed by an Iceberg table by using <code>table_format iceberg</code> like so:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = '&lt;external volume name&gt;'\n  )\n);\n</code></pre> <p>To prevent having to specify <code>catalog = 'snowflake'</code> and <code>external_volume = '&lt;external volume name&gt;'</code> on every model, see the Snowflake documentation for:</p> <ul> <li>Configuring a default Catalog</li> <li>Configuring a default External Volume</li> </ul> <p>Alternatively you can also use model defaults to set defaults at the Vulcan level instead.</p> <p>To utilize the wide variety of optional properties that Snowflake makes available for Iceberg tables, simply specify them as <code>physical_properties</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = 'my_external_volume',\n    base_location = 'my/product_reviews/'\n  )\n);\n</code></pre> <p>External catalogs</p> <p>Setting <code>catalog = 'snowflake'</code> to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake does not support writing to Iceberg tables configured under external catalogs.</p> <p>You can however still reference a table from an external catalog in your model as a normal external table.</p>"},{"location":"references/integrations/engines/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"references/integrations/engines/snowflake/#frequent-authentication-prompts","title":"Frequent Authentication Prompts","text":"<p>When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.</p> <p>To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:</p> <ul> <li>For general authentication, see Connection Caching Documentation</li> <li>For MFA specifically, see MFA Token Caching Documentation.</li> </ul>"},{"location":"references/integrations/engines/spark/","title":"Spark","text":""},{"location":"references/integrations/engines/spark/#spark","title":"Spark","text":""},{"location":"references/integrations/engines/spark/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>spark</code></p> <p>NOTE: Spark may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/spark/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>spark</code> string Y <code>config_dir</code> Value to set for <code>SPARK_CONFIG_DIR</code> string N <code>catalog</code> The catalog to use when issuing commands. See Catalog Support for details string N <code>config</code> Key/value pairs to set for the Spark Configuration. dict N"},{"location":"references/integrations/engines/spark/#catalog-support","title":"Catalog Support","text":"<p>Vulcan's Spark integration is only designed/tested with a single catalog usage in mind.  Therefore all Vulcan models must be defined with a single catalog.</p> <p>If <code>catalog</code> is not set, then the behavior changes based on spark release:</p> <ul> <li>If &gt;=3.4, then the default catalog is determined at runtime</li> <li>If &lt;3.4, then the default catalog is <code>spark_catalog</code> </li> </ul>"},{"location":"references/integrations/engines/trino/","title":"Trino","text":""},{"location":"references/integrations/engines/trino/#trino","title":"Trino","text":""},{"location":"references/integrations/engines/trino/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>trino</code></p> <p>NOTE: Trino may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/trino/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[trino]\"\n</code></pre> <p>If you are using Oauth for Authentication, it is recommended to install keyring cache: </p><pre><code>pip install \"trino[external-authentication-token-cache]\"\n</code></pre><p></p>"},{"location":"references/integrations/engines/trino/#trino-connector-support","title":"Trino Connector Support","text":"<p>The trino engine adapter has been tested against the Hive Connector, Iceberg Connector, and Delta Lake Connector.</p> <p>Please let us know on Slack if you are wanting to use another connector or have tried another connector.</p>"},{"location":"references/integrations/engines/trino/#hive-connector-configuration","title":"Hive Connector Configuration","text":"<p>Recommended hive catalog properties configuration (<code>&lt;catalog_name&gt;.properties</code>):</p> <pre><code>hive.metastore-cache-ttl=0s\nhive.metastore-refresh-interval=5s\nhive.metastore-timeout=10s\nhive.allow-drop-table=true\nhive.allow-add-column=true\nhive.allow-drop-column=true\nhive.allow-rename-column=true\nhive.allow-rename-table=true\n</code></pre>"},{"location":"references/integrations/engines/trino/#iceberg-connector-configuration","title":"Iceberg Connector Configuration","text":"<p>If you're using a hive metastore for the Iceberg catalog, the properties are mostly the same as the Hive connector.</p> <pre><code>iceberg.catalog.type=hive_metastore\n# metastore properties as per the Hive Connector Configuration above\n</code></pre> <p>Note: The Trino Iceberg Connector must be configured with an <code>iceberg.catalog.type</code> that supports views. At the time of this writing, this is <code>hive_metastore</code>, <code>glue</code>, and <code>rest</code>.</p> <p>The <code>jdbc</code> and <code>nessie</code> iceberg catalog types do not support views and are thus incompatible with Vulcan.</p> <p>Nessie</p> <p>Nessie is supported when used as an Iceberg REST Catalog (<code>iceberg.catalog.type=rest</code>). For more information on how to configure the Trino Iceberg connector for this, see the Nessie documentation.</p>"},{"location":"references/integrations/engines/trino/#delta-lake-connector-configuration","title":"Delta Lake Connector Configuration","text":"<p>The Trino adapter Delta Lake connector has only been tested with the Hive metastore catalog type.</p> <p>The properties file must include the Hive metastore URI and catalog name in addition to any other general properties.</p> <pre><code>hive.metastore.uri=thrift://example.net:9083\ndelta.hive-catalog-name=datalake_delta # example catalog name, can be any valid string\n</code></pre>"},{"location":"references/integrations/engines/trino/#aws-glue","title":"AWS Glue","text":"<p>AWS Glue provides an implementation of the Hive metastore catalog.</p> <p>Your Trino project's physical data objects are stored in a specific location, such as an AWS S3 bucket. Hive provides a default location, which you can override in its configuration file.</p> <p>Set the default location for your project's tables in the Hive catalog configuration's <code>hive.metastore.glue.default-warehouse-dir</code> parameter.</p> <p>For example:</p> <pre><code>hive.metastore=glue\nhive.metastore.glue.default-warehouse-dir=s3://my-bucket/\n</code></pre>"},{"location":"references/integrations/engines/trino/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>trino</code> string Y <code>user</code> The username (of the account) to log in to your cluster. When connecting to Starburst Galaxy clusters, you must include the role of the user as a suffix to the username. string Y <code>host</code> The hostname of your cluster. Don't include the <code>http://</code> or <code>https://</code> prefix. string Y <code>catalog</code> The name of a catalog in your cluster. string Y <code>http_scheme</code> The HTTP scheme to use when connecting to your cluster. By default, it's <code>https</code> and can only be <code>http</code> for no-auth or basic auth. string N <code>port</code> The port to connect to your cluster. By default, it's <code>443</code> for <code>https</code> scheme and <code>80</code> for <code>http</code> int N <code>roles</code> Mapping of catalog name to a role dict N <code>http_headers</code> Additional HTTP headers to send with each request. dict N <code>session_properties</code> Trino session properties. Run <code>SHOW SESSION</code> to see all options. dict N <code>retries</code> Number of retries to attempt when a request fails. Default: <code>3</code> int N <code>timezone</code> Timezone to use for the connection. Default: client-side local timezone string N <code>schema_location_mapping</code> A mapping of regex patterns to S3 locations to use for the <code>LOCATION</code> property when creating schemas. See Table and Schema locations for more details. dict N <code>catalog_type_overrides</code> A mapping of catalog names to their connector type. This is used to enable/disable connector specific behavior. See Catalog Type Overrides for more details. dict N"},{"location":"references/integrations/engines/trino/#table-and-schema-locations","title":"Table and Schema locations","text":"<p>When using connectors that are decoupled from their storage (such as the Iceberg, Hive or Delta connectors), when creating new tables Trino needs to know the location in the physical storage it should write the table data to.</p> <p>This location gets stored against the table in the metastore so that any engine trying to read the data knows where to look.</p>"},{"location":"references/integrations/engines/trino/#default-behaviour","title":"Default behaviour","text":"<p>Trino allows you to optionally configure a <code>default-warehouse-dir</code> property at the Metastore level. When creating objects, Trino will infer schema locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;</code> and table locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;/&lt;table name&gt;</code>.</p> <p>However, if you dont set this property, Trino can still infer table locations if a schema location is explicitly set.</p> <p>For example, if you specify the <code>LOCATION</code> property when creating a schema like so:</p> <pre><code>CREATE SCHEMA staging_data\nWITH (LOCATION = 's3://warehouse/production/staging_data')\n</code></pre> <p>Then any tables created under that schema will have their location inferred as <code>&lt;schema location&gt;/&lt;table name&gt;</code>.</p> <p>If you specify neither a <code>default-warehouse-dir</code> in the metastore config nor a schema location when creating the schema, you must specify an explicit table location when creating the table or Trino will produce an error.</p> <p>Creating a table in a specific location is very similar to creating a schema in a specific location:</p> <pre><code>CREATE TABLE staging_data.customers (customer_id INT)\nWITH (LOCATION = 's3://warehouse/production/staging_data/customers')\n</code></pre>"},{"location":"references/integrations/engines/trino/#configuring-in-vulcan","title":"Configuring in Vulcan","text":"<p>Within Vulcan, you can configure the value to use for the <code>LOCATION</code> property when Vulcan creates tables and schemas. This overrides what Trino would have inferred based on the cluster configuration.</p>"},{"location":"references/integrations/engines/trino/#schemas","title":"Schemas","text":"<p>To configure the <code>LOCATION</code> property that Vulcan will specify when issuing <code>CREATE SCHEMA</code> statements, you can use the <code>schema_location_mapping</code> connection property. This applies to all schemas that Vulcan creates, including its internal ones.</p> <p>The simplest example is to emulate a <code>default-warehouse-dir</code>:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '.*': 's3://warehouse/production/@{schema_name}'\n</code></pre> <p>This will cause all schemas to get created with their location set to <code>s3://warehouse/production/&lt;schema name&gt;</code>. The table locations will be inferred by Trino as <code>s3://warehouse/production/&lt;schema name&gt;/&lt;table name&gt;</code> so all objects will effectively be created under <code>s3://warehouse/production/</code>.</p> <p>It's worth mentioning that if your models are using fully qualified three part names, eg <code>&lt;catalog&gt;.&lt;schema&gt;.&lt;name&gt;</code> then string being matched against the <code>schema_location_mapping</code> regex will be <code>&lt;catalog&gt;.&lt;schema&gt;</code> and not just the <code>&lt;schema&gt;</code> itself. This allows you to set different locations for the same schema name if that schema name is used across multiple catalogs.</p> <p>If your models are using two part names, eg <code>&lt;schema&gt;.&lt;table&gt;</code> then only the <code>&lt;schema&gt;</code> part will be matched against the regex.</p> <p>Here's an example:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '^utils$': 's3://utils-bucket/@{schema_name}'\n        '^landing\\..*$': 's3://raw-data/@{catalog_name}/@{schema_name}'\n        '^staging.*$': 's3://bucket/@{schema_name}_dev'\n        '^vulcan.*$': 's3://vulcan-internal/dev/@{schema_name}'\n</code></pre> <p>This would perform the following mappings:</p> <ul> <li>a schema called <code>sales</code> would not be mapped to a location at all because it doesnt match any of the patterns. It would be created without a <code>LOCATION</code> property</li> <li>a schema called <code>utils</code> would be mapped to the location <code>s3://utils-bucket/utils</code> because it directly matches the <code>^utils$</code> pattern</li> <li>a schema called <code>transactions</code> in a catalog called <code>landing</code> would be mapped to the location <code>s3://raw-data/landing/transactions</code> because the string <code>landing.transactions</code> matches the <code>^landing\\..*$</code> pattern</li> <li>schemas called <code>staging_customers</code> and <code>staging_accounts</code> would be mapped to the locations <code>s3://bucket/staging_customers_dev</code> and <code>s3://bucket/staging_accounts_dev</code> respectively because they match the <code>^staging.*$</code> pattern</li> <li>a schema called <code>accounts</code> in a catalog called <code>staging</code> would be mapped to the location <code>s3://bucket/accounts_dev</code> because the string <code>staging.accounts</code> matches the <code>^staging.*$</code> pattern</li> <li>schemas called <code>vulcan__staging_customers</code> and <code>vulcan__staging_utils</code> would be mapped to the locations <code>s3://vulcan-internal/dev/vulcan__staging_customers</code> and <code>s3://vulcan-internal/dev/vulcan__staging_utils</code> respectively because they match the <code>^vulcan.*$</code> pattern</li> </ul> <p>Placeholders</p> <p>You may use the <code>@{catalog_name}</code> and <code>@{schema_name}</code> placeholders in the mapping value.</p> <p>If there is a match on one of the patterns then the catalog / schema that Vulcan is about to use in the <code>CREATE SCHEMA</code> statement will be substituted into these placeholders.</p> <p>Note the use of curly brace syntax <code>@{}</code> when referencing these placeholders - learn more here.</p>"},{"location":"references/integrations/engines/trino/#tables","title":"Tables","text":"<p>Often, you don't need to configure an explicit table location because if you have configured explicit schema locations, table locations are automatically inferred by Trino to be a subdirectory under the schema location.</p> <p>However, if you need to, you can configure an explicit table location by adding a <code>location</code> property to the model <code>physical_properties</code>.</p> <p>Note that you need to use the @resolve_template macro to generate a unique table location for each model version. Otherwise, all model versions will be written to the same location and clobber each other.</p> <pre><code>MODEL (\n  name staging.customers,\n  kind FULL,\n  physical_properties (\n    location = @resolve_template('s3://warehouse/@{catalog_name}/@{schema_name}/@{table_name}')\n  )\n);\n\nSELECT ...\n</code></pre> <p>This will cause Vulcan to set the specified <code>LOCATION</code> when issuing a <code>CREATE TABLE</code> statement.</p>"},{"location":"references/integrations/engines/trino/#catalog-type-overrides","title":"Catalog Type Overrides","text":"<p>Vulcan attempts to determine the connector type of a catalog by querying the <code>system.metadata.catalogs</code> table and checking the <code>connector_name</code> column. It checks if the connector name is <code>hive</code> for Hive connector behavior or contains <code>iceberg</code> or <code>delta_lake</code> for Iceberg or Delta Lake connector behavior respectively. However, the connector name may not always be a reliable way to determine the connector type, for example when using a custom connector or a fork of an existing connector. To handle such cases, you can use the <code>catalog_type_overrides</code> connection property to explicitly specify the connector type for specific catalogs. For example, to specify that the <code>datalake</code> catalog is using the Iceberg connector and the <code>analytics</code> catalog is using the Hive connector, you can configure the connection as follows:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      catalog_type_overrides:\n        datalake: iceberg\n        analytics: hive\n</code></pre>"},{"location":"references/integrations/engines/trino/#authentication","title":"Authentication","text":"No AuthBasic AuthLDAPKerberosJWTCertificateOauth Option Description Type Required <code>method</code> <code>no-auth</code> (Default) string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    user: [user]\n    host: [host]\n    catalog: [catalog]\n    # Most likely you will want http for scheme when not using auth\n    http_scheme: http\n</code></pre> Option Description Type Required <code>method</code> <code>basic</code> string Y <code>password</code> The password to use when authenticating. string Y <code>verify</code> Boolean flag for whether SSL verification should occur. Default: trinodb Python client default (<code>true</code> as of this writing) bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: basic\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on Basic Authentication</li> <li>Python Client Basic Authentication</li> </ul> Option Description Type Required <code>method</code> <code>ldap</code> string Y <code>password</code> The password to use when authenticating. string Y <code>impersonation_user</code> Override the provided username. This lets you impersonate another user. string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: ldap\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on LDAP Authentication</li> <li>Python Client LDAP Authentication</li> </ul> Option Description Type Required <code>method</code> <code>kerberos</code> string Y <code>keytab</code> Path to keytab. Ex: <code>/tmp/trino.keytab</code> string Y <code>krb5_config</code> Path to config. Ex: <code>/tmp/krb5.conf</code> string Y <code>principal</code> Principal.  Ex: <code>user@company.com</code> string Y <code>service_name</code> Service name (default is <code>trino</code>) string N <code>hostname_override</code> Kerberos hostname for a host whose DNS name doesn't match string N <code>mutual_authentication</code> Boolean flag for mutual authentication. Default: <code>false</code> bool N <code>force_preemptive</code> Boolean flag to preemptively initiate the Kerberos GSS exchange. Default: <code>false</code> bool N <code>sanitize_mutual_error_response</code> Boolean flag to strip content and headers from error responses. Default: <code>true</code> bool N <code>delegate</code> Boolean flag for credential delegation (<code>GSS_C_DELEG_FLAG</code>). Default: <code>false</code> bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: kerberos\n    user: user\n    keytab: /tmp/trino.keytab\n    krb5_config: /tmp/krb5.conf\n    principal: trino@company.com\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Kerberos Authentication</li> <li>Python Client Kerberos Authentication</li> </ul> Option Description Type Required <code>method</code> <code>jwt</code> string Y <code>jwt_token</code> The JWT string. string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: jwt\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on JWT Authentication</li> <li>Python Client JWT Authentication</li> </ul> Option Description Type Required <code>method</code> <code>certificate</code> string Y <code>cert</code> The full path to a certificate file string Y <code>client_certificate</code> Path to client certificate. Ex: <code>/tmp/client.crt</code> string Y <code>client_private_key</code> Path to client private key. Ex: <code>/tmp/client.key</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: certificate\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n    cert: [path/to/cert_file]\n    client_certificate: [path/to/client/cert]\n    client_private_key: [path/to/client/key]\n</code></pre> Option Description Type Required <code>method</code> <code>oauth</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: oauth\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Oauth Authentication</li> <li>Python Client Oauth Authentication</li> </ul>"}]}