{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/_]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About","text":""},{"location":"#about","title":"About","text":"<p>Vulcan is a complete stack for building data products. Think of it as your all-in-one toolkit for transforming data, whether you're working with SQL or Python, small datasets or massive pipelines.</p>"},{"location":"#what-is-vulcan","title":"What is Vulcan?","text":"<p>Vulcan is a next-generation data transformation framework that helps you ship data products quickly, efficiently, and reliably. It's designed for data teams who want to move fast without breaking things.</p> <p>Here's what makes it special:</p> <ul> <li>Write in SQL or Python: Use whichever language you're comfortable with (or both!)</li> <li>Visibility and control: Know exactly what's happening in your pipelines, at any scale</li> <li>Ship without errors: Built-in validation and testing help catch problems before they reach production</li> <li>Works at any size: From small projects to enterprise-scale data platforms</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>Ready to dive in? The quickstart guide will have you up and running in minutes. It walks you through setting up Vulcan and creating your first data transformation.</p> <p>Python version note</p> <p>Depending on your system, you might need to use <code>python3</code> or <code>pip3</code> instead of <code>python</code> or <code>pip</code>. If you run into issues, try the <code>3</code> versions first.</p> <p>Once you've got Vulcan running, you'll find guides and examples throughout this documentation to help you build exactly what you need. Welcome aboard!</p>"},{"location":"FILE_REFERENCES/","title":"Documentation File References and Hyperlinks","text":""},{"location":"FILE_REFERENCES/#documentation-file-references-and-hyperlinks","title":"Documentation File References and Hyperlinks","text":"<p>This document provides a comprehensive catalog of all hyperlinks and file references in the Vulcan documentation, organized for easy navigation and link management.</p>"},{"location":"FILE_REFERENCES/#quick-statistics","title":"Quick Statistics","text":"<ul> <li>Total files scanned: 83</li> <li>Files with links: 71</li> <li>Total links: 992</li> <li>Internal links: 752</li> <li>External links: 240</li> <li>Potentially broken links: 383</li> </ul>"},{"location":"FILE_REFERENCES/#navigation","title":"Navigation","text":"<ul> <li>Broken Links by Pattern - Grouped broken links for easier fixing</li> <li>Link Relationships - See what links to what</li> <li>Most Linked Pages - Popular internal pages</li> <li>External Resources - External domains and links</li> <li>Links by File - Complete listing organized by file</li> <li>Link Index - Quick reference index</li> </ul>"},{"location":"FILE_REFERENCES/#broken-links-by-pattern","title":"Broken Links by Pattern","text":"<p>Broken links grouped by common patterns to help identify systematic issues:</p>"},{"location":"FILE_REFERENCES/#pattern-relative_paths-349-links","title":"Pattern: <code>relative_paths</code> (349 links)","text":"<ul> <li>comparisons.md (Line 65): <code>[Virtual Environments](./concepts/plans.md#plan-application)</code></li> <li>comparisons.md (Line 164): <code>[Unit and integration tests](./concepts/tests.md)</code></li> <li>configurations/overview.md (Line 98): <code>[Configuration Reference](./configuration.md#gateways)</code></li> <li>configurations/overview.md (Line 112): <code>[Model Defaults](./model_defaults.md)</code></li> <li>configurations/overview.md (Line 118): <code>[Variables](./variables.md)</code></li> <li>configurations/overview.md (Line 124): <code>[Execution Hooks](./hooks.md)</code></li> <li>configurations/overview.md (Line 130): <code>[Linter](./linter.md)</code></li> <li>configurations/overview.md (Line 136): <code>[Notifications](./notifications.md)</code></li> <li>configurations/overview.md (Line 142): <code>[PostgreSQL](./integrations/engines/postgres.md)</code></li> <li>configurations/overview.md (Line 143): <code>[Snowflake](./integrations/engines/snowflake.md)</code></li> <li>configurations/overview.md (Line 149): <code>[Configuration Reference](./configuration.md)</code></li> <li>configurations/overview.md (Line 150): <code>[Variables](./variables.md)</code></li> <li>configurations/overview.md (Line 151): <code>[Model Defaults](./model_defaults.md)</code></li> <li>configurations/overview.md (Line 152): <code>[Execution Hooks](./hooks.md)</code></li> <li>configurations/overview.md (Line 153): <code>[Linter](./linter.md)</code></li> <li>configurations/overview.md (Line 154): <code>[Notifications](./notifications.md)</code></li> <li>configurations/options/notifications.md (Line 11): <code>[Audit](../concepts/audits.md)</code></li> <li>configurations/options/notifications.md (Line 133): <code>[</code>plan<code>application](../concepts/plans.md)</code></li> <li>configurations/options/notifications.md (Line 133): <code>[</code>run<code>](../reference/cli.md#run)</code></li> <li>configurations/options/notifications.md (Line 133): <code>[</code>audit<code>](../concepts/audits.md)</code></li> </ul> <p>... and 329 more links with this pattern</p>"},{"location":"FILE_REFERENCES/#pattern-concepts-15-links","title":"Pattern: <code>concepts/</code> (15 links)","text":"<ul> <li>comparisons.md (Line 24): <code>[\u2705](concepts/models/overview.md)</code></li> <li>comparisons.md (Line 25): <code>[\u2705\u2705](concepts/models/python_models.md)</code></li> <li>comparisons.md (Line 27): <code>[\u2705](concepts/macros/jinja_macros.md)</code></li> <li>comparisons.md (Line 28): <code>[\u2705](concepts/macros/vulcan_macros.md)</code></li> <li>comparisons.md (Line 30): <code>[\u2705](concepts/glossary.md#semantic-understanding)</code></li> <li>comparisons.md (Line 31): <code>[\u2705](concepts/tests.md)</code></li> <li>comparisons.md (Line 33): <code>[\u2705](concepts/audits.md)</code></li> <li>comparisons.md (Line 34): <code>[\u2705](concepts/plans.md)</code></li> <li>comparisons.md (Line 35): <code>[\u2705](concepts/plans.md)</code></li> <li>comparisons.md (Line 37): <code>[\u2705](concepts/environments.md)</code></li> <li>comparisons.md (Line 51): <code>[\u2705](concepts/models/sql_models.md#transpilation)</code></li> <li>comparisons.md (Line 152): <code>[batch](concepts/models/overview.md#batch_size)</code></li> <li>comparisons.md (Line 169): <code>[Python models](concepts/models/python_models.md)</code></li> <li>comparisons.md (Line 174): <code>[</code>vulcan plan<code>](concepts/plans.md)</code></li> <li>comparisons.md (Line 176): <code>[Virtual Preview](concepts/glossary.md#virtual-preview)</code></li> </ul>"},{"location":"FILE_REFERENCES/#pattern-other-8-links","title":"Pattern: <code>other</code> (8 links)","text":"<ul> <li>configurations/options/model_defaults.md (Line 5): <code>[models configuration reference page](model_configuration.md#model-defaults)</code></li> <li>components/checks/checks.md (Line 5): <code>[audits](audits.md)</code></li> <li>components/tests/tests.md (Line 17): <code>[plan](plans.md)</code></li> <li>components/semantics/overview.md (Line 69): <code>[Business Metrics](metrics.md)</code></li> <li>components/semantics/overview.md (Line 159): <code>[Business Metrics](metrics.md)</code></li> <li>concepts-old/glossary.md (Line 13): <code>[tests](tests.md)</code></li> <li>concepts-old/glossary.md (Line 13): <code>[audits](audits.md)</code></li> <li>concepts-old/glossary.md (Line 16): <code>[tests](tests.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#pattern-models-8-links","title":"Pattern: <code>models/</code> (8 links)","text":"<ul> <li>concepts-old/environments.md (Line 6): <code>[Models](models/overview.md)</code></li> <li>concepts-old/plans.md (Line 336): <code>[INCREMENTAL_BY_UNIQUE_KEY](models/model_kinds.md#incremental_by_unique_key)</code></li> <li>concepts-old/plans.md (Line 337): <code>[INCREMENTAL_BY_PARTITION](models/model_kinds.md#incremental_by_partition)</code></li> <li>concepts-old/plans.md (Line 338): <code>[SCD_TYPE_2_BY_TIME](models/model_kinds.md#scd-type-2-by-time-recommended)</code></li> <li>concepts-old/plans.md (Line 339): <code>[SCD_TYPE_2_BY_COLUMN](models/model_kinds.md#scd-type-2-by-column)</code></li> <li>concepts-old/plans.md (Line 562): <code>[forward-only](models/overview.md#forward_only)</code></li> <li>concepts-old/plans.md (Line 633): <code>[disable_restatement](models/overview.md#disable_restatement)</code></li> <li>concepts-old/glossary.md (Line 49): <code>[Model Kinds](models/model_kinds.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#pattern-reference-1-links","title":"Pattern: <code>reference/</code> (1 links)","text":"<ul> <li>comparisons.md (Line 41): <code>[\u2705](reference/cli.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#pattern-getting_started-1-links","title":"Pattern: <code>getting_started/</code> (1 links)","text":"<ul> <li>index.md (Line 104): <code>[quickstart guide](getting_started/docker.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#pattern-plans-1-links","title":"Pattern: <code>plans/</code> (1 links)","text":"<ul> <li>concepts-old/plans.md (Line 286): <code>[Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#link-relationships","title":"Link Relationships","text":"<p>See which files link to each target page:</p>"},{"location":"FILE_REFERENCES/#conceptsplansmd","title":"<code>concepts/plans.md</code>","text":"<p>Linked from 18 file(s):</p> <ul> <li>comparisons.md (Line 34): <code>\u2705</code></li> <li>comparisons.md (Line 35): <code>\u2705</code></li> <li>comparisons.md (Line 65): <code>Virtual Environments</code></li> <li>comparisons.md (Line 174): <code>vulcan plan</code></li> <li>guides-old/notifications.md (Line 133): <code>`plan</code> application`</li> <li>guides-old/configuration.md (Line 573): <code>preview</code></li> <li>guides-old/configuration.md (Line 647): <code>categorize</code></li> <li>guides-old/configuration.md (Line 651): <code>breaking</code></li> <li>guides-old/configuration.md (Line 1102): <code>plans</code></li> <li>configurations-old/configuration.md (Line 90): <code>categorize</code></li> </ul> <p>... and 8 more references</p>"},{"location":"FILE_REFERENCES/#referenceconfigurationmd","title":"<code>reference/configuration.md</code>","text":"<p>Linked from 18 file(s):</p> <ul> <li>guides-old/configuration.md (Line 7): <code>configuration reference page</code></li> <li>guides-old/configuration.md (Line 281): <code>Vulcan configuration reference page</code></li> <li>guides-old/configuration.md (Line 283): <code>Project</code></li> <li>guides-old/configuration.md (Line 284): <code>Environment</code></li> <li>guides-old/configuration.md (Line 285): <code>Gateways</code></li> <li>guides-old/configuration.md (Line 286): <code>Gateway/connection defaults</code></li> <li>guides-old/configuration.md (Line 288): <code>Debug mode</code></li> <li>guides-old/configuration.md (Line 292): <code>configuration reference page</code></li> <li>guides-old/configuration.md (Line 328): <code>environments</code></li> <li>guides-old/configuration.md (Line 412): <code>environment_suffix_target</code></li> </ul> <p>... and 8 more references</p>"},{"location":"FILE_REFERENCES/#guidesconfigurationmd","title":"<code>guides/configuration.md</code>","text":"<p>Linked from 16 file(s):</p> <ul> <li>configurations-old/configuration.md (Line 3): <code>configuration guide</code></li> <li>configurations-old/configuration.md (Line 34): <code>will be placed</code></li> <li>configurations-old/configuration.md (Line 35): <code>additional details</code></li> <li>configurations-old/configuration.md (Line 46): <code>additional details</code></li> <li>configurations-old/configuration.md (Line 77): <code>the configuration guide</code></li> <li>configurations-old/configuration.md (Line 90): <code>additional details</code></li> <li>configurations-old/configuration.md (Line 158): <code>gateways section</code></li> <li>configurations-old/configuration.md (Line 214): <code>configuration overview scheduler section</code></li> <li>configurations-old/configuration.md (Line 224): <code>gateway/connection defaults section</code></li> <li>components/model/properties.md (Line 1666): <code>configuration guide</code></li> </ul> <p>... and 6 more references</p>"},{"location":"FILE_REFERENCES/#referenceclimd","title":"<code>reference/cli.md</code>","text":"<p>Linked from 12 file(s):</p> <ul> <li>comparisons.md (Line 41): <code>\u2705</code></li> <li>guides-old/notifications.md (Line 133): <code>run</code></li> <li>guides-old/projects.md (Line 69): <code>CLI</code></li> <li>guides-old/configuration.md (Line 1148): <code>accept a gateway option</code></li> <li>guides/run_and_scheduling.md (Line 646): <code>Run Command</code></li> <li>concepts-old/overview.md (Line 10): <code>CLI</code></li> <li>concepts-old/overview.md (Line 21): <code>`plan</code> command`</li> <li>concepts-old/overview.md (Line 28): <code>plan</code></li> <li>concepts-old/overview.md (Line 55): <code>`test</code> command`</li> <li>concepts-old/overview.md (Line 66): <code>`audit</code> command`</li> </ul> <p>... and 2 more references</p>"},{"location":"FILE_REFERENCES/#conceptsmodelsmodel_kindsmd","title":"<code>concepts/models/model_kinds.md</code>","text":"<p>Linked from 12 file(s):</p> <ul> <li>guides-old/configuration.md (Line 1269): <code>model concepts page</code></li> <li>guides-old/table_migration.md (Line 9): <code>external models</code></li> <li>guides-old/table_migration.md (Line 42): <code>`EXTERNAL</code> model`</li> <li>guides-old/table_migration.md (Line 44): <code>`VIEW</code> model`</li> <li>guides/models.md (Line 677): <code>Model Kinds</code></li> <li>guides/incremental_by_time.md (Line 5): <code>model kinds page</code></li> <li>guides/incremental_by_time.md (Line 1155): <code>Model Kinds</code></li> <li>guides/plan.md (Line 932): <code>Model Kinds</code></li> <li>getting_started/cli.md (Line 418): <code>kinds</code></li> <li>getting_started/cli.md (Line 420): <code>`SEED</code> models`</li> </ul> <p>... and 2 more references</p>"},{"location":"FILE_REFERENCES/#guidesreferenceclimd","title":"<code>guides/reference/cli.md</code>","text":"<p>Linked from 11 file(s):</p> <ul> <li>guides/get-started/docker.md (Line 106): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 114): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 121): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 133): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 139): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 147): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 155): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 161): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 173): <code>*Click here*</code></li> <li>guides/get-started/docker.md (Line 179): <code>*Click here*</code></li> </ul> <p>... and 1 more references</p>"},{"location":"FILE_REFERENCES/#concepts-oldplansmd","title":"<code>concepts-old/plans.md</code>","text":"<p>Linked from 10 file(s):</p> <ul> <li>concepts-old/environments.md (Line 14): <code>plan</code></li> <li>concepts-old/environments.md (Line 16): <code>vulcan plan</code></li> <li>concepts-old/environments.md (Line 29): <code>plans</code></li> <li>concepts-old/environments.md (Line 32): <code>plan</code></li> <li>concepts-old/overview.md (Line 25): <code>plans</code></li> <li>concepts-old/overview.md (Line 28): <code>apply</code></li> <li>concepts-old/glossary.md (Line 82): <code>plan application</code></li> <li>concepts-old/glossary.md (Line 88): <code>Virtual Update</code></li> <li>concepts-old/state.md (Line 9): <code>promoted</code></li> <li>concepts-old/architecture/snapshots.md (Line 10): <code>change categories</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldmacrosmacro_variablesmd","title":"<code>concepts-old/macros/macro_variables.md</code>","text":"<p>Linked from 10 file(s):</p> <ul> <li>concepts-old/macros/overview.md (Line 11): <code>Pre-defined macro variables</code></li> <li>concepts-old/macros/jinja_macros.md (Line 55): <code>predefined macro variables</code></li> <li>concepts-old/macros/jinja_macros.md (Line 57): <code>runtime_stage</code></li> <li>concepts-old/macros/jinja_macros.md (Line 57): <code>this_model</code></li> <li>concepts-old/macros/jinja_macros.md (Line 59): <code>temporal</code></li> <li>concepts-old/macros/jinja_macros.md (Line 326): <code>Predefined Vulcan macro variables</code></li> <li>concepts-old/macros/vulcan_macros.md (Line 32): <code>Vulcan pre-defined</code></li> <li>concepts-old/macros/vulcan_macros.md (Line 617): <code>runtime stage</code></li> <li>concepts-old/macros/vulcan_macros.md (Line 1155): <code>runtime stages</code></li> <li>concepts-old/macros/vulcan_macros.md (Line 1692): <code>Pre-defined variables</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptsglossarymd","title":"<code>concepts/glossary.md</code>","text":"<p>Linked from 9 file(s):</p> <ul> <li>comparisons.md (Line 30): <code>\u2705</code></li> <li>comparisons.md (Line 176): <code>Virtual Preview</code></li> <li>guides-old/configuration.md (Line 582): <code>catalog</code></li> <li>guides-old/configuration.md (Line 589): <code>virtual layer</code></li> <li>guides-old/configuration.md (Line 589): <code>physical layer</code></li> <li>configurations-old/configuration.md (Line 28): <code>physical layer</code></li> <li>configurations-old/configuration.md (Line 39): <code>virtual layer</code></li> <li>concepts-old/macros/macro_variables.md (Line 160): <code>virtual layer</code></li> <li>concepts-old/macros/macro_variables.md (Line 161): <code>virtual layer</code></li> </ul>"},{"location":"FILE_REFERENCES/#referencemodel_configurationmd","title":"<code>reference/model_configuration.md</code>","text":"<p>Linked from 9 file(s):</p> <ul> <li>guides-old/configuration.md (Line 287): <code>Model defaults</code></li> <li>guides-old/configuration.md (Line 1242): <code>models configuration reference page</code></li> <li>guides-old/configuration.md (Line 1329): <code>models configuration reference page</code></li> <li>guides-old/configuration.md (Line 1355): <code>models configuration reference page</code></li> <li>components/model/model_kinds.md (Line 7): <code>model configuration reference page</code></li> <li>guides/model_selection.md (Line 625): <code>Model Configuration</code></li> <li>concepts-old/plans.md (Line 574): <code>model defaults</code></li> <li>concepts-old/plans.md (Line 575): <code>model defaults</code></li> <li>concepts-old/macros/macro_variables.md (Line 141): <code>physical properties in model defaults</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-oldguidesconfigurationmd","title":"<code>configurations-old/guides/configuration.md</code>","text":"<p>Linked from 9 file(s):</p> <ul> <li>configurations-old/integrations/engines/snowflake.md (Line 177): <code>environment variables that the configuration file loads dynamically</code></li> <li>configurations-old/integrations/engines/snowflake.md (Line 208): <code>here</code></li> <li>configurations-old/integrations/engines/snowflake.md (Line 594): <code>model defaults</code></li> <li>configurations-old/integrations/engines/motherduck.md (Line 57): <code>environment variables that the configuration file loads dynamically</code></li> <li>configurations-old/integrations/engines/databricks.md (Line 109): <code>environment variables that the configuration file loads dynamically</code></li> <li>configurations-old/integrations/engines/databricks.md (Line 149): <code>environment variables that the configuration file loads dynamically</code></li> <li>configurations-old/integrations/engines/databricks.md (Line 182): <code>here</code></li> <li>configurations-old/integrations/engines/clickhouse.md (Line 70): <code>`environment_suffix_target</code> key in your project configuration`</li> <li>configurations-old/integrations/engines/clickhouse.md (Line 78): <code>`physical_schema_mapping</code> key in your project configuration`</li> </ul>"},{"location":"FILE_REFERENCES/#getting_starteddockermd","title":"<code>getting_started/docker.md</code>","text":"<p>Linked from 7 file(s):</p> <ul> <li>index.md (Line 104): <code>quickstart guide</code></li> <li>guides/models.md (Line 9): <code>Created your project</code></li> <li>guides/plan.md (Line 131): <code>Docker Quickstart</code></li> <li>concepts-old/macros/vulcan_macros.md (Line 267): <code>Vulcan quickstart guide</code></li> <li>getting_started/cli.md (Line 26): <code>Docker Quickstart</code></li> <li>getting_started/cli.md (Line 64): <code>Docker Quickstart</code></li> <li>getting_started/index.md (Line 15): <code>Start with Docker Quickstart</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-oldconceptsmodelsmodel_kindsmd","title":"<code>configurations-old/concepts/models/model_kinds.md</code>","text":"<p>Linked from 7 file(s):</p> <ul> <li>configurations-old/integrations/engines/athena.md (Line 69): <code>INCREMENTAL_BY_UNIQUE_KEY</code></li> <li>configurations-old/integrations/engines/athena.md (Line 69): <code>SCD_TYPE_2</code></li> <li>configurations-old/integrations/engines/mssql.md (Line 16): <code>incremental by unique key</code></li> <li>configurations-old/integrations/engines/clickhouse.md (Line 341): <code>`FULL</code> models`</li> <li>configurations-old/integrations/engines/clickhouse.md (Line 342): <code>`INCREMENTAL_BY_TIME_RANGE</code> models`</li> <li>configurations-old/integrations/engines/clickhouse.md (Line 343): <code>`INCREMENTAL_BY_UNIQUE_KEY</code> models`</li> <li>configurations-old/integrations/engines/clickhouse.md (Line 344): <code>`INCREMENTAL_BY_PARTITION</code> models`</li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldglossarymd","title":"<code>concepts-old/glossary.md</code>","text":"<p>Linked from 7 file(s):</p> <ul> <li>concepts-old/plans.md (Line 206): <code>Direct</code></li> <li>concepts-old/plans.md (Line 206): <code>Indirect</code></li> <li>concepts-old/plans.md (Line 206): <code>Backfill</code></li> <li>concepts-old/plans.md (Line 207): <code>Direct</code></li> <li>concepts-old/plans.md (Line 207): <code>Backfill</code></li> <li>concepts-old/plans.md (Line 208): <code>Indirect</code></li> <li>concepts-old/plans.md (Line 208): <code>No Backfill</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldmodelsmodel_kindsmd","title":"<code>concepts-old/models/model_kinds.md</code>","text":"<p>Linked from 7 file(s):</p> <ul> <li>concepts-old/plans.md (Line 336): <code>INCREMENTAL_BY_UNIQUE_KEY</code></li> <li>concepts-old/plans.md (Line 337): <code>INCREMENTAL_BY_PARTITION</code></li> <li>concepts-old/plans.md (Line 338): <code>SCD_TYPE_2_BY_TIME</code></li> <li>concepts-old/plans.md (Line 339): <code>SCD_TYPE_2_BY_COLUMN</code></li> <li>concepts-old/glossary.md (Line 49): <code>Model Kinds</code></li> <li>concepts-old/state.md (Line 13): <code>incremental models</code></li> <li>concepts-old/macros/macro_variables.md (Line 60): <code>here</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldmacrosvulcan_macrosmd","title":"<code>concepts-old/macros/vulcan_macros.md</code>","text":"<p>Linked from 7 file(s):</p> <ul> <li>concepts-old/macros/macro_variables.md (Line 9): <code>Vulcan macros page</code></li> <li>concepts-old/macros/macro_variables.md (Line 45): <code>Vulcan macros</code></li> <li>concepts-old/macros/macro_variables.md (Line 153): <code>Vulcan macros documentation</code></li> <li>concepts-old/macros/overview.md (Line 12): <code>Vulcan macros</code></li> <li>concepts-old/macros/jinja_macros.md (Line 87): <code>Vulcan macros documentation</code></li> <li>concepts-old/macros/jinja_macros.md (Line 119): <code>Vulcan macros documentation</code></li> <li>concepts-old/macros/jinja_macros.md (Line 324): <code>Vulcan</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptsmacrosvulcan_macrosmd","title":"<code>concepts/macros/vulcan_macros.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>comparisons.md (Line 28): <code>\u2705</code></li> <li>guides-old/isolated_systems.md (Line 65): <code>`@IF</code> macro operator`</li> <li>guides-old/isolated_systems.md (Line 81): <code>here</code></li> <li>configurations-old/configuration.md (Line 65): <code>`@VAR</code> macro function`</li> <li>configurations-old/configuration.md (Line 65): <code>`evaluator.var</code> method`</li> <li>configurations-old/configuration.md (Line 67): <code>Vulcan macros concepts page</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptsauditsmd","title":"<code>concepts/audits.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>comparisons.md (Line 33): <code>\u2705</code></li> <li>guides-old/notifications.md (Line 11): <code>Audit</code></li> <li>guides-old/notifications.md (Line 133): <code>audit</code></li> <li>guides/data_quality.md (Line 580): <code>Built-in Audits</code></li> <li>getting_started/cli.md (Line 254): <code>here</code></li> <li>getting_started/cli.md (Line 472): <code>audits</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptsenvironmentsmd","title":"<code>concepts/environments.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>comparisons.md (Line 37): <code>\u2705</code></li> <li>guides-old/isolated_systems.md (Line 19): <code>Vulcan environments</code></li> <li>guides/run_and_scheduling.md (Line 648): <code>Environments</code></li> <li>guides/models.md (Line 11): <code>dev environment</code></li> <li>guides/plan.md (Line 931): <code>Environments</code></li> <li>getting_started/cli.md (Line 300): <code>Vulcan environment</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentsmodelmodel_kindsmd","title":"<code>components/model/model_kinds.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>components/model/properties.md (Line 120): <code>Model Kinds</code></li> <li>components/model/properties.md (Line 1200): <code>Model Kinds</code></li> <li>components/model/properties.md (Line 1259): <code>Model Kinds documentation</code></li> <li>components/model/properties.md (Line 1334): <code>Model Kinds documentation</code></li> <li>components/model/properties.md (Line 1416): <code>Model Kinds documentation</code></li> <li>components/model/properties.md (Line 1457): <code>Model Kinds documentation</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentsadvanced-featuresmacrosbuilt_inmd","title":"<code>components/advanced-features/macros/built_in.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>components/advanced-features/macros/overview.md (Line 19): <code>Vulcan macros</code></li> <li>components/advanced-features/macros/jinja.md (Line 78): <code>Vulcan macros documentation</code></li> <li>components/advanced-features/macros/jinja.md (Line 108): <code>Vulcan macros documentation</code></li> <li>components/advanced-features/macros/jinja.md (Line 326): <code>Vulcan macros</code></li> <li>components/advanced-features/macros/variables.md (Line 8): <code>Vulcan macros page</code></li> <li>components/advanced-features/macros/variables.md (Line 154): <code>Vulcan macros documentation</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldmodelsoverviewmd","title":"<code>concepts-old/models/overview.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>concepts-old/environments.md (Line 6): <code>Models</code></li> <li>concepts-old/plans.md (Line 322): <code>the model definition</code></li> <li>concepts-old/plans.md (Line 562): <code>forward-only</code></li> <li>concepts-old/plans.md (Line 633): <code>disable_restatement</code></li> <li>concepts-old/state.md (Line 7): <code>Model Version</code></li> <li>concepts-old/state.md (Line 10): <code>auto restatements</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldarchitecturesnapshotsmd","title":"<code>concepts-old/architecture/snapshots.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>concepts-old/environments.md (Line 27): <code>fingerprint</code></li> <li>concepts-old/environments.md (Line 27): <code>snapshots</code></li> <li>concepts-old/plans.md (Line 224): <code>snapshot</code></li> <li>concepts-old/plans.md (Line 224): <code>fingerprint</code></li> <li>concepts-old/architecture/serialization.md (Line 3): <code>snapshot</code></li> <li>concepts-old/architecture/serialization.md (Line 9): <code>snapshot fingerprinting</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldenvironmentsmd","title":"<code>concepts-old/environments.md</code>","text":"<p>Linked from 6 file(s):</p> <ul> <li>concepts-old/plans.md (Line 3): <code>environment</code></li> <li>concepts-old/plans.md (Line 222): <code>environment</code></li> <li>concepts-old/plans.md (Line 671): <code>development environment</code></li> <li>concepts-old/state.md (Line 8): <code>Virtual Data Environment</code></li> <li>concepts-old/state.md (Line 9): <code>Virtual Data Environment</code></li> <li>concepts-old/macros/macro_variables.md (Line 159): <code>environment</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptsmodelspython_modelsmd","title":"<code>concepts/models/python_models.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>comparisons.md (Line 25): <code>\u2705\u2705</code></li> <li>comparisons.md (Line 169): <code>Python models</code></li> <li>guides-old/configuration.md (Line 1374): <code>Python models concepts page</code></li> <li>configurations-old/configuration.md (Line 65): <code>`context.var</code> method`</li> <li>concepts-old/architecture/serialization.md (Line 3): <code>Python models</code></li> </ul>"},{"location":"FILE_REFERENCES/#conceptstestsmd","title":"<code>concepts/tests.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>comparisons.md (Line 31): <code>\u2705</code></li> <li>comparisons.md (Line 164): <code>Unit and integration tests</code></li> <li>guides-old/connections.md (Line 52): <code>tests</code></li> <li>guides/data_quality.md (Line 582): <code>Testing</code></li> <li>getting_started/cli.md (Line 256): <code>here</code></li> </ul>"},{"location":"FILE_REFERENCES/#integrationsenginespostgresmd","title":"<code>integrations/engines/postgres.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>guides-old/configuration.md (Line 633): <code>Postgres</code></li> <li>guides-old/configuration.md (Line 917): <code>Postgres</code></li> <li>guides-old/configuration.md (Line 939): <code>Postgres</code></li> <li>guides-old/connections.md (Line 88): <code>Postgres</code></li> <li>concepts-old/state.md (Line 19): <code>PostgreSQL</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-oldgetting_starteddockermd","title":"<code>configurations-old/getting_started/docker.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>configurations-old/integrations/engines/snowflake.md (Line 18): <code>Vulcan Quickstart</code></li> <li>configurations-old/integrations/engines/motherduck.md (Line 18): <code>Vulcan Quickstart</code></li> <li>configurations-old/integrations/engines/databricks.md (Line 44): <code>Vulcan Quickstart</code></li> <li>configurations-old/integrations/engines/bigquery.md (Line 7): <code>quickstart project</code></li> <li>configurations-old/integrations/engines/bigquery.md (Line 20): <code>quickstart guide</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-oldconceptsmodelsoverviewmd","title":"<code>configurations-old/concepts/models/overview.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>configurations-old/integrations/engines/athena.md (Line 39): <code>properties</code></li> <li>configurations-old/integrations/engines/athena.md (Line 46): <code>physical_properties</code></li> <li>configurations-old/integrations/engines/athena.md (Line 71): <code>properties</code></li> <li>configurations-old/integrations/engines/mssql.md (Line 22): <code>physical_properties</code></li> <li>configurations-old/integrations/engines/clickhouse.md (Line 390): <code>partitioned_by</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentsmodeltypesmodel_kindsmd","title":"<code>components/model/types/model_kinds.md</code>","text":"<p>Linked from 5 file(s):</p> <ul> <li>components/model/types/managed_models.md (Line 13): <code>model kind</code></li> <li>components/model/types/managed_models.md (Line 19): <code>materialized views</code></li> <li>components/model/types/managed_models.md (Line 56): <code>MANAGED</code></li> <li>components/model/types/python_models.md (Line 16): <code>model kinds</code></li> <li>components/model/types/sql_models.md (Line 324): <code>external models</code></li> </ul>"},{"location":"FILE_REFERENCES/#most-linked-pages","title":"Most Linked Pages","text":"<p>Internal pages sorted by number of incoming links:</p> <ul> <li><code>concepts/plans.md</code> - 18 incoming links</li> <li><code>reference/configuration.md</code> - 18 incoming links</li> <li><code>guides/configuration.md</code> - 16 incoming links</li> <li><code>reference/cli.md</code> - 12 incoming links</li> <li><code>concepts/models/model_kinds.md</code> - 12 incoming links</li> <li><code>guides/reference/cli.md</code> - 11 incoming links</li> <li><code>concepts-old/plans.md</code> - 10 incoming links</li> <li><code>concepts-old/macros/macro_variables.md</code> - 10 incoming links</li> <li><code>concepts/glossary.md</code> - 9 incoming links</li> <li><code>reference/model_configuration.md</code> - 9 incoming links</li> <li><code>configurations-old/guides/configuration.md</code> - 9 incoming links</li> <li><code>getting_started/docker.md</code> - 7 incoming links</li> <li><code>configurations-old/concepts/models/model_kinds.md</code> - 7 incoming links</li> <li><code>concepts-old/glossary.md</code> - 7 incoming links</li> <li><code>concepts-old/models/model_kinds.md</code> - 7 incoming links</li> <li><code>concepts-old/macros/vulcan_macros.md</code> - 7 incoming links</li> <li><code>concepts/macros/vulcan_macros.md</code> - 6 incoming links</li> <li><code>concepts/audits.md</code> - 6 incoming links</li> <li><code>concepts/environments.md</code> - 6 incoming links</li> <li><code>components/model/model_kinds.md</code> - 6 incoming links</li> </ul>"},{"location":"FILE_REFERENCES/#external-resources","title":"External Resources","text":""},{"location":"FILE_REFERENCES/#external-domains","title":"External Domains","text":"<ul> <li><code>github.com</code> - 35 references</li> <li><code>sqlglot.com</code> - 21 references</li> <li><code>en.wikipedia.org</code> - 20 references</li> <li><code>docs.snowflake.com</code> - 19 references</li> <li><code>docs.databricks.com</code> - 18 references</li> <li><code>trino.io</code> - 15 references</li> <li><code>vulcan.readthedocs.io</code> - 12 references</li> <li><code>clickhouse.com</code> - 12 references</li> <li><code>duckdb.org</code> - 11 references</li> <li><code>cloud.google.com</code> - 9 references</li> <li><code>jinja.palletsprojects.com</code> - 8 references</li> <li><code>tobikodata.com</code> - 7 references</li> <li><code>docs.aws.amazon.com</code> - 6 references</li> <li><code>api.slack.com</code> - 4 references</li> <li><code>docs.python.org</code> - 4 references</li> <li><code>learn.microsoft.com</code> - 4 references</li> <li><code>google-auth.readthedocs.io</code> - 4 references</li> <li><code>www.docker.com</code> - 4 references</li> <li><code>docs.docker.com</code> - 3 references</li> <li><code>dateparser.readthedocs.io</code> - 2 references</li> <li><code>boto3.amazonaws.com</code> - 2 references</li> <li><code>aws.amazon.com</code> - 2 references</li> <li><code>docs.risingwave.com</code> - 2 references</li> <li><code>www.getdbt.com</code> - 1 references</li> <li><code>www.postgresql.org</code> - 1 references</li> <li><code>regex101.com</code> - 1 references</li> <li><code>chat.openai.com</code> - 1 references</li> <li><code>en.m.wikipedia.org</code> - 1 references</li> <li><code>filesystem-spec.readthedocs.io</code> - 1 references</li> <li><code>fsspec.github.io</code> - 1 references</li> <li><code>projectnessie.org</code> - 1 references</li> <li><code>zookeeper.apache.org</code> - 1 references</li> <li><code>azure.microsoft.com</code> - 1 references</li> <li><code>pypi.org</code> - 1 references</li> <li><code>googleapis.dev</code> - 1 references</li> <li><code>risingwave.com</code> - 1 references</li> <li><code>www.medcalc.org</code> - 1 references</li> <li><code>pandas.pydata.org</code> - 1 references</li> <li><code>packaging.python.org</code> - 1 references</li> </ul>"},{"location":"FILE_REFERENCES/#external-links-by-category","title":"External Links by Category","text":"<ul> <li>GitHub: 16 links</li> <li>Documentation sites: 54 links</li> <li>Wikipedia: 12 links</li> </ul>"},{"location":"FILE_REFERENCES/#links-by-file","title":"Links by File","text":""},{"location":"FILE_REFERENCES/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Root files</li> <li>comparisons.md (23 links)</li> <li>index.md (1 links)</li> <li>components/advanced-features/</li> <li>components/advanced-features/custom_materializations.md (6 links)</li> <li>components/advanced-features/signals.md (2 links)</li> <li>components/advanced-features/macros/</li> <li>components/advanced-features/macros/built_in.md (69 links)</li> <li>components/advanced-features/macros/jinja.md (6 links)</li> <li>components/advanced-features/macros/overview.md (5 links)</li> <li>components/advanced-features/macros/variables.md (16 links)</li> <li>components/audits/</li> <li>components/audits/audits.md (7 links)</li> <li>components/checks/</li> <li>components/checks/checks.md (1 links)</li> <li>components/model/</li> <li>components/model/model_kinds.md (12 links)</li> <li>components/model/overview.md (3 links)</li> <li>components/model/properties.md (14 links)</li> <li>components/model/statements.md (1 links)</li> <li>components/model/types/</li> <li>components/model/types/external_models.md (2 links)</li> <li>components/model/types/managed_models.md (11 links)</li> <li>components/model/types/python_models.md (13 links)</li> <li>components/model/types/sql_models.md (12 links)</li> <li>components/semantics/</li> <li>components/semantics/business_metrics.md (2 links)</li> <li>components/semantics/models.md (2 links)</li> <li>components/semantics/overview.md (5 links)</li> <li>components/tests/</li> <li>components/tests/tests.md (3 links)</li> <li>concepts-old/</li> <li>concepts-old/environments.md (7 links)</li> <li>concepts-old/glossary.md (11 links)</li> <li>concepts-old/overview.md (10 links)</li> <li>concepts-old/plans.md (50 links)</li> <li>concepts-old/state.md (10 links)</li> <li>concepts-old/architecture/</li> <li>concepts-old/architecture/serialization.md (4 links)</li> <li>concepts-old/architecture/snapshots.md (1 links)</li> <li>concepts-old/macros/</li> <li>concepts-old/macros/jinja_macros.md (12 links)</li> <li>concepts-old/macros/macro_variables.md (18 links)</li> <li>concepts-old/macros/overview.md (5 links)</li> <li>concepts-old/macros/vulcan_macros.md (69 links)</li> <li>configurations/</li> <li>configurations/overview.md (14 links)</li> <li>configurations-old/</li> <li>configurations-old/configuration.md (39 links)</li> <li>configurations-old/integrations/engines/</li> <li>configurations-old/integrations/engines/athena.md (15 links)</li> <li>configurations-old/integrations/engines/azuresql.md (2 links)</li> <li>configurations-old/integrations/engines/bigquery.md (31 links)</li> <li>configurations-old/integrations/engines/clickhouse.md (31 links)</li> <li>configurations-old/integrations/engines/databricks.md (74 links)</li> <li>configurations-old/integrations/engines/duckdb.md (15 links)</li> <li>configurations-old/integrations/engines/fabric.md (2 links)</li> <li>configurations-old/integrations/engines/motherduck.md (8 links)</li> <li>configurations-old/integrations/engines/mssql.md (4 links)</li> <li>configurations-old/integrations/engines/redshift.md (1 links)</li> <li>configurations-old/integrations/engines/risingwave.md (6 links)</li> <li>configurations-old/integrations/engines/snowflake.md (32 links)</li> <li>configurations-old/integrations/engines/spark.md (2 links)</li> <li>configurations-old/integrations/engines/trino.md (28 links)</li> <li>configurations/engines/snowflake/</li> <li>configurations/engines/snowflake/snowflake.md (32 links)</li> <li>configurations/options/</li> <li>configurations/options/linter.md (3 links)</li> <li>configurations/options/model_defaults.md (5 links)</li> <li>configurations/options/notifications.md (12 links)</li> <li>getting_started/</li> <li>getting_started/cli.md (28 links)</li> <li>getting_started/index.md (2 links)</li> <li>getting_started/prerequisites.md (4 links)</li> <li>guides/</li> <li>guides/data_quality.md (4 links)</li> <li>guides/incremental_by_time.md (7 links)</li> <li>guides/model_selection.md (6 links)</li> <li>guides/models.md (8 links)</li> <li>guides/plan.md (7 links)</li> <li>guides/run_and_scheduling.md (7 links)</li> <li>guides/transpiling_semantics.md (3 links)</li> <li>guides-old/</li> <li>guides-old/configuration.md (85 links)</li> <li>guides-old/connections.md (13 links)</li> <li>guides-old/customizing_vulcan.md (1 links)</li> <li>guides-old/isolated_systems.md (10 links)</li> <li>guides-old/notifications.md (12 links)</li> <li>guides-old/projects.md (3 links)</li> <li>guides-old/table_migration.md (5 links)</li> <li>guides/get-started/</li> <li>guides/get-started/docker.md (18 links)</li> </ul>"},{"location":"FILE_REFERENCES/#root-files","title":"Root Files","text":""},{"location":"FILE_REFERENCES/#comparisons","title":"comparisons.md","text":"<p>Internal Links (20):</p> <ul> <li>Line 7: <code>[Why Vulcan](index.md#why-vulcan)</code></li> <li>Line 7: <code>[What is Vulcan](index.md#what-is-vulcan)</code></li> <li>Line 24: <code>[\u2705](concepts/models/overview.md)</code> \u26a0\ufe0f</li> <li>Line 25: <code>[\u2705\u2705](concepts/models/python_models.md)</code> \u26a0\ufe0f</li> <li>Line 27: <code>[\u2705](concepts/macros/jinja_macros.md)</code> \u26a0\ufe0f</li> <li>Line 28: <code>[\u2705](concepts/macros/vulcan_macros.md)</code> \u26a0\ufe0f</li> <li>Line 30: <code>[\u2705](concepts/glossary.md#semantic-understanding)</code> \u26a0\ufe0f</li> <li>Line 31: <code>[\u2705](concepts/tests.md)</code> \u26a0\ufe0f</li> <li>Line 33: <code>[\u2705](concepts/audits.md)</code> \u26a0\ufe0f</li> <li>Line 34: <code>[\u2705](concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 35: <code>[\u2705](concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 37: <code>[\u2705](concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 41: <code>[\u2705](reference/cli.md)</code> \u26a0\ufe0f</li> <li>Line 51: <code>[\u2705](concepts/models/sql_models.md#transpilation)</code> \u26a0\ufe0f</li> <li>Line 65: <code>[Virtual Environments](./concepts/plans.md#plan-application)</code> \u26a0\ufe0f</li> <li>Line 152: <code>[batch](concepts/models/overview.md#batch_size)</code> \u26a0\ufe0f</li> <li>Line 164: <code>[Unit and integration tests](./concepts/tests.md)</code> \u26a0\ufe0f</li> <li>Line 169: <code>[Python models](concepts/models/python_models.md)</code> \u26a0\ufe0f</li> <li>Line 174: <code>[</code>vulcan plan<code>](concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 176: <code>[Virtual Preview](concepts/glossary.md#virtual-preview)</code> \u26a0\ufe0f</li> </ul> <p>External Links (3):</p> <ul> <li>Line 10: <code>[dbt](https://www.getdbt.com/)</code></li> <li>Line 155: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> <li>Line 157: <code>[SQLGlot](https://github.com/tobymao/sqlglot)</code></li> </ul>"},{"location":"FILE_REFERENCES/#index","title":"index.md","text":"<p>Internal Links (1):</p> <ul> <li>Line 104: <code>[quickstart guide](getting_started/docker.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#componentsadvanced-features","title":"components/advanced-features/","text":""},{"location":"FILE_REFERENCES/#components-advanced-features-custom_materializations","title":"components/advanced-features/custom_materializations.md","text":"<p>Internal Links (3):</p> <ul> <li>Line 3: <code>[model kinds](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 35: <code>[Python model](../concepts/models/python_models.md)</code> \u26a0\ufe0f</li> <li>Line 37: <code>[Python package](#python-packaging)</code></li> </ul> <p>External Links (3):</p> <ul> <li>Line 8: <code>[community slack](https://tobikodata.com/community.html)</code></li> <li>Line 400: <code>[setuptools entrypoints](https://packaging.python.org/en/latest/guides/creating-and-discovering-plugins/#using-package-metadata)</code></li> <li>Line 424: <code>[custom_materializations](https://github.com/TobikoData/vulcan/tree/main/examples/custom_materializations)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-advanced-features-signals","title":"components/advanced-features/signals.md","text":"<p>Internal Links (2):</p> <ul> <li>Line 3: <code>[built-in scheduler](./scheduling.md#built-in-scheduler)</code> \u26a0\ufe0f</li> <li>Line 55: <code>[Vulcan macros](../concepts/macros/vulcan_macros.md#typed-macros)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#componentsadvanced-featuresmacros","title":"components/advanced-features/macros/","text":""},{"location":"FILE_REFERENCES/#components-advanced-features-macros-built_in","title":"components/advanced-features/macros/built_in.md","text":"<p>Internal Links (46):</p> <ul> <li>Line 29: <code>[user-defined macro variables](#user-defined-variables)</code></li> <li>Line 30: <code>[Vulcan pre-defined](./macro_variables.md)</code> \u26a0\ufe0f</li> <li>Line 30: <code>[user-defined local](#local-variables)</code> \u26a0\ufe0f</li> <li>Line 30: <code>[user-defined global](#global-variables)</code> \u26a0\ufe0f</li> <li>Line 31: <code>[Vulcan's](#macro-operators)</code></li> <li>Line 31: <code>[user-defined](#user-defined-macro-functions)</code></li> <li>Line 51: <code>[gateway variable](#gateway-variables)</code></li> <li>Line 95: <code>[global](#global-variables)</code></li> <li>Line 95: <code>[gateway](#gateway-variables)</code></li> <li>Line 95: <code>[blueprint](#blueprint-variables)</code></li> <li>Line 95: <code>[local](#local-variables)</code></li> <li>Line 105: <code>[</code>variables<code>key](../../reference/configuration.md#variables)</code> \u26a0\ufe0f</li> <li>Line 171: <code>[Python macro functions](#accessing-global-variable-values)</code> \u26a0\ufe0f</li> <li>Line 171: <code>[Python models](../models/python_models.md#user-defined-variables)</code> \u26a0\ufe0f</li> <li>Line 204: <code>[global variables](#global-variables)</code></li> <li>Line 210: <code>[global](#global-variables)</code></li> <li>Line 210: <code>[gateway-specific](#gateway-variables)</code></li> <li>Line 212: <code>[creating model templates](../models/sql_models.md)</code> \u26a0\ufe0f</li> <li>Line 247: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 253: <code>[global](#global-variables)</code></li> <li>Line 253: <code>[blueprint](#blueprint-variables)</code></li> <li>Line 253: <code>[gateway-specific](#gateway-variables)</code></li> <li>Line 267: <code>[Vulcan quickstart guide](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 527: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 601: <code>[Macro rendering](#vulcan-macro-approach)</code></li> <li>Line 615: <code>[runtime stage](./macro_variables.md#predefined-variables)</code> \u26a0\ufe0f</li> <li>Line 646: <code>[</code>@IF<code>operator](#if)</code></li> <li>Line 672: <code>[</code>@EACH<code>](#each)</code></li> <li>Line 672: <code>[</code>@REDUCE<code>](#reduce)</code></li> <li>Line 676: <code>[</code>@IF<code>](#if)</code></li> <li>Line 753: <code>[below](#positional-and-keyword-arguments)</code></li> <li>Line 1052: <code>[below](#positional-and-keyword-arguments)</code></li> <li>Line 1153: <code>[runtime stages](./macro_variables.md#runtime-variables)</code> \u26a0\ufe0f</li> <li>Line 1166: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 1243: <code>[User-defined Variables](#user-defined-variables)</code></li> <li>Line 1475: <code>[Jinja templating system](./jinja.md#user-defined-macro-functions)</code></li> <li>Line 1492: <code>[argument type annotations are provided](#argument-data-types)</code></li> <li>Line 1496: <code>[return a list of strings or expressions](#returning-more-than-one-value)</code></li> <li>Line 1548: <code>[below](#typed-macros)</code></li> <li>Line 1550: <code>[mentioned above](#inputs-and-outputs)</code></li> <li>Line 1617: <code>[previous section](#positional-and-keyword-arguments)</code></li> <li>Line 1690: <code>[Pre-defined variables](./macro_variables.md#predefined-variables)</code> \u26a0\ufe0f</li> <li>Line 1690: <code>[user-defined local variables](#local-variables)</code> \u26a0\ufe0f</li> <li>Line 1746: <code>[User-defined global variables](#global-variables)</code></li> <li>Line 1831: <code>[external model](../models/external_models.md)</code> \u26a0\ufe0f</li> <li>Line 2120: <code>[Jinja macros](./jinja.md)</code></li> </ul> <p>External Links (23):</p> <ul> <li>Line 5: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> <li>Line 9: <code>[sqlglot](https://github.com/tobymao/sqlglot)</code></li> <li>Line 547: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li> <li>Line 676: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li> <li>Line 817: <code>[</code>MD5<code>](https://en.wikipedia.org/wiki/MD5)</code></li> <li>Line 823: <code>[</code>MD5()<code>hash function](https://en.wikipedia.org/wiki/MD5)</code></li> <li>Line 1006: <code>[haversine distance](https://en.wikipedia.org/wiki/Haversine_formula)</code></li> <li>Line 1111: <code>[</code>date_spine<code>](https://github.com/dbt-labs/dbt-utils?tab=readme-ov-file#date_spine-source)</code></li> <li>Line 1151: <code>[</code>$properties<code>](https://trino.io/docs/current/connector/iceberg.html#metadata-tables)</code><li>Line 1239: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li><li>Line 1343: <code>[common table expressions](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression)</code></li><li>Line 1646: <code>[</code>Literal<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)</code></li><li>Line 1647: <code>[</code>Literal<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)</code></li><li>Line 1648: <code>[</code>Column<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Column)</code></li><li>Line 1650: <code>[</code>Array<code>expression](https://sqlglot.com/sqlglot/expressions.html#Array)</code></li><li>Line 1878: <code>[DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)</code></li><li>Line 1902: <code>[SQLGlot](https://github.com/tobymao/sqlglot)</code></li><li>Line 1932: <code>[SQLGLot expression](https://github.com/tobymao/sqlglot/blob/main/sqlglot/expressions.py)</code></li><li>Line 1942: <code>[Column expression](https://sqlglot.com/sqlglot/expressions.html#Column)</code></li><li>Line 1944: <code>[Condition class](https://sqlglot.com/sqlglot/expressions.html#Condition)</code></li><li>Line 1944: <code>[</code>between<code>](https://sqlglot.com/sqlglot/expressions.html#Condition.between)</code></li><li>Line 1944: <code>[</code>like<code>](https://sqlglot.com/sqlglot/expressions.html#Condition.like)</code></li><li>Line 2112: <code>[here](https://github.com/TobikoData/vulcan/blob/main/tests/core/test_macros.py)</code></li></li></ul>"},{"location":"FILE_REFERENCES/#components-advanced-features-macros-jinja","title":"components/advanced-features/macros/jinja.md","text":"<p>Internal Links (5):</p><ul> <li>Line 54: <code>[predefined macro variables](./variables.md)</code></li> <li>Line 78: <code>[Vulcan macros documentation](./built_in.md#global-variables)</code></li> <li>Line 108: <code>[Vulcan macros documentation](./built_in.md#gateway-variables)</code></li> <li>Line 326: <code>[Vulcan macros](./built_in.md)</code></li> <li>Line 328: <code>[predefined Vulcan macro variables](./variables.md)</code></li> </ul><p>External Links (1):</p><ul> <li>Line 3: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-advanced-features-macros-overview","title":"components/advanced-features/macros/overview.md","text":"<p>Internal Links (4):</p><ul> <li>Line 14: <code>[pre-defined variables](./variables.md)</code></li> <li>Line 18: <code>[Pre-defined macro variables](./variables.md)</code></li> <li>Line 19: <code>[Vulcan macros](./built_in.md)</code></li> <li>Line 20: <code>[Jinja macros](./jinja.md)</code></li> </ul><p>External Links (1):</p><ul> <li>Line 3: <code>[declarative language](https://en.wikipedia.org/wiki/Declarative_programming)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-advanced-features-macros-variables","title":"components/advanced-features/macros/variables.md","text":"<p>Internal Links (13):</p><ul> <li>Line 8: <code>[Vulcan macros page](./built_in.md#user-defined-variables)</code></li> <li>Line 8: <code>[Jinja macros page](./jinja.md#user-defined-variables)</code></li> <li>Line 57: <code>[here](../model/model_kinds.md#timezones)</code> \u26a0\ufe0f</li> <li>Line 137: <code>[pre/post-statements](../model/sql_models.md#optional-prepost-statements)</code> \u26a0\ufe0f</li> <li>Line 139: <code>[gateway](../../guides/connections.md)</code> \u26a0\ufe0f</li> <li>Line 141: <code>[generic audits](../audits.md#generic-audits)</code> \u26a0\ufe0f</li> <li>Line 141: <code>[on_virtual_update statements](../model/sql_models.md#optional-on-virtual-update-statements)</code> \u26a0\ufe0f</li> <li>Line 143: <code>[physical properties in model defaults](../../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 154: <code>[Vulcan macros documentation](./built_in.md#embedding-variables-in-strings)</code></li> <li>Line 158: <code>[</code>before_all<code>and</code>after_all<code>statements](../../guides/configuration.md#before_all-and-after_all-statements)</code> \u26a0\ufe0f</li> <li>Line 160: <code>[environment](../environments.md)</code> \u26a0\ufe0f</li> <li>Line 161: <code>[virtual layer](../../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 162: <code>[virtual layer](../../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 52: <code>[datetime module](https://docs.python.org/3/library/datetime.html)</code></li> <li>Line 52: <code>[Unix epoch](https://en.wikipedia.org/wiki/Unix_time)</code></li> <li>Line 55: <code>[UTC time zone](https://en.wikipedia.org/wiki/Coordinated_Universal_Time)</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentsaudits","title":"components/audits/","text":""},{"location":"FILE_REFERENCES/#components-audits-audits","title":"components/audits/audits.md","text":"<p>Internal Links (4):</p><ul> <li>Line 5: <code>[tests](../tests/tests.md)</code> \u26a0\ufe0f</li> <li>Line 5: <code>[plan](./plans.md)</code> \u26a0\ufe0f</li> <li>Line 75: <code>[restatement plan](./plans.md#restatement-plans)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[macros](./macros/overview.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 743: <code>[symmetrised Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence)</code></li> <li>Line 760: <code>[chi-square statistic](https://en.wikipedia.org/wiki/Chi-squared_test)</code></li> <li>Line 764: <code>[chi-square table](https://www.medcalc.org/manual/chi-square-table.php)</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentschecks","title":"components/checks/","text":""},{"location":"FILE_REFERENCES/#components-checks-checks","title":"components/checks/checks.md","text":"<p>Internal Links (1):</p><ul> <li>Line 5: <code>[audits](audits.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#componentsmodel","title":"components/model/","text":""},{"location":"FILE_REFERENCES/#components-model-model_kinds","title":"components/model/model_kinds.md","text":"<p>Internal Links (11):</p><ul> <li>Line 7: <code>[model configuration reference page](../../reference/model_configuration.md)</code> \u26a0\ufe0f</li> <li>Line 33: <code>[Macros documentation](../macros/macro_variables.md)</code> \u26a0\ufe0f</li> <li>Line 408: <code>[above](#timezones)</code></li> <li>Line 505: <code>[idempotent](../glossary.md#idempotency)</code> \u26a0\ufe0f</li> <li>Line 535: <code>[SCD Type 2](#scd-type-2)</code></li> <li>Line 696: <code>[non-idempotent](../glossary.md#idempotency)</code> \u26a0\ufe0f</li> <li>Line 1627: <code>[Processing Source Table with Historical Data](#processing-source-table-with-historical-data)</code></li> <li>Line 1658: <code>[Processing Source Table with Historical Data](#processing-source-table-with-historical-data)</code></li> <li>Line 1858: <code>[External Models documentation](./external_models.md)</code> \u26a0\ufe0f</li> <li>Line 1886: <code>[Managed Models documentation](./managed_models.md)</code> \u26a0\ufe0f</li> <li>Line 1903: <code>[non-idempotent](../glossary.md#idempotency)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 767: <code>[Redshift documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_MERGE.html#r_MERGE-parameters)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-model-overview","title":"components/model/overview.md","text":"<p>Internal Links (3):</p><ul> <li>Line 129: <code>[Model Properties](./properties.md)</code></li> <li>Line 343: <code>[Python Models](./python_models.md)</code> \u26a0\ufe0f</li> <li>Line 401: <code>[macros documentation](../macros/overview.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#components-model-properties","title":"components/model/properties.md","text":"<p>Internal Links (13):</p><ul> <li>Line 48: <code>[name inference](#model-naming)</code></li> <li>Line 120: <code>[Model Kinds](model_kinds.md)</code></li> <li>Line 505: <code>[inline column comments](./overview.md#inline-column-comments)</code></li> <li>Line 547: <code>[Python models](./python_models.md)</code> \u26a0\ufe0f</li> <li>Line 599: <code>[audits](../audits.md)</code> \u26a0\ufe0f</li> <li>Line 1200: <code>[Model Kinds](model_kinds.md)</code></li> <li>Line 1208: <code>[forward-only](../plans.md#forward-only-plans)</code> \u26a0\ufe0f</li> <li>Line 1211: <code>[data restatement](../plans.md#restatement-plans)</code> \u26a0\ufe0f</li> <li>Line 1259: <code>[Model Kinds documentation](model_kinds.md#incremental_by_time_range)</code></li> <li>Line 1334: <code>[Model Kinds documentation](model_kinds.md#incremental_by_unique_key)</code></li> <li>Line 1416: <code>[Model Kinds documentation](model_kinds.md#incremental_by_partition)</code></li> <li>Line 1457: <code>[Model Kinds documentation](model_kinds.md#scd-type-2)</code></li> <li>Line 1666: <code>[configuration guide](../../guides/configuration.md#model-naming)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 555: <code>[SQLGlot dialects](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/__init__.py)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-model-statements","title":"components/model/statements.md","text":"<p>Internal Links (1):</p><ul> <li>Line 504: <code>[Macro Variables](../macros/macro_variables.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#componentsmodeltypes","title":"components/model/types/","text":""},{"location":"FILE_REFERENCES/#components-model-types-external_models","title":"components/model/types/external_models.md","text":"<p>Internal Links (2):</p><ul> <li>Line 79: <code>[isolated systems with multiple gateways](../../guides/isolated_systems.md#multiple-gateways)</code> \u26a0\ufe0f</li> <li>Line 158: <code>[assertions](../audits.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#components-model-types-managed_models","title":"components/model/types/managed_models.md","text":"<p>Internal Links (7):</p><ul> <li>Line 9: <code>[External Models](./external_models.md)</code></li> <li>Line 13: <code>[model kind](./model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 19: <code>[materialized views](./model_kinds.md#materialized-views)</code> \u26a0\ufe0f</li> <li>Line 54: <code>[Snowflake](../../configurations/engines/snowflake.md)</code> \u26a0\ufe0f</li> <li>Line 56: <code>[</code>MANAGED<code>](./model_kinds.md#managed)</code> \u26a0\ufe0f</li> <li>Line 102: <code>[</code>physical_properties<code>](../overview.md#physical_properties)</code></li> <li>Line 117: <code>[standard model property](../models/overview.md#clustered_by)</code> \u26a0\ufe0f</li> </ul><p>External Links (4):</p><ul> <li>Line 46: <code>[let us know](https://tobikodata.com/slack)</code></li> <li>Line 54: <code>[Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)</code></li> <li>Line 60: <code>[Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)</code></li> <li>Line 100: <code>[Snowflake documentation](https://docs.snowflake.com/sql-reference/sql/create-dynamic-table)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-model-types-python_models","title":"components/model/types/python_models.md","text":"<p>Internal Links (12):</p><ul> <li>Line 16: <code>[model kinds](./model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 105: <code>[model configuration reference](../../reference/model_configuration.md#model-kind-properties)</code> \u26a0\ufe0f</li> <li>Line 139: <code>[idempotent](../glossary.md#idempotency)</code> \u26a0\ufe0f</li> <li>Line 158: <code>[model configuration reference](../../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 238: <code>[model configuration reference](../../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 269: <code>[environment](../environments.md)</code> \u26a0\ufe0f</li> <li>Line 319: <code>[global variables](../../reference/configuration.md#variables)</code> \u26a0\ufe0f</li> <li>Line 319: <code>[blueprint variables](#python-model-blueprinting)</code> \u26a0\ufe0f</li> <li>Line 357: <code>[User-defined global variables](../../reference/configuration.md#variables)</code> \u26a0\ufe0f</li> <li>Line 447: <code>[here](../../concepts/macros/vulcan_macros.md#embedding-variables-in-strings)</code> \u26a0\ufe0f</li> <li>Line 521: <code>[metadata properties](./overview.md#model-properties)</code> \u26a0\ufe0f</li> <li>Line 772: <code>[serialization framework](../architecture/serialization.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 685: <code>[Bigframe](https://cloud.google.com/bigquery/docs/use-bigquery-dataframes#pandas-examples)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-model-types-sql_models","title":"components/model/types/sql_models.md","text":"<p>Internal Links (10):</p><ul> <li>Line 52: <code>[Model Properties](./overview.md#model-properties)</code> \u26a0\ufe0f</li> <li>Line 89: <code>[model configuration reference](../../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 151: <code>[Jinja expressions](../macros/jinja_macros.md)</code> \u26a0\ufe0f</li> <li>Line 227: <code>[here](../../concepts/macros/vulcan_macros.md#embedding-variables-in-strings)</code> \u26a0\ufe0f</li> <li>Line 279: <code>[Python Models](./python_models.md)</code></li> <li>Line 297: <code>[signals](../../guides/signals.md)</code> \u26a0\ufe0f</li> <li>Line 324: <code>[</code>create_external_models<code>](../../reference/cli.md#create_external_models)</code> \u26a0\ufe0f</li> <li>Line 324: <code>[external models](./model_kinds.md#external)</code> \u26a0\ufe0f</li> <li>Line 348: <code>[macro syntax](../macros/overview.md)</code> \u26a0\ufe0f</li> <li>Line 352: <code>[Macros documentation](../macros/overview.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (2):</p><ul> <li>Line 334: <code>[SQLGlot](https://github.com/tobymao/sqlglot)</code></li> <li>Line 348: <code>[Jinja templates](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentssemantics","title":"components/semantics/","text":""},{"location":"FILE_REFERENCES/#components-semantics-business_metrics","title":"components/semantics/business_metrics.md","text":"<p>Internal Links (2):</p><ul> <li>Line 273: <code>[Semantic Models](models.md)</code></li> <li>Line 274: <code>[Semantics Overview](overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-semantics-models","title":"components/semantics/models.md","text":"<p>Internal Links (2):</p><ul> <li>Line 346: <code>[Business Metrics](business_metrics.md)</code></li> <li>Line 348: <code>[Semantics Overview](overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#components-semantics-overview","title":"components/semantics/overview.md","text":"<p>Internal Links (5):</p><ul> <li>Line 43: <code>[Semantic Models](models.md)</code></li> <li>Line 69: <code>[Business Metrics](metrics.md)</code> \u26a0\ufe0f</li> <li>Line 158: <code>[Semantic Models](models.md)</code></li> <li>Line 159: <code>[Business Metrics](metrics.md)</code> \u26a0\ufe0f</li> <li>Line 160: <code>[Transpiling Semantic Queries](../../guides/transpiling_semantics.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#componentstests","title":"components/tests/","text":""},{"location":"FILE_REFERENCES/#components-tests-tests","title":"components/tests/tests.md","text":"<p>Internal Links (2):</p><ul> <li>Line 5: <code>[audits](../audits/audits.md)</code></li> <li>Line 17: <code>[plan](plans.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 635: <code>[pandas read_csv documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old","title":"concepts-old/","text":""},{"location":"FILE_REFERENCES/#concepts-old-environments","title":"concepts-old/environments.md","text":"<p>Internal Links (7):</p><ul> <li>Line 6: <code>[Models](models/overview.md)</code> \u26a0\ufe0f</li> <li>Line 14: <code>[plan](plans.md)</code></li> <li>Line 16: <code>[</code>vulcan plan<code>](plans.md)</code></li> <li>Line 27: <code>[fingerprint](architecture/snapshots.md#fingerprinting)</code></li> <li>Line 27: <code>[snapshots](architecture/snapshots.md)</code></li> <li>Line 29: <code>[plans](plans.md#plan-application)</code></li> <li>Line 32: <code>[plan](plans.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-glossary","title":"concepts-old/glossary.md","text":"<p>Internal Links (10):</p><ul> <li>Line 13: <code>[tests](tests.md)</code> \u26a0\ufe0f</li> <li>Line 13: <code>[audits](audits.md)</code> \u26a0\ufe0f</li> <li>Line 16: <code>[tests](tests.md)</code> \u26a0\ufe0f</li> <li>Line 34: <code>[Indirect Modification](#indirect-modification)</code></li> <li>Line 49: <code>[Model Kinds](models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 52: <code>[Direct Modification](#direct-modification)</code></li> <li>Line 61: <code>[Vulcan virtual layer's](#virtual-layer)</code></li> <li>Line 82: <code>[plan application](plans.md#plan-application)</code></li> <li>Line 85: <code>[physical layer and physical data storage](#physical-layer)</code></li> <li>Line 88: <code>[Virtual Update](plans.md#virtual-update)</code></li> </ul><p>External Links (1):</p><ul> <li>Line 67: <code>[SQLGlot](https://github.com/tobymao/sqlglot)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-overview","title":"concepts-old/overview.md","text":"<p>Internal Links (10):</p><ul> <li>Line 10: <code>[CLI](../reference/cli.md)</code> \u26a0\ufe0f</li> <li>Line 21: <code>[</code>plan<code>command](../reference/cli.md#plan)</code> \u26a0\ufe0f</li> <li>Line 25: <code>[plans](./plans.md)</code></li> <li>Line 28: <code>[</code>plan<code>](../reference/cli.md#plan)</code> \u26a0\ufe0f</li> <li>Line 28: <code>[</code>apply<code>](./plans.md#plan-application)</code> \u26a0\ufe0f</li> <li>Line 52: <code>[Tests](./tests.md)</code> \u26a0\ufe0f</li> <li>Line 55: <code>[</code>test<code>command](../reference/cli.md#test)</code> \u26a0\ufe0f</li> <li>Line 57: <code>[Audits](./audits.md)</code> \u26a0\ufe0f</li> <li>Line 62: <code>[macros](./macros/overview.md)</code></li> <li>Line 66: <code>[</code>audit<code>command](../reference/cli.md#audit)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-plans","title":"concepts-old/plans.md","text":"<p>Internal Links (50):</p><ul> <li>Line 3: <code>[environment](environments.md)</code></li> <li>Line 107: <code>[categorize changes](#change-categories)</code> \u26a0\ufe0f</li> <li>Line 107: <code>[configuration](../reference/configuration.md#plan)</code> \u26a0\ufe0f</li> <li>Line 191: <code>[backfilling](#backfilling)</code></li> <li>Line 206: <code>[Breaking](#breaking-change)</code></li> <li>Line 206: <code>[Direct](glossary.md#direct-modification)</code></li> <li>Line 206: <code>[Indirect](glossary.md#indirect-modification)</code></li> <li>Line 206: <code>[Backfill](glossary.md#backfill)</code></li> <li>Line 207: <code>[Non-breaking](#non-breaking-change)</code></li> <li>Line 207: <code>[Direct](glossary.md#direct-modification)</code></li> <li>Line 207: <code>[Backfill](glossary.md#backfill)</code></li> <li>Line 208: <code>[Non-breaking](#non-breaking-change)</code></li> <li>Line 208: <code>[Indirect](glossary.md#indirect-modification)</code></li> <li>Line 208: <code>[No Backfill](glossary.md#backfill)</code></li> <li>Line 217: <code>[Forward-only Plans](#forward-only-plans)</code></li> <li>Line 219: <code>[forward-only plan](#forward-only-plans)</code></li> <li>Line 222: <code>[environment](environments.md)</code></li> <li>Line 224: <code>[snapshot](architecture/snapshots.md)</code></li> <li>Line 224: <code>[fingerprint](architecture/snapshots.md#fingerprinting)</code></li> <li>Line 224: <code>[forward-only](#forward-only-plans)</code></li> <li>Line 286: <code>[Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)</code> \u26a0\ufe0f</li> <li>Line 286: <code>[Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)</code> \u26a0\ufe0f</li> <li>Line 292: <code>[Virtual Update](#virtual-update)</code></li> <li>Line 309: <code>[forward-only plan](#forward-only-plans)</code></li> <li>Line 322: <code>[the model definition](./models/overview.md#start)</code> \u26a0\ufe0f</li> <li>Line 322: <code>[project configuration's</code>model_defaults<code>](../guides/configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 324: <code>[restatement plans](#restatement-plans)</code></li> <li>Line 336: <code>[INCREMENTAL_BY_UNIQUE_KEY](models/model_kinds.md#incremental_by_unique_key)</code> \u26a0\ufe0f</li> <li>Line 337: <code>[INCREMENTAL_BY_PARTITION](models/model_kinds.md#incremental_by_partition)</code> \u26a0\ufe0f</li> <li>Line 338: <code>[SCD_TYPE_2_BY_TIME](models/model_kinds.md#scd-type-2-by-time-recommended)</code> \u26a0\ufe0f</li> <li>Line 339: <code>[SCD_TYPE_2_BY_COLUMN](models/model_kinds.md#scd-type-2-by-column)</code> \u26a0\ufe0f</li> <li>Line 522: <code>[forward-only changes](#forward-only-change)</code></li> <li>Line 540: <code>[restatements](#restatement-plans)</code></li> <li>Line 554: <code>[breaking and non-breaking changes](#change-categories)</code></li> <li>Line 562: <code>[forward-only](models/overview.md#forward_only)</code> \u26a0\ufe0f</li> <li>Line 564: <code>[effective date](#effective-date)</code></li> <li>Line 568: <code>[forward-only models](../guides/incremental_time.md#forward-only-models)</code> \u26a0\ufe0f</li> <li>Line 568: <code>[here](../guides/incremental_time.md#destructive-changes)</code> \u26a0\ufe0f</li> <li>Line 574: <code>[model's</code>on_destructive_change<code>value](../guides/incremental_time.md#schema-changes)</code> \u26a0\ufe0f</li> <li>Line 574: <code>[model defaults](../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 575: <code>[model's</code>on_additive_change<code>value](../guides/incremental_time.md#schema-changes)</code> \u26a0\ufe0f</li> <li>Line 575: <code>[model defaults](../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 590: <code>[here](../guides/model_selection.md)</code></li> <li>Line 608: <code>[forward-only plan](#forward-only-plans)</code></li> <li>Line 613: <code>[selector](../guides/model_selection.md)</code></li> <li>Line 613: <code>[below](#restatement-examples)</code></li> <li>Line 623: <code>[external model](./models/external_models.md)</code> \u26a0\ufe0f</li> <li>Line 625: <code>[below](#model-kind-limitations)</code></li> <li>Line 633: <code>[disable_restatement](models/overview.md#disable_restatement)</code> \u26a0\ufe0f</li> <li>Line 671: <code>[development environment](./environments.md#how-to-use-environments)</code></li> </ul><p>Images (1):</p><ul> <li>Line 286: <code>![Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-state","title":"concepts-old/state.md","text":"<p>Internal Links (10):</p><ul> <li>Line 7: <code>[Model Version](./models/overview.md)</code> \u26a0\ufe0f</li> <li>Line 8: <code>[Virtual Data Environment](./environments.md)</code></li> <li>Line 9: <code>[promoted](./plans.md#plan-application)</code></li> <li>Line 9: <code>[Virtual Data Environment](./environments.md)</code></li> <li>Line 10: <code>[auto restatements](./models/overview.md#auto_restatement_cron)</code> \u26a0\ufe0f</li> <li>Line 13: <code>[incremental models](./models/model_kinds.md#incremental_by_time_range)</code> \u26a0\ufe0f</li> <li>Line 19: <code>[PostgreSQL](../integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 23: <code>[configuration guide](../guides/configuration.md#state-connection)</code> \u26a0\ufe0f</li> <li>Line 173: <code>[multiple gateways](../guides/configuration.md#gateways)</code> \u26a0\ufe0f</li> <li>Line 173: <code>[state_connection](../guides/configuration.md#state-connection)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldarchitecture","title":"concepts-old/architecture/","text":""},{"location":"FILE_REFERENCES/#concepts-old-architecture-serialization","title":"concepts-old/architecture/serialization.md","text":"<p>Internal Links (4):</p><ul> <li>Line 3: <code>[macros](../macros/overview.md)</code> \u26a0\ufe0f</li> <li>Line 3: <code>[Python models](../../concepts/models/python_models.md)</code> \u26a0\ufe0f</li> <li>Line 3: <code>[snapshot](../architecture/snapshots.md)</code> \u26a0\ufe0f</li> <li>Line 9: <code>[snapshot fingerprinting](../architecture/snapshots.md#fingerprinting)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-architecture-snapshots","title":"concepts-old/architecture/snapshots.md","text":"<p>Internal Links (1):</p><ul> <li>Line 10: <code>[change categories](../plans.md#change-categories)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-oldmacros","title":"concepts-old/macros/","text":""},{"location":"FILE_REFERENCES/#concepts-old-macros-jinja_macros","title":"concepts-old/macros/jinja_macros.md","text":"<p>Internal Links (10):</p><ul> <li>Line 55: <code>[predefined macro variables](./macro_variables.md)</code></li> <li>Line 57: <code>[</code>runtime_stage<code>](./macro_variables.md#runtime-variables)</code></li> <li>Line 57: <code>[</code>this_model<code>](./macro_variables.md#runtime-variables)</code></li> <li>Line 59: <code>[temporal](./macro_variables.md#temporal-variables)</code></li> <li>Line 87: <code>[Vulcan macros documentation](./vulcan_macros.md#global-variables)</code></li> <li>Line 119: <code>[Vulcan macros documentation](./vulcan_macros.md#gateway-variables)</code></li> <li>Line 121: <code>[global variables](#global-variables)</code></li> <li>Line 127: <code>[creating model templates](../models/sql_models.md)</code> \u26a0\ufe0f</li> <li>Line 324: <code>[Vulcan](./vulcan_macros.md)</code></li> <li>Line 326: <code>[Predefined Vulcan macro variables](./macro_variables.md)</code></li> </ul><p>External Links (2):</p><ul> <li>Line 3: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> <li>Line 178: <code>[Python methods](https://jinja.palletsprojects.com/en/3.1.x/templates/#python-methods)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-macros-macro_variables","title":"concepts-old/macros/macro_variables.md","text":"<p>Internal Links (15):</p><ul> <li>Line 9: <code>[Vulcan macros page](./vulcan_macros.md#user-defined-variables)</code></li> <li>Line 45: <code>[Vulcan macros](./vulcan_macros.md#user-defined-variables)</code></li> <li>Line 45: <code>[Jinja macros](./jinja_macros.md#user-defined-variables)</code></li> <li>Line 50: <code>[other predefined variables](#runtime-variables)</code></li> <li>Line 60: <code>[here](../models/model_kinds.md#timezones)</code> \u26a0\ufe0f</li> <li>Line 131: <code>[here](../models/sql_models.md#optional-prepost-statements)</code> \u26a0\ufe0f</li> <li>Line 139: <code>[gateway](../../guides/connections.md)</code> \u26a0\ufe0f</li> <li>Line 140: <code>[generic audits](../audits.md#generic-audits)</code> \u26a0\ufe0f</li> <li>Line 140: <code>[on_virtual_update statements](../models/sql_models.md#optional-on-virtual-update-statements)</code> \u26a0\ufe0f</li> <li>Line 141: <code>[physical properties in model defaults](../../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 153: <code>[Vulcan macros documentation](./vulcan_macros.md#embedding-variables-in-strings)</code></li> <li>Line 157: <code>[</code>before_all<code>and</code>after_all<code>statements](../../guides/configuration.md#before_all-and-after_all-statements)</code> \u26a0\ufe0f</li> <li>Line 159: <code>[environment](../environments.md)</code></li> <li>Line 160: <code>[virtual layer](../../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 161: <code>[virtual layer](../../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 54: <code>[datetime module](https://docs.python.org/3/library/datetime.html)</code></li> <li>Line 54: <code>[Unix epoch](https://en.wikipedia.org/wiki/Unix_time)</code></li> <li>Line 58: <code>[UTC time zone](https://en.wikipedia.org/wiki/Coordinated_Universal_Time)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-macros-overview","title":"concepts-old/macros/overview.md","text":"<p>Internal Links (3):</p><ul> <li>Line 11: <code>[Pre-defined macro variables](./macro_variables.md)</code></li> <li>Line 12: <code>[Vulcan macros](./vulcan_macros.md)</code></li> <li>Line 13: <code>[Jinja macros](./jinja_macros.md)</code></li> </ul><p>External Links (2):</p><ul> <li>Line 3: <code>[declarative language](https://en.wikipedia.org/wiki/Declarative_programming)</code></li> <li>Line 7: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> </ul>"},{"location":"FILE_REFERENCES/#concepts-old-macros-vulcan_macros","title":"concepts-old/macros/vulcan_macros.md","text":"<p>Internal Links (46):</p><ul> <li>Line 31: <code>[user-defined macro variables](#user-defined-variables)</code></li> <li>Line 32: <code>[Vulcan pre-defined](./macro_variables.md)</code></li> <li>Line 32: <code>[user-defined local](#local-variables)</code></li> <li>Line 32: <code>[user-defined global](#global-variables)</code></li> <li>Line 33: <code>[Vulcan's](#macro-operators)</code></li> <li>Line 33: <code>[user-defined](#user-defined-macro-functions)</code></li> <li>Line 53: <code>[gateway variable](#gateway-variables)</code></li> <li>Line 97: <code>[global](#global-variables)</code></li> <li>Line 97: <code>[gateway](#gateway-variables)</code></li> <li>Line 97: <code>[blueprint](#blueprint-variables)</code></li> <li>Line 97: <code>[local](#local-variables)</code></li> <li>Line 105: <code>[</code>variables<code>key](../../reference/configuration.md#variables)</code> \u26a0\ufe0f</li> <li>Line 171: <code>[Python macro functions](#accessing-global-variable-values)</code> \u26a0\ufe0f</li> <li>Line 171: <code>[Python models](../models/python_models.md#user-defined-variables)</code> \u26a0\ufe0f</li> <li>Line 204: <code>[global variables](#global-variables)</code></li> <li>Line 210: <code>[global](#global-variables)</code></li> <li>Line 210: <code>[gateway-specific](#gateway-variables)</code></li> <li>Line 212: <code>[creating model templates](../models/sql_models.md)</code> \u26a0\ufe0f</li> <li>Line 247: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 253: <code>[global](#global-variables)</code></li> <li>Line 253: <code>[blueprint](#blueprint-variables)</code></li> <li>Line 253: <code>[gateway-specific](#gateway-variables)</code></li> <li>Line 267: <code>[Vulcan quickstart guide](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 529: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 603: <code>[Macro rendering](#vulcan-macro-approach)</code></li> <li>Line 617: <code>[runtime stage](./macro_variables.md#predefined-variables)</code></li> <li>Line 648: <code>[</code>@IF<code>operator](#if)</code></li> <li>Line 674: <code>[</code>@EACH<code>](#each)</code></li> <li>Line 674: <code>[</code>@REDUCE<code>](#reduce)</code></li> <li>Line 678: <code>[</code>@IF<code>](#if)</code></li> <li>Line 755: <code>[below](#positional-and-keyword-arguments)</code></li> <li>Line 1054: <code>[below](#positional-and-keyword-arguments)</code></li> <li>Line 1155: <code>[runtime stages](./macro_variables.md#runtime-variables)</code></li> <li>Line 1168: <code>[above](#embedding-variables-in-strings)</code></li> <li>Line 1245: <code>[User-defined Variables](#user-defined-variables)</code></li> <li>Line 1477: <code>[Jinja templating system](./jinja_macros.md#user-defined-macro-functions)</code></li> <li>Line 1494: <code>[argument type annotations are provided](#argument-data-types)</code></li> <li>Line 1498: <code>[return a list of strings or expressions](#returning-more-than-one-value)</code></li> <li>Line 1550: <code>[below](#typed-macros)</code></li> <li>Line 1552: <code>[mentioned above](#inputs-and-outputs)</code></li> <li>Line 1619: <code>[previous section](#positional-and-keyword-arguments)</code></li> <li>Line 1692: <code>[Pre-defined variables](./macro_variables.md#predefined-variables)</code></li> <li>Line 1692: <code>[user-defined local variables](#local-variables)</code></li> <li>Line 1748: <code>[User-defined global variables](#global-variables)</code></li> <li>Line 1833: <code>[external model](../models/external_models.md)</code> \u26a0\ufe0f</li> <li>Line 2122: <code>[Jinja](./jinja_macros.md)</code></li> </ul><p>External Links (23):</p><ul> <li>Line 5: <code>[Jinja](https://jinja.palletsprojects.com/en/3.1.x/)</code></li> <li>Line 13: <code>[sqlglot](https://github.com/tobymao/sqlglot)</code></li> <li>Line 549: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li> <li>Line 678: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li> <li>Line 819: <code>[</code>MD5<code>](https://en.wikipedia.org/wiki/MD5)</code></li> <li>Line 825: <code>[</code>MD5()<code>hash function](https://en.wikipedia.org/wiki/MD5)</code></li> <li>Line 1008: <code>[haversine distance](https://en.wikipedia.org/wiki/Haversine_formula)</code></li> <li>Line 1113: <code>[</code>date_spine<code>](https://github.com/dbt-labs/dbt-utils?tab=readme-ov-file#date_spine-source)</code></li> <li>Line 1153: <code>[</code></li></ul> $properties<code>](https://trino.io/docs/current/connector/iceberg.html#metadata-tables)</code><li>Line 1241: <code>[SQLGlot's](https://github.com/tobymao/sqlglot)</code></li><li>Line 1345: <code>[common table expressions](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression)</code></li><li>Line 1648: <code>[</code>Literal<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)</code></li><li>Line 1649: <code>[</code>Literal<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)</code></li><li>Line 1650: <code>[</code>Column<code>expressions](https://sqlglot.com/sqlglot/expressions.html#Column)</code></li><li>Line 1652: <code>[</code>Array<code>expression](https://sqlglot.com/sqlglot/expressions.html#Array)</code></li><li>Line 1880: <code>[DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)</code></li><li>Line 1904: <code>[SQLGlot](https://github.com/tobymao/sqlglot)</code></li><li>Line 1934: <code>[SQLGLot expression](https://github.com/tobymao/sqlglot/blob/main/sqlglot/expressions.py)</code></li><li>Line 1944: <code>[Column expression](https://sqlglot.com/sqlglot/expressions.html#Column)</code></li><li>Line 1946: <code>[Condition class](https://sqlglot.com/sqlglot/expressions.html#Condition)</code></li><li>Line 1946: <code>[</code>between<code>](https://sqlglot.com/sqlglot/expressions.html#Condition.between)</code></li><li>Line 1946: <code>[</code>like<code>](https://sqlglot.com/sqlglot/expressions.html#Condition.like)</code></li><li>Line 2114: <code>[here](https://github.com/TobikoData/vulcan/blob/main/tests/core/test_macros.py)</code></li>"},{"location":"FILE_REFERENCES/#configurations","title":"configurations/","text":""},{"location":"FILE_REFERENCES/#configurations-overview","title":"configurations/overview.md","text":"<p>Internal Links (14):</p><ul> <li>Line 98: <code>[Configuration Reference](./configuration.md#gateways)</code> \u26a0\ufe0f</li> <li>Line 112: <code>[Model Defaults](./model_defaults.md)</code> \u26a0\ufe0f</li> <li>Line 118: <code>[Variables](./variables.md)</code> \u26a0\ufe0f</li> <li>Line 124: <code>[Execution Hooks](./hooks.md)</code> \u26a0\ufe0f</li> <li>Line 130: <code>[Linter](./linter.md)</code> \u26a0\ufe0f</li> <li>Line 136: <code>[Notifications](./notifications.md)</code> \u26a0\ufe0f</li> <li>Line 142: <code>[PostgreSQL](./integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 143: <code>[Snowflake](./integrations/engines/snowflake.md)</code> \u26a0\ufe0f</li> <li>Line 149: <code>[Configuration Reference](./configuration.md)</code> \u26a0\ufe0f</li> <li>Line 150: <code>[Variables](./variables.md)</code> \u26a0\ufe0f</li> <li>Line 151: <code>[Model Defaults](./model_defaults.md)</code> \u26a0\ufe0f</li> <li>Line 152: <code>[Execution Hooks](./hooks.md)</code> \u26a0\ufe0f</li> <li>Line 153: <code>[Linter](./linter.md)</code> \u26a0\ufe0f</li> <li>Line 154: <code>[Notifications](./notifications.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old","title":"configurations-old/","text":""},{"location":"FILE_REFERENCES/#configurations-old-configuration","title":"configurations-old/configuration.md","text":"<p>Internal Links (36):</p><ul> <li>Line 3: <code>[configuration guide](../guides/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 5: <code>[model configuration reference page](./model_configuration.md)</code> \u26a0\ufe0f</li> <li>Line 11: <code>[</code>gateways<code>](#gateways)</code></li> <li>Line 11: <code>[gateway/connection defaults](#gatewayconnection-defaults)</code></li> <li>Line 28: <code>[physical layer](../concepts/glossary.md#physical-layer)</code> \u26a0\ufe0f</li> <li>Line 34: <code>[will be placed](../guides/configuration.md#physical-table-schemas)</code> \u26a0\ufe0f</li> <li>Line 35: <code>[additional details](../guides/configuration.md#physical-table-naming-convention)</code> \u26a0\ufe0f</li> <li>Line 39: <code>[virtual layer](../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 46: <code>[additional details](../guides/configuration.md#disable-environment-specific-schemas)</code> \u26a0\ufe0f</li> <li>Line 57: <code>[properties](./model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 61: <code>[model configuration reference page](./model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 65: <code>[</code>@VAR<code>macro function](../concepts/macros/vulcan_macros.md#global-variables)</code> \u26a0\ufe0f</li> <li>Line 65: <code>[</code>context.var<code>method](../concepts/models/python_models.md#user-defined-variables)</code> \u26a0\ufe0f</li> <li>Line 65: <code>[</code>evaluator.var<code>method](../concepts/macros/vulcan_macros.md#accessing-global-variable-values)</code> \u26a0\ufe0f</li> <li>Line 67: <code>[Vulcan macros concepts page](../concepts/macros/vulcan_macros.md#global-variables)</code> \u26a0\ufe0f</li> <li>Line 77: <code>[the configuration guide](../guides/configuration.md#before_all-and-after_all-statements)</code> \u26a0\ufe0f</li> <li>Line 90: <code>[categorize](../concepts/plans.md#change-categories)</code> \u26a0\ufe0f</li> <li>Line 90: <code>[additional details](../guides/configuration.md#auto-categorize-model-changes)</code> \u26a0\ufe0f</li> <li>Line 93: <code>[forward-only](../concepts/plans.md#forward-only-plans)</code> \u26a0\ufe0f</li> <li>Line 94: <code>[data preview](../concepts/plans.md#data-preview-for-forward-only-changes)</code> \u26a0\ufe0f</li> <li>Line 101: <code>[builtin](#builtin)</code></li> <li>Line 138: <code>[gateway defaults](#gatewayconnection-defaults)</code></li> <li>Line 158: <code>[gateways section](../guides/configuration.md#gateways)</code> \u26a0\ufe0f</li> <li>Line 170: <code>[</code>default_connection<code>](#default-connectionsscheduler)</code></li> <li>Line 178: <code>[connection configuration](#connection)</code></li> <li>Line 178: <code>[</code>default_connection<code>](#default-connectionsscheduler)</code></li> <li>Line 179: <code>[connection configuration](#connection)</code></li> <li>Line 181: <code>[connection configuration](#connection)</code></li> <li>Line 182: <code>[scheduler configuration](#scheduler)</code></li> <li>Line 183: <code>[variables](#variables)</code></li> <li>Line 189: <code>[below](#engine-specific)</code></li> <li>Line 205: <code>[Postgres](./integrations/engines/postgres.md)</code></li> <li>Line 206: <code>[Snowflake](./integrations/engines/snowflake.md)</code></li> <li>Line 210: <code>[plans](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 214: <code>[configuration overview scheduler section](../guides/configuration.md#scheduler)</code> \u26a0\ufe0f</li> <li>Line 224: <code>[gateway/connection defaults section](../guides/configuration.md#gatewayconnection-defaults)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 32: <code>[relative dates](https://dateparser.readthedocs.io/en/latest/)</code></li> <li>Line 43: <code>[relative dates](https://dateparser.readthedocs.io/en/latest/)</code></li> <li>Line 55: <code>[python format codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-oldintegrationsengines","title":"configurations-old/integrations/engines/","text":""},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-athena","title":"configurations-old/integrations/engines/athena.md","text":"<p>Internal Links (7):</p><ul> <li>Line 35: <code>[S3 Locations](#s3-locations)</code></li> <li>Line 39: <code>[properties](../../concepts/models/overview.md#model-properties)</code> \u26a0\ufe0f</li> <li>Line 46: <code>[physical_properties](../../concepts/models/overview.md#physical_properties)</code> \u26a0\ufe0f</li> <li>Line 67: <code>[forward only changes](../../concepts/plans.md#forward-only-change)</code> \u26a0\ufe0f</li> <li>Line 69: <code>[</code>INCREMENTAL_BY_UNIQUE_KEY<code>](../../concepts/models/model_kinds.md#incremental_by_unique_key)</code> \u26a0\ufe0f</li> <li>Line 69: <code>[</code>SCD_TYPE_2<code>](../../concepts/models/model_kinds.md#scd-type-2)</code> \u26a0\ufe0f</li> <li>Line 71: <code>[properties](../../concepts/models/overview.md#model-properties)</code> \u26a0\ufe0f</li> </ul><p>External Links (8):</p><ul> <li>Line 13: <code>[PyAthena](https://github.com/laughingman7743/PyAthena)</code></li> <li>Line 14: <code>[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)</code></li> <li>Line 14: <code>[boto3 environment variables](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)</code></li> <li>Line 24: <code>[workgroup](https://docs.aws.amazon.com/athena/latest/ug/workgroups-manage-queries-control-costs.html)</code></li> <li>Line 43: <code>[table_type](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html#ctas-table-properties)</code></li> <li>Line 44: <code>[STORED AS](https://docs.aws.amazon.com/athena/latest/ug/create-table.html#parameters)</code></li> <li>Line 44: <code>[format](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html#ctas-table-properties)</code></li> <li>Line 69: <code>[Apache Iceberg](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-azuresql","title":"configurations-old/integrations/engines/azuresql.md","text":"<p>External Links (2):</p><ul> <li>Line 3: <code>[Azure SQL](https://azure.microsoft.com/en-us/products/azure-sql)</code></li> <li>Line 36: <code>[here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-bigquery","title":"configurations-old/integrations/engines/bigquery.md","text":"<p>Internal Links (17):</p><ul> <li>Line 7: <code>[quickstart project](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 20: <code>[quickstart guide](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 72: <code>[</code>oauth<code>authentication method](#authentication-methods)</code></li> <li>Line 72: <code>[described below](#authentication-methods)</code></li> <li>Line 76: <code>[BigQuery Dashboard](./bigquery/bigquery-1.png)</code></li> <li>Line 76: <code>[BigQuery Dashboard](./bigquery/bigquery-1.png)</code></li> <li>Line 80: <code>[BigQuery Dashboard: selecting your project](./bigquery/bigquery-2.png)</code></li> <li>Line 80: <code>[BigQuery Dashboard: selecting your project](./bigquery/bigquery-2.png)</code></li> <li>Line 96: <code>[Terminal Output](./bigquery/bigquery-3.png)</code></li> <li>Line 96: <code>[Terminal Output](./bigquery/bigquery-3.png)</code></li> <li>Line 102: <code>[Terminal Output with warnings](./bigquery/bigquery-4.png)</code></li> <li>Line 102: <code>[Terminal Output with warnings](./bigquery/bigquery-4.png)</code></li> <li>Line 126: <code>[Steps to the Studio](./bigquery/bigquery-5.png)</code></li> <li>Line 126: <code>[Steps to the Studio](./bigquery/bigquery-5.png)</code></li> <li>Line 130: <code>[New Models](./bigquery/bigquery-6.png)</code></li> <li>Line 130: <code>[New Models](./bigquery/bigquery-6.png)</code></li> <li>Line 148: <code>[allowed values below](#authentication-methods)</code></li> </ul><p>External Links (14):</p><ul> <li>Line 14: <code>[CLI/API access is enabled](https://cloud.google.com/endpoints/docs/openapi/enable-api)</code></li> <li>Line 15: <code>[billing is configured](https://cloud.google.com/billing/docs/how-to/manage-billing-account)</code></li> <li>Line 30: <code>[</code>google-cloud-bigquery<code>library](https://pypi.org/project/google-cloud-bigquery/)</code></li> <li>Line 30: <code>[Google Cloud SDK</code>gcloud<code>tool](https://cloud.google.com/sdk/docs)</code></li> <li>Line 30: <code>[authenticating with BigQuery](https://googleapis.dev/python/google-api-core/latest/auth.html)</code></li> <li>Line 34: <code>[Google Cloud installation guide](https://cloud.google.com/sdk/docs/install)</code></li> <li>Line 53: <code>[</code>gcloud init<code>to setup authentication](https://cloud.google.com/sdk/gcloud/reference/init)</code></li> <li>Line 169: <code>[oauth](https://google-auth.readthedocs.io/en/master/reference/google.auth.html#google.auth.default)</code></li> <li>Line 172: <code>[oauth-secrets](https://google-auth.readthedocs.io/en/stable/reference/google.oauth2.credentials.html)</code></li> <li>Line 180: <code>[service-account](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html#google.oauth2.service_account.IDTokenCredentials.from_service_account_file)</code></li> <li>Line 184: <code>[service-account-json](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html#google.oauth2.service_account.IDTokenCredentials.from_service_account_info)</code></li> <li>Line 194: <code>[sufficient permissions to impersonate the service account](https://cloud.google.com/docs/authentication/use-service-account-impersonation)</code></li> <li>Line 199: <code>[</code>BigQuery Data Owner<code>](https://cloud.google.com/bigquery/docs/access-control#bigquery.dataOwner)</code></li> <li>Line 200: <code>[</code>BigQuery User<code>](https://cloud.google.com/bigquery/docs/access-control#bigquery.user)</code></li> </ul><p>Images (6):</p><ul> <li>Line 76: <code>![BigQuery Dashboard](./bigquery/bigquery-1.png)</code></li> <li>Line 80: <code>![BigQuery Dashboard: selecting your project](./bigquery/bigquery-2.png)</code></li> <li>Line 96: <code>![Terminal Output](./bigquery/bigquery-3.png)</code></li> <li>Line 102: <code>![Terminal Output with warnings](./bigquery/bigquery-4.png)</code></li> <li>Line 126: <code>![Steps to the Studio](./bigquery/bigquery-5.png)</code></li> <li>Line 130: <code>![New Models](./bigquery/bigquery-6.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-clickhouse","title":"configurations-old/integrations/engines/clickhouse.md","text":"<p>Internal Links (17):</p><ul> <li>Line 6: <code>[state connection](../../reference/configuration.md#connections)</code> \u26a0\ufe0f</li> <li>Line 52: <code>[below](#cluster-specification)</code></li> <li>Line 70: <code>[</code>environment_suffix_target<code>key in your project configuration](../../guides/configuration.md#disable-environment-specific-schemas)</code> \u26a0\ufe0f</li> <li>Line 78: <code>[</code>physical_schema_mapping<code>key in your project configuration](../../guides/configuration.md#physical-table-schemas)</code> \u26a0\ufe0f</li> <li>Line 97: <code>[below](#localbuilt-in-scheduler)</code></li> <li>Line 228: <code>[partitioned tables to improve performance](#performance-considerations)</code></li> <li>Line 260: <code>[</code>order_by<code>](#order-by)</code></li> <li>Line 260: <code>[</code>primary_key<code>](#primary-key)</code></li> <li>Line 341: <code>[</code>FULL<code>models](../../concepts/models/model_kinds.md#full)</code> \u26a0\ufe0f</li> <li>Line 342: <code>[</code>INCREMENTAL_BY_TIME_RANGE<code>models](../../concepts/models/model_kinds.md#incremental_by_time_range)</code> \u26a0\ufe0f</li> <li>Line 343: <code>[</code>INCREMENTAL_BY_UNIQUE_KEY<code>models](../../concepts/models/model_kinds.md#incremental_by_unique_key)</code> \u26a0\ufe0f</li> <li>Line 344: <code>[</code>INCREMENTAL_BY_PARTITION<code>models](../../concepts/models/model_kinds.md#incremental_by_partition)</code> \u26a0\ufe0f</li> <li>Line 358: <code>[for partitioned tables](#partition-swap)</code></li> <li>Line 370: <code>[ClickHouse table swap steps](./clickhouse/clickhouse_table-swap-steps.png)</code></li> <li>Line 370: <code>[ClickHouse table swap steps](./clickhouse/clickhouse_table-swap-steps.png)</code></li> <li>Line 390: <code>[</code>partitioned_by<code>](../../concepts/models/overview.md#partitioned_by)</code> \u26a0\ufe0f</li> <li>Line 414: <code>[</code>ORDER_BY<code>expression](#order-by)</code></li> </ul><p>External Links (14):</p><ul> <li>Line 10: <code>[ClickHouse](https://clickhouse.com/)</code></li> <li>Line 26: <code>[\"table engine\" that controls how the table's data is stored and queried](https://clickhouse.com/docs/en/engines/table-engines)</code></li> <li>Line 28: <code>[</code>MergeTree<code>engine family](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree)</code></li> <li>Line 44: <code>[ClickHouse Keeper](https://clickhouse.com/docs/en/architecture/horizontal-scaling)</code></li> <li>Line 44: <code>[Apache ZooKeeper](https://zookeeper.apache.org/)</code></li> <li>Line 56: <code>[ClickHouse Cloud](https://clickhouse.com/cloud)</code></li> <li>Line 58: <code>[occur in two steps on ClickHouse Cloud](https://clickhouse.com/docs/en/sql-reference/statements/create/table#from-select-query)</code></li> <li>Line 188: <code>[TTL expression that triggers actions](https://clickhouse.com/docs/en/guides/developer/ttl)</code></li> <li>Line 232: <code>[immense number of settings](https://clickhouse.com/docs/en/operations/settings)</code></li> <li>Line 238: <code>[</code>clickhouse-connect<code>library](https://clickhouse.com/docs/en/integrations/python)</code></li> <li>Line 260: <code>[</code>physical_properties<code>key](https://vulcan.readthedocs.io/en/stable/concepts/models/overview/?h=physical#physical_properties)</code></li> <li>Line 400: <code>[specifically warns against tables having too many partitions](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/custom-partitioning-key)</code></li> <li>Line 448: <code>[connection settings](https://clickhouse.com/docs/integrations/python#settings-argument)</code></li> <li>Line 449: <code>[options](https://clickhouse.com/docs/integrations/python#customizing-the-http-connection-pool)</code></li> </ul><p>Images (1):</p><ul> <li>Line 370: <code>![ClickHouse table swap steps](./clickhouse/clickhouse_table-swap-steps.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-databricks","title":"configurations-old/integrations/engines/databricks.md","text":"<p>Internal Links (56):</p><ul> <li>Line 5: <code>[Connection Quickstart](#connection-quickstart)</code></li> <li>Line 5: <code>[built-in](#localbuilt-in-scheduler)</code></li> <li>Line 19: <code>[Databricks Connect](#databricks-connect)</code></li> <li>Line 27: <code>[more configuration details below](#databricks-connect)</code></li> <li>Line 33: <code>[more configuration details below](#databricks-notebook-interface)</code></li> <li>Line 44: <code>[Vulcan Quickstart](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 70: <code>[next section](#get-jdbcodbc-info)</code></li> <li>Line 74: <code>[Databricks Workspace default view](./databricks/db-guide_workspace.png)</code></li> <li>Line 74: <code>[Databricks Workspace default view](./databricks/db-guide_workspace.png)</code></li> <li>Line 78: <code>[Databricks Compute default view](./databricks/db-guide_compute.png)</code></li> <li>Line 78: <code>[Databricks Compute default view](./databricks/db-guide_compute.png)</code></li> <li>Line 82: <code>[Databricks Create Compute view](./databricks/db-guide_compute-create.png)</code></li> <li>Line 82: <code>[Databricks Create Compute view](./databricks/db-guide_compute-create.png)</code></li> <li>Line 88: <code>[Databricks Compute Advanced Options link](./databricks/db-guide_compute-advanced-options-link.png)</code></li> <li>Line 88: <code>[Databricks Compute Advanced Options link](./databricks/db-guide_compute-advanced-options-link.png)</code></li> <li>Line 92: <code>[Databricks Compute Advanced Options JDBC/ODBC tab](./databricks/db-guide_advanced-options.png)</code></li> <li>Line 92: <code>[Databricks Compute Advanced Options JDBC/ODBC tab](./databricks/db-guide_advanced-options.png)</code></li> <li>Line 96: <code>[Project config.yaml databricks gateway](./databricks/db-guide_config-yaml.png)</code></li> <li>Line 96: <code>[Project config.yaml databricks gateway](./databricks/db-guide_config-yaml.png)</code></li> <li>Line 100: <code>[Copy server_hostname and http_path to config.yaml](./databricks/db-guide_copy-server-http.png)</code></li> <li>Line 100: <code>[Copy server_hostname and http_path to config.yaml](./databricks/db-guide_copy-server-http.png)</code></li> <li>Line 109: <code>[environment variables that the configuration file loads dynamically](../../guides/configuration.md#environment-variables)</code> \u26a0\ufe0f</li> <li>Line 124: <code>[Navigate to profile Settings page](./databricks/db-guide_profile-settings-link.png)</code></li> <li>Line 124: <code>[Navigate to profile Settings page](./databricks/db-guide_profile-settings-link.png)</code></li> <li>Line 128: <code>[Navigate to User Developer view](./databricks/db-guide_profile-settings-developer.png)</code></li> <li>Line 128: <code>[Navigate to User Developer view](./databricks/db-guide_profile-settings-developer.png)</code></li> <li>Line 132: <code>[Navigate to Access Tokens management](./databricks/db-guide_access-tokens-link.png)</code></li> <li>Line 132: <code>[Navigate to Access Tokens management](./databricks/db-guide_access-tokens-link.png)</code></li> <li>Line 136: <code>[Open the token generation menu](./databricks/db-guide_access-tokens-generate-button.png)</code></li> <li>Line 136: <code>[Open the token generation menu](./databricks/db-guide_access-tokens-generate-button.png)</code></li> <li>Line 140: <code>[Generate a new token](./databricks/db-guide_access-tokens-generate.png)</code></li> <li>Line 140: <code>[Generate a new token](./databricks/db-guide_access-tokens-generate.png)</code></li> <li>Line 144: <code>[Copy token to config.yaml access_token key](./databricks/db-guide_copy-token.png)</code></li> <li>Line 144: <code>[Copy token to config.yaml access_token key](./databricks/db-guide_copy-token.png)</code></li> <li>Line 149: <code>[environment variables that the configuration file loads dynamically](../../guides/configuration.md#environment-variables)</code> \u26a0\ufe0f</li> <li>Line 169: <code>[Run vulcan info command in CLI](./databricks/db-guide_sqlmesh-info.png)</code></li> <li>Line 169: <code>[Run vulcan info command in CLI](./databricks/db-guide_sqlmesh-info.png)</code></li> <li>Line 173: <code>[Successful data warehouse connection](./databricks/db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 173: <code>[Successful data warehouse connection](./databricks/db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 177: <code>[Databricks state connection warning](./databricks/db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 177: <code>[Databricks state connection warning](./databricks/db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 182: <code>[here](../../guides/configuration.md#state-connection)</code> \u26a0\ufe0f</li> <li>Line 190: <code>[Specify DuckDB state connection](./databricks/db-guide_state-connection.png)</code></li> <li>Line 190: <code>[Specify DuckDB state connection](./databricks/db-guide_state-connection.png)</code></li> <li>Line 194: <code>[No state connection warning](./databricks/db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 194: <code>[No state connection warning](./databricks/db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 200: <code>[Specify databricks as default gateway](./databricks/db-guide_default-gateway.png)</code></li> <li>Line 200: <code>[Specify databricks as default gateway](./databricks/db-guide_default-gateway.png)</code></li> <li>Line 204: <code>[Run vulcan plan in databricks](./databricks/db-guide_sqlmesh-plan.png)</code></li> <li>Line 204: <code>[Run vulcan plan in databricks](./databricks/db-guide_sqlmesh-plan.png)</code></li> <li>Line 208: <code>[Vulcan plan objects in databricks](./databricks/db-guide_sqlmesh-plan-objects.png)</code></li> <li>Line 208: <code>[Vulcan plan objects in databricks](./databricks/db-guide_sqlmesh-plan-objects.png)</code></li> <li>Line 213: <code>[</code>catalog<code>parameter](#connection-options)</code></li> <li>Line 225: <code>[section above](#databricks-connection-methods)</code></li> <li>Line 229: <code>[more above](#databricks-sql-connector)</code></li> <li>Line 276: <code>[forward only](../../guides/incremental_time.md#forward-only-models)</code> \u26a0\ufe0f</li> </ul><p>External Links (18):</p><ul> <li>Line 13: <code>[Databricks SQL Connector](https://docs.databricks.com/dev-tools/python-sql-connector.html)</code></li> <li>Line 23: <code>[Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html)</code></li> <li>Line 25: <code>[install the version of Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/python/install.html)</code></li> <li>Line 39: <code>[All-Purpose Compute](https://docs.databricks.com/en/compute/index.html)</code></li> <li>Line 51: <code>[personal access tokens](https://docs.databricks.com/en/dev-tools/auth/pat.html)</code></li> <li>Line 51: <code>[Community Edition workspaces do not](https://docs.databricks.com/en/admin/access-control/tokens.html)</code></li> <li>Line 53: <code>[Unity Catalog](https://docs.databricks.com/aws/en/data-governance/unity-catalog/)</code></li> <li>Line 62: <code>[Unity Catalog](https://docs.databricks.com/aws/en/data-governance/unity-catalog/)</code></li> <li>Line 229: <code>[Databricks SQL Connector](https://docs.databricks.com/dev-tools/python-sql-connector.html)</code></li> <li>Line 233: <code>[Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html)</code></li> <li>Line 235: <code>[install the version of Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/python/install.html)</code></li> <li>Line 241: <code>[Databricks SQL Warehouse](https://docs.databricks.com/sql/admin/create-sql-warehouse.html)</code></li> <li>Line 244: <code>[requirements](https://docs.databricks.com/dev-tools/databricks-connect.html#requirements)</code></li> <li>Line 244: <code>[limitations](https://docs.databricks.com/dev-tools/databricks-connect.html#limitations)</code></li> <li>Line 260: <code>[Defaults to use Databricks cluster default](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#the-default-catalog-configuration-when-unity-catalog-is-enabled)</code></li> <li>Line 262: <code>[M2M](https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication)</code></li> <li>Line 263: <code>[M2M](https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication)</code></li> <li>Line 290: <code>[Databricks Documentation for more details](https://docs.databricks.com/en/delta/column-mapping.html#requirements)</code></li> </ul><p>Images (21):</p><ul> <li>Line 74: <code>![Databricks Workspace default view](./databricks/db-guide_workspace.png)</code></li> <li>Line 78: <code>![Databricks Compute default view](./databricks/db-guide_compute.png)</code></li> <li>Line 82: <code>![Databricks Create Compute view](./databricks/db-guide_compute-create.png)</code></li> <li>Line 88: <code>![Databricks Compute Advanced Options link](./databricks/db-guide_compute-advanced-options-link.png)</code></li> <li>Line 92: <code>![Databricks Compute Advanced Options JDBC/ODBC tab](./databricks/db-guide_advanced-options.png)</code></li> <li>Line 96: <code>![Project config.yaml databricks gateway](./databricks/db-guide_config-yaml.png)</code></li> <li>Line 100: <code>![Copy server_hostname and http_path to config.yaml](./databricks/db-guide_copy-server-http.png)</code></li> <li>Line 124: <code>![Navigate to profile Settings page](./databricks/db-guide_profile-settings-link.png)</code></li> <li>Line 128: <code>![Navigate to User Developer view](./databricks/db-guide_profile-settings-developer.png)</code></li> <li>Line 132: <code>![Navigate to Access Tokens management](./databricks/db-guide_access-tokens-link.png)</code></li> <li>Line 136: <code>![Open the token generation menu](./databricks/db-guide_access-tokens-generate-button.png)</code></li> <li>Line 140: <code>![Generate a new token](./databricks/db-guide_access-tokens-generate.png)</code></li> <li>Line 144: <code>![Copy token to config.yaml access_token key](./databricks/db-guide_copy-token.png)</code></li> <li>Line 169: <code>![Run vulcan info command in CLI](./databricks/db-guide_sqlmesh-info.png)</code></li> <li>Line 173: <code>![Successful data warehouse connection](./databricks/db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 177: <code>![Databricks state connection warning](./databricks/db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 190: <code>![Specify DuckDB state connection](./databricks/db-guide_state-connection.png)</code></li> <li>Line 194: <code>![No state connection warning](./databricks/db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 200: <code>![Specify databricks as default gateway](./databricks/db-guide_default-gateway.png)</code></li> <li>Line 204: <code>![Run vulcan plan in databricks](./databricks/db-guide_sqlmesh-plan.png)</code></li> <li>Line 208: <code>![Vulcan plan objects in databricks](./databricks/db-guide_sqlmesh-plan-objects.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-duckdb","title":"configurations-old/integrations/engines/duckdb.md","text":"<p>Internal Links (3):</p><ul> <li>Line 6: <code>[Postgres](./postgres.md)</code></li> <li>Line 17: <code>[attach DuckDB catalogs](#duckdb-catalogs-example)</code></li> <li>Line 17: <code>[catalogs for other connections](#other-connection-catalogs-example)</code></li> </ul><p>External Links (12):</p><ul> <li>Line 4: <code>[single user](https://duckdb.org/docs/connect/concurrency.html#writing-to-duckdb-from-multiple-processes)</code></li> <li>Line 6: <code>[Tobiko Cloud](https://tobikodata.com/product.html)</code></li> <li>Line 119: <code>[DuckDB can be attached to](https://duckdb.org/docs/sql/statements/attach.html)</code></li> <li>Line 189: <code>[See DuckDB Documentation for more information](https://duckdb.org/docs/extensions/postgres#configuring-via-environment-variables)</code></li> <li>Line 193: <code>[httpfs](https://duckdb.org/docs/extensions/httpfs/s3api)</code></li> <li>Line 193: <code>[azure](https://duckdb.org/docs/extensions/azure)</code></li> <li>Line 195: <code>[Secrets Manager](https://duckdb.org/docs/configuration/secrets_manager.html)</code></li> <li>Line 195: <code>[legacy authentication method](https://duckdb.org/docs/stable/extensions/httpfs/s3api_legacy_authentication.html)</code></li> <li>Line 336: <code>[supported S3 secret parameters](https://duckdb.org/docs/stable/extensions/httpfs/s3api.html#overview-of-s3-secret-parameters)</code></li> <li>Line 336: <code>[Secrets Manager configuration](https://duckdb.org/docs/configuration/secrets_manager.html)</code></li> <li>Line 370: <code>[fsspec.filesystem](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.filesystem)</code></li> <li>Line 370: <code>[adlfs.AzureBlobFileSystem](https://fsspec.github.io/adlfs/api/#api-reference)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-fabric","title":"configurations-old/integrations/engines/fabric.md","text":"<p>Internal Links (1):</p><ul> <li>Line 9: <code>[state connection](../../reference/configuration.md#connections)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 37: <code>[here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-motherduck","title":"configurations-old/integrations/engines/motherduck.md","text":"<p>Internal Links (8):</p><ul> <li>Line 5: <code>[Connection Quickstart](#connection-quickstart)</code></li> <li>Line 18: <code>[Vulcan Quickstart](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 54: <code>[DuckDB can be attached to](./duckdb.md#other-connection-catalogs-example)</code></li> <li>Line 57: <code>[environment variables that the configuration file loads dynamically](../../guides/configuration.md#environment-variables)</code> \u26a0\ufe0f</li> <li>Line 75: <code>[](./motherduck/sqlmesh_info.png)</code></li> <li>Line 79: <code>[](./motherduck/info_output.png)</code></li> <li>Line 85: <code>[](./motherduck/sqlmesh_plan.png)</code></li> <li>Line 89: <code>[](./motherduck/motherduck_ui.png)</code></li> </ul><p>Images (4):</p><ul> <li>Line 75: <code>![](./motherduck/sqlmesh_info.png)</code></li> <li>Line 79: <code>![](./motherduck/info_output.png)</code></li> <li>Line 85: <code>![](./motherduck/sqlmesh_plan.png)</code></li> <li>Line 89: <code>![](./motherduck/motherduck_ui.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-mssql","title":"configurations-old/integrations/engines/mssql.md","text":"<p>Internal Links (2):</p><ul> <li>Line 16: <code>[incremental by unique key](../../concepts/models/model_kinds.md#incremental_by_unique_key)</code> \u26a0\ufe0f</li> <li>Line 22: <code>[</code>physical_properties<code>](../../concepts/models/overview.md#physical_properties)</code> \u26a0\ufe0f</li> </ul><p>External Links (2):</p><ul> <li>Line 42: <code>[MSSQL</code>EXCEPT<code>statement documentation](https://learn.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql?view=sql-server-ver17#arguments)</code></li> <li>Line 65: <code>[here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-redshift","title":"configurations-old/integrations/engines/redshift.md","text":"<p>External Links (1):</p><ul> <li>Line 24: <code>[TCP keepalive](https://en.wikipedia.org/wiki/Keepalive#TCP_keepalive)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-risingwave","title":"configurations-old/integrations/engines/risingwave.md","text":"<p>Internal Links (3):</p><ul> <li>Line 20: <code>[Postgres](./postgres.md)</code></li> <li>Line 41: <code>[pre / post statements](../../concepts/models/sql_models.md#optional-prepost-statements)</code> \u26a0\ufe0f</li> <li>Line 74: <code>[here](../../concepts/macros/macro_variables.md#runtime-variables)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 3: <code>[RisingWave](https://risingwave.com/)</code></li> <li>Line 38: <code>[Sources](https://docs.risingwave.com/sql/commands/sql-create-source)</code></li> <li>Line 39: <code>[Sinks](https://docs.risingwave.com/sql/commands/sql-create-sink)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-snowflake","title":"configurations-old/integrations/engines/snowflake.md","text":"<p>Internal Links (23):</p><ul> <li>Line 5: <code>[Connection Quickstart](#connection-quickstart)</code></li> <li>Line 5: <code>[built-in](#localbuilt-in-scheduler)</code></li> <li>Line 13: <code>[described below](#snowflake-authorization-methods)</code></li> <li>Line 18: <code>[Vulcan Quickstart](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 117: <code>[Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code></li> <li>Line 117: <code>[Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code></li> <li>Line 177: <code>[environment variables that the configuration file loads dynamically](../../guides/configuration.md#environment-variables)</code> \u26a0\ufe0f</li> <li>Line 195: <code>[Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code></li> <li>Line 195: <code>[Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code></li> <li>Line 199: <code>[Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 199: <code>[Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 203: <code>[Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 203: <code>[Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 208: <code>[here](../../guides/configuration.md#state-connection)</code> \u26a0\ufe0f</li> <li>Line 239: <code>[No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 239: <code>[No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 245: <code>[Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code></li> <li>Line 245: <code>[Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code></li> <li>Line 249: <code>[Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code></li> <li>Line 249: <code>[Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code></li> <li>Line 482: <code>[Private Key Base64](#private-key-base64)</code></li> <li>Line 594: <code>[model defaults](../../guides/configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 615: <code>[external table](../../concepts/models/external_models.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (9):</p><ul> <li>Line 25: <code>[warehouse](https://docs.snowflake.com/en/user-guide/warehouses-overview)</code></li> <li>Line 259: <code>[on Github](https://github.com/snowflakedb/snowflake-connector-python/issues/645)</code></li> <li>Line 573: <code>[External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume)</code></li> <li>Line 591: <code>[Configuring a default Catalog](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-catalog-integration#set-a-default-catalog-at-the-account-database-or-schema-level)</code></li> <li>Line 592: <code>[Configuring a default External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume#set-a-default-external-volume-at-the-account-database-or-schema-level)</code></li> <li>Line 596: <code>[optional properties](https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table-snowflake#optional-parameters)</code></li> <li>Line 613: <code>[does not support](https://docs.snowflake.com/en/user-guide/tables-iceberg#catalog-options)</code></li> <li>Line 625: <code>[Connection Caching Documentation](https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-use#using-connection-caching-to-minimize-the-number-of-prompts-for-authentication-optional)</code></li> <li>Line 626: <code>[MFA Token Caching Documentation](https://docs.snowflake.com/en/user-guide/security-mfa#using-mfa-token-caching-to-minimize-the-number-of-prompts-during-authentication-optional)</code></li> </ul><p>Images (7):</p><ul> <li>Line 117: <code>![Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code></li> <li>Line 195: <code>![Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code></li> <li>Line 199: <code>![Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 203: <code>![Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 239: <code>![No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 245: <code>![Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code></li> <li>Line 249: <code>![Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-spark","title":"configurations-old/integrations/engines/spark.md","text":"<p>Internal Links (2):</p><ul> <li>Line 6: <code>[state connection](../../reference/configuration.md#connections)</code> \u26a0\ufe0f</li> <li>Line 14: <code>[Catalog Support](#catalog-support)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-old-integrations-engines-trino","title":"configurations-old/integrations/engines/trino.md","text":"<p>Internal Links (5):</p><ul> <li>Line 6: <code>[state connection](../../reference/configuration.md#connections)</code> \u26a0\ufe0f</li> <li>Line 97: <code>[Table and Schema locations](#table-and-schema-locations)</code></li> <li>Line 98: <code>[Catalog Type Overrides](#catalog-type-overrides)</code></li> <li>Line 185: <code>[here](../../concepts/macros/vulcan_macros.md#embedding-variables-in-strings)</code> \u26a0\ufe0f</li> <li>Line 193: <code>[@resolve_template](../../concepts/macros/vulcan_macros.md#resolve_template)</code> \u26a0\ufe0f</li> </ul><p>External Links (23):</p><ul> <li>Line 20: <code>[Hive Connector](https://trino.io/docs/current/connector/hive.html)</code></li> <li>Line 20: <code>[Iceberg Connector](https://trino.io/docs/current/connector/iceberg.html)</code></li> <li>Line 20: <code>[Delta Lake Connector](https://trino.io/docs/current/connector/delta-lake.html)</code></li> <li>Line 22: <code>[Slack](https://tobikodata.com/slack)</code></li> <li>Line 41: <code>[properties](https://trino.io/docs/current/connector/metastores.html#general-metastore-configuration-properties)</code></li> <li>Line 54: <code>[Nessie documentation](https://projectnessie.org/nessie-latest/trino/)</code></li> <li>Line 60: <code>[properties file](https://trino.io/docs/current/connector/delta-lake.html#general-configuration)</code></li> <li>Line 60: <code>[general properties](https://trino.io/docs/current/object-storage/metastores.html#general-metastore-properties)</code></li> <li>Line 69: <code>[AWS Glue](https://aws.amazon.com/glue/)</code></li> <li>Line 71: <code>[AWS S3](https://aws.amazon.com/s3/)</code></li> <li>Line 73: <code>[</code>hive.metastore.glue.default-warehouse-dir<code>parameter](https://trino.io/docs/current/object-storage/metastores.html#aws-glue-catalog-configuration-properties)</code></li> <li>Line 108: <code>[Metastore](https://trino.io/docs/current/object-storage/metastores.html)</code></li> <li>Line 253: <code>[trinodb Python client](https://github.com/trinodb/trino-python-client)</code></li> <li>Line 267: <code>[Trino Documentation on Basic Authentication](https://trino.io/docs/current/security/password-file.html)</code></li> <li>Line 268: <code>[Python Client Basic Authentication](https://github.com/trinodb/trino-python-client#basic-authentication)</code></li> <li>Line 289: <code>[Trino Documentation on LDAP Authentication](https://trino.io/docs/current/security/ldap.html)</code></li> <li>Line 290: <code>[Python Client LDAP Authentication](https://github.com/trinodb/trino-python-client#basic-authentication)</code></li> <li>Line 320: <code>[Trino Documentation on Kerberos Authentication](https://trino.io/docs/current/security/kerberos.html)</code></li> <li>Line 321: <code>[Python Client Kerberos Authentication](https://github.com/trinodb/trino-python-client#kerberos-authentication)</code></li> <li>Line 341: <code>[Trino Documentation on JWT Authentication](https://trino.io/docs/current/security/jwt.html)</code></li> <li>Line 342: <code>[Python Client JWT Authentication](https://github.com/trinodb/trino-python-client#jwt-authentication)</code></li> <li>Line 383: <code>[Trino Documentation on Oauth Authentication](https://trino.io/docs/current/security/oauth2.html)</code></li> <li>Line 384: <code>[Python Client Oauth Authentication](https://github.com/trinodb/trino-python-client#oauth2-authentication)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurationsenginessnowflake","title":"configurations/engines/snowflake/","text":""},{"location":"FILE_REFERENCES/#configurations-engines-snowflake-snowflake","title":"configurations/engines/snowflake/snowflake.md","text":"<p>Internal Links (23):</p><ul> <li>Line 5: <code>[Connection Quickstart](#connection-quickstart)</code></li> <li>Line 5: <code>[built-in](#localbuilt-in-scheduler)</code></li> <li>Line 13: <code>[described below](#snowflake-authorization-methods)</code></li> <li>Line 18: <code>[Vulcan Quickstart](../../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 117: <code>[Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code> \u26a0\ufe0f</li> <li>Line 117: <code>[Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code> \u26a0\ufe0f</li> <li>Line 177: <code>[environment variables that the configuration file loads dynamically](../../guides/configuration.md#environment-variables)</code> \u26a0\ufe0f</li> <li>Line 195: <code>[Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code> \u26a0\ufe0f</li> <li>Line 195: <code>[Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code> \u26a0\ufe0f</li> <li>Line 199: <code>[Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code> \u26a0\ufe0f</li> <li>Line 199: <code>[Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code> \u26a0\ufe0f</li> <li>Line 203: <code>[Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code> \u26a0\ufe0f</li> <li>Line 203: <code>[Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code> \u26a0\ufe0f</li> <li>Line 208: <code>[here](../../guides/configuration.md#state-connection)</code> \u26a0\ufe0f</li> <li>Line 239: <code>[No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code> \u26a0\ufe0f</li> <li>Line 239: <code>[No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code> \u26a0\ufe0f</li> <li>Line 245: <code>[Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code> \u26a0\ufe0f</li> <li>Line 245: <code>[Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code> \u26a0\ufe0f</li> <li>Line 249: <code>[Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code> \u26a0\ufe0f</li> <li>Line 249: <code>[Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code> \u26a0\ufe0f</li> <li>Line 482: <code>[Private Key Base64](#private-key-base64)</code></li> <li>Line 594: <code>[model defaults](../../guides/configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 615: <code>[external table](../../concepts/models/external_models.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (9):</p><ul> <li>Line 25: <code>[warehouse](https://docs.snowflake.com/en/user-guide/warehouses-overview)</code></li> <li>Line 259: <code>[on Github](https://github.com/snowflakedb/snowflake-connector-python/issues/645)</code></li> <li>Line 573: <code>[External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume)</code></li> <li>Line 591: <code>[Configuring a default Catalog](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-catalog-integration#set-a-default-catalog-at-the-account-database-or-schema-level)</code></li> <li>Line 592: <code>[Configuring a default External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume#set-a-default-external-volume-at-the-account-database-or-schema-level)</code></li> <li>Line 596: <code>[optional properties](https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table-snowflake#optional-parameters)</code></li> <li>Line 613: <code>[does not support](https://docs.snowflake.com/en/user-guide/tables-iceberg#catalog-options)</code></li> <li>Line 625: <code>[Connection Caching Documentation](https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-use#using-connection-caching-to-minimize-the-number-of-prompts-for-authentication-optional)</code></li> <li>Line 626: <code>[MFA Token Caching Documentation](https://docs.snowflake.com/en/user-guide/security-mfa#using-mfa-token-caching-to-minimize-the-number-of-prompts-during-authentication-optional)</code></li> </ul><p>Images (7):</p><ul> <li>Line 117: <code>![Snowflake account info in web URL](./snowflake/snowflake_db-guide_account-url.png)</code></li> <li>Line 195: <code>![Run vulcan info command in CLI](./snowflake/snowflake_db-guide_sqlmesh-info.png)</code></li> <li>Line 199: <code>![Successful data warehouse connection](./snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png)</code></li> <li>Line 203: <code>![Snowflake state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-warning.png)</code></li> <li>Line 239: <code>![No state connection warning](./snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png)</code></li> <li>Line 245: <code>![Run vulcan plan in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan.png)</code></li> <li>Line 249: <code>![Vulcan plan objects in snowflake](./snowflake/snowflake_db-guide_sqlmesh-plan-objects.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurationsoptions","title":"configurations/options/","text":""},{"location":"FILE_REFERENCES/#configurations-options-linter","title":"configurations/options/linter.md","text":"<p>Internal Links (2):</p><ul> <li>Line 99: <code>[configuration file](#applying-linting-rules)</code></li> <li>Line 130: <code>[configuration file](./configuration.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 17: <code>[full source code](https://github.com/TobikoData/vulcan/blob/main/vulcan/core/linter/rule.py)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-options-model_defaults","title":"configurations/options/model_defaults.md","text":"<p>Internal Links (2):</p><ul> <li>Line 5: <code>[models configuration reference page](model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 34: <code>[model concepts page](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 3: <code>[supported by the SQLGlot library](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/dialect.py)</code></li> <li>Line 42: <code>[respecting each dialect's resolution rules](https://sqlglot.com/sqlglot/dialects/dialect.html#Dialect.normalize_identifier)</code></li> <li>Line 55: <code>[here](https://sqlglot.com/sqlglot/dialects/dialect.html#NormalizationStrategy)</code></li> </ul>"},{"location":"FILE_REFERENCES/#configurations-options-notifications","title":"configurations/options/notifications.md","text":"<p>Internal Links (8):</p><ul> <li>Line 9: <code>[event type](#vulcan-event-types)</code></li> <li>Line 9: <code>[override is configured for development](#notifications-during-development)</code></li> <li>Line 11: <code>[Audit](../concepts/audits.md)</code> \u26a0\ufe0f</li> <li>Line 21: <code>[Slack notification methods](#slack-notifications)</code></li> <li>Line 21: <code>[email notification](#email-notifications)</code></li> <li>Line 133: <code>[</code>plan<code>application](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[</code>run<code>](../reference/cli.md#run)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[</code>audit<code>](../concepts/audits.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (4):</p><ul> <li>Line 7: <code>[configuration](https://vulcan.readthedocs.io/en/stable/reference/configuration/)</code></li> <li>Line 157: <code>[create an incoming webhook](https://api.slack.com/messaging/webhooks)</code></li> <li>Line 186: <code>[Slack's official documentation](https://api.slack.com/tutorials/tracks/getting-a-token)</code></li> <li>Line 259: <code>[here](https://github.com/TobikoData/vulcan/blob/main/vulcan/core/notification_target.py)</code></li> </ul>"},{"location":"FILE_REFERENCES/#getting_started","title":"getting_started/","text":""},{"location":"FILE_REFERENCES/#getting_started-cli","title":"getting_started/cli.md","text":"<p>Internal Links (26):</p><ul> <li>Line 7: <code>[prerequisites](./prerequisites.md)</code></li> <li>Line 26: <code>[Docker Quickstart](./docker.md)</code> \u26a0\ufe0f</li> <li>Line 64: <code>[Docker Quickstart](./docker.md)</code> \u26a0\ufe0f</li> <li>Line 85: <code>[</code>vulcan init<code>command](../reference/cli.md#init)</code> \u26a0\ufe0f</li> <li>Line 95: <code>[below](#2-create-a-prod-environment)</code></li> <li>Line 238: <code>[here](../reference/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 248: <code>[here](../guides/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 250: <code>[here](../concepts/models/overview.md)</code> \u26a0\ufe0f</li> <li>Line 252: <code>[here](../concepts/models/seed_models.md)</code> \u26a0\ufe0f</li> <li>Line 254: <code>[here](../concepts/audits.md)</code> \u26a0\ufe0f</li> <li>Line 256: <code>[here](../concepts/tests.md)</code> \u26a0\ufe0f</li> <li>Line 258: <code>[here](../concepts/macros/overview.md)</code> \u26a0\ufe0f</li> <li>Line 300: <code>[Vulcan environment](../concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 304: <code>[Vulcan plan](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 418: <code>[kinds](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 420: <code>[</code>SEED<code>models](../concepts/models/model_kinds.md#seed)</code> \u26a0\ufe0f</li> <li>Line 421: <code>[</code>FULL<code>models](../concepts/models/model_kinds.md#full)</code> \u26a0\ufe0f</li> <li>Line 422: <code>[</code>INCREMENTAL_BY_TIME_RANGE<code>models](../concepts/models/model_kinds.md#incremental_by_time_range)</code> \u26a0\ufe0f</li> <li>Line 472: <code>[</code>audits<code>](../concepts/audits.md)</code> \u26a0\ufe0f</li> <li>Line 530: <code>[Example project physical layer tables in the DuckDB CLI](./cli/cli-quickstart_duckdb-tables.png)</code></li> <li>Line 530: <code>[Example project physical layer tables in the DuckDB CLI](./cli/cli-quickstart_duckdb-tables.png)</code></li> <li>Line 534: <code>[Example project virtual layer views in the DuckDB CLI](./cli/cli-quickstart_duckdb-views.png)</code></li> <li>Line 534: <code>[Example project virtual layer views in the DuckDB CLI](./cli/cli-quickstart_duckdb-views.png)</code></li> <li>Line 758: <code>[Learn more about Vulcan CLI commands](../reference/cli.md)</code> \u26a0\ufe0f</li> <li>Line 759: <code>[Set up a connection to a database or SQL engine](../guides/connections.md)</code> \u26a0\ufe0f</li> <li>Line 760: <code>[Learn more about Vulcan concepts](../concepts/overview.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (2):</p><ul> <li>Line 5: <code>[DuckDB](https://duckdb.org/)</code></li> <li>Line 761: <code>[Join our Slack community](https://tobikodata.com/slack)</code></li> </ul><p>Images (2):</p><ul> <li>Line 530: <code>![Example project physical layer tables in the DuckDB CLI](./cli/cli-quickstart_duckdb-tables.png)</code></li> <li>Line 534: <code>![Example project virtual layer views in the DuckDB CLI](./cli/cli-quickstart_duckdb-views.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#getting_started-index","title":"getting_started/index.md","text":"<p>Internal Links (2):</p><ul> <li>Line 15: <code>[Start with Docker Quickstart](./docker.md)</code> \u26a0\ufe0f</li> <li>Line 19: <code>[prerequisites](./prerequisites.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#getting_started-prerequisites","title":"getting_started/prerequisites.md","text":"<p>External Links (4):</p><ul> <li>Line 13: <code>[Docker Desktop for Mac](https://www.docker.com/products/docker-desktop/)</code></li> <li>Line 14: <code>[Docker Desktop for Windows](https://www.docker.com/products/docker-desktop/)</code></li> <li>Line 15: <code>[official Docker installation guide](https://docs.docker.com/engine/install/)</code></li> <li>Line 40: <code>[Docker Compose installation guide](https://docs.docker.com/compose/install/)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides","title":"guides/","text":""},{"location":"FILE_REFERENCES/#guides-data_quality","title":"guides/data_quality.md","text":"<p>Internal Links (4):</p><ul> <li>Line 580: <code>[Built-in Audits](../concepts/audits.md#built-in-audits)</code> \u26a0\ufe0f</li> <li>Line 581: <code>[Check Dimensions](../concepts/checks.md#data-quality-dimensions)</code> \u26a0\ufe0f</li> <li>Line 582: <code>[Testing](../concepts/tests.md)</code> \u26a0\ufe0f</li> <li>Line 583: <code>[Orders360 Example](../examples/overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-incremental_by_time","title":"guides/incremental_by_time.md","text":"<p>Internal Links (7):</p><ul> <li>Line 5: <code>[models guide](./models.md)</code> \u26a0\ufe0f</li> <li>Line 5: <code>[model kinds page](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 1155: <code>[Model Kinds](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 1156: <code>[Models Guide](./models.md)</code></li> <li>Line 1157: <code>[Plan Guide](./plan.md)</code></li> <li>Line 1158: <code>[Run Guide](./run.md)</code> \u26a0\ufe0f</li> <li>Line 1159: <code>[Orders360 Example](../examples/overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-model_selection","title":"guides/model_selection.md","text":"<p>Internal Links (6):</p><ul> <li>Line 5: <code>[</code>--allow-destructive-model<code>and</code>--allow-additive-model<code>selectors](../concepts/plans.md#destructive-changes)</code> \u26a0\ufe0f</li> <li>Line 11: <code>[plan](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 623: <code>[Plans](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 624: <code>[Plan Guide](./plan.md)</code></li> <li>Line 625: <code>[Model Configuration](../reference/model_configuration.md)</code> \u26a0\ufe0f</li> <li>Line 626: <code>[Orders360 Example](../examples/overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-models","title":"guides/models.md","text":"<p>Internal Links (8):</p><ul> <li>Line 9: <code>[Created your project](../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 10: <code>[Applied your first plan](./plan.md#scenario-1-first-plan-initializing-production)</code></li> <li>Line 11: <code>[dev environment](../concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 677: <code>[Model Kinds](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 678: <code>[Model Properties](../concepts/models/properties.md)</code> \u26a0\ufe0f</li> <li>Line 679: <code>[Plan Guide](./plan.md)</code></li> <li>Line 680: <code>[Testing Guide](./testing.md)</code> \u26a0\ufe0f</li> <li>Line 681: <code>[Orders360 Example](../examples/overview.md)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-plan","title":"guides/plan.md","text":"<p>Internal Links (7):</p><ul> <li>Line 130: <code>[Examples Overview](../examples/overview.md)</code></li> <li>Line 131: <code>[Docker Quickstart](../getting_started/docker.md)</code> \u26a0\ufe0f</li> <li>Line 930: <code>[Plans Concepts](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 931: <code>[Environments](../concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 932: <code>[Model Kinds](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 933: <code>[Run Guide](./run.md)</code> \u26a0\ufe0f</li> <li>Line 934: <code>[Notifications](./notifications.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#guides-run_and_scheduling","title":"guides/run_and_scheduling.md","text":"<p>Internal Links (7):</p><ul> <li>Line 616: <code>[Connections Guide](./connections.md#state-connection)</code> \u26a0\ufe0f</li> <li>Line 627: <code>[notifications](./notifications.md)</code> \u26a0\ufe0f</li> <li>Line 645: <code>[Plan Guide](./plan.md)</code></li> <li>Line 646: <code>[Run Command](../reference/cli.md#run)</code> \u26a0\ufe0f</li> <li>Line 647: <code>[Notifications](./notifications.md)</code> \u26a0\ufe0f</li> <li>Line 648: <code>[Environments](../concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 649: <code>[Connections](./connections.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#guides-transpiling_semantics","title":"guides/transpiling_semantics.md","text":"<p>Internal Links (3):</p><ul> <li>Line 491: <code>[Semantic Models](../concepts/semantics/models.md)</code> \u26a0\ufe0f</li> <li>Line 492: <code>[Business Metrics](../concepts/semantics/metrics.md)</code> \u26a0\ufe0f</li> <li>Line 493: <code>[Semantics Overview](../concepts/semantics/index.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#guides-old","title":"guides-old/","text":""},{"location":"FILE_REFERENCES/#guides-old-configuration","title":"guides-old/configuration.md","text":"<p>Internal Links (66):</p><ul> <li>Line 7: <code>[configuration reference page](../reference/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 14: <code>[</code>model_defaults<code></code>dialect<code>key](#models)</code></li> <li>Line 59: <code>[gateways section](#gateways)</code></li> <li>Line 96: <code>[notifications guide](../guides/notifications.md)</code> \u26a0\ufe0f</li> <li>Line 123: <code>[overrides](#overrides)</code></li> <li>Line 253: <code>[noted above](#configuration-files)</code></li> <li>Line 281: <code>[Vulcan configuration reference page](../reference/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 283: <code>[Project](../reference/configuration.md#projects)</code> \u26a0\ufe0f</li> <li>Line 284: <code>[Environment](../reference/configuration.md#environments-virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 285: <code>[Gateways](../reference/configuration.md#gateways)</code> \u26a0\ufe0f</li> <li>Line 286: <code>[Gateway/connection defaults](../reference/configuration.md#gatewayconnection-defaults)</code> \u26a0\ufe0f</li> <li>Line 287: <code>[Model defaults](../reference/model_configuration.md)</code> \u26a0\ufe0f</li> <li>Line 288: <code>[Debug mode](../reference/configuration.md#debug-mode)</code> \u26a0\ufe0f</li> <li>Line 292: <code>[configuration reference page](../reference/configuration.md)</code> \u26a0\ufe0f</li> <li>Line 328: <code>[environments](../reference/configuration.md#environments-virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 412: <code>[environment_suffix_target](../reference/configuration.md#environments-virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 573: <code>[preview](../concepts/plans.md#data-preview-for-forward-only-changes)</code> \u26a0\ufe0f</li> <li>Line 577: <code>[Table Migration Guide](./table_migration.md)</code></li> <li>Line 582: <code>[catalog](../concepts/glossary.md#catalog)</code> \u26a0\ufe0f</li> <li>Line 589: <code>[virtual layer](../concepts/glossary.md#virtual-layer)</code> \u26a0\ufe0f</li> <li>Line 589: <code>[physical layer](../concepts/glossary.md#physical-layer)</code> \u26a0\ufe0f</li> <li>Line 589: <code>[Isolated Systems Guide](../guides/isolated_systems.md)</code> \u26a0\ufe0f</li> <li>Line 632: <code>[MySQL](../integrations/engines/mysql.md)</code> \u26a0\ufe0f</li> <li>Line 633: <code>[Postgres](../integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 634: <code>[GCP Postgres](../integrations/engines/gcp-postgres.md)</code> \u26a0\ufe0f</li> <li>Line 647: <code>[categorize](../concepts/plans.md#change-categories)</code> \u26a0\ufe0f</li> <li>Line 647: <code>[plan](../reference/configuration.md#plan)</code> \u26a0\ufe0f</li> <li>Line 651: <code>[breaking](../concepts/plans.md#breaking-change)</code> \u26a0\ufe0f</li> <li>Line 807: <code>[gateway](../reference/configuration.md#gateway)</code> \u26a0\ufe0f</li> <li>Line 830: <code>[engine-specific connection config](#engine-connection-configuration)</code></li> <li>Line 830: <code>[scheduler config](#scheduler)</code></li> <li>Line 854: <code>[gateway defaults below](#gatewayconnection-defaults)</code></li> <li>Line 858: <code>[connection](../reference/configuration.md#connection)</code> \u26a0\ufe0f</li> <li>Line 863: <code>[below](#engine-connection-configuration)</code></li> <li>Line 909: <code>[Athena](../integrations/engines/athena.md)</code> \u26a0\ufe0f</li> <li>Line 910: <code>[BigQuery](../integrations/engines/bigquery.md)</code> \u26a0\ufe0f</li> <li>Line 911: <code>[Databricks](../integrations/engines/databricks.md)</code> \u26a0\ufe0f</li> <li>Line 912: <code>[DuckDB](../integrations/engines/duckdb.md)</code> \u26a0\ufe0f</li> <li>Line 913: <code>[Fabric](../integrations/engines/fabric.md)</code> \u26a0\ufe0f</li> <li>Line 914: <code>[MotherDuck](../integrations/engines/motherduck.md)</code> \u26a0\ufe0f</li> <li>Line 915: <code>[MySQL](../integrations/engines/mysql.md)</code> \u26a0\ufe0f</li> <li>Line 916: <code>[MSSQL](../integrations/engines/mssql.md)</code> \u26a0\ufe0f</li> <li>Line 917: <code>[Postgres](../integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 918: <code>[GCP Postgres](../integrations/engines/gcp-postgres.md)</code> \u26a0\ufe0f</li> <li>Line 919: <code>[Redshift](../integrations/engines/redshift.md)</code> \u26a0\ufe0f</li> <li>Line 920: <code>[Snowflake](../integrations/engines/snowflake.md)</code> \u26a0\ufe0f</li> <li>Line 921: <code>[Spark](../integrations/engines/spark.md)</code> \u26a0\ufe0f</li> <li>Line 922: <code>[Trino](../integrations/engines/trino.md)</code> \u26a0\ufe0f</li> <li>Line 939: <code>[Postgres](../integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 940: <code>[GCP Postgres](../integrations/engines/gcp-postgres.md)</code> \u26a0\ufe0f</li> <li>Line 944: <code>[DuckDB](../integrations/engines/duckdb.md)</code> \u26a0\ufe0f</li> <li>Line 946: <code>[MySQL](../integrations/engines/mysql.md)</code> \u26a0\ufe0f</li> <li>Line 947: <code>[MSSQL](../integrations/engines/mssql.md)</code> \u26a0\ufe0f</li> <li>Line 951: <code>[ClickHouse](../integrations/engines/clickhouse.md)</code> \u26a0\ufe0f</li> <li>Line 952: <code>[Spark](../integrations/engines/spark.md)</code> \u26a0\ufe0f</li> <li>Line 953: <code>[Trino](../integrations/engines/trino.md)</code> \u26a0\ufe0f</li> <li>Line 1102: <code>[plans](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 1104: <code>[scheduler](../reference/configuration.md#scheduler)</code> \u26a0\ufe0f</li> <li>Line 1146: <code>[gateway/connection defaults](../reference/configuration.md#gatewayconnection-defaults)</code> \u26a0\ufe0f</li> <li>Line 1148: <code>[accept a gateway option](../reference/cli.md#cli)</code> \u26a0\ufe0f</li> <li>Line 1242: <code>[models configuration reference page](../reference/model_configuration.md#model-defaults)</code> \u26a0\ufe0f</li> <li>Line 1269: <code>[model concepts page](../concepts/models/model_kinds.md)</code> \u26a0\ufe0f</li> <li>Line 1329: <code>[models configuration reference page](../reference/model_configuration.md#model-kind-properties)</code> \u26a0\ufe0f</li> <li>Line 1355: <code>[models configuration reference page](../reference/model_configuration.md#model-kind-properties)</code> \u26a0\ufe0f</li> <li>Line 1374: <code>[Python models concepts page](../concepts/models/python_models.md#model-specification)</code> \u26a0\ufe0f</li> <li>Line 1480: <code>[linting guide](./linter.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (19):</p><ul> <li>Line 84: <code>[API documentation](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config.html)</code></li> <li>Line 88: <code>[Model defaults configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/model.html)</code></li> <li>Line 89: <code>[Gateway configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/gateway.html)</code></li> <li>Line 90: <code>[Connection configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/connection.html)</code></li> <li>Line 91: <code>[Scheduler configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/scheduler.html)</code></li> <li>Line 92: <code>[Plan change categorization configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/categorizer.html#CategorizerConfig)</code></li> <li>Line 93: <code>[User configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/user.html#User)</code></li> <li>Line 94: <code>[Notification configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/notification_target.html)</code></li> <li>Line 333: <code>[regex pattern](https://docs.python.org/3/library/re.html#regular-expression-syntax)</code></li> <li>Line 466: <code>[silently truncate](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS)</code></li> <li>Line 591: <code>[regex patterns](https://en.wikipedia.org/wiki/Regular_expression)</code></li> <li>Line 637: <code>[regex101](https://regex101.com/)</code></li> <li>Line 638: <code>[ChatGPT](https://chat.openai.com)</code></li> <li>Line 935: <code>[Tobiko Cloud](https://tobikodata.com/product.html)</code></li> <li>Line 945: <code>[single user](https://duckdb.org/docs/connect/concurrency.html#writing-to-duckdb-from-multiple-processes)</code></li> <li>Line 1240: <code>[supported by the SQLGlot library](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/dialect.py)</code></li> <li>Line 1277: <code>[respecting each dialect's resolution rules](https://sqlglot.com/sqlglot/dialects/dialect.html#Dialect.normalize_identifier)</code></li> <li>Line 1290: <code>[here](https://sqlglot.com/sqlglot/dialects/dialect.html#NormalizationStrategy)</code></li> <li>Line 1523: <code>[Tobiko Cloud](https://tobikodata.com/product.html)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-connections","title":"guides-old/connections.md","text":"<p>Internal Links (13):</p><ul> <li>Line 52: <code>[tests](../concepts/tests.md)</code> \u26a0\ufe0f</li> <li>Line 82: <code>[BigQuery](../integrations/engines/bigquery.md)</code> \u26a0\ufe0f</li> <li>Line 83: <code>[Databricks](../integrations/engines/databricks.md)</code> \u26a0\ufe0f</li> <li>Line 84: <code>[DuckDB](../integrations/engines/duckdb.md)</code> \u26a0\ufe0f</li> <li>Line 85: <code>[MotherDuck](../integrations/engines/motherduck.md)</code> \u26a0\ufe0f</li> <li>Line 86: <code>[MySQL](../integrations/engines/mysql.md)</code> \u26a0\ufe0f</li> <li>Line 87: <code>[MSSQL](../integrations/engines/mssql.md)</code> \u26a0\ufe0f</li> <li>Line 88: <code>[Postgres](../integrations/engines/postgres.md)</code> \u26a0\ufe0f</li> <li>Line 89: <code>[GCP Postgres](../integrations/engines/gcp-postgres.md)</code> \u26a0\ufe0f</li> <li>Line 90: <code>[Redshift](../integrations/engines/redshift.md)</code> \u26a0\ufe0f</li> <li>Line 91: <code>[Snowflake](../integrations/engines/snowflake.md)</code> \u26a0\ufe0f</li> <li>Line 92: <code>[Spark](../integrations/engines/spark.md)</code> \u26a0\ufe0f</li> <li>Line 93: <code>[Trino](../integrations/engines/trino.md)</code> \u26a0\ufe0f</li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-customizing_vulcan","title":"guides-old/customizing_vulcan.md","text":"<p>Internal Links (1):</p><ul> <li>Line 23: <code>[Python configuration format](./configuration.md#python)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-isolated_systems","title":"guides-old/isolated_systems.md","text":"<p>Internal Links (9):</p><ul> <li>Line 19: <code>[Vulcan environments](../concepts/environments.md)</code> \u26a0\ufe0f</li> <li>Line 27: <code>[separate database](./configuration.md#state-connection)</code></li> <li>Line 33: <code>[gateways](./configuration.md#gateways)</code></li> <li>Line 33: <code>[connections](./connections.md)</code></li> <li>Line 63: <code>[</code>@gateway<code>macro variable](../concepts/macros/macro_variables.md#runtime-variables)</code> \u26a0\ufe0f</li> <li>Line 65: <code>[</code>@IF<code>macro operator](../concepts/macros/vulcan_macros.md#if)</code> \u26a0\ufe0f</li> <li>Line 81: <code>[here](../concepts/macros/vulcan_macros.md#embedding-variables-in-strings)</code> \u26a0\ufe0f</li> <li>Line 91: <code>[Vulcan project files link systems](./isolated_systems/isolated-systems_linkage.png)</code> \u26a0\ufe0f</li> <li>Line 91: <code>[Vulcan project files link systems](./isolated_systems/isolated-systems_linkage.png)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 144: <code>[blue-green deployment](https://en.m.wikipedia.org/wiki/Blue%E2%80%93green_deployment)</code></li> </ul><p>Images (1):</p><ul> <li>Line 91: <code>![Vulcan project files link systems](./isolated_systems/isolated-systems_linkage.png)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-notifications","title":"guides-old/notifications.md","text":"<p>Internal Links (8):</p><ul> <li>Line 9: <code>[event type](#vulcan-event-types)</code></li> <li>Line 9: <code>[override is configured for development](#notifications-during-development)</code></li> <li>Line 11: <code>[Audit](../concepts/audits.md)</code> \u26a0\ufe0f</li> <li>Line 21: <code>[Slack notification methods](#slack-notifications)</code></li> <li>Line 21: <code>[email notification](#email-notifications)</code></li> <li>Line 133: <code>[</code>plan<code>application](../concepts/plans.md)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[</code>run<code>](../reference/cli.md#run)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[</code>audit<code>](../concepts/audits.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (4):</p><ul> <li>Line 7: <code>[configuration](https://vulcan.readthedocs.io/en/stable/reference/configuration/)</code></li> <li>Line 157: <code>[create an incoming webhook](https://api.slack.com/messaging/webhooks)</code></li> <li>Line 186: <code>[Slack's official documentation](https://api.slack.com/tutorials/tracks/getting-a-token)</code></li> <li>Line 259: <code>[here](https://github.com/TobikoData/vulcan/blob/main/vulcan/core/notification_target.py)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-projects","title":"guides-old/projects.md","text":"<p>Internal Links (2):</p><ul> <li>Line 7: <code>[prerequisites](../getting_started/prerequisites.md)</code></li> <li>Line 69: <code>[CLI](../reference/cli.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 49: <code>[SQL dialect supported by sqlglot](https://sqlglot.com/sqlglot/dialects.html)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guides-old-table_migration","title":"guides-old/table_migration.md","text":"<p>Internal Links (4):</p><ul> <li>Line 9: <code>[external models](../concepts/models/model_kinds.md#external)</code> \u26a0\ufe0f</li> <li>Line 42: <code>[</code>EXTERNAL<code>model](../concepts/models/model_kinds.md#external)</code> \u26a0\ufe0f</li> <li>Line 44: <code>[</code>VIEW<code>model](../concepts/models/model_kinds.md#view)</code> \u26a0\ufe0f</li> <li>Line 128: <code>[forward only](./incremental_time.md#forward-only-models)</code> \u26a0\ufe0f</li> </ul><p>External Links (1):</p><ul> <li>Line 114: <code>[interval approach](https://vulcan.readthedocs.io/en/stable/guides/incremental_time/#counting-time)</code></li> </ul>"},{"location":"FILE_REFERENCES/#guidesget-started","title":"guides/get-started/","text":""},{"location":"FILE_REFERENCES/#guides-get-started-docker","title":"guides/get-started/docker.md","text":"<p>Internal Links (15):</p><ul> <li>Line 34: <code>[:material-download: Download for Mac/Linux](zip-mac/vulcan-project.zip)</code></li> <li>Line 68: <code>[:material-download: Download for Windows](zip-window/vulcan-project.zip)</code></li> <li>Line 106: <code>[*Click here*](../reference/cli.md#init)</code> \u26a0\ufe0f</li> <li>Line 114: <code>[*Click here*](../reference/cli.md#info)</code> \u26a0\ufe0f</li> <li>Line 121: <code>[*Click here*](../reference/cli.md#plan)</code> \u26a0\ufe0f</li> <li>Line 133: <code>[*Click here*](../reference/cli.md#fetchdf)</code> \u26a0\ufe0f</li> <li>Line 139: <code>[*Click here*](../reference/cli.md#transpile)</code> \u26a0\ufe0f</li> <li>Line 147: <code>[*Click here*](../reference/cli.md#init)</code> \u26a0\ufe0f</li> <li>Line 155: <code>[*Click here*](../reference/cli.md#info)</code> \u26a0\ufe0f</li> <li>Line 161: <code>[*Click here*](../reference/cli.md#plan)</code> \u26a0\ufe0f</li> <li>Line 173: <code>[*Click here*](../reference/cli.md#fetchdf)</code> \u26a0\ufe0f</li> <li>Line 179: <code>[*Click here*](../reference/cli.md#transpile)</code> \u26a0\ufe0f</li> <li>Line 251: <code>[Learn more about Vulcan CLI commands](../reference/cli.md)</code> \u26a0\ufe0f</li> <li>Line 252: <code>[Explore Vulcan concepts](../concepts/overview.md)</code> \u26a0\ufe0f</li> <li>Line 253: <code>[Set up connections to different warehouses](../guides/connections.md)</code> \u26a0\ufe0f</li> </ul><p>External Links (3):</p><ul> <li>Line 16: <code>[Docker Desktop for Mac](https://www.docker.com/products/docker-desktop/)</code></li> <li>Line 18: <code>[Docker for Linux](https://docs.docker.com/engine/install/)</code></li> <li>Line 28: <code>[Docker Desktop for Windows](https://www.docker.com/products/docker-desktop/)</code></li> </ul>"},{"location":"FILE_REFERENCES/#link-index","title":"Link Index","text":"<p>Quick reference for finding specific links:</p>"},{"location":"FILE_REFERENCES/#c","title":"C","text":"<ul> <li>comparisons.md (23)</li> <li>components/advanced-features/custom_materializations.md (6)</li> <li>components/advanced-features/macros/built_in.md (69)</li> <li>components/advanced-features/macros/jinja.md (6)</li> <li>components/advanced-features/macros/overview.md (5)</li> <li>components/advanced-features/macros/variables.md (16)</li> <li>components/advanced-features/signals.md (2)</li> <li>components/audits/audits.md (7)</li> <li>components/checks/checks.md (1)</li> <li>components/model/model_kinds.md (12)</li> <li>components/model/overview.md (3)</li> <li>components/model/properties.md (14)</li> <li>components/model/statements.md (1)</li> <li>components/model/types/external_models.md (2)</li> <li>components/model/types/managed_models.md (11)</li> <li>components/model/types/python_models.md (13)</li> <li>components/model/types/sql_models.md (12)</li> <li>components/semantics/business_metrics.md (2)</li> <li>components/semantics/models.md (2)</li> <li>components/semantics/overview.md (5)</li> <li>components/tests/tests.md (3)</li> <li>concepts-old/architecture/serialization.md (4)</li> <li>concepts-old/architecture/snapshots.md (1)</li> <li>concepts-old/environments.md (7)</li> <li>concepts-old/glossary.md (11)</li> <li>concepts-old/macros/jinja_macros.md (12)</li> <li>concepts-old/macros/macro_variables.md (18)</li> <li>concepts-old/macros/overview.md (5)</li> <li>concepts-old/macros/vulcan_macros.md (69)</li> <li>concepts-old/overview.md (10)</li> <li>concepts-old/plans.md (50)</li> <li>concepts-old/state.md (10)</li> <li>configurations-old/configuration.md (39)</li> <li>configurations-old/integrations/engines/athena.md (15)</li> <li>configurations-old/integrations/engines/azuresql.md (2)</li> <li>configurations-old/integrations/engines/bigquery.md (31)</li> <li>configurations-old/integrations/engines/clickhouse.md (31)</li> <li>configurations-old/integrations/engines/databricks.md (74)</li> <li>configurations-old/integrations/engines/duckdb.md (15)</li> <li>configurations-old/integrations/engines/fabric.md (2)</li> <li>configurations-old/integrations/engines/motherduck.md (8)</li> <li>configurations-old/integrations/engines/mssql.md (4)</li> <li>configurations-old/integrations/engines/redshift.md (1)</li> <li>configurations-old/integrations/engines/risingwave.md (6)</li> <li>configurations-old/integrations/engines/snowflake.md (32)</li> <li>configurations-old/integrations/engines/spark.md (2)</li> <li>configurations-old/integrations/engines/trino.md (28)</li> <li>configurations/engines/snowflake/snowflake.md (32)</li> <li>configurations/options/linter.md (3)</li> <li>configurations/options/model_defaults.md (5)</li> <li>configurations/options/notifications.md (12)</li> <li>configurations/overview.md (14)</li> </ul>"},{"location":"FILE_REFERENCES/#g","title":"G","text":"<ul> <li>getting_started/cli.md (28)</li> <li>getting_started/index.md (2)</li> <li>getting_started/prerequisites.md (4)</li> <li>guides-old/configuration.md (85)</li> <li>guides-old/connections.md (13)</li> <li>guides-old/customizing_vulcan.md (1)</li> <li>guides-old/isolated_systems.md (10)</li> <li>guides-old/notifications.md (12)</li> <li>guides-old/projects.md (3)</li> <li>guides-old/table_migration.md (5)</li> <li>guides/data_quality.md (4)</li> <li>guides/get-started/docker.md (18)</li> <li>guides/incremental_by_time.md (7)</li> <li>guides/model_selection.md (6)</li> <li>guides/models.md (8)</li> <li>guides/plan.md (7)</li> <li>guides/run_and_scheduling.md (7)</li> <li>guides/transpiling_semantics.md (3)</li> </ul>"},{"location":"FILE_REFERENCES/#i","title":"I","text":"<ul> <li>index.md (1)</li> </ul>"},{"location":"comparisons/","title":"Comparisons","text":""},{"location":"comparisons/#comparisons","title":"Comparisons","text":"<p>This documentation is a work in progress.</p> <p>There are many tools and frameworks in the data ecosystem. This page tries to make sense of it all.</p> <p>If you are not familiar with Vulcan, it will be helpful to first read about Vulcan to better understand the comparisons. </p>"},{"location":"comparisons/#dbt","title":"dbt","text":"<p>dbt is a tool for data transformations. It is a pioneer in this space and has shown how valuable transformation frameworks can be. Although dbt is a fantastic tool, it has trouble scaling with data and organizational size.</p> <p>dbt built their product focused on simple data transformations. By default, it fully refreshes data warehouses by executing templated SQL in the correct order.</p> <p>Over time dbt has seen that data transformations are not enough to operate a scalable and robust data product. As a result, advanced features are patched in, such as state management (defer) and incremental loads, to try to address these needs while pushing the burden of correctness onto users with increased complexity. These \"advanced\" features make up some of the fundamental building blocks of a DataOps framework.</p> <p>In other words, the challenge of implementing these features in dbt falls primarily on you: more jinja macro blocks, more manual configuration, more custom tooling, and more opportunities for error. We needed an easier, more reliable way, so we designed Vulcan from the ground up to be a robust DataOps framework.</p> <p>Vulcan aims to be dbt format-compatible. Importing existing dbt projects with minor changes is in development.</p>"},{"location":"comparisons/#feature-comparisons","title":"Feature comparisons","text":"Feature dbt Vulcan Modeling <code>SQL models</code> \u2705 \u2705 <code>Python models</code> \u2705 \u2705\u2705 <code>Jinja support</code> \u2705 \u2705 <code>Jinja macros</code> \u2705 \u2705 <code>Python macros</code> \u274c \u2705 Validation <code>SQL semantic validation</code> \u274c \u2705 <code>Unit tests</code> \u274c \u2705 <code>Table diff</code> \u274c \u2705 <code>Data audits</code> \u2705 \u2705 <code>Schema contracts</code> \u2705 \u2705 <code>Data contracts</code> \u274c \u2705 Deployment <code>Virtual Data Environments</code> \u274c \u2705 <code>Open-source CI/CD bot</code> \u274c \u2705 <code>Data consistency enforcement</code> \u274c \u2705 Interfaces <code>CLI</code> \u2705 \u2705 <code>Paid UI</code> \u2705 \u274c <code>Open-source UI</code> \u274c \u2705 <code>Native Notebook Support</code> \u274c \u2705 Visualization <code>Documentation generation</code> \u2705 \u2705 <code>Column-level lineage</code> \u274c \u2705 Miscellaneous <code>Package manager</code> \u2705 \u274c <code>Multi-repository support</code> \u274c \u2705 <code>SQL transpilation</code> \u274c \u2705"},{"location":"comparisons/#environments","title":"Environments","text":"<p>Development and staging environments in dbt are costly to make and not fully representative of what will go into production.</p> <p>The standard approach to creating a new environment in dbt is to rerun your entire warehouse in a new environment. This may work at small scales, but even then it wastes time and money. Here's why:</p> <p>The first part of running a data transformation system is repeatedly iterating through three steps: create or modify model code, execute the models, evaluate the outputs. Practitioners may repeat these steps many times in a day's work.</p> <p>These steps incur costs to organizations: compute costs to run the models and staff time spent waiting on them to run. Inefficiencies compound rapidly because the steps are repeated so frequently. dbt's default full refresh approach leads to the most costly version of this loop: recomputing every model every time.</p> <p>Vulcan takes another approach. It examines the code modifications and the dependency structure among the models to determine which models are affected -- and executes only those models. This results in the least costly version of the loop: computing only what is required every time through.</p> <p>This enables Vulcan to provide efficient isolated Virtual Environments. Environments in dbt cost compute and storage, but creating a development environment in Vulcan is free -- you can quickly access a full replica of any other environment with a single command.</p> <p>Additionally, Vulcan ensures that promotion of staging environments to production is predictable and consistent. There is no concept of promotion in dbt, so queries are all rerun when it's time to deploy something. In Vulcan, promotions are simple pointer swaps so there is no wasted compute.</p>"},{"location":"comparisons/#incremental-models","title":"Incremental models","text":"<p>Implementing incremental models is difficult and error-prone in dbt because it does not keep track of state.</p>"},{"location":"comparisons/#complexity","title":"Complexity","text":"<p>Since there is no state of which incremental intervals have already run in dbt, users must write and maintain subqueries to find missing date boundaries themselves:</p> <pre><code>-- dbt incremental\nSELECT *\nFROM {{ ref(raw.events) }} e\nJOIN {{ ref(raw.event_dims) }} d\n  ON e.id = d.id\n-- must specify the is_incremental flag because this predicate will fail if the model has never run before\n{% if is_incremental() %}\n    -- this filter dynamically scans the current model to find the date boundary\n    AND d.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n{% if is_incremental() %}\n  WHERE e.ds &gt;= (SELECT MAX(ds) FROM {{ this }})\n{% endif %}\n</code></pre> <p>Manually specifying macros to find date boundaries is repetitive and error-prone.</p> <p>The example above shows how incremental models behave differently in dbt depending on whether they have been run before. As models become more complex, the cognitive burden of having two run times, \"first time full refresh\" vs. \"subsequent incremental\", increases.</p> <p>Vulcan keeps track of which date ranges exist, producing a simplified and efficient query as follows:</p> <pre><code>-- Vulcan incremental\nSELECT *\nFROM raw.events e\nJOIN raw.event_dims d\n  -- date ranges are handled automatically by Vulcan\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n</code></pre>"},{"location":"comparisons/#data-leakage","title":"Data leakage","text":"<p>dbt does not check whether the data inserted into an incremental table should be there or not. This can lead to problems and consistency issues, such as late-arriving data overriding past partitions. These problems are called \"data leakage.\"</p> <p>Vulcan wraps all queries in a subquery with a time filter under the hood to enforce that the data inserted for a particular batch is as expected and reproducible every time.</p> <p>In addition, dbt only supports the 'insert/overwrite' incremental load pattern for systems that natively support it. Vulcan enables 'insert/overwrite' on any system, because it is the most robust approach to incremental loading, while 'Append' pipelines risk data inaccuracy in the variety of scenarios where your pipelines may run more than once for a given date.</p> <p>This example shows the time filtering subquery Vulcan applies to all queries as a guard against data leakage: </p><pre><code>-- original query\nSELECT *\nFROM raw.events\nJOIN raw.event_dims d\n  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\nWHERE d.ds BETWEEN @start_ds AND @end_ds\n\n-- with automated data leakage guard\nSELECT *\nFROM (\n  SELECT *\n  FROM raw.events\n  JOIN raw.event_dims d\n    ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds\n  WHERE d.ds BETWEEN @start_ds AND @end_ds\n)\nWHERE ds BETWEEN @start_ds AND @end_ds\n</code></pre><p></p>"},{"location":"comparisons/#data-gaps","title":"Data gaps","text":"<p>The main pattern used to implement incremental models in dbt is checking for the most recent data with MAX(date). This pattern does not catch missing data from the past, or \"data gaps.\"</p> <p>Vulcan stores each date interval a model has been run with, so it knows exactly what dates are missing: </p><pre><code>Expected dates: 2022-01-01, 2022-01-02, 2022-01-03\nMissing past data: ?, 2022-01-02, 2022-01-03\nData gap: 2022-01-01, ?, 2022-01-03\n</code></pre><p></p> <p>Vulcan will automatically fill these data gaps on the next run.</p>"},{"location":"comparisons/#performance","title":"Performance","text":"<p>Subqueries that look for MAX(date) could have a performance impact on the primary query. Vulcan is able to avoid these extra subqueries.</p> <p>Additionally, dbt expects an incremental model to be able to fully refresh the first time it runs. For some large data sets, this is cost-prohibitive or infeasible.</p> <p>Vulcan is able to batch up backfills into more manageable chunks.</p>"},{"location":"comparisons/#sql-understanding","title":"SQL understanding","text":"<p>dbt heavily relies on Jinja. It has no understanding of SQL and treats all queries as raw strings without context. This means that simple syntax errors like trailing commas are difficult to debug and require a full run to detect.</p> <p>Vulcan supports Jinja, but it does not rely on it - instead, it parses/understands SQL through SQLGlot. Simple errors can be detected at compile time, so you no longer have to wait minutes or even longer to see that you've referenced a column incorrectly or missed a comma.</p> <p>Additionally, having a first-class understanding of SQL supercharges Vulcan with features such as transpilation, column-level lineage, and automatic change categorization.</p>"},{"location":"comparisons/#testing","title":"Testing","text":"<p>Data quality checks such as detecting NULL values and duplicated rows are extremely valuable for detecting upstream data issues and large scale problems. However, they are not meant for testing edge cases or business logic, and they are not sufficient for creating robust data pipelines.</p> <p>Unit and integration tests are the tools to use to validate business logic. Vulcan encourages users to add unit tests to all of their models to ensure changes don't unexpectedly break assumptions. Unit tests are designed to be fast and self contained so that they can run in continuous integration (CI) frameworks.</p>"},{"location":"comparisons/#python-models","title":"Python models","text":"<p>dbt's Python models only run remotely on adapters of data platforms that have a full Python runtime, limiting the number of users that can take advantage of them and making the models difficult to debug.</p> <p>Vulcan's Python models run locally and can be used with any data warehouse. Breakpoints can be added to debug the model.</p>"},{"location":"comparisons/#data-contracts","title":"Data contracts","text":"<p>dbt offers manually configured schema contracts that will check the model's schema against the yaml schema at runtime. Models can be versioned to allow downstream teams time to migrate to the latest version, at the risk of a fragmented source of truth during the migration period.</p> <p>Vulcan provides automatic schema contracts and data contracts via <code>vulcan plan</code>, which checks the model's schema and query logic for changes that affect downstream users. <code>vulcan plan</code> will show which models have breaking changes and which downstream models are affected.</p> <p>While breaking changes can be rolled out as separate models to allow for a migration period, Vulcan's Virtual Preview empowers teams to collaborate on migrations before the changes are deployed to prod, maintaining a single source of truth across the business.</p>"},{"location":"cli-command/cli/","title":"CLI Commands","text":""},{"location":"cli-command/cli/#cli-commands","title":"CLI Commands","text":"<pre><code>Usage: vulcan [OPTIONS] COMMAND [ARGS]...\n\n  Vulcan command line tool.\n\nOptions:\n  --version            Show the version and exit.\n  -p, --paths TEXT     Path(s) to the Vulcan config/project.\n  --config TEXT        Name of the config object. Only applicable to\n                       configuration defined using Python script.\n  --gateway TEXT       The name of the gateway.\n  --ignore-warnings    Ignore warnings.\n  --debug              Enable debug mode.\n  --log-to-stdout      Display logs in stdout.\n  --log-file-dir TEXT  The directory to write log files to.\n  --dotenv PATH        Path to a custom .env file to load environment\n                       variables.\n  --help               Show this message and exit.\n\nCommands:\n  api                     Start the Vulcan API server (models, metrics,...\n  audit                   Run audits for the target model(s).\n  check_intervals         Show missing intervals in an environment,...\n  clean                   Clears the Vulcan cache and any build artifacts.\n  create_external_models  Create a schema file containing external model...\n  create_test             Generate a unit test fixture for a given model.\n  dag                     Render the DAG as an html file.\n  destroy                 The destroy command removes all project resources.\n  diff                    Show the diff between the local state and the...\n  dlt_refresh             Attaches to a DLT pipeline with the option to...\n  environments            Prints the list of Vulcan environments with its...\n  evaluate                Evaluate a model and return a dataframe with a...\n  fetchdf                 Run a SQL query and display the results.\n  format                  Format all SQL models and audits.\n  graphql                 Manage the GraphQL service (subcommands: up,...\n  info                    Print information about a Vulcan project.\n  invalidate              Invalidate the target environment, forcing its...\n  janitor                 Run the janitor process on-demand.\n  lint                    Run the linter for the target model(s).\n  migrate                 Migrate Vulcan to the current running version.\n  plan                    Apply local changes to the target environment.\n  render                  Render a model's query, optionally expanding...\n  rollback                Rollback Vulcan to the previous migration.\n  run                     Evaluate missing intervals for the target...\n  semantic                Semantic layer operations.\n  state                   Commands for interacting with state\n  table_diff              Show the diff between two tables or a selection...\n  table_name              Prints the name of the physical table for the...\n  test                    Run model unit tests.\n  transpile               Transpile a semantic SQL or REST-style semantic...\n  transpiler              Manage the Transpiler service (subcommands: up,...\n</code></pre>"},{"location":"cli-command/cli/#audit","title":"audit","text":"<pre><code>Usage: vulcan audit [OPTIONS]\n\n  Run audits for the target model(s).\n\nOptions:\n  --model TEXT           A model to audit. Multiple models can be audited.\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan audit\n  Found 11 audit(s).\n  unique_values on model sales.daily_sales \u2705 PASS.\n  not_null on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  unique_values on model raw.raw_products \u2705 PASS.\n  not_null on model raw.raw_products \u2705 PASS.\n  unique_values on model raw.raw_customers \u2705 PASS.\n  not_null on model raw.raw_customers \u2705 PASS.\n  unique_values on model raw.raw_orders \u2705 PASS.\n  not_null on model raw.raw_orders \u2705 PASS.\n  positive_values on model raw.raw_orders \u2705 PASS.\n\n  Finished with 0 audit errors and 0 audits skipped.\n  Done.\n</code></pre>"},{"location":"cli-command/cli/#check_intervals","title":"check_intervals","text":"<pre><code>Usage: vulcan check_intervals [OPTIONS] [ENVIRONMENT]\n\n  Show missing intervals in an environment, respecting signals.\n\nOptions:\n  --no-signals         Disable signal checks and only show missing intervals.\n  --select-model TEXT  Select specific models to show missing intervals for.\n  -s, --start TEXT     The start datetime of the interval for which this\n                       command will be applied.\n  -e, --end TEXT       The end datetime of the interval for which this command\n                       will be applied.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#clean","title":"clean","text":"<pre><code>Usage: vulcan clean [OPTIONS]\n\n  Clears the Vulcan cache and any build artifacts.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#create_external_models","title":"create_external_models","text":"<pre><code>Usage: vulcan create_external_models [OPTIONS]\n\n  Create a schema file containing external model schemas.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_external_models\n</code></pre>"},{"location":"cli-command/cli/#create_test","title":"create_test","text":"<pre><code>Usage: vulcan create_test [OPTIONS] MODEL\n\n  Generate a unit test fixture for a given model.\n\nOptions:\n  -q, --query &lt;TEXT TEXT&gt;...  Queries that will be used to generate data for\n                              the model's dependencies.\n  -o, --overwrite             When true, the fixture file will be overwritten\n                              in case it already exists.\n  -v, --var &lt;TEXT TEXT&gt;...    Key-value pairs that will define variables\n                              needed by the model.\n  -p, --path TEXT             The file path corresponding to the fixture,\n                              relative to the test directory. By default, the\n                              fixture will be created under the test directory\n                              and the file name will be inferred based on the\n                              test's name.\n  -n, --name TEXT             The name of the test that will be created. By\n                              default, it's inferred based on the model's\n                              name.\n  --include-ctes              When true, CTE fixtures will also be generated.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_test sales.daily_sales --query raw.raw_orders \"SELECT * FROM raw.raw_orders\"\n</code></pre>"},{"location":"cli-command/cli/#dag","title":"dag","text":"<pre><code>Usage: vulcan dag [OPTIONS] FILE\n\n  Render the DAG as an html file.\n\nOptions:\n  --select-model TEXT  Select specific models to include in the dag.\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan dag ./dag.html\n</code></pre>"},{"location":"cli-command/cli/#destroy","title":"destroy","text":"<pre><code>Usage: vulcan destroy\n\n  Removes all state tables, the Vulcan cache and all project resources, including warehouse objects. This includes all tables, views and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.\n\nOptions:\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan destroy\n[WARNING] This will permanently delete all engine-managed objects, state tables and Vulcan cache.\nThe operation may disrupt any currently running or scheduled plans.\n\nSchemas to be deleted:\n  \u2022 warehouse.raw\n  \u2022 warehouse.sales\n\nSnapshot tables to be deleted:\n  \u2022 warehouse.vulcan__raw.raw__raw_customers__1474975870\n  \u2022 warehouse.vulcan__raw.raw__raw_orders__1032938324\n  \u2022 warehouse.vulcan__raw.raw__raw_products__3337559381\n  \u2022 warehouse.vulcan__sales.sales__daily_sales__2671854529\n\nThis action will DELETE ALL the above resources managed by Vulcan AND\npotentially external resources created by other tools in these schemas.\n\nAre you ABSOLUTELY SURE you want to proceed with deletion? [y/n]: y\nEnvironment 'prod' invalidated.\n\nDeleted object warehouse.raw\nDeleted object warehouse.sales\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381__dev\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324__dev\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324\nState tables removed.\nDestroy completed successfully.\n</code></pre>"},{"location":"cli-command/cli/#dlt_refresh","title":"dlt_refresh","text":"<pre><code>Usage: dlt_refresh PIPELINE [OPTIONS]\n\n  Attaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.\n\nOptions:\n  -t, --table TEXT  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.\n  -f, --force       If set it will overwrite existing models with the new generated models from the DLT tables.\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#diff","title":"diff","text":"<pre><code>Usage: vulcan diff [OPTIONS] ENVIRONMENT\n\n  Show the diff between the local state and the target environment.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan diff prod\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n        --- .../daily_sales.sql\n\n        +++ .../daily_sales.sql\n\n        @@ -20,10 +20,11 @@\n\n          grains (order_date)\n        )\n        SELECT\n          CAST(order_date AS TIMESTAMP) AS order_date,\n          CAST(COUNT(order_id) AS INT) AS total_orders,\n          CAST(SUM(total_amount) AS DOUBLE PRECISION) AS total_revenue,\n        -  CAST(MAX(order_id) AS VARCHAR) AS last_order_id\n        +  CAST(MAX(order_id) AS VARCHAR) AS last_order_id,\n        +  COUNT(DISTINCT product_id) AS total_products\n        FROM raw.raw_orders\n        GROUP BY\n          order_date\nSemantics:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 semantic-model:sales.daily_sales\n    \u251c\u2500\u2500 semantic-metric:order_volume\n    \u2514\u2500\u2500 semantic-metric:revenue_trends\nQuality Checks:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:accuracy\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:timeliness\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:completeness\n    \u2514\u2500\u2500 check-suite:sales.daily_sales:validity\n</code></pre>"},{"location":"cli-command/cli/#environments","title":"environments","text":"<pre><code>Usage: vulcan environments [OPTIONS]\n\n  Prints the list of Vulcan environments with its expiry datetime.\n\nOptions:\n  --help             Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan environments\nNumber of Vulcan environments are: 2\nprod - No Expiry\ndev - 2025-12-23 00:00:00\n</code></pre>"},{"location":"cli-command/cli/#evaluate","title":"evaluate","text":"<pre><code>Usage: vulcan evaluate [OPTIONS] MODEL\n\n  Evaluate a model and return a dataframe with a default limit of 1000.\n\nOptions:\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --limit INTEGER        The number of rows which the query should be limited\n                         to.\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan evaluate sales.daily_sales\n   order_date  total_orders  total_revenue last_order_id  total_products\n0  2024-01-05             1          70.77          O001               1\n1  2024-01-10             1          44.22          O002               1\n2  2024-01-15             1          65.52          O003               1\n3  2024-01-20             1          79.42          O004               1\n4  2024-02-01             1          91.35          O005               1\n....\n19 2024-05-15             1          38.38          O020               1\n</code></pre>"},{"location":"cli-command/cli/#fetchdf","title":"fetchdf","text":"<pre><code>Usage: vulcan fetchdf [OPTIONS] SQL\n\n  Run a SQL query and display the results.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan fetchdf \"select count(*) from sales.daily_sales\"\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 count \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 20    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli-command/cli/#format","title":"format","text":"<pre><code>Usage: vulcan format [OPTIONS]\n\n  Format all SQL models and audits.\n\nOptions:\n  -t, --transpile TEXT        Transpile project models to the specified\n                              dialect.\n  --append-newline            Include a newline at the end of each file.\n  --no-rewrite-casts          Preserve the existing casts, without rewriting\n                              them to use the :: syntax.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --check                     Whether or not to check formatting (but not\n                              actually format anything).\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#info","title":"info","text":"<pre><code>Usage: vulcan info [OPTIONS]\n\n  Print information about a Vulcan project.\n\n  Includes counts of project models and macros and connection tests for the\n  data warehouse.\n\nOptions:\n  --skip-connection  Skip the connection test.\n  -v, --verbose      Verbose output.\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan info\nModels: 4\nMacros: 0\nData warehouse connection succeeded\nState backend connection succeeded\n</code></pre>"},{"location":"cli-command/cli/#init","title":"init","text":"<pre><code>Usage: vulcan init [OPTIONS] [ENGINE]\n\n  Create a new Vulcan repository.\n\nOptions:\n  -t, --template TEXT  Project template. Supported values: dbt, dlt, default,\n                       empty.\n  --dlt-pipeline TEXT  DLT pipeline for which to generate a Vulcan project.\n                       Use alongside template: dlt\n  --dlt-path TEXT      The directory where the DLT pipeline resides. Use\n                       alongside template: dlt\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan init postgres\n</code></pre>"},{"location":"cli-command/cli/#invalidate","title":"invalidate","text":"<pre><code>Usage: vulcan invalidate [OPTIONS] ENVIRONMENT\n\n  Invalidate the target environment, forcing its removal during the next run\n  of the janitor process.\n\nOptions:\n  -s, --sync  Wait for the environment to be deleted before returning. If not\n              specified, the environment will be deleted asynchronously by the\n              janitor process. This option requires a connection to the data\n              warehouse.\n  --help      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan invalidate dev\nEnvironment 'dev' invalidated.\n</code></pre>"},{"location":"cli-command/cli/#janitor","title":"janitor","text":"<pre><code>Usage: vulcan janitor [OPTIONS]\n\n  Run the janitor process on-demand.\n\n  The janitor cleans up old environments and expired snapshots.\n\nOptions:\n  --ignore-ttl  Cleanup snapshots that are not referenced in any environment,\n                regardless of when they're set to expire\n  --help        Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan janitor\nDeleted object warehouse.sales__dev\nDeleted object warehouse.raw__dev\nCleanup complete.\n</code></pre>"},{"location":"cli-command/cli/#migrate","title":"migrate","text":"<pre><code>Usage: vulcan migrate [OPTIONS]\n\n  Migrate Vulcan to the current running version.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>migrate</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-command/cli/#plan","title":"plan","text":"<pre><code>Usage: vulcan plan [OPTIONS] [ENVIRONMENT]\n\n  Apply local changes to the target environment.\n\nOptions:\n  -s, --start TEXT                The start datetime of the interval for which\n                                  this command will be applied.\n  -e, --end TEXT                  The end datetime of the interval for which\n                                  this command will be applied.\n  --execution-time TEXT           The execution time (defaults to now).\n  --create-from TEXT              The environment to create the target\n                                  environment from if it doesn't exist.\n                                  Default: prod.\n  --skip-tests                    Skip tests prior to generating the plan if\n                                  they are defined.\n  --skip-linter                   Skip linting prior to generating the plan if\n                                  the linter is enabled.\n  -r, --restate-model TEXT        Restate data for specified models and models\n                                  downstream from the one specified. For\n                                  production environment, all related model\n                                  versions will have their intervals wiped,\n                                  but only the current versions will be\n                                  backfilled. For development environment,\n                                  only the current model versions will be\n                                  affected.\n  --no-gaps                       Ensure that new snapshots have no data gaps\n                                  when comparing to existing snapshots for\n                                  matching models in the target environment.\n  --skip-backfill, --dry-run      Skip the backfill step and only create a\n                                  virtual update for the plan.\n  --empty-backfill                Produce empty backfill. Like --skip-backfill\n                                  no models will be backfilled, unlike --skip-\n                                  backfill missing intervals will be recorded\n                                  as if they were backfilled.\n  --forward-only                  Create a plan for forward-only changes.\n  --allow-destructive-model TEXT  Allow destructive forward-only changes to\n                                  models whose names match the expression.\n  --allow-additive-model TEXT     Allow additive forward-only changes to\n                                  models whose names match the expression.\n  --effective-from TEXT           The effective date from which to apply\n                                  forward-only changes on production.\n  --no-prompts                    Disable interactive prompts for the backfill\n                                  time range. Please note that if this flag is\n                                  set and there are uncategorized changes,\n                                  plan creation will fail.\n  --auto-apply                    Automatically apply the new plan after\n                                  creation.\n  --no-auto-categorization        Disable automatic change categorization.\n  --include-unmodified            Include unmodified models in the target\n                                  environment.\n  --select-model TEXT             Select specific model changes that should be\n                                  included in the plan.\n  --backfill-model TEXT           Backfill only the models whose names match\n                                  the expression.\n  --no-diff                       Hide text differences for changed models.\n  --run                           Run latest intervals as part of the plan\n                                  application (prod environment only).\n  --enable-preview                Enable preview for forward-only models when\n                                  targeting a development environment.\n  --diff-rendered                 Output text differences for the rendered\n                                  versions of the models and standalone\n                                  audits.\n  --explain                       Explain the plan instead of applying it.\n  --ignore-cron                   Run all missing intervals, ignoring\n                                  individual cron schedules. Only applies if\n                                  --run is set.\n  --min-intervals INTEGER         For every model, ensure at least this many\n                                  intervals are covered by a missing intervals\n                                  check regardless of the plan start date\n  -v, --verbose                   Verbose output. Use -vv for very verbose\n                                  output.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#api","title":"api","text":"<pre><code>Usage: vulcan api [OPTIONS]\n\n  Start the Vulcan API server (models, metrics, lineage, telemetry).\n\nOptions:\n  --host TEXT        Bind socket to this host. Default: 0.0.0.0\n  --port INTEGER     Bind socket to this port. Default: 8000\n  --reload           Enable auto-reload on file changes. Default: False\n  --workers INTEGER  Number of worker processes. Default: 1\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#render","title":"render","text":"<pre><code>Usage: vulcan render [OPTIONS] MODEL\n\n  Render a model's query, optionally expanding referenced models.\n\nOptions:\n  -s, --start TEXT            The start datetime of the interval for which\n                              this command will be applied.\n  -e, --end TEXT              The end datetime of the interval for which this\n                              command will be applied.\n  --execution-time TEXT       The execution time (defaults to now).\n  --expand TEXT               Whether or not to expand materialized models\n                              (defaults to False). If True, all referenced\n                              models are expanded as raw queries. Multiple\n                              model names can also be specified, in which case\n                              only they will be expanded as raw queries.\n  --dialect TEXT              The SQL dialect to render the query as.\n  --no-format                 Disable fancy formatting of the query.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan render sales.daily_sales\n\nSELECT\n  CAST(\"raw_orders\".\"order_date\" AS TIMESTAMP) AS \"order_date\",\n  CAST(COUNT(\"raw_orders\".\"order_id\") AS INT) AS \"total_orders\",\n  CAST(SUM(\"raw_orders\".\"total_amount\") AS DOUBLE PRECISION) AS \"total_revenue\",\n  CAST(MAX(\"raw_orders\".\"order_id\") AS VARCHAR) AS \"last_order_id\",\n  COUNT(DISTINCT \"raw_orders\".\"product_id\") AS \"total_products\"\nFROM \"warehouse\".\"vulcan__raw\".\"raw__raw_orders__1032938324\" AS \"raw_orders\" /* warehouse.raw.raw_orders */\nGROUP BY\n  \"raw_orders\".\"order_date\"\nORDER BY\n  \"order_date\"\n</code></pre>"},{"location":"cli-command/cli/#rollback","title":"rollback","text":"<pre><code>Usage: vulcan rollback [OPTIONS]\n\n  Rollback Vulcan to the previous migration.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>rollback</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-command/cli/#run","title":"run","text":"<pre><code>Usage: vulcan run [OPTIONS] [ENVIRONMENT]\n\n  Evaluate missing intervals for the target environment.\n\nOptions:\n  -s, --start TEXT              The start datetime of the interval for which\n                                this command will be applied.\n  -e, --end TEXT                The end datetime of the interval for which\n                                this command will be applied.\n  --skip-janitor                Skip the janitor task.\n  --ignore-cron                 Run for all missing intervals, ignoring\n                                individual cron schedules.\n  --select-model TEXT           Select specific models to run. Note: this\n                                always includes upstream dependencies.\n  --exit-on-env-update INTEGER  If set, the command will exit with the\n                                specified code if the run is interrupted by an\n                                update to the target environment.\n  --no-auto-upstream            Do not automatically include upstream models.\n                                Only applicable when --select-model is used.\n                                Note: this may result in missing / invalid\n                                data for the selected models.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#state","title":"state","text":"<pre><code>Usage: vulcan state [OPTIONS] COMMAND [ARGS]...\n\n  Commands for interacting with state\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  export  Export the state database to a file\n  import  Import a state export file back into the state database\n</code></pre>"},{"location":"cli-command/cli/#export","title":"export","text":"<pre><code>Usage: vulcan state export [OPTIONS]\n\n  Export the state database to a file\n\nOptions:\n  -o, --output-file FILE  Path to write the state export to  [required]\n  --environment TEXT      Name of environment to export. Specify multiple\n                          --environment arguments to export multiple\n                          environments\n  --local                 Export local state only. Note that the resulting\n                          file will not be importable\n  --no-confirm            Do not prompt for confirmation before exporting\n                          existing state\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#import","title":"import","text":"<pre><code>Usage: vulcan state import [OPTIONS]\n\n  Import a state export file back into the state database\n\nOptions:\n  -i, --input-file FILE  Path to the state file  [required]\n  --replace              Clear the remote state before loading the file. If\n                         omitted, a merge is performed instead\n  --no-confirm           Do not prompt for confirmation before updating\n                         existing state\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#table_diff","title":"table_diff","text":"<pre><code>Usage: vulcan table_diff [OPTIONS] SOURCE:TARGET [MODEL]\n\n  Show the diff between two tables or a selection of models when they are\n  specified.\n\nOptions:\n  -o, --on TEXT            The column to join on. Can be specified multiple\n                           times. The model grain will be used if not\n                           specified.\n  -s, --skip-columns TEXT  The column(s) to skip when comparing the source and\n                           target table.\n  --where TEXT             An optional where statement to filter results.\n  --limit INTEGER          The limit of the sample dataframe.\n  --show-sample            Show a sample of the rows that differ. With many\n                           columns, the output can be very wide.\n  -d, --decimals INTEGER   The number of decimal places to keep when comparing\n                           floating point columns. Default: 3\n  --skip-grain-check       Disable the check for a primary key (grain) that is\n                           missing or is not unique.\n  --warn-grain-check       Warn if any selected model is missing a grain,\n                           and compute diffs for the remaining models.\n  --temp-schema TEXT       Schema used for temporary tables. It can be\n                           `CATALOG.SCHEMA` or `SCHEMA`. Default:\n                           `vulcan_temp`\n  -m, --select-model TEXT  Specify one or more models to data diff. Use\n                           wildcards to diff multiple models. Ex: '*' (all\n                           models with applied plan diffs), 'demo.model+'\n                           (this and downstream models),\n                           'git:feature_branch' (models with direct\n                           modifications in this branch only)\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#table_name","title":"table_name","text":"<pre><code>Usage: vulcan table_name [OPTIONS] MODEL_NAME\n\n  Prints the name of the physical table for the given model.\n\nOptions:\n  --environment, --env TEXT  The environment to source the model version from.\n  --prod                     If set, return the name of the physical table\n                             that will be used in production for the model\n                             version promoted in the target environment.\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#test","title":"test","text":"<pre><code>Usage: vulcan test [OPTIONS] [TESTS]...\n\n  Run model unit tests.\n\nOptions:\n  -k TEXT              Only run tests that match the pattern of substring.\n  -v, --verbose        Verbose output.\n  --preserve-fixtures  Preserve the fixture tables in the testing database,\n                       useful for debugging.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#semantic","title":"semantic","text":"<pre><code>Usage: vulcan semantic [OPTIONS] {export} [ENVIRONMENT]\n\n  Semantic layer operations.\n\n  This command provides semantic layer export functionality, allowing users to\n  convert semantic models and metrics into CubeJS-compatible YAML schemas.\n\nOptions:\n  -o, --output PATH   Output file path for the CubeJS schema.  [required]\n  --strict            Strict mode: export only explicitly defined semantic\n                      models.\n  --no-auto-measures  Disable automatic generation of measures (e.g., _count)\n                      for models with grains.\n  --no-confirm        Do not prompt for confirmation before overwriting\n                      existing output file.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#transpile","title":"transpile","text":"<pre><code>Usage: vulcan transpile [OPTIONS] [QUERY]\n\n  Transpile a semantic SQL or REST-style semantic query to executable SQL.\n\nOptions:\n  --format [sql|rest]        Input type: semantic SQL ('sql') or REST-style\n                             semantic payload ('rest').  [required]\n  --file TEXT                Read query or REST payload from file. Use '-' to\n                             read from stdin.\n  --user TEXT                User id to propagate in the X-User header\n                             (defaults to 'cli').\n  --disable-post-processing  Disable post-processing in the Transpiler.\n  --style [pretty|compact]   SQL output style: 'pretty' (formatted with\n                             indentation), 'compact' (unformatted but\n                             processed),\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#transpiler","title":"transpiler","text":"<pre><code>Usage: vulcan transpiler [OPTIONS] {up|down}\n\n  Manage the Transpiler service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#graphql","title":"graphql","text":"<pre><code>Usage: vulcan graphql [OPTIONS] {up|down}\n\n  Manage the GraphQL service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-command/cli/#lint","title":"lint","text":"<pre><code>Usage: vulcan lint [OPTIONS]\n  Run linter for the target model(s).\n\nOptions:\n  --model TEXT           A model to lint. Multiple models can be linted.  If no models are specified, every model will be linted.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/","title":"Custom materializations","text":""},{"location":"components/advanced-features/custom_materializations/#custom-materializations","title":"Custom materializations","text":"<p>Vulcan comes with a variety of model kinds that handle the most common ways to evaluate and materialize your data transformations. But what if you need something different?</p> <p>Sometimes, your specific use case doesn't quite fit any of the built-in model kinds. Maybe you need custom logic for how data gets inserted, or you want to implement a materialization strategy that's unique to your workflow. That's where custom materializations come in, they let you write your own Python code to control exactly how your models get materialized.</p> <p>Advanced Feature</p> <p>Custom materializations are powerful, but they're also advanced. Before diving in, make sure you've exhausted all other options. If you're considering this path, we'd love to hear from you in our community slack. If an existing model kind can solve your problem, we want to improve our docs; if a built-in kind is almost what you need, we might be able to enhance it for everyone.</p>"},{"location":"components/advanced-features/custom_materializations/#what-is-a-materialization","title":"What is a materialization?","text":"<p>Think of a materialization as the \"how\" behind your model execution. When Vulcan runs a model, it needs to figure out how to actually get that data into your database. The materialization is the set of methods that handle executing your transformation logic and managing the resulting data.</p> <p>Some materializations are straightforward. For example, a <code>FULL</code> model kind completely replaces the table each time it runs, so its materialization is essentially just <code>CREATE OR REPLACE TABLE [name] AS [your query]</code>. Simple!</p> <p>Other materializations are more complex. An <code>INCREMENTAL_BY_TIME_RANGE</code> model needs to figure out which time intervals to process, query only that data, and then merge it into the existing table. That requires more logic.</p> <p>The materialization logic can also vary by SQL engine. PostgreSQL doesn't support <code>CREATE OR REPLACE TABLE</code>, so <code>FULL</code> models on Postgres use <code>DROP</code> then <code>CREATE</code> instead. Vulcan handles all these engine-specific details for built-in model kinds, but with custom materializations, you're in control.</p>"},{"location":"components/advanced-features/custom_materializations/#how-custom-materializations-work","title":"How custom materializations work","text":"<p>Custom materializations are like creating your own model kind. You define them in Python, give them a name, and then reference that name in your model's <code>MODEL</code> block. They can even accept configuration arguments that you pass in from your model definition.</p> <p>Here's what every custom materialization needs:</p> <ul> <li>Python code: Written as a Python class</li> <li>Base class: Must inherit from Vulcan's <code>CustomMaterialization</code> class</li> <li>Insert method: At minimum, you need to implement the <code>insert</code> method</li> <li>Auto-loading: Vulcan automatically discovers materializations in your <code>materializations/</code> directory</li> </ul> <p>You can also:</p> <ul> <li>Override other methods from <code>MaterializableStrategy</code> or <code>EngineAdapter</code> classes</li> <li>Execute arbitrary SQL using the engine adapter</li> <li>Perform Python processing with Pandas or other libraries (though for most cases, you'd want that logic in a Python model instead)</li> </ul> <p>Vulcan will automatically load any Python files in your project's <code>materializations/</code> directory. Or, if you prefer, you can package your materialization as a Python package and install it like any other dependency.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-materialization","title":"Creating a custom materialization","text":"<p>To create a custom materialization, just add a <code>.py</code> file to your project's <code>materializations/</code> folder. Vulcan will automatically import all Python modules in this folder when your project loads, so your materializations will be ready to use.</p> <p>Your materialization class needs to inherit from <code>CustomMaterialization</code> and implement at least the <code>insert</code> method. Let's look at some examples to see how this works.</p>"},{"location":"components/advanced-features/custom_materializations/#simple-example","title":"Simple example","text":"<p>Here's a complete example that shows custom insert logic with some helpful logging:</p> <pre><code>import typing as t\nfrom sqlalchemy import text\nfrom vulcan import CustomMaterialization\nfrom vulcan import Model\n\nclass SimpleCustomMaterialization(CustomMaterialization):\n    \"\"\"Simple custom materialization - demonstrates custom insert logic\"\"\"\n\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Union[str, t.Any],\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        \"\"\"Custom insert logic for tables\"\"\"\n\n        print(f\"Custom materialization: Processing table {table_name}\")\n        print(f\"Model: {model.name}\")\n        print(f\"Is first insert: {is_first_insert}\")\n\n        if is_first_insert:\n            print(\"Creating table for the first time\")\n            # Create the table normally using the adapter\n            self.adapter.create_table(\n                table_name,\n                columns=model.columns_to_types,\n                target_columns_to_types=model.columns_to_types,\n                partitioned_by=model.partitioned_by,\n            )\n\n        # Insert data with custom logic\n        if isinstance(query_or_df, str):\n            print(\"Executing SQL query\")\n            # Execute the query - Vulcan provides the INSERT INTO ... SELECT query\n            self.adapter.execute(text(query_or_df))\n        else:\n            print(\"Inserting DataFrame\")\n            # Insert DataFrame normally - useful for Python models that return DataFrames\n            self.adapter.insert_append(table_name, query_or_df)\n\n        print(f\"Custom materialization completed for {table_name}\")\n</code></pre> <p>Let's break down what's happening here:</p> Component What It Does <code>NAME</code> The identifier you'll use in your model definition (like <code>simple_custom</code>) <code>table_name</code> The target table where your data will be inserted <code>query_or_df</code> Either a SQL query string or a DataFrame (works with Pandas, PySpark, Snowpark) <code>model</code> The full model definition object, gives you access to all model properties <code>is_first_insert</code> <code>True</code> if this is the first time inserting data for this model version <code>render_kwargs</code> Dictionary of arguments used to render the model query <code>self.adapter</code> The engine adapter, your interface to execute SQL and interact with the database"},{"location":"components/advanced-features/custom_materializations/#minimal-example","title":"Minimal example","text":"<p>If you just want a simple full-refresh materialization, here's the minimal version:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre> <p>That's it! This will completely replace the table contents each time the model runs, just like a <code>FULL</code> model kind.</p>"},{"location":"components/advanced-features/custom_materializations/#controlling-table-creation-and-deletion","title":"Controlling table creation and deletion","text":"<p>You can also customize how tables and views are created and deleted by overriding the <code>create</code> and <code>delete</code> methods:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(self, table_name: str, query_or_df: t.Any, model: Model, \n               is_first_insert: bool, render_kwargs: t.Dict[str, t.Any], **kwargs: t.Any) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n\n    def create(\n        self,\n        table_name: str,\n        model: Model,\n        is_table_deployable: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Custom table/view creation logic\n        # Uses self.adapter methods like create_table, create_view, or ctas\n        self.adapter.create_table(\n            table_name,\n            columns=model.columns_to_types,\n            target_columns_to_types=model.columns_to_types,\n        )\n\n    def delete(self, name: str, **kwargs: t.Any) -&gt; None:\n        # Custom table/view deletion logic\n        self.adapter.drop_table(name)\n</code></pre> <p>This gives you full control over the lifecycle of your data objects.</p>"},{"location":"components/advanced-features/custom_materializations/#using-a-custom-materialization","title":"Using a custom materialization","text":"<p>Once you've created your materialization, using it is straightforward. In your model definition, set the <code>kind</code> to <code>CUSTOM</code> and specify the <code>materialization</code> name (the <code>NAME</code> from your Python class):</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom'\n  ),\n  grain (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers c\nLEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name\nORDER BY total_spent DESC\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.custom_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.CUSTOM,\n        materialization=\"simple_custom\",\n    ),\n    grain=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Python model using custom materialization with dynamic dependencies\"\"\"\n\n    # Simple customer summary\n    query = \"\"\"\n    SELECT \n        c.customer_id,\n        c.name as customer_name,\n        COUNT(DISTINCT o.order_id) as total_orders,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_spent\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id, c.name\n    ORDER BY total_spent DESC\n    \"\"\"\n\n    # Execute query and return results\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#passing-properties-to-the-materialization","title":"Passing properties to the materialization","text":"<p>You can pass configuration to your materialization using <code>materialization_properties</code>. This is useful when you want to customize behavior per model:</p> <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom',\n    materialization_properties (\n      'config_key' = 'config_value',\n      'batch_size' = 1000\n    )\n  )\n);\n</code></pre> <p>Then access these properties in your materialization code via <code>model.custom_materialization_properties</code>:</p> <pre><code>class SimpleCustomMaterialization(CustomMaterialization):\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Access custom properties\n        config_value = model.custom_materialization_properties.get(\"config_key\")\n        batch_size = model.custom_materialization_properties.get(\"batch_size\", 500)\n\n        print(f\"Config value: {config_value}, Batch size: {batch_size}\")\n\n        # Proceed with insert logic\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre> <p>This lets you create flexible materializations that can adapt to different use cases.</p>"},{"location":"components/advanced-features/custom_materializations/#extending-customkind","title":"Extending <code>CustomKind</code>","text":"<p>Warning</p> <p>This is advanced territory. You're working with Vulcan's internals here, so there's extra complexity involved. If the basic custom materialization approach works for you, stick with that. Only dive into this if you really need the extra control.</p> <p>Most of the time, the standard custom materialization approach is all you need. But sometimes you want tighter integration with Vulcan's internals, maybe you need to validate custom properties before any database connections are made, or you want to leverage functionality that depends on specific properties being present.</p> <p>In those cases, you can create a subclass of <code>CustomKind</code> that Vulcan will use instead of the default. When your project loads, Vulcan will detect your subclass and use it instead of the standard <code>CustomKind</code>.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-kind","title":"Creating a custom kind","text":"<p>Here's how you'd create a custom kind that validates a <code>primary_key</code> property:</p> <pre><code>import typing as t\nfrom typing_extensions import Self\nfrom pydantic import model_validator\nfrom sqlglot import exp\nfrom vulcan import CustomKind\nfrom vulcan.utils.pydantic import list_of_fields_validator\nfrom vulcan.utils.errors import ConfigError\n\nclass MyCustomKind(CustomKind):\n\n    _primary_key: t.List[exp.Expression]\n\n    @model_validator(mode=\"after\")\n    def _validate_model(self) -&gt; Self:\n        self._primary_key = list_of_fields_validator(\n            self.materialization_properties.get(\"primary_key\"),\n            {\"dialect\": self.dialect}\n        )\n        if not self.primary_key:\n            raise ConfigError(\"primary_key must be specified\")\n        return self\n\n    @property\n    def primary_key(self) -&gt; t.List[exp.Expression]:\n        return self._primary_key\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#using-the-custom-kind-in-a-model","title":"Using the custom kind in a model","text":"<p>Use it in your model like this:</p> <pre><code>MODEL (\n  name vulcan_demo.my_model,\n  kind CUSTOM (\n    materialization 'my_custom_full',\n    materialization_properties (\n      primary_key = (col1, col2)\n    )\n  )\n);\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#linking-to-your-materialization","title":"Linking to your materialization","text":"<p>To connect your custom kind to your materialization, specify it as a generic type parameter:</p> <pre><code>class CustomFullMaterialization(CustomMaterialization[MyCustomKind]):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        assert isinstance(model.kind, MyCustomKind)\n\n        self.adapter.merge(\n            ...,\n            unique_key=model.kind.primary_key\n        )\n</code></pre> <p>When Vulcan loads your materialization, it inspects the type signature for generic parameters that are subclasses of <code>CustomKind</code>. If it finds one, it uses your subclass when building <code>model.kind</code> instead of the default.</p> <p>Why would you want this? Two main benefits:</p> <ul> <li>Early validation: Your <code>primary_key</code> validation happens at load time, not evaluation time. Issues get caught before you even create a plan.</li> <li>Type safety: <code>model.kind</code> resolves to your custom kind object, so you get access to extra properties without additional validation.</li> </ul>"},{"location":"components/advanced-features/custom_materializations/#sharing-custom-materializations","title":"Sharing custom materializations","text":"<p>Once you've built a custom materialization, you'll probably want to use it across multiple projects. You have a couple of options.</p>"},{"location":"components/advanced-features/custom_materializations/#copying-files","title":"Copying files","text":"<p>The simplest approach is to copy the materialization code into each project's <code>materializations/</code> directory. It works, but it's not the most maintainable approach, you'll need to manually update each copy when you make changes.</p> <p>If you go this route, we strongly recommend keeping the materialization code in version control and setting up a reliable way to notify users when updates are available.</p>"},{"location":"components/advanced-features/custom_materializations/#python-packaging","title":"Python packaging","text":"<p>A more robust approach is to package your materialization as a Python package. This is especially useful if you're using Airflow or other external schedulers where the scheduler cluster doesn't have direct access to your project's <code>materializations/</code> folder.</p> <p>Package your materialization using setuptools entrypoints:</p> pyproject.tomlsetup.py <pre><code>[project.entry-points.\"vulcan.materializations\"]\nmy_materialization = \"my_package.my_materialization:CustomFullMaterialization\"\n</code></pre> <pre><code>setup(\n    ...,\n    entry_points={\n        \"vulcan.materializations\": [\n            \"my_materialization = my_package.my_materialization:CustomFullMaterialization\",\n        ],\n    },\n)\n</code></pre> <p>Once the package is installed, Vulcan automatically discovers and loads your materialization from the entrypoint list. No manual configuration needed!</p> <p>For more details on Python packaging, check out the Vulcan GitHub custom_materializations example.</p>"},{"location":"components/advanced-features/signals/","title":"Signals","text":""},{"location":"components/advanced-features/signals/#signals","title":"Signals","text":"<p>Vulcan's built-in scheduler is pretty smart, it knows when to run your models based on their <code>cron</code> schedules. If you have a model set to run <code>@daily</code>, it checks whether a day has passed since the last run and evaluates the model if needed.</p> <p>But here's the thing: real-world data doesn't always follow our schedules. Sometimes data arrives late, maybe your upstream system had an issue, or a batch job ran behind schedule. When that happens, your daily model might have already run for the day, and that late data won't get processed until tomorrow's scheduled run.</p> <p>Signals solve this problem by letting you add custom conditions that must be met before a model runs. Think of them as extra gates that the scheduler checks, beyond just \"has enough time passed?\" and \"are upstream dependencies done?\"</p>"},{"location":"components/advanced-features/signals/#what-is-a-signal","title":"What is a signal?","text":"<p>By default, Vulcan's scheduler uses two criteria to decide if a model should run:</p> <ol> <li>Has the model's <code>cron</code> interval elapsed since the last evaluation?</li> <li>Have all upstream dependencies finished running?</li> </ol> <p>Signals let you add a third criterion: your own custom check. A signal is just a Python function that examines a batch of time intervals and decides whether they're ready for evaluation.</p> <p>Here's how it works under the hood: The scheduler doesn't actually evaluate \"a model\", it evaluates a model over specific time intervals. For incremental models, this is obvious (you're processing a date range). But even non-temporal models like <code>FULL</code> and <code>VIEW</code> are evaluated based on time intervals, their <code>cron</code> frequency determines the interval.</p> <p>The scheduler looks at candidate intervals, groups them into batches (controlled by your model's <code>batch_size</code> parameter), and then checks signals to see if those batches are ready. Your signal function gets called with a batch of time intervals and can return:</p> <ul> <li><code>True</code> if all intervals in the batch are ready</li> <li><code>False</code> if none are ready</li> <li>A list of specific intervals if only some are ready</li> </ul> <p>One model, multiple signals</p> <p>You can specify multiple signals for a single model. When you do, Vulcan requires that all signal functions agree an interval is ready before it gets evaluated. Think of it as an AND gate, every signal must give the green light.</p>"},{"location":"components/advanced-features/signals/#defining-a-signal","title":"Defining a signal","text":"<p>To create a signal, add a <code>signals</code> directory to your project and create your signal function in <code>__init__.py</code> (you can organize signals across multiple Python files if you prefer).</p> <p>A signal function needs to: - Accept a batch of time intervals (<code>DateTimeRanges: t.List[t.Tuple[datetime, datetime]]</code>) - Return either a boolean or a list of intervals - Use the <code>@signal</code> decorator</p> <p>Let's look at some examples, starting simple and building up to more complex use cases.</p>"},{"location":"components/advanced-features/signals/#simple-example","title":"Simple example","text":"<p>Here's a basic signal that randomly decides whether intervals are ready (useful for testing, maybe not so much for production!):</p> <pre><code>import random\nimport typing as t\nfrom vulcan import signal, DatetimeRanges\n\n\n@signal()\ndef random_signal(batch: DatetimeRanges, threshold: float) -&gt; t.Union[bool, DatetimeRanges]:\n    return random.random() &gt; threshold\n</code></pre> <p>This signal takes a <code>threshold</code> argument (you'll pass this from your model definition) and returns <code>True</code> if a random number exceeds that threshold. Notice how the function signature includes <code>threshold: float</code>, Vulcan will automatically extract this from your model definition and pass it to the function. The type inference works the same way as Vulcan macros.</p> <p>To use this signal in a model, add it to the <code>signals</code> key in your <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind FULL,\n  signals (\n    random_signal(threshold := 0.5), # specify threshold value\n  )\n);\n\nSELECT 1\n</code></pre> <p>The <code>signals</code> key accepts a list of signal calls, each with its own arguments. When you run <code>vulcan run</code>, this signal will essentially flip a coin, if the random number is greater than 0.5, the model runs; otherwise, it waits.</p>"},{"location":"components/advanced-features/signals/#advanced-example","title":"Advanced example","text":"<p>Sometimes you want more fine-grained control. Instead of saying \"all intervals are ready\" or \"none are ready,\" you can return specific intervals from the batch. Here's an example that only allows intervals from at least one week ago:</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges\nfrom vulcan.utils.date import to_datetime\n\n\n# signal that returns only intervals that are &lt;= 1 week ago\n@signal()\ndef one_week_ago(batch: DatetimeRanges) -&gt; t.Union[bool, DatetimeRanges]:\n    dt = to_datetime(\"1 week ago\")\n\n    return [\n        (start, end)\n        for start, end in batch\n        if start &lt;= dt\n    ]\n</code></pre> <p>Instead of returning <code>True</code> or <code>False</code> for the entire batch, this function filters the batch and returns only the intervals that meet the criteria. It compares each interval's start time to \"1 week ago\" and includes only those that are old enough.</p> <p>Use it in an incremental model like this:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column ds,\n  ),\n  start '2 week ago',\n  signals (\n    one_week_ago(),\n  )\n);\n\nSELECT @start_ds AS ds\n</code></pre> <p>This ensures that only data from at least a week ago gets processed, useful if you want to wait for late-arriving data to stabilize before processing it.</p>"},{"location":"components/advanced-features/signals/#accessing-execution-context","title":"Accessing execution context","text":"<p>Sometimes you need to check something in your database or access the execution context. You can do that by adding a <code>context</code> parameter to your signal function:</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges, ExecutionContext\n\n\n# add the context argument to your function\n@signal()\ndef one_week_ago(batch: DatetimeRanges, context: ExecutionContext) -&gt; t.Union[bool, DatetimeRanges]:\n    return len(context.engine_adapter.fetchdf(\"SELECT 1\")) &gt; 1\n</code></pre> <p>The <code>context</code> parameter gives you access to the engine adapter, so you can query your warehouse, check if certain tables exist, verify data freshness, or perform any other checks you need.</p>"},{"location":"components/advanced-features/signals/#testing-signals","title":"Testing signals","text":"<p>Signals only evaluate when you run <code>vulcan run</code> or use the <code>check_intervals</code> command. To test your signals without actually running models:</p> <ol> <li>Deploy your changes to an environment: <code>vulcan plan my_dev</code></li> <li>Check which intervals would be evaluated: <code>vulcan check_intervals my_dev</code>    - Use <code>--select-model</code> to check specific models    - Use <code>--no-signals</code> to see what would run without signal checks</li> <li>Iterate by making changes to your signal and redeploying</li> </ol> <p>Note</p> <p>The <code>check_intervals</code> command only works with remote models that have been deployed to an environment. Local signal changes won't be tested until you deploy them.</p> <p>This workflow lets you verify your signal logic before it affects your actual model runs.</p>"},{"location":"components/advanced-features/macros/built_in/","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#built-in","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros work differently than templating systems like Jinja. Here's the key difference: templating systems are all about string substitution, they scan your code, find special characters, and replace them with other text. That's their whole job.</p> <p>Templating systems are intentionally language-agnostic. They work for blog posts, HTML, SQL, or pretty much anything. They have control flow (if-then, loops) and other features, but those are just tools to help them substitute the right strings.</p> <p>Vulcan macros are different. They're built specifically for SQL, and they understand what your SQL actually means. Instead of just swapping strings, Vulcan macros analyze your SQL using the sqlglot library to build a semantic representation of your query. Then they modify that representation. This means they can do things templating systems can't, like knowing whether something is a column name or a string literal, or understanding the structure of your query.</p> <p>Plus, you can write macro logic in Python, which gives you way more power than simple string substitution.</p>"},{"location":"components/advanced-features/macros/built_in/#how-vulcan-macros-work","title":"How Vulcan macros work","text":"<p>This section explains what happens under the hood when Vulcan processes your macros. You don't need to read this to use macros, but it's super helpful when you're debugging something that's not working as expected. Feel free to skip it and come back later if you need to.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</li> <li>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</li> <li>Macro functions, both Vulcan's and user-defined</li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"components/advanced-features/macros/built_in/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></li> <li>Identifiers, which reference database objects like column, table, alias, and function names<ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint, and local.</p> <p>Here's how they're organized: - Global and gateway variables are defined in your project configuration file and can be used in any model - Blueprint and local variables are defined in a specific model and only work in that model</p> <p>What happens if you have variables with the same name at different levels? The most specific one wins. Local variables override blueprint or gateway variables, gateway variables override global variables, and so on. This lets you set defaults globally but override them when needed.</p>"},{"location":"components/advanced-features/macros/built_in/#global-variables","title":"Global variables","text":"<p>Global variables live in your project configuration file under the <code>variables</code> key. They're perfect for values you want to use across multiple models.</p> <p>You can store numbers (<code>int</code>, <code>float</code>), booleans (<code>bool</code>), strings (<code>str</code>), or even lists and dictionaries containing these types.</p> <p>Access them in your models using either <code>@VAR_NAME</code> (the simple syntax) or <code>@VAR('var_name')</code> (the function syntax). The function syntax is handy because you can provide a default value as the second argument, useful if the variable might not be defined.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"components/advanced-features/macros/built_in/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"components/advanced-features/macros/built_in/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system comes with a bunch of operators that let you add dynamic behavior to your models. These are the built-in tools that make your SQL adapt to different situations.</p>"},{"location":"components/advanced-features/macros/built_in/#each","title":"@EACH","text":"<p><code>@EACH</code> is like a <code>for</code> loop for your SQL. It takes a list of items and applies a function to each one, transforming them into whatever you need.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>You can put macro values at the end of a column name using <code>column_@x</code>, but if you want to put the variable anywhere else in the identifier, use curly braces <code>@{}</code> to avoid confusion. For example: <code>@{x}_column</code> or <code>my_@{x}_column</code> work great.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"components/advanced-features/macros/built_in/#if","title":"@IF","text":"<p><code>@IF</code> lets you conditionally include parts of your SQL based on a logical condition. It's like an if-then statement, but for your query.</p> <p>It has three parts:</p> <ol> <li>A condition that evaluates to <code>TRUE</code> or <code>FALSE</code> (written in SQL)</li> <li>What to return if the condition is <code>TRUE</code></li> <li>What to return if the condition is <code>FALSE</code> (this is optional, if you omit it and the condition is false, nothing gets included)</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</li> <li>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</li> <li>Between: <code>[number] BETWEEN [low number] AND [high number]</code></li> <li>Membership: <code>[item] IN ([comma-separated list of items])</code></li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li><code>'a' = 'a'</code></li> <li><code>'a' != 'b'</code></li> <li><code>0 &lt; 1</code></li> <li><code>1 &gt;= 1</code></li> <li><code>2 BETWEEN 1 AND 3</code></li> <li><code>'a' IN ('a', 'b')</code></li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"components/advanced-features/macros/built_in/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"components/advanced-features/macros/built_in/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The relation/table whose columns are being selected</li> <li><code>alias</code> (optional): The alias of the relation (if it has one)</li> <li><code>exclude</code> (optional): A list of columns to exclude</li> <li><code>prefix</code> (optional): A string to use as a prefix for all selected column names</li> <li><code>suffix</code> (optional): A string to use as a suffix for all selected column names</li> <li><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</li> <li>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</li> <li>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</li> <li>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></li> <li>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</li> <li><code>my_column</code> is present in the query</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by: 1. <code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type) 2. Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column 3. Concatenating the column values after steps (1) and (2) 4. Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</li> <li><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>lat1</code>: Latitude of the first point</li> <li><code>lon1</code>: Longitude of the first point</li> <li><code>lat2</code>: Latitude of the second point</li> <li><code>lon2</code>: Longitude of the second point</li> <li><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>column</code>: The column to pivot</li> <li><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</li> <li><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</li> <li><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></li> <li><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></li> <li><code>prefix</code> (optional): A prefix to use for all aliases</li> <li><code>suffix</code> (optional): A suffix to use for all aliases</li> <li><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></li> <li><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></li> <li><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</li> <li><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The table or CTE name to deduplicate</li> <li><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</li> <li><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</li> <li><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</li> <li><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>. - For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</p>"},{"location":"components/advanced-features/macros/built_in/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</li> <li>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li><code>template</code> - The string template to render into an AST node</li> <li><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></li> <li><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></li> <li><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li><code>@WITH</code>: common table expression <code>WITH</code> clause</li> <li><code>@JOIN</code>: table <code>JOIN</code> clause(s)</li> <li><code>@WHERE</code>: filtering <code>WHERE</code> clause</li> <li><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</li> <li><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</li> <li><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</li> <li><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"components/advanced-features/macros/built_in/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"components/advanced-features/macros/built_in/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>Macro functions let you write reusable logic that you can call from multiple models. Instead of copying the same code everywhere, you define it once and reuse it.</p> <p>Vulcan supports macro functions in two languages:</p> <ul> <li>SQL functions use the Jinja templating system</li> <li>Python functions use SQLGlot and give you way more power, you can do complex operations that go beyond what variables and operators can handle alone</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#python-macro-functions","title":"Python macro functions","text":""},{"location":"components/advanced-features/macros/built_in/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"components/advanced-features/macros/built_in/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li><code>@add_args(1)</code> returns 1</li> <li><code>@add_args(1, 2)</code> returns 3</li> <li><code>@add_args(1, 2, 3)</code> returns 6</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li>Numbers are parsed into <code>Literal</code> expressions</li> <li>Quoted strings are parsed into <code>Literal</code> expressions</li> <li>Unquoted strings are parsed into <code>Column</code> expressions</li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></li> <li>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in the <code>dev</code> environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in any environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</li> <li>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</li> <li>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#typed-macros","title":"Typed macros","text":"<p>Typed macros bring Python's type hinting to your SQL macros. By specifying what types your macro expects, you make your code more readable, easier to maintain, and less prone to errors. Plus, IDEs can give you better autocomplete and catch mistakes before you run your code.</p>"},{"location":"components/advanced-features/macros/built_in/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"components/advanced-features/macros/built_in/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"components/advanced-features/macros/built_in/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</li> <li><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li><code>exp.Table</code></li> <li><code>exp.Column</code></li> <li><code>exp.Literal</code></li> <li><code>exp.Identifier</code></li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"components/advanced-features/macros/built_in/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p> <p>See examples of the coercion function in action in the test suite here.</p>"},{"location":"components/advanced-features/macros/built_in/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"components/advanced-features/macros/built_in/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan macros and Jinja macros, but we strongly recommend picking one system per model. If you mix them, things can get confusing or break in unexpected ways. Pick the one that fits your needs and stick with it.</p>"},{"location":"components/advanced-features/macros/jinja/","title":"Jinja","text":""},{"location":"components/advanced-features/macros/jinja/#jinja","title":"Jinja","text":"<p>Vulcan supports macros from the Jinja templating system. If you're already familiar with Jinja (maybe from dbt or other tools), you'll feel right at home here.</p> <p>Jinja works differently than Vulcan's native macros. While Vulcan macros understand the semantic structure of your SQL, Jinja macros are pure string substitution, they assemble SQL text by replacing placeholders, without building a semantic representation of the query.</p> <p>dbt compatibility</p> <p>Vulcan supports the standard Jinja function library, but not dbt-specific functions like <code>{{ ref() }}</code>. If you're working with a dbt project using the Vulcan adapter, dbt-specific functions will work there, but not in native Vulcan projects.</p>"},{"location":"components/advanced-features/macros/jinja/#the-basics","title":"The basics","text":"<p>Jinja uses curly braces <code>{}</code> to mark macro code. The second character after the opening brace tells Jinja what to do:</p> <ul> <li><code>{{...}}</code> - Expressions: These get replaced with values in your rendered SQL. Use them for variables and function calls.</li> <li><code>{%...%}</code> - Statements: These control flow and logic. Use them for <code>if</code> statements, <code>for</code> loops, and setting variables.</li> <li><code>{#...#}</code> - Comments: These are stripped out and won't appear in your final SQL.</li> </ul> <p>Since Jinja syntax isn't valid SQL, you need to wrap your Jinja queries in special blocks so Vulcan knows to process them. For queries, use <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>For pre/post-statements (code that runs before or after your query), use <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#using-vulcans-predefined-variables","title":"Using Vulcan's predefined variables","text":"<p>You can use all of Vulcan's predefined macro variables in your Jinja code. Some give you information about the Vulcan project itself (like <code>runtime_stage</code> or <code>this_model</code>), while others are temporal (like <code>start_ds</code> and <code>execution_date</code> for incremental models).</p> <p>Access them by putting the variable name (unquoted) inside curly braces:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Notice the single quotes around the variable references? That's because <code>start_ds</code> and <code>end_ds</code> return string values. For numeric variables like <code>start_epoch</code>, you wouldn't need the quotes.</p> <p>One special case: the <code>gateway</code> variable is a function call, so you need parentheses: <code>{{ gateway() }}</code> instead of just <code>{{ gateway }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#user-defined-variables","title":"User-defined variables","text":"<p>Beyond the predefined variables, you can create your own. Vulcan supports global variables (defined in your project config) and local variables (defined in a specific model).</p>"},{"location":"components/advanced-features/macros/jinja/#global-variables","title":"Global variables","text":"<p>Global variables are defined in your project configuration file and can be used in any model. Learn more about setting them up in the Vulcan macros documentation.</p> <p>Access them using the <code>{{ var() }}</code> function. Pass the variable name (in single quotes) as the first argument, and optionally a default value as the second:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>If the variable might not exist, provide a default:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre> <p>If <code>missing_var</code> isn't defined, this will use <code>0</code> as the fallback value.</p>"},{"location":"components/advanced-features/macros/jinja/#gateway-variables","title":"Gateway variables","text":"<p>Gateway variables work just like global variables, but they're defined in a specific gateway's configuration. They take precedence over global variables with the same name. Learn more in the Vulcan macros documentation.</p> <p>Access them the same way as global variables using <code>{{ var() }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables let you create model templates. They're defined in the <code>MODEL</code> block and can be used to generate multiple models from one template:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Use <code>{{ blueprint_var() }}</code> to access them, with an optional default value just like <code>{{ var() }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#local-variables","title":"Local variables","text":"<p>Define variables that are only available in the current model using <code>{% set ... %}</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>The <code>{% set %}</code> statement goes after the <code>MODEL</code> block and before your SQL query.</p> <p>Jinja variables can be strings, numbers, or even complex data structures like lists, tuples, or dictionaries. They support Python methods too, so you can call <code>.upper()</code> on strings, iterate over lists, and so on.</p>"},{"location":"components/advanced-features/macros/jinja/#control-flow","title":"Control flow","text":"<p>Jinja gives you control flow operators to make your SQL dynamic.</p>"},{"location":"components/advanced-features/macros/jinja/#for-loops","title":"For loops","text":"<p>For loops let you iterate over collections to generate repetitive SQL. They start with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>.</p> <p>Here's an example that creates indicator columns for different vehicle types:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  {% for vehicle_type in ['car', 'truck', 'bus'] %}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>A few things to notice: - The values in the list are quoted: <code>['car', 'truck', 'bus']</code> - When you use <code>{{ vehicle_type }}</code> in the <code>CASE WHEN</code>, you need quotes around it: <code>'{{ vehicle_type }}'</code> - When you use it in an identifier name like <code>vehicle_{{ vehicle_type }}</code>, no quotes needed - There's a trailing comma after the <code>CASE WHEN</code> line, Vulcan's semantic understanding will remove it automatically</p> <p>This renders to:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>It's usually better to define your lists separately:</p> <pre><code>JINJA_QUERY_BEGIN;\n\n{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types %}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>Same result, but easier to maintain.</p>"},{"location":"components/advanced-features/macros/jinja/#if-statements","title":"If statements","text":"<p>If statements let you conditionally include SQL based on some condition. They start with <code>{% if ... %}</code> and end with <code>{% endif %}</code>.</p> <p>The condition needs to evaluate to <code>True</code> or <code>False</code>. Things like <code>True</code>, <code>1 + 1 == 2</code>, or <code>'a' in ['a', 'b']</code> all work.</p> <p>Here's an example that conditionally includes a testing column:</p> <pre><code>JINJA_QUERY_BEGIN;\n\n{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>Since <code>testing</code> is <code>True</code>, this renders to:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>Macro functions let you reuse code across multiple models. Define them in <code>.sql</code> files in your project's <code>macros</code> directory (you can put multiple functions in one file or split them up).</p> <p>Define a function with <code>{% macro %}</code> and <code>{% endmacro %}</code>:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>Call it in your model with <code>{{ print_text() }}</code>, and it gets replaced with <code>text</code>.</p> <p>Functions can take arguments:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>Use it like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n\nJINJA_END;\n</code></pre> <p>This renders to:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Notice that even though you quoted the arguments in the function call, they're not quoted in the output. Vulcan's semantic understanding recognizes that <code>item_id</code> is a column name and handles it appropriately.</p> <p>If you want to select a string literal instead of a column, use double quotes around the string in the function call:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n\nJINJA_END;\n</code></pre> <p>This renders to:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes tell Vulcan \"this is a string literal, not a column name.\" You can also use <code>'\"item_id\"'</code> if you want double quotes in the output (useful for some SQL dialects).</p>"},{"location":"components/advanced-features/macros/jinja/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Jinja and Vulcan macros, but we strongly recommend picking one system per model. Mixing them can lead to confusing behavior or errors.</p> <p>You can use predefined Vulcan macro variables in Jinja queries, but if you're passing them as arguments to a Jinja macro function, use the Jinja syntax <code>{{ start_ds }}</code> instead of the Vulcan <code>@start_ds</code> syntax. You may need to add quotes depending on what you're doing.</p>"},{"location":"components/advanced-features/macros/overview/","title":"Overview","text":""},{"location":"components/advanced-features/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language, which means you describe what you want, not how to get it. That's great for clarity, but it also means SQL doesn't have built-in features like variables or control flow (if-then statements, loops) that let your queries adapt to different situations.</p> <p>The problem? Data pipelines are dynamic. You need different behavior depending on context, maybe filter by a different date each day, or include different columns based on configuration. That's where macros come in.</p> <p>Macros let you make your SQL dynamic. Instead of hardcoding values, you can use variables that get substituted at runtime. Instead of writing repetitive code, you can use functions that generate SQL for you.</p> <p>Vulcan supports two macro systems, each with its own strengths:</p> <ul> <li>Vulcan macros: Built specifically for SQL, with semantic understanding of your queries</li> <li>Jinja macros: The popular templating system, great if you're already familiar with it</li> </ul> <p>Both systems can use the same pre-defined variables that Vulcan provides, like <code>@execution_ds</code> for the current execution date or <code>@this_model</code> for the current model name.</p> <p>Ready to dive in? Here's where to go next:</p> <ul> <li>Pre-defined macro variables - Built-in variables available in both systems</li> <li>Vulcan macros - Vulcan's native macro system with SQL-aware features</li> <li>Jinja macros - The Jinja templating system for SQL</li> </ul>"},{"location":"components/advanced-features/macros/variables/","title":"Variables","text":""},{"location":"components/advanced-features/macros/variables/#variables","title":"Variables","text":"<p>Macro variables are placeholders that get replaced with actual values when Vulcan renders your SQL. They're what make your queries dynamic, instead of hardcoding values, you use variables that change based on context.</p> <p>Think of them like this: instead of writing <code>WHERE date &gt; '2023-01-01'</code> and manually updating it every day, you write <code>WHERE date &gt; @execution_ds</code> and it automatically uses today's date. Much better!</p> <p>Note</p> <p>This page covers Vulcan's built-in macro variables, the ones that come pre-configured and ready to use. If you want to create your own custom variables, check out the Vulcan macros page or Jinja macros page.</p>"},{"location":"components/advanced-features/macros/variables/#a-quick-example","title":"A quick example","text":"<p>Let's say you have a query that filters by date. Without macros, you'd write something like this:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>Every time you want to change the date, you have to edit the query. That's tedious and error-prone.</p> <p>With a macro variable, you can make it dynamic:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan \"this is a macro variable, replace it with a value before executing.\" The <code>@execution_ds</code> variable is predefined, so Vulcan automatically sets it to the date when execution started.</p> <p>If you run this model on February 1, 2023, Vulcan renders it as:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>The date updates automatically every time you run it. No manual editing needed!</p> <p>Vulcan comes with a bunch of predefined variables like this. You can also create your own custom variables if you need something specific, we'll cover those in the macro system pages.</p>"},{"location":"components/advanced-features/macros/variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan provides a set of predefined variables that are automatically available in your models. Most of them are related to time (dates, timestamps, etc.), which makes sense since time-based logic is super common in data pipelines.</p> <p>The time variables follow a consistent naming pattern: they combine a prefix (like <code>start</code>, <code>end</code>, or <code>execution</code>) with a postfix (like <code>ds</code>, <code>ts</code>, or <code>epoch</code>) to create variables like <code>@start_ds</code> or <code>@execution_epoch</code>.</p>"},{"location":"components/advanced-features/macros/variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses Python's datetime module under the hood and follows the standard Unix epoch (starting January 1, 1970).</p> <p>Important</p> <p>All time-related predefined variables use UTC time zone. If you need to work with other timezones, you'll handle that in your query logic.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes tell you what time period the variable represents:</p> <ul> <li><code>start</code> - The beginning of the time interval for this model run (inclusive)</li> <li><code>end</code> - The end of the time interval for this model run (inclusive)</li> <li><code>execution</code> - The exact timestamp when the execution started</li> </ul> <p>Postfixes tell you what format the value is in:</p> <ul> <li><code>dt</code> - A Python datetime object that becomes a SQL <code>TIMESTAMP</code></li> <li><code>dtntz</code> - A Python datetime object that becomes a SQL <code>TIMESTAMP WITHOUT TIME ZONE</code></li> <li><code>date</code> - A Python date object that becomes a SQL <code>DATE</code></li> <li><code>ds</code> - A date string formatted as <code>'YYYY-MM-DD'</code> (like <code>'2023-02-01'</code>)</li> <li><code>ts</code> - An ISO 8601 datetime string: <code>'YYYY-MM-DD HH:MM:SS'</code></li> <li><code>tstz</code> - An ISO 8601 datetime string with timezone: <code>'YYYY-MM-DD HH:MM:SS+00:00'</code></li> <li><code>hour</code> - An integer from 0-23 representing the hour of the day</li> <li><code>epoch</code> - An integer representing seconds since Unix epoch</li> <li><code>millis</code> - An integer representing milliseconds since Unix epoch</li> </ul> <p>Here are all the temporal variables you can use:</p> <p>dt (datetime objects): - <code>@start_dt</code> - <code>@end_dt</code> - <code>@execution_dt</code></p> <p>dtntz (datetime without timezone): - <code>@start_dtntz</code> - <code>@end_dtntz</code> - <code>@execution_dtntz</code></p> <p>date (date objects): - <code>@start_date</code> - <code>@end_date</code> - <code>@execution_date</code></p> <p>ds (date strings): - <code>@start_ds</code> - <code>@end_ds</code> - <code>@execution_ds</code></p> <p>ts (timestamp strings): - <code>@start_ts</code> - <code>@end_ts</code> - <code>@execution_ts</code></p> <p>tstz (timestamp strings with timezone): - <code>@start_tstz</code> - <code>@end_tstz</code> - <code>@execution_tstz</code></p> <p>hour (hour integers): - <code>@start_hour</code> - <code>@end_hour</code> - <code>@execution_hour</code></p> <p>epoch (Unix epoch seconds): - <code>@start_epoch</code> - <code>@end_epoch</code> - <code>@execution_epoch</code></p> <p>millis (Unix epoch milliseconds): - <code>@start_millis</code> - <code>@end_millis</code> - <code>@execution_millis</code></p>"},{"location":"components/advanced-features/macros/variables/#runtime-variables","title":"Runtime variables","text":"<p>Beyond time, Vulcan provides variables that give you information about the current execution context:</p> <ul> <li><code>@runtime_stage</code> - A string telling you what stage Vulcan is currently in. Useful for conditionally running code based on whether you're creating tables, evaluating queries, or promoting views. Possible values:</li> <li><code>'loading'</code> - Project is being loaded into Vulcan's runtime</li> <li><code>'creating'</code> - Model tables are being created for the first time</li> <li><code>'evaluating'</code> - Model query is being evaluated and data inserted</li> <li><code>'promoting'</code> - Model is being promoted (view created in virtual layer)</li> <li><code>'demoting'</code> - Model is being demoted (view dropped from virtual layer)</li> <li><code>'auditing'</code> - Audit is being run</li> <li><code>'testing'</code> - Model is being evaluated in a unit test context</li> </ul> <p>Learn more about using this in pre/post-statements.</p> <ul> <li> <p><code>@gateway</code> - The name of the current gateway (your database connection)</p> </li> <li> <p><code>@this_model</code> - The physical table name that the model's view selects from. Handy for creating generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</p> </li> <li> <p><code>@model_kind_name</code> - The name of the current model kind (like <code>'FULL'</code> or <code>'INCREMENTAL_BY_TIME_RANGE'</code>). Useful when you need to control physical properties in model defaults based on the model kind.</p> </li> </ul> <p>Embedding variables in strings</p> <p>Sometimes you'll see variables written with curly braces like <code>@{variable}</code> instead of just <code>@variable</code>. They do different things!</p> <p>The curly brace syntax tells Vulcan to treat the rendered value as a SQL identifier (like a table or column name), not a string literal. So if <code>variable</code> contains <code>foo.bar</code>, then: - <code>@variable</code> produces <code>foo.bar</code> (as a literal value) - <code>@{variable}</code> produces <code>\"foo.bar\"</code> (as an identifier, with quotes)</p> <p>You'll most often use <code>@{variable}</code> when you want to interpolate a value into an identifier name, like <code>@{schema}_table</code>. The regular <code>@variable</code> syntax is for plain value substitution.</p> <p>Learn more in the Vulcan macros documentation.</p>"},{"location":"components/advanced-features/macros/variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>These variables are available in <code>before_all</code> and <code>after_all</code> statements, as well as in any macros called within those statements:</p> <ul> <li><code>@this_env</code> - The name of the current environment</li> <li><code>@schemas</code> - A list of schema names in the virtual layer for the current environment</li> <li><code>@views</code> - A list of view names in the virtual layer for the current environment</li> </ul> <p>These are handy when you need to perform setup or cleanup operations that depend on the environment context.</p>"},{"location":"components/audits/audits/","title":"Audits","text":""},{"location":"components/audits/audits/#audits","title":"Audits","text":"<p>Audits are your data quality bouncers, they stop bad data at the door before it can cause problems downstream. Think of them as strict validators that run after every model execution and halt your pipeline if something's wrong.</p> <p>Unlike tests (which you run manually to verify logic), audits run automatically whenever you apply a plan. They're perfect for catching data quality issues early, whether they come from external vendors, upstream teams, or your own model changes.</p> <p>Here's the key thing: All audits in Vulcan are blocking. When an audit fails, Vulcan stops everything, no plan application, no run execution, nothing. This might sound strict, but it's actually a good thing. It prevents bad data from propagating through your entire pipeline and causing headaches later.</p> <p>A comprehensive suite of audits helps you catch problems upstream, builds trust in your data across the organization, and lets your team work with confidence knowing that invalid data won't slip through.</p> <p>Note: For incremental by time range models, audits only run on the intervals being processed, not the entire table. This keeps things fast and focused on what actually changed.</p>"},{"location":"components/audits/audits/#terminology-audits-and-assertions","title":"Terminology: Audits and Assertions","text":"<p>Before we dive in, let's clear up some terminology. Vulcan uses two related but distinct concepts:</p> <ul> <li>AUDIT - The validation rule itself (the SQL query that checks for problems)</li> <li>ASSERTION - Attaching an audit to a model (claiming it should pass)</li> </ul> <p>Think of it this way: an audit is the rule (\"prices must be positive\"), and an assertion is you saying \"this model follows that rule.\"</p> <p>In MODEL definitions:</p> <pre><code>-- Define the AUDIT (the rule)\nAUDIT (name check_positive_price);\nSELECT * FROM @this_model WHERE price &lt;= 0;\n\n-- Make ASSERTIONS about your model (attach the audit)\nMODEL (\n  name products,\n  assertions (check_positive_price)  -- Declaring this audit should pass\n);\n</code></pre> <p>Note: You might see older code using <code>audits</code> instead of <code>assertions</code> in MODEL definitions. Both work identically, but <code>assertions</code> is clearer, you're asserting that your model passes these audits. This documentation uses <code>assertions</code> throughout.</p>"},{"location":"components/audits/audits/#how-audits-work","title":"How Audits Work","text":"<p>When an audit fails, Vulcan stops everything. No ifs, ands, or buts. This is by design, it's better to catch problems early than to let bad data flow downstream and cause bigger issues.</p> <p>Here's what happens when you run a model:</p> <ol> <li>Evaluate the model - Vulcan runs your model SQL (inserts new data, rebuilds the table, etc.)</li> <li>Run the audit query - Vulcan executes your audit SQL against the newly updated table. For incremental models, this only checks the intervals you're processing (keeps things fast!)</li> <li>Check the results - If the query returns any rows, the audit fails and everything stops</li> </ol> <p>Why this matters: Audits query for bad data. If your audit finds bad data (returns rows), that's a problem. If it finds nothing (returns zero rows), you're good to go.</p>"},{"location":"components/audits/audits/#plan-vs-run","title":"Plan vs. Run","text":"<p>The difference between <code>plan</code> and <code>run</code> matters a lot when it comes to audits:</p> <p><code>plan</code> - The safe way: - Vulcan evaluates and audits all modified models before promoting them to production - If an audit fails, the plan stops and your production table is untouched - Invalid data stays in an isolated table and never reaches production - This is like testing in a sandbox before deploying</p> <p><code>run</code> - The direct way: - Vulcan evaluates and audits models directly against the production environment - If an audit fails, the run stops, but the invalid data is already in production - The blocking prevents this bad data from being used to build downstream models - This is like deploying directly, faster, but riskier</p> <p>Which should you use? For production changes, use <code>plan</code>. It's safer and gives you a chance to fix issues before they hit production. Use <code>run</code> when you're confident or doing quick iterations.</p>"},{"location":"components/audits/audits/#fixing-a-failed-audit","title":"Fixing a Failed Audit","text":"<p>So an audit failed. Don't panic! Here's how to fix it:</p> <ol> <li> <p>Find the root cause - Look at the audit query results. What data failed? Check upstream models and data sources.</p> </li> <li> <p>Fix the source - This depends on where the problem came from:    - External data source? Fix it at the source, then run a restatement plan on the first Vulcan model that ingests it. This will restate all downstream models automatically.    - Vulcan model? Update the model's logic, then apply the change with a <code>plan</code>. Vulcan will automatically re-evaluate all downstream models.</p> </li> </ol> <p>The key is fixing the root cause, not just the symptom. If bad data is coming from upstream, fixing it downstream won't help long-term.</p>"},{"location":"components/audits/audits/#user-defined-audits","title":"User-Defined Audits","text":"<p>You can write your own audits! They're just SQL queries that should return zero rows. If they return rows, that means they found bad data and the audit fails.</p> <p>Audits live in <code>.sql</code> files in an <code>audits</code> directory in your project. You can put multiple audits in one file (organize them however makes sense) or define them inline in your model files.</p>"},{"location":"components/audits/audits/#your-first-audit","title":"Your First Audit","text":"<p>Let's create a simple audit. Here's the basic structure:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  dialect spark\n);\nSELECT * from sushi.items\nWHERE\n  ds BETWEEN @start_ds AND @end_ds\n  AND price IS NULL;\n</code></pre> <p>This audit checks that every sushi item has a price. If any items are missing prices (the query returns rows), the audit fails.</p> <p>A few things to note: - The <code>name</code> is what you'll reference when attaching it to a model - If your query uses a different SQL dialect than your project, specify it with <code>dialect</code> (like <code>spark</code> in the example) - The <code>@start_ds</code> and <code>@end_ds</code> macros are automatically filled in for incremental models</p> <p>To actually use this audit, attach it to a model:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (assert_item_price_is_not_null)\n);\n</code></pre> <p>Now this audit runs every time the <code>sushi.items</code> model runs. Pretty straightforward!</p>"},{"location":"components/audits/audits/#generic-audits","title":"Generic Audits","text":"<p>Here's where audits get really powerful. You can create parameterized audits that work across multiple models. This saves you from writing the same audit over and over.</p> <p>Consider this audit that checks if a column exceeds a threshold:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>This uses macros to make it flexible: - <code>@this_model</code> is a special macro that refers to the model being audited (and handles incremental models correctly) - <code>@column</code> and <code>@threshold</code> are parameters you'll specify when you use the audit</p> <p>Now you can use this same audit for different columns and thresholds:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    does_not_exceed_threshold(column := id, threshold := 1000),\n    does_not_exceed_threshold(column := price, threshold := 100)\n  )\n);\n</code></pre> <p>See how we're using the same audit twice with different parameters? That's the power of generic audits. You can even use the same audit multiple times on the same model with different parameters.</p> <p>Default values:</p> <p>You can set default values for parameters:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold,\n  defaults (\n    threshold = 10,\n    column = id\n  )\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>Now if someone uses the audit without specifying parameters, it'll use these defaults. Handy for common cases!</p> <p>Global audits:</p> <p>You can also apply audits globally using model defaults:</p> <pre><code>model_defaults:\n  assertions:\n    - assert_positive_order_ids\n    - does_not_exceed_threshold(column := id, threshold := 1000)\n</code></pre> <p>This applies these audits to all models by default. Useful for organization-wide rules!</p> <p>Note: In <code>model_defaults</code>, you can use either <code>audits</code> or <code>assertions</code>, both work for backward compatibility.</p>"},{"location":"components/audits/audits/#naming","title":"Naming","text":"<p>Avoid SQL keywords when naming audit parameters. If you must use a keyword, quote it.</p> <p>For example, if your audit uses a <code>values</code> parameter (which is a SQL keyword), you'll need quotes:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    my_audit(column := a, \"values\" := (1,2,3))\n  )\n)\n</code></pre> <p>It's easier to just avoid keywords in the first place, but if you need them, quotes work fine.</p>"},{"location":"components/audits/audits/#inline-audits","title":"Inline Audits","text":"<p>You can also define audits right in your model file. This is handy when an audit is specific to one model:</p> <pre><code>MODEL (\n    name sushi.items,\n    assertions(does_not_exceed_threshold(column := id, threshold := 1000), price_is_not_null)\n);\nSELECT id, price\nFROM sushi.seed;\n\nAUDIT (name does_not_exceed_threshold);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n\nAUDIT (name price_is_not_null);\nSELECT * FROM @this_model\nWHERE price IS NULL;\n</code></pre> <p>You can define multiple audits in the same file. Just make sure they're defined before (or alongside) the MODEL that uses them.</p>"},{"location":"components/audits/audits/#built-in-audits","title":"Built-in Audits","text":"<p>Vulcan comes with a whole suite of built-in audits that cover most common use cases. These are ready to use, no need to write SQL yourself for these scenarios.</p> <p>All built-in audits are blocking (they stop execution when they fail), and they're grouped by what they check. Let's walk through them:</p>"},{"location":"components/audits/audits/#generic-assertion-audit","title":"Generic Assertion Audit","text":""},{"location":"components/audits/audits/#forall","title":"<code>forall</code>","text":"<p>The most flexible built-in audit. It lets you write arbitrary boolean SQL expressions:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    forall(criteria := (\n      price &gt; 0,\n      LENGTH(name) &gt; 0\n    ))\n  )\n);\n</code></pre> <p>This checks that all rows have a <code>price</code> greater than 0 AND a <code>name</code> with at least one character. You can add as many criteria as you want, they all need to pass.</p>"},{"location":"components/audits/audits/#row-counts-and-null-value-audits","title":"Row Counts and NULL Value Audits","text":"<p>These audits check that you have enough data and that required fields aren't missing.</p>"},{"location":"components/audits/audits/#number_of_rows","title":"<code>number_of_rows</code>","text":"<p>Make sure you have enough rows. Useful for catching cases where a model didn't run properly or data didn't load:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    number_of_rows(threshold := 10)\n  )\n);\n</code></pre> <p>This ensures your model has more than 10 rows. If you have 10 or fewer, something's probably wrong.</p>"},{"location":"components/audits/audits/#not_null","title":"<code>not_null</code>","text":"<p>The classic \"required field\" check. Ensures specified columns don't have NULL values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    not_null(columns := (id, customer_id, waiter_id))\n  )\n);\n</code></pre> <p>This checks that <code>id</code>, <code>customer_id</code>, and <code>waiter_id</code> are never NULL. If any of them are NULL, the audit fails.</p>"},{"location":"components/audits/audits/#at_least_one","title":"<code>at_least_one</code>","text":"<p>Sometimes you just need at least one non-NULL value, not all of them. This is useful for optional fields that should have some data:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    at_least_one(column := zip)\n    )\n);\n</code></pre> <p>This ensures the <code>zip</code> column has at least one non-NULL value. Maybe most customers don't have zip codes, but at least some should.</p>"},{"location":"components/audits/audits/#not_null_proportion","title":"<code>not_null_proportion</code>","text":"<p>Check that NULL values don't exceed a certain percentage. Useful when some NULLs are okay, but too many is a problem:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    not_null_proportion(column := zip, threshold := 0.8)\n    )\n);\n</code></pre> <p>This ensures that at least 80% of rows have a zip code. The other 20% can be NULL, but if more than 20% are missing, that's a problem.</p>"},{"location":"components/audits/audits/#specific-data-values-audits","title":"Specific Data Values Audits","text":"<p>These audits check the actual values in your data, not just whether they exist.</p>"},{"location":"components/audits/audits/#not_constant","title":"<code>not_constant</code>","text":"<p>Make sure a column has variety. If every row has the same value, something might be wrong:</p> <pre><code>MODEL (\n  name sushi.customer_revenue_by_day,\n  assertions (\n    not_constant(column := customer_id)\n    )\n);\n</code></pre> <p>This ensures <code>customer_id</code> has at least two different non-NULL values. If every row has the same customer ID, that's suspicious.</p>"},{"location":"components/audits/audits/#unique_values","title":"<code>unique_values</code>","text":"<p>The classic uniqueness check. Ensures no duplicate values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_values(columns := (id, item_id))\n  )\n);\n</code></pre> <p>This checks that <code>id</code> and <code>item_id</code> each have unique values. No duplicates allowed!</p>"},{"location":"components/audits/audits/#unique_combination_of_columns","title":"<code>unique_combination_of_columns</code>","text":"<p>Check uniqueness across multiple columns. Maybe individual columns can repeat, but combinations must be unique:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_combination_of_columns(columns := (id, ds))\n  )\n);\n</code></pre> <p>This ensures that the combination of <code>id</code> and <code>ds</code> is unique. So <code>id</code> can repeat across different dates, but the same <code>id</code> can't appear twice on the same date.</p>"},{"location":"components/audits/audits/#accepted_values","title":"<code>accepted_values</code>","text":"<p>Make sure values are in an allowed set. Like an enum check:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_values(column := name, is_in := ('Hamachi', 'Unagi', 'Sake'))\n  )\n);\n</code></pre> <p>This ensures that <code>name</code> is one of the three allowed values. Anything else fails the audit.</p> <p>Note</p> <p>Rows with <code>NULL</code> values will pass this audit in most databases. If you want to reject NULLs, combine this with a <code>not_null</code> audit.</p>"},{"location":"components/audits/audits/#not_accepted_values","title":"<code>not_accepted_values</code>","text":"<p>The opposite, make sure certain values are NOT present:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_accepted_values(column := name, is_in := ('Hamburger', 'French fries'))\n  )\n);\n</code></pre> <p>This ensures that <code>name</code> is never 'Hamburger' or 'French fries'. Useful for catching data that shouldn't be there.</p> <p>Note</p> <p>This audit doesn't support rejecting <code>NULL</code> values. Use <code>not_null</code> if you need to ensure no NULLs.</p>"},{"location":"components/audits/audits/#numeric-data-audits","title":"Numeric Data Audits","text":"<p>These audits check numeric ranges and distributions.</p>"},{"location":"components/audits/audits/#sequential_values","title":"<code>sequential_values</code>","text":"<p>Check that values are sequential. Useful for IDs or sequence numbers:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    sequential_values(column := item_id, interval := 1)\n  )\n);\n</code></pre> <p>This ensures that <code>item_id</code> values are sequential (1, 2, 3, 4...). If you have gaps or duplicates, the audit fails.</p>"},{"location":"components/audits/audits/#accepted_range","title":"<code>accepted_range</code>","text":"<p>Check that values are within a numeric range:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 1, max_v := 100)\n  )\n);\n</code></pre> <p>This ensures all prices are between 1 and 100 (inclusive). Values outside this range fail the audit.</p> <p>Exclusive ranges:</p> <p>You can make the range exclusive (not including the boundaries):</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 0, max_v := 100, inclusive := false)\n  )\n);\n</code></pre> <p>Now prices must be greater than 0 and less than 100 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#mutually_exclusive_ranges","title":"<code>mutually_exclusive_ranges</code>","text":"<p>Check that ranges don't overlap. Useful for pricing tiers or time slots:</p> <pre><code>MODEL (\n  name pricing.tier_ranges,\n  assertions (\n    mutually_exclusive_ranges(lower_bound_column := min_price, upper_bound_column := max_price)\n  )\n);\n</code></pre> <p>This ensures that each row's price range [min_price, max_price] doesn't overlap with any other row's range. Perfect for ensuring pricing tiers don't conflict.</p>"},{"location":"components/audits/audits/#character-data-audits","title":"Character Data Audits","text":"<p>These audits check string formats and patterns.</p> <p>Warning</p> <p>Different databases may behave differently with character sets or languages. Test your audits!</p>"},{"location":"components/audits/audits/#not_empty_string","title":"<code>not_empty_string</code>","text":"<p>Make sure strings aren't empty. NULL is okay, but empty strings <code>''</code> are not:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_empty_string(column := name)\n  )\n);\n</code></pre> <p>This ensures no <code>name</code> is an empty string. NULL values pass, but <code>''</code> fails.</p>"},{"location":"components/audits/audits/#string_length_equal","title":"<code>string_length_equal</code>","text":"<p>Check that all strings have the exact same length:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_equal(column := zip, v := 5)\n    )\n);\n</code></pre> <p>This ensures all <code>zip</code> values are exactly 5 characters. Useful for fixed-length codes.</p>"},{"location":"components/audits/audits/#string_length_between","title":"<code>string_length_between</code>","text":"<p>Check that string lengths are within a range:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := name, min_v := 5, max_v := 50)\n    )\n);\n</code></pre> <p>This ensures all <code>name</code> values are between 5 and 50 characters (inclusive).</p> <p>Exclusive ranges:</p> <p>You can make the range exclusive:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := zip, min_v := 4, max_v := 60, inclusive := false)\n    )\n);\n</code></pre> <p>Now names must be longer than 4 characters and shorter than 60 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#valid_uuid","title":"<code>valid_uuid</code>","text":"<p>Check that values match UUID format:</p> <pre><code>MODEL (\n  name events.user_sessions,\n  assertions (\n    valid_uuid(column := uuid)\n    )\n);\n</code></pre> <p>This ensures all <code>uuid</code> values match the UUID structure (like <code>550e8400-e29b-41d4-a716-446655440000</code>).</p>"},{"location":"components/audits/audits/#valid_email","title":"<code>valid_email</code>","text":"<p>Check email format:</p> <pre><code>MODEL (\n  name dim.users,\n  assertions (\n    valid_email(column := email)\n    )\n);\n</code></pre> <p>This ensures all <code>email</code> values look like valid email addresses (has <code>@</code>, has domain, etc.).</p>"},{"location":"components/audits/audits/#valid_url","title":"<code>valid_url</code>","text":"<p>Check URL format:</p> <pre><code>MODEL (\n  name dim.products,\n  assertions (\n    valid_url(column := url)\n    )\n);\n</code></pre> <p>This ensures all <code>url</code> values are valid URLs (starts with <code>http://</code>, <code>https://</code>, or <code>ftp://</code>, etc.).</p>"},{"location":"components/audits/audits/#valid_http_method","title":"<code>valid_http_method</code>","text":"<p>Check that values are valid HTTP methods:</p> <pre><code>MODEL (\n  name logs.api_requests,\n  assertions (\n    valid_http_method(column := http_method)\n  )\n);\n</code></pre> <p>This ensures <code>http_method</code> is one of: <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>PATCH</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.</p>"},{"location":"components/audits/audits/#match_regex_pattern_list","title":"<code>match_regex_pattern_list</code>","text":"<p>Check that values match at least one regex pattern:</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    match_regex_pattern_list(column := todo, patterns := ('^\\d.*', '.*!$'))\n  )\n);\n</code></pre> <p>This ensures all <code>todo</code> values match at least one pattern: either start with a digit (<code>^\\d.*</code>) or end with an exclamation mark (<code>.*!$</code>).</p>"},{"location":"components/audits/audits/#not_match_regex_pattern_list","title":"<code>not_match_regex_pattern_list</code>","text":"<p>The opposite, make sure values don't match any pattern:</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    not_match_regex_pattern_list(column := todo, patterns := ('^!.*', '.*\\d$'))\n  )\n);\n</code></pre> <p>This ensures no <code>todo</code> values start with <code>!</code> or end with a digit.</p>"},{"location":"components/audits/audits/#match_like_pattern_list","title":"<code>match_like_pattern_list</code>","text":"<p>Check that values match at least one SQL LIKE pattern:</p> <pre><code>MODEL (\n  name sales.customers,\n  assertions (\n    match_like_pattern_list(column := name, patterns := ('jim%', 'pam%'))\n  )\n);\n</code></pre> <p>This ensures all <code>name</code> values start with 'jim' or 'pam'. Uses SQL LIKE syntax, so <code>%</code> matches any characters.</p>"},{"location":"components/audits/audits/#not_match_like_pattern_list","title":"<code>not_match_like_pattern_list</code>","text":"<p>Make sure values don't match any LIKE pattern:</p> <pre><code>MODEL (\n  name products.catalog,\n  assertions (\n    not_match_like_pattern_list(column := name, patterns := ('%doe', '%smith'))\n  )\n);\n</code></pre> <p>This ensures no <code>name</code> values end with 'doe' or 'smith'.</p>"},{"location":"components/audits/audits/#statistical-audits","title":"Statistical Audits","text":"<p>These audits check statistical properties of your data. They're powerful but require some tuning to get the thresholds right.</p> <p>Note</p> <p>Statistical audit thresholds usually need fine-tuning through trial and error. Start with wide ranges and tighten them as you learn what's normal for your data.</p>"},{"location":"components/audits/audits/#mean_in_range","title":"<code>mean_in_range</code>","text":"<p>Check that a column's average is within a range:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 21, max_v := 50)\n    )\n);\n</code></pre> <p>This ensures the average <code>age</code> is between 21 and 50. Useful for catching when your data distribution shifts unexpectedly.</p> <p>Exclusive ranges:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 18, max_v := 65, inclusive := false)\n    )\n);\n</code></pre> <p>Now the mean must be greater than 18 and less than 65 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#stddev_in_range","title":"<code>stddev_in_range</code>","text":"<p>Check that standard deviation is within a range:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 2, max_v := 5)\n  )\n);\n</code></pre> <p>This ensures the standard deviation of <code>age</code> is between 2 and 5. Useful for detecting when your data becomes more or less spread out than expected.</p> <p>Exclusive ranges:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 3, max_v := 6, inclusive := false)\n  )\n);\n</code></pre> <p>Now the standard deviation must be greater than 3 and less than 6.</p>"},{"location":"components/audits/audits/#z_score","title":"<code>z_score</code>","text":"<p>Check for statistical outliers. Values with high z-scores are far from the mean:</p> <pre><code>MODEL (\n  name sales.transactions,\n  assertions (\n    z_score(column := age, threshold := 3)\n    )\n);\n</code></pre> <p>This ensures no <code>age</code> values have a z-score greater than 3 (meaning they're more than 3 standard deviations from the mean). Useful for catching outliers that might indicate data quality issues.</p> <p>The z-score is calculated as: <code>ABS(([row value] - [column mean]) / NULLIF([column standard deviation], 0))</code></p>"},{"location":"components/audits/audits/#kl_divergence","title":"<code>kl_divergence</code>","text":"<p>Check how different two distributions are. Useful for comparing current data to a reference:</p> <pre><code>MODEL (\n  name analytics.cohort_comparison,\n  assertions (\n    kl_divergence(column := age, target_column := reference_age, threshold := 0.1)\n    )\n);\n</code></pre> <p>This ensures the symmetrised Kullback-Leibler divergence (also called \"Jeffreys divergence\" or \"Population Stability Index\") between <code>age</code> and <code>reference_age</code> is less than or equal to 0.1.</p> <p>Lower values mean the distributions are more similar. This is great for detecting when your data distribution has shifted significantly from a known good reference.</p>"},{"location":"components/audits/audits/#chi_square","title":"<code>chi_square</code>","text":"<p>Check the relationship between two categorical columns:</p> <pre><code>MODEL (\n  name analytics.user_segments,\n  assertions (\n    chi_square(column := user_state, target_column := user_type, critical_value := 6.635)\n    )\n);\n</code></pre> <p>This ensures the chi-square statistic for <code>user_state</code> and <code>user_type</code> doesn't exceed 6.635.</p> <p>Finding critical values:</p> <p>You can look up critical values in a chi-square table or calculate them with Python:</p> <pre><code>from scipy.stats import chi2\n\n# critical value for p-value := 0.95 and degrees of freedom := 1\nchi2.ppf(0.95, 1)\n</code></pre> <p>This is useful for detecting when the relationship between two categorical variables has changed unexpectedly.</p>"},{"location":"components/audits/audits/#running-audits","title":"Running Audits","text":""},{"location":"components/audits/audits/#the-cli-audit-command","title":"The CLI Audit Command","text":"<p>You can run audits manually with the <code>vulcan audit</code> command:</p> <pre><code>$ vulcan -p project audit --start 2022-01-01 --end 2022-01-02\nFound 1 audit(s).\nassert_item_price_is_not_null FAIL.\n\nFinished with 1 audit error(s).\n\nFailure in audit assert_item_price_is_not_null for model sushi.items (audits/items.sql).\nGot 3 results, expected 0.\nSELECT * FROM vulcan.sushi__items__1836721418_83893210 WHERE ds BETWEEN '2022-01-01' AND '2022-01-02' AND price IS NULL\nDone.\n</code></pre> <p>This is useful for testing audits before running a full plan, or for debugging why an audit is failing. The output shows you exactly what query failed and how many rows it found.</p>"},{"location":"components/audits/audits/#automated-auditing","title":"Automated Auditing","text":"<p>When you apply a plan, Vulcan automatically runs all audits for models being evaluated. You don't need to do anything special, just run your plan and audits happen automatically.</p> <p>If any audit fails, Vulcan halts the pipeline immediately. This prevents bad data from propagating downstream and causing bigger problems. It might be annoying when it happens, but trust us, it's better than finding out later that bad data made it into production.</p>"},{"location":"components/audits/audits/#advanced-usage","title":"Advanced Usage","text":""},{"location":"components/audits/audits/#skipping-audits","title":"Skipping Audits","text":"<p>Sometimes you need to temporarily disable an audit. Maybe you're debugging, or you know there's a temporary data issue you're working on fixing. You can skip audits by setting <code>skip</code> to <code>true</code>:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  skip true\n);\nSELECT * from sushi.items\nWHERE ds BETWEEN @start_ds AND @end_ds AND\n   price IS NULL;\n</code></pre> <p>Use this sparingly! Skipped audits won't run, which means they won't catch problems. It's better to fix the underlying issue than to skip the audit. But sometimes you need it for debugging or temporary situations.</p>"},{"location":"components/audits/audits/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/audits/audits/#audit-fails-unexpectedly","title":"Audit Fails Unexpectedly","text":"<p>Problem: Your audit is failing, but you're not sure why.</p> <p>Solution: Run the audit query manually to see what it's finding:</p> <pre><code>vulcan -p project audit --start 2022-01-01 --end 2022-01-02 --verbose\n</code></pre> <p>This will show you the exact query and the rows that failed. Once you see what data is causing the failure, you can either fix the data or adjust the audit.</p>"},{"location":"components/audits/audits/#audit-too-strict","title":"Audit Too Strict","text":"<p>Problem: Your audit is failing during normal operation, even though the data is actually fine.</p> <p>Solution: Review your thresholds. Maybe your <code>accepted_range</code> is too narrow, or your <code>number_of_rows</code> threshold is too high. Statistical audits especially need tuning, start with wide ranges and tighten them as you learn what's normal.</p>"},{"location":"components/audits/audits/#performance-issues","title":"Performance Issues","text":"<p>Problem: Audits are slowing down your plan execution.</p> <p>Solution:  - Make sure your audit queries use indexes on the columns they're checking - For incremental models, audits only run on processed intervals (which helps), but you can also add date filters to your audit queries - Consider if you really need all those audits, sometimes less is more</p>"},{"location":"components/audits/audits/#understanding-audit-results","title":"Understanding Audit Results","text":"<p>When an audit fails, Vulcan shows you: - Which audit failed - Which model it was attached to - The exact query that was run - How many rows were returned (when it expected 0)</p> <p>Use this information to understand what went wrong. The query results tell you exactly what data failed the check.</p>"},{"location":"components/checks/checks/","title":"Checks","text":""},{"location":"components/checks/checks/#checks","title":"Checks","text":"<p>Quality checks are validation rules that monitor your data quality over time without blocking your pipelines. Think of them as your data's health checkup, they'll warn you when something looks off, but they won't stop the show.</p> <p>Unlike audits (which block pipeline execution when they fail), checks run separately or alongside your models and provide non-blocking validation. They're perfect for tracking trends, detecting anomalies, and building up a historical picture of your data quality.</p> <p>What makes checks special: - Configured in simple YAML files in the <code>checks/</code> directory - Don't block pipelines (your models keep running even if checks fail) - Track historical patterns and trends - Support complex statistical analysis - Integrate with Activity API for monitoring and alerting</p>"},{"location":"components/checks/checks/#checks-vs-audits-vs-profiles","title":"Checks vs Audits vs Profiles","text":"<p>Before we dive in, let's clear up the confusion around these three data quality mechanisms. They all serve different purposes, and understanding when to use each one will save you headaches later.</p> Feature Audits Checks Profiles Purpose Critical validation Monitoring &amp; analysis Observation &amp; tracking When runs With model (inline) Separately or with models With model Blocks pipeline? Yes (always) No No Configuration In MODEL DDL or .sql files YAML files (<code>checks/</code>) In MODEL DDL Output Pass/fail Pass/fail + samples Statistical metrics Best for Business rules, data integrity Trend monitoring, anomalies Understanding data Historical tracking No Yes (Activity API) Yes (<code>_check_profiles</code>) <p>The Three-Layer Strategy:</p> <p>Think of it like a security system for your data:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUDITS (Critical - Blocks Pipeline)   \u2502\n\u2502  \u2022 Primary keys must be unique          \u2502\n\u2502  \u2022 Revenue must be non-negative         \u2502\n\u2502  \u2022 Foreign key relationships valid      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CHECKS (Monitoring - Non-Blocking)     \u2502\n\u2502  \u2022 Row count within expected range      \u2502\n\u2502  \u2022 Anomaly detection on metrics         \u2502\n\u2502  \u2022 Cross-table consistency              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PROFILES (Observation - Metrics)       \u2502\n\u2502  \u2022 Track null percentages               \u2502\n\u2502  \u2022 Monitor column distributions         \u2502\n\u2502  \u2022 Detect data drift                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why this matters: Audits are your bouncers, they stop bad data at the door. Checks are your security cameras, they watch for problems but don't interfere. Profiles are your data scientists, they observe patterns and help you understand what's normal.</p>"},{"location":"components/checks/checks/#when-to-use-checks","title":"When to Use Checks","text":"<p>\u2705 Use Quality Checks for: - Monitoring data quality trends over time (is completeness getting worse?) - Statistical anomaly detection (did revenue suddenly spike?) - Cross-model validation (do orders match customers?) - Non-critical validation (warnings, not blockers) - Complex validation requiring historical context - Building data quality dashboards</p> <p>\u274c Use Audits Instead for: - Critical business rules that must pass (revenue can't be negative) - Model-specific validation (runs inline with the model) - Simple SQL assertions - Blocking invalid data from flowing downstream</p> <p>\u274c Use Profiles Instead for: - Understanding data characteristics (what does this column look like?) - Discovering patterns (not validation) - Detecting data drift over time - Informing which checks/audits to add</p> <p>Example: Revenue validation strategy</p> <p>Here's how you'd layer all three for a revenue table:</p> <pre><code>-- AUDIT (Critical - blocks if fails)\n-- This stops the pipeline if revenue is invalid\nMODEL (\n  name analytics.revenue,\n  assertions (\n    not_null(columns := (customer_id, revenue)),\n    accepted_range(column := revenue, min_v := 0, max_v := 100000000)\n  )\n);\n</code></pre> <pre><code># CHECK (Monitoring - warns if unusual)\n# This watches for anomalies but doesn't block\nchecks:\n  analytics.revenue:\n    accuracy:\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n      - change for row_count &gt;= -30%:\n          name: row_count_drop_alert\n</code></pre> <pre><code>-- PROFILE (Observation - tracks over time)\n-- This just watches and records what it sees\nMODEL (\n  name analytics.revenue,\n  profiles (revenue, order_count, customer_tier)\n);\n</code></pre>"},{"location":"components/checks/checks/#quick-start","title":"Quick Start","text":""},{"location":"components/checks/checks/#your-first-check","title":"Your First Check","text":"<p>Let's create your first check. It's simpler than you might think!</p> <p>Create a file <code>checks/customers.yml</code>:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_emails\n          attributes:\n            description: \"All customers must have an email address\"\n</code></pre> <p>That's it! This check ensures that every customer has an email address. When you run your models, this check will run automatically and warn you if any emails are missing.</p> <p>What happens when it runs:</p> <p>Checks and profiles run automatically when models are executed, either through a plan or run command. Here's what the execution output looks like:</p> <pre><code>Check Executions (1 Models)\n\u2514\u2500\u2500 hello.subscriptions\n    \u251c\u2500\u2500 \u2713 completeness (4/4)\n    \u251c\u2500\u2500 \u2713 uniqueness (1/1)\n    \u2514\u2500\u2500 \u2713 validity (3/3)\n\nProfiled 1 model (3 columns):\n  \u2713 warehouse.hello.subscriptions: 3 columns\n</code></pre> <p>Pretty straightforward, right? Now let's look at some common patterns you'll want to use.</p>"},{"location":"components/checks/checks/#common-check-patterns","title":"Common Check Patterns","text":"<p>Here are the patterns you'll use most often. Copy these, tweak them for your tables, and you're good to go!</p>"},{"location":"components/checks/checks/#pattern-1-completeness-checks","title":"Pattern 1: Completeness Checks","text":"<p>Make sure required data is present:</p> <pre><code>checks:\n  analytics.orders:\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: customer_id_required\n\n      - missing_percent(email) &lt; 5:\n          name: email_mostly_complete\n\n      - row_count &gt; 1000:\n          name: sufficient_orders\n</code></pre> <p>The first check ensures every order has a customer ID (zero tolerance). The second allows up to 5% missing emails (sometimes that's okay). The third makes sure you have enough data to work with.</p>"},{"location":"components/checks/checks/#pattern-2-validity-checks","title":"Pattern 2: Validity Checks","text":"<p>Validate data format and values:</p> <pre><code>checks:\n  analytics.users:\n    validity:\n      - failed rows:\n          name: invalid_emails\n          fail query: |\n            SELECT user_id, email\n            FROM analytics.users\n            WHERE email NOT LIKE '%@%'\n          samples limit: 10\n\n      - failed rows:\n          name: invalid_ages\n          fail query: |\n            SELECT user_id, age\n            FROM analytics.users\n            WHERE age &lt; 0 OR age &gt; 120\n</code></pre> <p>The <code>failed rows</code> check type is super flexible, you can write any SQL query. If it returns rows, the check fails and captures those rows as samples so you can see what went wrong.</p>"},{"location":"components/checks/checks/#pattern-3-uniqueness-checks","title":"Pattern 3: Uniqueness Checks","text":"<p>Ensure no duplicates:</p> <pre><code>checks:\n  analytics.customers:\n    uniqueness:\n      - duplicate_count(email) = 0:\n          name: unique_emails\n\n      - duplicate_count(customer_id, order_date) = 0:\n          name: unique_customer_date_combination\n</code></pre> <p>The second example shows composite keys, maybe a customer can have multiple orders, but only one per day.</p>"},{"location":"components/checks/checks/#pattern-4-anomaly-detection","title":"Pattern 4: Anomaly Detection","text":"<p>Detect unusual patterns automatically:</p> <pre><code>checks:\n  analytics.daily_revenue:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly\n</code></pre> <p>This is where checks get really cool. Anomaly detection learns from historical data and flags when something looks unusual. It needs to run a few times first to build up a baseline, but after that it's pretty smart about spotting problems.</p>"},{"location":"components/checks/checks/#pattern-5-change-monitoring","title":"Pattern 5: Change Monitoring","text":"<p>Track changes over time:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - change for row_count &gt;= -50%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if row count drops more than 50%\"\n</code></pre> <p>This compares the current value to the previous run and alerts you if it changes too much. Perfect for catching sudden drops or spikes.</p>"},{"location":"components/checks/checks/#check-configuration","title":"Check Configuration","text":""},{"location":"components/checks/checks/#file-structure","title":"File Structure","text":"<p>Checks live in YAML files in the <code>checks/</code> directory. You can organize them however makes sense for your project:</p> <pre><code>project/\n\u251c\u2500\u2500 models/\n\u251c\u2500\u2500 checks/\n\u2502   \u251c\u2500\u2500 users.yml           # Checks for user tables\n\u2502   \u251c\u2500\u2500 orders.yml          # Checks for order tables\n\u2502   \u251c\u2500\u2500 revenue.yml         # Checks for revenue tables\n\u2502   \u2514\u2500\u2500 cross_model.yml     # Checks spanning multiple tables\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming: - Must end with <code>.yml</code> or <code>.yaml</code> - The name doesn't matter (Vulcan reads all files in the directory) - Organize by domain or table for clarity, whatever helps you find things</p>"},{"location":"components/checks/checks/#basic-check-syntax","title":"Basic Check Syntax","text":"<p>Here's the basic structure of a check:</p> <pre><code>checks:\n  &lt;fully_qualified_table_name&gt;:\n    &lt;dimension&gt;:\n      - &lt;check_expression&gt;:\n          name: &lt;check_name&gt;\n          attributes:\n            description: &lt;human_readable_description&gt;\n            severity: &lt;warning|error&gt;\n            tags: [&lt;tag1&gt;, &lt;tag2&gt;]\n</code></pre> <p>Example:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_customers\n          attributes:\n            description: \"At least 100 customers expected in production\"\n            severity: warning\n            tags: [critical, daily]\n</code></pre> <p>The <code>name</code> field is required and should be descriptive. The <code>attributes</code> section is optional but super useful for documentation and filtering.</p>"},{"location":"components/checks/checks/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Checks are organized by 8 standard dimensions (based on ODPS v3.1). Each dimension focuses on a different aspect of data quality:</p>"},{"location":"components/checks/checks/#1-completeness","title":"1. Completeness","text":"<p>No missing required data. This is probably the most common dimension you'll use.</p> <pre><code>completeness:\n  - missing_count(customer_id) = 0\n  - missing_percent(email) &lt; 5\n  - row_count &gt; 1000\n</code></pre>"},{"location":"components/checks/checks/#2-validity","title":"2. Validity","text":"<p>Data conforms to format/syntax. Is that email actually an email? Is that date in the right format?</p> <pre><code>validity:\n  - failed rows:\n      fail query: |\n        SELECT * FROM table\n        WHERE email NOT LIKE '%@%'\n</code></pre>"},{"location":"components/checks/checks/#3-accuracy","title":"3. Accuracy","text":"<p>Data matches reality. Is the average age reasonable? Is revenue in the expected range?</p> <pre><code>accuracy:\n  - anomaly detection for avg(revenue)\n  - avg(age) between 18 and 65\n</code></pre>"},{"location":"components/checks/checks/#4-consistency","title":"4. Consistency","text":"<p>Data agrees across sources. Do orders match customers? Are totals consistent?</p> <pre><code>consistency:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders o\n        LEFT JOIN customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n</code></pre>"},{"location":"components/checks/checks/#5-uniqueness","title":"5. Uniqueness","text":"<p>No duplicates. Is that email really unique? Can customers have multiple orders per day?</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0\n  - duplicate_count(order_id) = 0\n</code></pre>"},{"location":"components/checks/checks/#6-timeliness","title":"6. Timeliness","text":"<p>Data is current. Is the data fresh? Are updates happening on time?</p> <pre><code>timeliness:\n  - change for row_count &gt;= -30%\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders\n        WHERE updated_at &lt; CURRENT_DATE - INTERVAL '7 days'\n</code></pre>"},{"location":"components/checks/checks/#7-conformity","title":"7. Conformity","text":"<p>Follows standards. Does the zip code have the right format? Are codes valid?</p> <pre><code>conformity:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM addresses\n        WHERE LENGTH(zip_code) != 5\n</code></pre>"},{"location":"components/checks/checks/#8-coverage","title":"8. Coverage","text":"<p>All records are present. Did we get all the data we expected?</p> <pre><code>coverage:\n  - row_count &gt;= 95% of historical_avg(row_count)\n</code></pre>"},{"location":"components/checks/checks/#filtering-checks","title":"Filtering Checks","text":"<p>Sometimes you want to apply checks to a subset of your data. Maybe you only care about completed orders, or US customers. That's where filters come in:</p> <pre><code>checks:\n  analytics.orders:\n    filter: \"status = 'completed' AND order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: completed_orders_have_customers\n</code></pre> <p>Multiple filters:</p> <p>You can define the same table multiple times with different filters:</p> <pre><code>checks:\n  analytics.customers:\n    filter: \"country = 'US'\"\n    completeness:\n      - row_count &gt; 1000\n\n  analytics.customers:\n    filter: \"country = 'EU'\"\n    completeness:\n      - row_count &gt; 500\n</code></pre> <p>This lets you have different expectations for different regions, which is pretty common in real-world scenarios.</p>"},{"location":"components/checks/checks/#check-attributes","title":"Check Attributes","text":"<p>Add metadata to your checks to make them easier to manage and understand:</p> <pre><code>checks:\n  analytics.revenue:\n    completeness:\n      - row_count &gt; 1000:\n          name: sufficient_revenue_data\n          attributes:\n            description: \"Revenue table must have at least 1000 rows for analysis\"\n            severity: error\n            tags: [critical, daily, revenue]\n            owner: data-team\n            jira: DATA-1234\n            sla: \"&lt; 1 hour\"\n</code></pre> <p>Standard attributes: - <code>description</code> - Human-readable explanation (super helpful for your teammates) - <code>severity</code> - <code>error</code> (default) or <code>warning</code> (warnings are less urgent) - <code>tags</code> - List of tags for filtering/organization (find all \"critical\" checks easily) - <code>owner</code> - Team or person responsible (who do I call when this fails?) - Custom attributes - Any key-value pairs (add whatever metadata you need)</p>"},{"location":"components/checks/checks/#built-in-check-types","title":"Built-in Check Types","text":"<p>Vulcan provides several built-in check types that cover most common scenarios. Let's walk through them:</p>"},{"location":"components/checks/checks/#missing-data-checks","title":"Missing Data Checks","text":""},{"location":"components/checks/checks/#missing_countcolumn","title":"<code>missing_count(column)</code>","text":"<p>Count of NULL values. Simple and straightforward:</p> <pre><code>completeness:\n  - missing_count(email) = 0:\n      name: no_missing_emails\n\n  - missing_count(phone) &lt;= 100:\n      name: phone_mostly_complete\n</code></pre> <p>The first ensures zero missing emails (strict). The second allows up to 100 missing phone numbers (maybe phones are optional for some customers).</p>"},{"location":"components/checks/checks/#missing_percentcolumn","title":"<code>missing_percent(column)</code>","text":"<p>Percentage of NULL values. Useful when you care about proportions rather than absolute counts:</p> <pre><code>completeness:\n  - missing_percent(email) &lt; 5:\n      name: email_95_percent_complete\n\n  - missing_percent(optional_field) &lt; 50:\n      name: optional_field_half_complete\n</code></pre> <p>This is handy when table sizes vary, 5% missing might be fine for a million-row table but concerning for a hundred-row table.</p>"},{"location":"components/checks/checks/#row-count-checks","title":"Row Count Checks","text":""},{"location":"components/checks/checks/#row_count","title":"<code>row_count</code>","text":"<p>Total rows in table. Great for ensuring you have enough data:</p> <pre><code>completeness:\n  - row_count &gt; 1000:\n      name: sufficient_data\n\n  - row_count between 1000 and 100000:\n      name: expected_row_range\n</code></pre> <p>The second example shows a range check, maybe you know your table should be between 1K and 100K rows, and anything outside that range is suspicious.</p>"},{"location":"components/checks/checks/#row_count-with-filter","title":"<code>row_count</code> with filter","text":"<p>You can also check row counts on filtered data:</p> <pre><code>completeness:\n  - row_count &gt; 500:\n      name: sufficient_active_users\n      filter: \"status = 'active'\"\n</code></pre> <p>This checks that you have at least 500 active users, regardless of how many total users you have.</p>"},{"location":"components/checks/checks/#duplicate-count-checks","title":"Duplicate Count Checks","text":""},{"location":"components/checks/checks/#duplicate_countcolumn","title":"<code>duplicate_count(column)</code>","text":"<p>Count of duplicate values. Perfect for ensuring uniqueness:</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0:\n      name: unique_emails\n\n  - duplicate_count(customer_id) = 0:\n      name: unique_customer_ids\n</code></pre> <p>If this returns anything greater than zero, you've got duplicates. The check fails and you can investigate.</p>"},{"location":"components/checks/checks/#duplicate_countcolumn1-column2","title":"<code>duplicate_count(column1, column2)</code>","text":"<p>Composite key duplicates. Check combinations of columns:</p> <pre><code>uniqueness:\n  - duplicate_count(customer_id, order_date) = 0:\n      name: unique_customer_date\n      attributes:\n        description: \"Each customer can have at most one order per day\"\n</code></pre> <p>Maybe customers can have multiple orders, but only one per day. This check enforces that business rule.</p>"},{"location":"components/checks/checks/#failed-rows-checks","title":"Failed Rows Checks","text":""},{"location":"components/checks/checks/#sql-based-validation-with-samples","title":"SQL-based validation with samples","text":"<p>This is the most flexible check type, you can write any SQL query you want:</p> <pre><code>validity:\n  - failed rows:\n      name: invalid_revenue\n      fail query: |\n        SELECT customer_id, revenue, order_date\n        FROM analytics.orders\n        WHERE revenue &lt; 0 OR revenue &gt; 10000000\n      samples limit: 20\n      attributes:\n        description: \"Revenue must be between 0 and 10M\"\n</code></pre> <p>How it works: - <code>fail query</code> - A SELECT statement that returns invalid rows - <code>samples limit</code> - How many example rows to capture when the check fails (default: 5) - Returns empty = check passes (no invalid rows found) - Returns rows = check fails (captures samples so you can see what's wrong)</p> <p>Complex validation:</p> <p>You can get fancy with joins and CTEs:</p> <pre><code>validity:\n  - failed rows:\n      name: orphaned_orders\n      fail query: |\n        SELECT o.order_id, o.customer_id\n        FROM analytics.orders o\n        LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n      samples limit: 10\n</code></pre> <p>This finds orders that reference customers that don't exist, a classic referential integrity check.</p>"},{"location":"components/checks/checks/#threshold-checks","title":"Threshold Checks","text":""},{"location":"components/checks/checks/#numeric-aggregations","title":"Numeric aggregations","text":"<p>Check aggregated values against thresholds:</p> <pre><code>accuracy:\n  - avg(revenue) between 100 and 10000:\n      name: revenue_in_expected_range\n\n  - sum(amount) &gt; 1000000:\n      name: sufficient_total_revenue\n\n  - max(age) &lt;= 120:\n      name: age_within_human_range\n\n  - min(price) &gt;= 0:\n      name: non_negative_prices\n</code></pre> <p>You can use any aggregation function: <code>avg</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>count</code>, <code>distinct_count</code>, etc.</p>"},{"location":"components/checks/checks/#statistical-checks","title":"Statistical checks","text":"<p>Get fancy with statistical functions:</p> <pre><code>accuracy:\n  - stddev(revenue) &lt; 5000:\n      name: revenue_low_variance\n\n  - percentile(revenue, 95) &lt; 50000:\n      name: revenue_95th_percentile_check\n</code></pre> <p>These are great for detecting when your data distribution changes unexpectedly.</p>"},{"location":"components/checks/checks/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"components/checks/checks/#ml-based-anomaly-detection","title":"ML-based anomaly detection","text":"<p>This is where checks get really powerful. Anomaly detection uses historical check results to learn what's normal and flag unusual patterns:</p> <pre><code>accuracy:\n  - anomaly detection for row_count:\n      name: row_count_anomaly\n      attributes:\n        description: \"Detect unusual changes in row count\"\n\n  - anomaly detection for avg(revenue):\n      name: revenue_anomaly\n\n  - anomaly detection for distinct_count(customer_id):\n      name: customer_count_anomaly\n</code></pre> <p>How it works: 1. Collects historical metric values over time (every time the check runs) 2. Builds a statistical model (mean, standard deviation, trends) 3. Compares current value to expected range 4. Flags significant deviations (typically &gt; 3 standard deviations)</p> <p>Requirements: - Needs historical data (runs multiple times to build a baseline) - Works best with regular schedules (daily, hourly) - More accurate after 30+ data points (the more history, the better)</p> <p>So if you're setting up anomaly detection, be patient, it needs to run a few times before it's useful. But once it has enough data, it's really good at spotting problems you might not think to check for.</p>"},{"location":"components/checks/checks/#change-over-time-checks","title":"Change Over Time Checks","text":""},{"location":"components/checks/checks/#monitor-changes-compared-to-previous-run","title":"Monitor changes compared to previous run","text":"<p>Track how metrics change between runs:</p> <pre><code>timeliness:\n  - change for row_count &gt;= -50%:\n      name: row_count_drop_alert\n      attributes:\n        description: \"Alert if row count drops more than 50% from last week\"\n\n  - change for avg(revenue) &gt;= -20%:\n      name: revenue_drop_alert\n\n  - change for distinct_count(customer_id) &gt;= 10%:\n      name: customer_growth_check\n</code></pre> <p>Change calculation: </p><pre><code>change = (current_value - previous_value) / previous_value * 100\n</code></pre><p></p> <p>Examples: - <code>change &gt;= -30%</code> - Alert if metric drops more than 30% (negative change) - <code>change &gt;= 10%</code> - Alert if metric grows more than 10% (positive change) - <code>change between -10% and 10%</code> - Alert if metric changes more than 10% either way</p> <p>This is super useful for catching sudden changes that might indicate a problem (or an opportunity!).</p>"},{"location":"components/checks/checks/#data-profiling","title":"Data Profiling","text":""},{"location":"components/checks/checks/#what-is-profiling","title":"What is Profiling?","text":"<p>Profiles automatically collect statistical metrics about your data over time.</p> <p>Unlike checks (which validate), profiles observe and track data characteristics. They're like a data scientist watching your tables and taking notes:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind FULL,\n  grains (customer_id),\n  profiles (revenue, signup_date, customer_tier, order_count)\n);\n</code></pre> <p>What gets profiled:</p> <p>Table-level metrics: - Row count</p> <p>Column-level metrics (all columns): - Null count &amp; percentage - Distinct count - Duplicate count - Uniqueness percentage</p> <p>Numeric columns: - Min, max, avg, sum - Standard deviation, variance - Histogram buckets</p> <p>Text columns: - Min, max, avg length - Most frequent values</p> <p>Think of profiles as your data's health records, they track how things change over time so you can spot trends and drift.</p>"},{"location":"components/checks/checks/#profile-configuration","title":"Profile Configuration","text":"<p>Enable profiling in your MODEL definition:</p> <pre><code>MODEL (\n  name analytics.revenue_metrics,\n  kind INCREMENTAL_BY_TIME_RANGE (time_column metric_date),\n\n  -- Profile these columns\n  profiles (\n    revenue,\n    order_count,\n    customer_tier,\n    region\n  )\n);\n</code></pre> <p>Just list the columns you want to profile. Vulcan will automatically collect metrics for them every time the model runs.</p>"},{"location":"components/checks/checks/#profile-storage","title":"Profile Storage","text":"<p>Profiles are stored in the <code>_check_profiles</code> table, which you can query like any other table:</p> Column Meaning <code>id</code> Unique identifier for this metric row <code>run_id</code> Identifies which profiling run this metric belongs to <code>table_name</code> Name of the table being profiled <code>column_name</code> Name of the column being profiled (NULL for table-level metrics like row_count) <code>profile_type</code> The type of metric, e.g., row_count, distinct, missing_count, frequent_values, min, max, avg_length, etc. <code>value_number</code> Numeric metric value (for metrics like row_count, distinct, min, max, avg, etc.) <code>value_text</code> Used for text values (rare) <code>value_json</code> JSON-encoded metric (for histograms, frequent values, etc.) <code>value_type</code> Type of value stored (number, json, etc.) <code>profiled_at</code> When the profiling was performed (epoch ms) <code>created_ts</code> When the row was inserted"},{"location":"components/checks/checks/#querying-profiles","title":"Querying Profiles","text":""},{"location":"components/checks/checks/#track-missing-count-over-time","title":"Track missing count over time","text":"<p>See how null percentages change:</p> <pre><code>SELECT\n  to_timestamp(profiled_at/1000)::date AS date,\n  value_number AS missing_count\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\n  AND column_name = 'mrr'\n  AND profile_type = 'missing_count'\nORDER BY profiled_at DESC\nLIMIT 30;  -- Last 30 days\n</code></pre> <p>This shows you a time series of missing values, which is great for spotting trends.</p>"},{"location":"components/checks/checks/#monitor-data-drift","title":"Monitor data drift","text":"<p>Compare current values to historical averages:</p> <pre><code>WITH latest_profile AS (\n  -- Pick the most recent profiling timestamp for that table/column\n  SELECT profiled_at\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n  ORDER BY profiled_at DESC\n  LIMIT 1\n),\n\ncurrent AS (\n  -- Get the most recent distinct count and average value from that profiling run\n  SELECT\n    MAX(CASE WHEN profile_type = 'distinct' THEN value_number END)     AS distinct_count,\n    MAX(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_value\n  FROM _check_profiles p\n  JOIN latest_profile l ON p.profiled_at = l.profiled_at\n  WHERE p.table_name = 'warehouse.hello.subscriptions'\n    AND p.column_name = 'mrr'\n),\n\nhistorical AS (\n  -- 30-day historical averages (profiled_at stored as epoch ms \u2192 convert to timestamp)\n  SELECT\n    AVG(CASE WHEN profile_type = 'distinct' THEN value_number END)      AS avg_distinct,\n    AVG(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_mrr\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n    AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days'\n)\n\nSELECT\n  c.distinct_count,\n  h.avg_distinct,\n  CASE\n    WHEN h.avg_distinct IS NULL THEN NULL\n    ELSE (c.distinct_count - h.avg_distinct) / NULLIF(h.avg_distinct, 0) * 100\n  END AS distinct_change_pct,\n  c.avg_value,\n  h.avg_mrr,\n  CASE\n    WHEN h.avg_mrr IS NULL THEN NULL\n    ELSE (c.avg_value - h.avg_mrr) / NULLIF(h.avg_mrr, 0) * 100\n  END AS mrr_change_pct\nFROM current c, historical h;\n</code></pre> <p>This query compares current metrics to 30-day historical averages and calculates percentage changes. Perfect for detecting drift!</p>"},{"location":"components/checks/checks/#using-profiles-to-inform-checks","title":"Using Profiles to Inform Checks","text":"<p>Workflow:</p> <ol> <li>Enable profiling on new models (just add <code>profiles (...)</code> to your MODEL)</li> <li>Observe patterns for 30+ days (let profiles collect data)</li> <li>Identify anomalies in profile data (query <code>_check_profiles</code> and look for trends)</li> <li>Create checks based on observed patterns (now you know what's normal)</li> </ol> <p>Example:</p> <pre><code>-- Step 1: Enable profiling\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <pre><code>-- Step 2: Query profiles after 30 days\nSELECT\n    MIN(value_number) AS min_revenue,\n    MAX(value_number) AS max_revenue,\n    AVG(value_number) AS typical_revenue,\n    STDDEV(value_number) AS revenue_stddev\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\n  AND column_name = 'mrr'\n  AND profile_type IN ('avg', 'mean', 'average', 'avg_value')\n  AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days';\n\n-- Results:\n-- min_revenue: 45000\n-- max_revenue: 75000\n-- typical_revenue: 58000\n-- revenue_stddev: 6000\n</code></pre> <pre><code># Step 3: Create checks based on observed patterns\nchecks:\n  analytics.orders:\n    accuracy:\n      - avg(revenue) between 40000 and 80000:\n          name: revenue_within_observed_range\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n</code></pre> <p>Now your checks are informed by actual data patterns, not guesses. Much better!</p>"},{"location":"components/checks/checks/#profile-best-practices","title":"Profile Best Practices","text":"<p>\u2705 DO: - Profile high-value production tables (the ones that matter) - Profile columns used in downstream analysis (if it's important, profile it) - Use profiles to understand new data sources (what does this data look like?) - Query profiles to detect data drift (is something changing?) - Use profiles to inform check thresholds (data-driven thresholds are better)</p> <p>\u274c DON'T: - Profile sensitive/PII columns (privacy risk, be careful!) - Profile every column (performance overhead, pick what matters) - Profile temporary/experimental models (waste of resources) - Use profiles as a replacement for checks (they serve different purposes) - Profile very high-frequency models (storage cost adds up)</p> <p>When to use profiles: - Building new models (understand the data first) - Monitoring production tables (watch for changes) - Detecting data drift (is the data changing?) - Informing audit/check strategy (what should we check?) - Debugging data quality issues (what's normal vs abnormal?)</p> <p>When to skip profiles: - Temporary models (they won't be around long) - Models with sensitive data (privacy concerns) - Very high-frequency models (&gt; 100 runs/day, storage costs) - Models where you only need pass/fail validation (profiles are overkill)</p>"},{"location":"components/checks/checks/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"components/checks/checks/#cross-model-validation","title":"Cross-Model Validation","text":"<p>Validate relationships between models. This is super useful for ensuring referential integrity:</p> <pre><code># checks/cross_model.yml\nchecks:\n  analytics.orders:\n    consistency:\n      - failed rows:\n          name: orphaned_orders\n          fail query: |\n            SELECT o.order_id, o.customer_id\n            FROM analytics.orders o\n            LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n            WHERE c.customer_id IS NULL\n          samples limit: 10\n          attributes:\n            description: \"All orders must have a valid customer\"\n\n      - failed rows:\n          name: revenue_mismatch\n          fail query: |\n            SELECT\n              o.order_id,\n              o.revenue as order_revenue,\n              r.revenue as revenue_table_revenue\n            FROM analytics.orders o\n            JOIN analytics.revenue r ON o.order_id = r.order_id\n            WHERE ABS(o.revenue - r.revenue) &gt; 0.01\n</code></pre> <p>The first check finds orders without valid customers (orphaned records). The second ensures revenue matches across tables (consistency check).</p>"},{"location":"components/checks/checks/#time-based-validation","title":"Time-Based Validation","text":"<p>Ensure data timeliness. Is your data fresh? Are updates happening on schedule?</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - failed rows:\n          name: stale_data\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE updated_at &lt; CURRENT_TIMESTAMP - INTERVAL '24 hours'\n              AND status != 'completed'\n          attributes:\n            description: \"Pending orders should update within 24 hours\"\n\n      - failed rows:\n          name: future_dates\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE order_date &gt; CURRENT_DATE\n</code></pre> <p>The first check finds stale pending orders (maybe something's stuck). The second catches future dates (data entry errors).</p>"},{"location":"components/checks/checks/#statistical-outlier-detection","title":"Statistical Outlier Detection","text":"<p>Custom outlier detection using SQL. Sometimes you need more control than anomaly detection provides:</p> <pre><code>checks:\n  analytics.revenue:\n    accuracy:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            WITH stats AS (\n              SELECT\n                AVG(revenue) as mean,\n                STDDEV(revenue) as stddev\n              FROM analytics.revenue\n            )\n            SELECT r.*,\n              (r.revenue - s.mean) / s.stddev as z_score\n            FROM analytics.revenue r, stats s\n            WHERE ABS((r.revenue - s.mean) / s.stddev) &gt; 3\n          samples limit: 20\n</code></pre> <p>This finds rows where revenue is more than 3 standard deviations from the mean (classic outlier detection). The z-score tells you how extreme each outlier is.</p>"},{"location":"components/checks/checks/#best-practices","title":"Best Practices","text":""},{"location":"components/checks/checks/#check-organization","title":"Check Organization","text":"<p>Organize your checks in a way that makes sense for your team. Here are two common approaches:</p> <p>By domain:</p> <pre><code>checks/\n\u251c\u2500\u2500 customers/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u251c\u2500\u2500 validity.yml\n\u2502   \u2514\u2500\u2500 consistency.yml\n\u251c\u2500\u2500 orders/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u2514\u2500\u2500 timeliness.yml\n\u2514\u2500\u2500 revenue/\n    \u2514\u2500\u2500 accuracy.yml\n</code></pre> <p>By priority:</p> <pre><code>checks/\n\u251c\u2500\u2500 critical.yml      # Must never fail\n\u251c\u2500\u2500 important.yml     # Should rarely fail\n\u251c\u2500\u2500 monitoring.yml    # Track trends\n\u2514\u2500\u2500 experimental.yml  # Testing new checks\n</code></pre> <p>Pick whatever works for your team. The important thing is consistency, if everyone knows where to find things, life is easier.</p>"},{"location":"components/checks/checks/#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive names:</p> <pre><code># \u274c Bad - what does \"check1\" tell you?\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: check1\n\n# \u2705 Good - clear and descriptive\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_customer_emails\n          attributes:\n            description: \"All customers must have an email for marketing\"\n</code></pre> <p>Naming pattern: - <code>&lt;dimension&gt;_&lt;what&gt;_&lt;constraint&gt;</code> or <code>&lt;what&gt;_&lt;constraint&gt;</code> - Examples:   - <code>completeness_email_required</code> or <code>no_missing_emails</code>   - <code>validity_email_format</code> or <code>valid_email_format</code>   - <code>uniqueness_email_no_duplicates</code> or <code>unique_emails</code>   - <code>timeliness_order_within_24hrs</code> or <code>orders_update_daily</code></p> <p>The key is that someone reading the name should understand what it checks without looking at the code.</p>"},{"location":"components/checks/checks/#threshold-selection","title":"Threshold Selection","text":"<p>Start conservative, adjust based on data:</p> <pre><code># Step 1: Start with wide range\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_orders_v1\n\n# Step 2: Monitor for 30 days, see actual range: 5000-10000\n\n# Step 3: Tighten based on observed patterns\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 4000 and 12000:\n          name: sufficient_orders_v2\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n</code></pre> <p>Don't set thresholds based on guesses, let the data tell you what's normal. Use profiles to understand your data first, then set checks based on what you learn.</p> <p>Use profiles to inform thresholds:</p> <pre><code>-- Query profiles to understand your data\nSELECT\n  MIN(value_number) as min_observed,\n  MAX(value_number) as max_observed,\n  AVG(value_number) as typical,\n  STDDEV(value_number) as stddev\nFROM check_results\nWHERE check_name = 'row_count'\n  AND executed_at &gt;= CURRENT_DATE - INTERVAL '90 days';\n\n-- Set threshold as: typical \u00b1 3*stddev\n</code></pre> <p>This gives you data-driven thresholds instead of wild guesses. Much better!</p>"},{"location":"components/checks/checks/#integration-strategy","title":"Integration Strategy","text":"<p>Layer validation:</p> <pre><code>-- LAYER 1: Audits (critical - blocks)\n-- Stop bad data at the door\nMODEL (\n  name analytics.orders,\n  assertions (\n    not_null(columns := (order_id, customer_id)),\n    unique_values(columns := (order_id))\n  )\n);\n</code></pre> <pre><code># LAYER 2: Checks (monitoring - warns)\n# Watch for problems but don't block\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 5000 and 15000:\n          name: order_count_in_range\n\n    timeliness:\n      - change for row_count &gt;= -30%:\n          name: order_count_stable\n</code></pre> <pre><code>-- LAYER 3: Profiles (observe - tracks)\n-- Just watch and learn\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <p>This three-layer approach gives you comprehensive data quality coverage: audits stop problems, checks warn about issues, and profiles help you understand what's normal.</p>"},{"location":"components/checks/checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/checks/checks/#check-failures","title":"Check Failures","text":""},{"location":"components/checks/checks/#investigate-failed-check","title":"Investigate failed check","text":"<p>When a check fails, you'll want to dig into why:</p> <pre><code># Run specific check with verbose output\nvulcan check --select analytics.customers.invalid_emails --verbose\n</code></pre> <p>This gives you more details about what went wrong.</p>"},{"location":"components/checks/checks/#query-failed-samples","title":"Query failed samples","text":"<p>If your check captures samples (like <code>failed rows</code> checks do), you can query them:</p> <pre><code>-- Get samples from last failed run\nSELECT *\nFROM check_samples\nWHERE check_name = 'invalid_emails'\n  AND status = 'failed'\nORDER BY executed_at DESC\nLIMIT 10;\n</code></pre> <p>This shows you actual rows that failed, which is super helpful for debugging.</p>"},{"location":"components/checks/checks/#performance-issues","title":"Performance Issues","text":""},{"location":"components/checks/checks/#slow-check-queries","title":"Slow check queries","text":"<p>Problem: Check takes too long to run</p> <p>Solution 1: Add filters</p> <pre><code># \u274c Slow - scans entire table\nchecks:\n  analytics.orders:\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n\n# \u2705 Fast - filters to recent data\nchecks:\n  analytics.orders:\n    filter: \"order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n</code></pre> <p>Filtering reduces the amount of data the check needs to scan, which makes it faster.</p> <p>Solution 2: Add indexes</p> <pre><code>-- Add index on frequently checked columns\nCREATE INDEX idx_orders_email ON analytics.orders(email);\nCREATE INDEX idx_orders_order_date ON analytics.orders(order_date);\n</code></pre> <p>Indexes help queries run faster, especially for <code>failed rows</code> checks that filter on specific columns.</p>"},{"location":"components/checks/checks/#false-positives","title":"False Positives","text":""},{"location":"components/checks/checks/#threshold-too-strict","title":"Threshold too strict","text":"<p>Problem: Check fails during normal variance</p> <pre><code># \u274c Too strict - exact match is unrealistic\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count = 10000  # Exact match\n\n# \u2705 Allow variance - more realistic\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 9000 and 11000  # \u00b110% variance\n</code></pre> <p>Real data has variance. Don't set thresholds that are too strict, you'll just get false positives.</p>"},{"location":"components/checks/checks/#use-anomaly-detection-instead","title":"Use anomaly detection instead","text":"<p>Sometimes strict thresholds aren't the right approach:</p> <pre><code># Replace strict threshold with ML-based detection\nchecks:\n  analytics.orders:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n</code></pre> <p>Anomaly detection learns what's normal and adapts to variance, which reduces false positives.</p>"},{"location":"components/checks/checks/#summary","title":"Summary","text":"<p>Quality checks provide a comprehensive way to monitor data quality over time without blocking your pipelines. Here's what we covered:</p>"},{"location":"components/checks/checks/#core-concepts","title":"Core Concepts","text":"<p>1. Quality Checks - YAML-configured validation rules - Non-blocking (don't stop pipelines) - Track trends over time - Integrate with Activity API</p> <p>2. Check Types - Missing data checks (<code>missing_count</code>, <code>missing_percent</code>) - Row count checks (<code>row_count</code>) - Duplicate checks (<code>duplicate_count</code>) - Failed rows (SQL-based, super flexible) - Anomaly detection (ML-based, learns from history) - Change monitoring (compare to previous runs)</p> <p>3. Data Profiling - Automatic statistical metric collection - Stored in <code>_check_profiles</code> table - Observe patterns without validation - Inform check threshold selection</p> <p>4. Data Quality Strategy - Audits - Critical, blocking (stop bad data) - Checks - Monitoring, non-blocking (watch for problems) - Profiles - Observation, tracking (understand what's normal)</p> <p>Remember: start simple, use profiles to understand your data, then create checks based on what you learn. And don't forget, checks are there to help you, not stress you out. If a check is giving you too many false positives, adjust the threshold or switch to anomaly detection. The goal is better data quality, not perfect check scores.</p>"},{"location":"components/model/model_kinds/","title":"Kinds","text":""},{"location":"components/model/model_kinds/#kinds","title":"Kinds","text":"<p>Model kinds determine how Vulcan loads and processes your data. Think of them as different strategies, each one optimized for different use cases. Some rebuild everything from scratch, others update incrementally, and some just create views that compute on-demand.</p> <p>Find information about all model kind configuration parameters in the model configuration reference page.</p>"},{"location":"components/model/model_kinds/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p><code>INCREMENTAL_BY_TIME_RANGE</code> models are perfect for time-series data, things like events, logs, transactions, or any data that arrives over time. Instead of rebuilding everything each run (like FULL models do), these models only process the time intervals that are missing or need updating.</p> <p>Why this matters: If you're processing daily sales data, you don't want to reprocess all of 2023 just to add today's data. With <code>INCREMENTAL_BY_TIME_RANGE</code>, Vulcan only processes the new intervals, which saves you time and money. Pretty smart, right?</p> <p>To use this kind, you need to tell Vulcan two things: 1. Which column has your time data - So Vulcan knows how to partition and filter 2. A WHERE clause - That filters your upstream data by time range using Vulcan's time macros</p> <p>You specify the time column in your <code>MODEL</code> DDL using the <code>time_column</code> key. Here's a simple example:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p> In addition to specifying a time column in the <code>MODEL</code> DDL, the model's query must contain a <code>WHERE</code> clause that filters the upstream records by time range. Vulcan provides special macros that represent the start and end of the time range being processed: <code>@start_date</code> / <code>@end_date</code> and <code>@start_ds</code> / <code>@end_ds</code>. Refer to Macros for more information.</p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>This example demonstrates incremental by time range models.</p> <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incrementals_demo,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    -- How does this model kind behave?\n    --   DELETE by time range, then INSERT\n    time_column transaction_date,\n\n    -- How do I handle late-arriving data?\n    --   Handle late-arriving events for the past 2 (2*1) days based on cron\n    --   interval. Each time it runs, it will process today, yesterday, and\n    --   the day before yesterday.\n    lookback 2,\n  ),\n\n  -- Don't backfill data before this date\n  start '2024-10-25',\n\n  -- What schedule should I run these at?\n  --   Daily at Midnight UTC\n  cron '@daily',\n\n  -- Good documentation for the primary key\n  grain transaction_id,\n\n  -- How do I test this data?\n  --   Validate that the `transaction_id` primary key values are both unique\n  --   and non-null. Data audit tests only run for the processed intervals,\n  --   not for the entire table.\n  -- audits (\n  --   UNIQUE_VALUES(columns = (transaction_id)),\n  --   NOT_NULL(columns = (transaction_id))\n  -- )\n);\n\nWITH sales_data AS (\n  SELECT\n    transaction_id,\n    product_id,\n    customer_id,\n    transaction_amount,\n    -- How do I account for UTC vs. PST (California baby) timestamps?\n    --   Make sure all time columns are in UTC and convert them to PST in the\n    --   presentation layer downstream.\n    transaction_timestamp,\n    payment_method,\n    currency\n  FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data\n  -- How do I make this run fast and only process the necessary intervals?\n  --   Use our date macros that will automatically run the necessary intervals.\n  --   Because Vulcan manages state, it will know what needs to run each time\n  --   you invoke `vulcan run`.\n  WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B\n  -- Include usage data from the 30 days before the interval\n  WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n)\n\nSELECT\n  s.transaction_id,\n  s.product_id,\n  s.customer_id,\n  s.transaction_amount,\n  -- Extract the date from the timestamp to partition by day\n  DATE(s.transaction_timestamp) as transaction_date,\n  -- Convert timestamp to PST using a SQL function in the presentation layer for end users\n  DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,\n  s.payment_method,\n  s.currency,\n  -- Product usage metrics\n  p.last_usage_date,\n  p.usage_count,\n  p.feature_utilization_score,\n  p.user_segment,\n  -- Derived metrics\n  CASE\n    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.8 THEN 'Power User'\n    WHEN p.usage_count &gt; 50 THEN 'Regular User'\n    WHEN p.usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type,\n  -- Time since last usage\n  DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage\nFROM sales_data s\nLEFT JOIN product_usage p\n  ON s.product_id = p.product_id\n  AND s.customer_id = p.customer_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>50975949</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (\n  `transaction_id` STRING,\n  `product_id` STRING,\n  `customer_id` STRING,\n  `transaction_amount` NUMERIC,\n  `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),\n  `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),\n  `payment_method` STRING,\n  `currency` STRING,\n  `last_usage_date` TIMESTAMP,\n  `usage_count` INT64,\n  `feature_utilization_score` FLOAT64,\n  `user_segment` STRING,\n  `user_type` STRING OPTIONS (description='Derived metrics'),\n  `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')\n  )\n  PARTITION BY `transaction_date`\n</code></pre> <p>Vulcan will validate the SQL before processing data (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps).</p> <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND\n    FALSE\n),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)\n    ) AND\n    FALSE\n)\n\nSELECT\n  `s`.`transaction_id` AS `transaction_id`,\n  `s`.`product_id` AS `product_id`,\n  `s`.`customer_id` AS `customer_id`,\n  CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n  DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n  DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n  `s`.`payment_method` AS `payment_method`,\n  `s`.`currency` AS `currency`,\n  `p`.`last_usage_date` AS `last_usage_date`,\n  `p`.`usage_count` AS `usage_count`,\n  `p`.`feature_utilization_score` AS `feature_utilization_score`,\n  `p`.`user_segment` AS `user_segment`,\n  CASE\n    WHEN `p`.`feature_utilization_score` &gt; 0.8 AND `p`.`usage_count` &gt; 100 THEN 'Power User'\n    WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n    WHEN `p`.`usage_count` IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END AS `user_type`,\n  DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\nFROM `sales_data` AS `s`\nLEFT JOIN `product_usage` AS `p`\n  ON `p`.`customer_id` = `s`.`customer_id` AND\n  `p`.`product_id` = `s`.`product_id`\nWHERE FALSE\nLIMIT 0\n</code></pre> <p>Vulcan will merge data into the empty table.</p> <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `transaction_id`,\n      `product_id`,\n      `customer_id`,\n      `transaction_amount`,\n      `transaction_timestamp`,\n      `payment_method`,\n      `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_id`,\n      `customer_id`,\n      `last_usage_date`,\n      `usage_count`,\n      `feature_utilization_score`,\n      `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  )\n\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`usage_count` &gt; 100 AND `p`.`feature_utilization_score` &gt; 0.8 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n      END AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p`\n      ON `s`.`product_id` = `p`.`product_id`\n      AND `s`.`customer_id` = `p`.`customer_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN\n  INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n  VALUES (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer to pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`\n</code></pre> <p>Important: Timezone Requirements</p> <p>Your <code>time_column</code> should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.</p> <p>Why UTC? It's a data engineering best practice, convert everything to UTC when it enters your system, then convert to local timezones only when data leaves for end users. This prevents timezone-related bugs as data flows between models.</p> <p>Important: The <code>cron_tz</code> flag doesn't change this requirement, it only affects when your model runs, not how time intervals are calculated.</p> <p>If you absolutely must use a different timezone, you can try to work around it using <code>lookback</code>, <code>allow_partials</code>, or cron offsets, but UTC is strongly recommended. Trust us on this one, timezone bugs are no fun!</p> <p>This example implements a complete <code>INCREMENTAL_BY_TIME_RANGE</code> model that specifies the time column name <code>order_date</code> in the <code>MODEL</code> DDL and includes a SQL <code>WHERE</code> clause to filter records by time range:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_time_range,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  grains (order_date, product_id),\n  cron '@daily'\n);\n\nSELECT\n  o.order_date,\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  COUNT(DISTINCT o.order_id) AS order_count,\n  SUM(oi.quantity) AS total_quantity,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p\n  ON oi.product_id = p.product_id\nWHERE\n  o.order_date BETWEEN @start_ds AND @end_ds\nGROUP BY\n  o.order_date, p.product_id, p.name, p.category\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_time_range_py\",\n    columns={\n        \"order_date\": \"date\",\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"total_sales_amount\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_date\", \"product_id\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\", \"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT o.order_date, p.product_id, p.name AS product_name,\n           SUM(oi.quantity * oi.unit_price) AS total_sales_amount\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    JOIN vulcan_demo.products p ON oi.product_id = p.product_id\n    WHERE o.order_date BETWEEN '{start}' AND '{end}'\n    GROUP BY o.order_date, p.product_id, p.name\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/model_kinds/#time-column","title":"Time Column","text":"<p>Vulcan needs to know which column in your model's output represents the timestamp or date for each record. This is your <code>time_column</code>.</p> <p>Remember: UTC Timezone</p> <p>Your <code>time_column</code> should be in UTC timezone. Learn more about why this matters above.</p> <p>The time column is used to determine which records will be overwritten during data restatement and provides a partition key for engines that support partitioning (such as Apache Spark). The name of the time column is specified in the <code>MODEL</code> DDL <code>kind</code> specification:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p>By default, Vulcan assumes your time column is in <code>%Y-%m-%d</code> format (like <code>2025-01-15</code>). If your dates are in a different format, you can specify it:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (order_date, '%Y-%m-%d')\n  )\n);\n</code></pre> <p>Format String Dialect</p> <p>Use the same SQL dialect for your format string as the one used in your model's query.</p> <p>Safety feature: Vulcan automatically adds a time range filter to your query's output to prevent data leakage. This means even if your <code>WHERE</code> clause has a bug, Vulcan won't accidentally store records outside the target interval.</p> <p>Here's how it works: - Your WHERE clause filters the input data as it's read from upstream tables (makes queries faster) - Vulcan's automatic filter filters the output data before it's stored (prevents data leakage)</p> <p>This is especially important when handling late-arriving data, you don't want to accidentally overwrite unrelated records!</p> <p>Here's a cool example: sometimes your upstream data uses a different time column than your model. In this case, you filter on the upstream column (<code>shipped_date</code>), but Vulcan still adds a filter on your model's time column (<code>order_date</code>):</p> <pre><code>MODEL (\n  name vulcan_demo.shipment_events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- `order_date` is model's time column\n  )\n);\n\nSELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds; -- Filter is based on the user-supplied `shipped_date` column\n</code></pre> <p>At runtime, Vulcan will automatically modify the model's query to look like this:</p> <pre><code>SELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds\n  AND o.order_date BETWEEN @start_ds AND @end_ds; -- `order_date` time column filter automatically added by Vulcan\n</code></pre>"},{"location":"components/model/model_kinds/#partitioning","title":"Partitioning","text":"<p>By default, Vulcan automatically adds your <code>time_column</code> to the partition key. This lets database engines do partition pruning (skipping partitions that don't match your query), which makes queries faster.</p> <p>Why this matters: If you're querying data from the last 7 days, the engine can skip scanning all the old partitions. That's a huge performance win!</p> <p>Sometimes you might not want this though, maybe you want to partition exclusively on another column, or you want to partition on <code>month(time_column)</code> but your engine doesn't support expression-based partitioning.</p> <p>To disable automatic time column partitioning, set <code>partition_by_time_column false</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    partition_by_time_column false\n  ),\n  partitioned_by (warehouse_id) -- order_date will no longer be automatically added here and the partition key will just be 'warehouse_id'\n);\n</code></pre>"},{"location":"components/model/model_kinds/#idempotency","title":"Idempotency","text":"<p>We recommend making sure incremental by time range model queries are idempotent to prevent unexpected results during data restatement.</p> <p>We recommend making your incremental by time range queries idempotent. This means running the same query multiple times produces the same result, which prevents surprises during data restatement.</p> <p>Watch out: Your upstream models can affect idempotency. If you reference a FULL model (which rebuilds everything each run), your incremental model becomes non-idempotent because that upstream data changes every time. This is usually fine, but it's good to be aware of.</p>"},{"location":"components/model/model_kinds/#materialization-strategy","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE by time column partition Databricks INSERT OVERWRITE by time column partition Snowflake DELETE by time range, then INSERT BigQuery DELETE by time range, then INSERT Redshift DELETE by time range, then INSERT Postgres DELETE by time range, then INSERT DuckDB DELETE by time range, then INSERT"},{"location":"components/model/model_kinds/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p><code>INCREMENTAL_BY_UNIQUE_KEY</code> models update data based on a unique key. Think of it like an upsert operation, if a key exists, update it; if it doesn't, insert it.</p> <p>Here's how it works: - New key? \u2192 Insert the row - Existing key? \u2192 Update the row with new data - Key missing from new data? \u2192 Leave the existing row alone</p> <p>Why use this? Perfect for dimension tables, customer records, or any data where you want to keep the latest version of each record without rebuilding everything. It's like updating a contact list, you update existing contacts and add new ones, but you don't delete contacts that aren't in your latest import.</p> <p>This kind is a good fit for datasets that have the following traits:</p> <ul> <li>Each record has a unique key associated with it.</li> <li>There is at most one record associated with each unique key.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their keys match.</li> </ul> <p>A Slowly Changing Dimension (SCD) is one approach that fits this description well. See the SCD Type 2 model kind for a specific model kind for SCD Type 2 models.</p> <p>The name of the unique key column must be provided as part of the <code>MODEL</code> DDL, as in this example:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_unique_key,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  MAX(o.order_date) AS last_order_date\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nWHERE\n  o.order_date IS NULL OR o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_unique_key_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"last_order_date\": \"date\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id,\n           SUM(oi.quantity * oi.unit_price) as total_spent,\n           MAX(o.order_date) as last_order_date\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>You can use composite keys (multiple columns) too:</p> <pre><code>MODEL (\n  name vulcan_demo.order_items_agg,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, product_id)\n  )\n);\n</code></pre> <p>You can also filter upstream records by time range using <code>@start_date</code>, <code>@end_date</code>, or other time macros (just like <code>INCREMENTAL_BY_TIME_RANGE</code>). This is useful when you only want to process records from a specific time period.</p> <p>Note: Vulcan's time macros are always in UTC timezone.</p> <pre><code>SELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(o.order_id) AS total_orders\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nWHERE\n  o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incremental_by_unique_key_example,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key id\n  ),\n  start '2020-01-01',\n  cron '@daily',\n);\n\nSELECT\n  id,\n  item_id,\n  event_date\nFROM demo.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>1161945221</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>FALSE LIMIT 0</code> in the <code>WHERE</code> statement and the placeholder dates).</p> <pre><code>SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE (`seed_model`.`event_date` &lt;= CAST('1970-01-01' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('1970-01-01' AS DATE)) AND FALSE LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE `seed_model`.`event_date` &lt;= CAST('2024-10-30' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('2020-01-01' AS DATE)) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incremental_by_unique_key_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221`\n</code></pre> <p>Note: Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are inherently non-idempotent, which should be taken into consideration during data restatement. As a result, partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated.</p>"},{"location":"components/model/model_kinds/#unique-key-expressions","title":"Unique Key Expressions","text":"<p>You're not limited to just column names, you can use SQL expressions too! This is handy when you need to create a key from multiple columns or transform values. Here's an example using <code>COALESCE</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_unique,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key COALESCE(\"email\", '')\n  )\n);\n</code></pre>"},{"location":"components/model/model_kinds/#when-matched-expression","title":"When Matched Expression","text":"<p>By default, when a key matches (source and target have the same key), Vulcan updates all columns. But sometimes you want more control, maybe you want to preserve certain values, or only update specific columns.</p> <p>You can customize this behavior with <code>when_matched</code> expressions:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched (\n      WHEN MATCHED THEN UPDATE SET target.email = COALESCE(source.email, target.email)\n    )\n  )\n);\n</code></pre> <p>Important: You must use <code>source</code> and <code>target</code> aliases to distinguish between the source (new data) and target (existing table) columns.</p> <p>You can also provide multiple <code>WHEN MATCHED</code> expressions for more complex logic:</p> <pre><code>MODEL (\n  name vulcan_demo.products_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key product_id,\n    when_matched (\n      WHEN MATCHED AND source.price IS NULL THEN UPDATE SET target.price = target.price\n      WHEN MATCHED THEN UPDATE SET target.category = COALESCE(source.category, target.category)\n    )\n  )\n);\n</code></pre> <p>Engine Support</p> <p><code>when_matched</code> only works on engines that support the <code>MERGE</code> statement. Supported engines include:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Postgres</li> <li>Redshift (requires <code>enable_merge: true</code> in connection config)</li> <li>Snowflake</li> <li>Spark</li> </ul> <p>Redshift users: You need to enable MERGE support by setting <code>enable_merge: true</code> in your connection config. It's disabled by default.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      enable_merge: true\n</code></pre> <p>Redshift supports only the <code>UPDATE</code> or <code>DELETE</code> actions for the <code>WHEN MATCHED</code> clause and does not allow multiple <code>WHEN MATCHED</code> expressions. For further information, refer to the Redshift documentation.</p>"},{"location":"components/model/model_kinds/#merge-filter-expression","title":"Merge Filter Expression","text":"<p>MERGE operations can be slow on large tables because they typically scan the entire existing table. If you're only updating a small subset of records, this is wasteful.</p> <p>Solution: Use <code>merge_filter</code> to add conditions to the MERGE's <code>ON</code> clause. This limits the scan to only the rows that might match, making things much faster.</p> <p>The <code>merge_filter</code> accepts predicates (single or combined with AND) that get added to the MERGE operation:</p> <pre><code>MODEL (\n  name vulcan_demo.orders_recent,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key order_id,\n    merge_filter source._operation IS NULL AND target.order_date &gt; dateadd(day, -7, current_date)\n  )\n);\n</code></pre> <p>Just like <code>when_matched</code>, use <code>source</code> and <code>target</code> aliases to reference the source and target tables.</p> <p>Coming from dbt? If your dbt project uses <code>incremental_predicates</code>, Vulcan will automatically convert them to <code>merge_filter</code> for you. Pretty convenient!</p>"},{"location":"components/model/model_kinds/#materialization-strategy_1","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are materialized using the following strategies:</p> Engine Strategy Spark not supported Databricks MERGE ON unique key Snowflake MERGE ON unique key BigQuery MERGE ON unique key Redshift MERGE ON unique key Postgres MERGE ON unique key DuckDB DELETE ON matched + INSERT new rows"},{"location":"components/model/model_kinds/#full","title":"FULL","text":"<p><code>FULL</code> models are the simplest kind, they rebuild everything from scratch every time they run. No incremental logic, no time columns, no unique keys. Just run the query and replace the entire table.</p> <p>When to use FULL: - Small datasets where rebuilding is fast and cheap - Aggregate tables without a time dimension - Tables that change completely each run (like a \"current state\" snapshot) - Development and testing (simpler is better when you're iterating)</p> <p>When NOT to use FULL: - Large datasets (you'll wait forever and pay a lot) - Time-series data (use <code>INCREMENTAL_BY_TIME_RANGE</code> instead) - Tables that only change partially (use incremental kinds)</p> <p>The trade-off is simplicity vs. performance. For small tables, FULL is perfect. For large tables, incremental kinds will save you time and money.</p> <p>This example specifies a <code>FULL</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  start '2025-01-01',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\nORDER BY total_spent DESC\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT p.product_id, p.name AS product_name, p.category,\n           COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.full_model_example,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM demo.incremental_model\nGROUP BY\n  item_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>2345651858</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` (`item_id` INT64, `num_orders` INT64)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>WHERE FALSE</code> and <code>LIMIT 0</code>).</p> <pre><code>SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nWHERE FALSE\nGROUP BY `incremental_model`.`item_id` LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` AS\nSELECT CAST(`item_id` AS INT64) AS `item_id`, CAST(`num_orders` AS INT64) AS `num_orders`\nFROM (SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nGROUP BY `incremental_model`.`item_id`) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`full_model_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858`\n</code></pre>"},{"location":"components/model/model_kinds/#materialization-strategy_2","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>FULL</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE Databricks INSERT OVERWRITE Snowflake CREATE OR REPLACE TABLE BigQuery CREATE OR REPLACE TABLE Redshift DROP TABLE, CREATE TABLE, INSERT Postgres DROP TABLE, CREATE TABLE, INSERT DuckDB CREATE OR REPLACE TABLE"},{"location":"components/model/model_kinds/#view","title":"VIEW","text":"<p>Unlike the other kinds, <code>VIEW</code> models don't store any data. Instead, they create a virtual table (a view) that runs your query every time someone queries it.</p> <p>How it works: When a downstream model or user queries your VIEW model, the database executes your query on-the-fly. No data is pre-computed or stored.</p> <p>When to use VIEW: - Simple transformations that are fast to compute - When you want always-fresh data (no caching) - When storage is expensive but compute is cheap - For lightweight transformations that don't need materialization</p> <p>When NOT to use VIEW: - Expensive queries that run frequently (you'll pay the compute cost every time) - Complex aggregations or joins (materialize these instead) - Python models (VIEW isn't supported for Python, use SQL)</p> <p>Default Kind</p> <p><code>VIEW</code> is the default model kind if you don't specify one. So if you write a model without a <code>kind</code>, it becomes a VIEW automatically.</p> <p>Performance Consideration</p> <p>Since VIEW queries run every time they're referenced, expensive queries can get costly fast. If your view is referenced by many downstream models, you might be running that expensive query dozens of times. Consider materializing expensive views as FULL or incremental models instead.</p> <p>This example specifies a <code>VIEW</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.view_model,\n  kind VIEW,\n  grains (warehouse_performance_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  r.region_name,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', o.order_date::TEXT) AS warehouse_performance_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.warehouses AS w\nLEFT JOIN vulcan_demo.regions AS r\n  ON w.region_id = r.region_id\nLEFT JOIN vulcan_demo.orders AS o\n  ON w.warehouse_id = o.warehouse_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY w.warehouse_id, w.name, r.region_name, o.order_date\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.example_view,\n  kind VIEW,\n  cron '@daily',\n);\n\nSELECT\n  'hello there' as a_column\n</code></pre> <p>Vulcan will execute this SQL to create a versioned view in the physical layer. Note that the view's version fingerprint, <code>1024042926</code>, is part of the view name.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n(`a_column`) AS SELECT 'hello there' AS `a_column`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned view in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`example_view` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n</code></pre>"},{"location":"components/model/model_kinds/#materialized-views","title":"Materialized Views","text":"<p>Want the best of both worlds? You can turn a VIEW into a materialized view by setting <code>materialized: true</code>. Materialized views store the query results (like a table) but automatically refresh when the underlying data changes (like a view).</p> <p>Set it up like this:</p> <pre><code>MODEL (\n  name vulcan_demo.sales_summary,\n  kind VIEW (\n    materialized true\n  )\n);\n</code></pre> <p>Engine Support</p> <p>Materialized views are only supported on:</p> <ul> <li>BigQuery</li> <li>Databricks</li> <li>Snowflake</li> </ul> <p>On other engines, this flag is ignored and you'll get a regular VIEW.</p> <p>Smart refresh: Vulcan only recreates the materialized view when your query changes or the view doesn't exist. This means you get all the performance benefits of materialized views without unnecessary refreshes. Pretty efficient!</p>"},{"location":"components/model/model_kinds/#embedded","title":"EMBEDDED","text":"<p><code>EMBEDDED</code> models are like reusable SQL snippets. They don't create tables or views, instead, their query gets injected directly into any downstream model that references them, as a subquery.</p> <p>Why use this? If you have common logic that multiple models need (like a CTE that filters active customers), you can define it once in an EMBEDDED model and reuse it everywhere. It's like a macro, but for SQL.</p> <p>Perfect for: - Common CTEs used across multiple models - Reusable business logic (like \"active customers\" or \"valid orders\") - Avoiding code duplication</p> <p>Python Models</p> <p>Python models don't support the <code>EMBEDDED</code> kind, use a SQL model instead.</p> <p>This example specifies an <code>EMBEDDED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.unique_customers,\n  kind EMBEDDED\n);\n\nSELECT DISTINCT\n  customer_id,\n  name AS customer_name,\n  email\nFROM vulcan_demo.customers\n</code></pre>"},{"location":"components/model/model_kinds/#seed","title":"SEED","text":"<p>The <code>SEED</code> model kind is used to specify seed models for using static CSV datasets in your Vulcan project.</p> <p>How it works: You point to a CSV file, define the schema, and Vulcan loads it into a table. The data only gets reloaded if you change the model definition or update the CSV file.</p> <p>Use cases: - Reference data (countries, states, categories) - Lookup tables - Static configuration data - Test data</p> <p>Python Models</p> <p>Python models don't support the <code>SEED</code> kind, use a SQL model instead.</p> <p>When Data Reloads</p> <p>Seed models are loaded once and stay loaded unless you update the model definition or change the CSV file. This keeps things efficient, no point reloading static data every run!</p> <p>This example specifies a <code>SEED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INT,\n    item_id INT,\n    event_date DATE\n  ),\n  grains (id),\n  assertions (\n    UNIQUE_COMBINATION_OF_COLUMNS(columns := (id, event_date)),\n    NOT_NULL(columns := (id, item_id, event_date))\n  )\n)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.seed_example,\n  kind SEED (\n    path '../../seeds/seed_example.csv'\n  ),\n  columns (\n    id INT64,\n    item_id INT64,\n    event_date DATE\n  ),\n  grain (id, event_date)\n)\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>3038173937</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will upload the seed as a temp table in the physical layer.</p> <pre><code>vulcan-public-demo.vulcan__demo.__temp_demo__seed_example__3038173937_9kzbpld7\n</code></pre> <p>Vulcan will create a versioned table in the physical layer from the temp table.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `id`, `item_id`, `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`) AS `_subquery`\n</code></pre> <p>Vulcan will drop the temp table in the physical layer.</p> <pre><code>DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`seed_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937`\n</code></pre>"},{"location":"components/model/model_kinds/#scd-type-2","title":"SCD Type 2","text":"<p>SCD Type 2 is a model kind that supports slowly changing dimensions (SCDs) in your Vulcan project. SCDs are a common pattern in data warehousing that allow you to track changes to records over time.</p> <p>Vulcan achieves this by adding a <code>valid_from</code> and <code>valid_to</code> column to your model. The <code>valid_from</code> column is the timestamp that the record became valid (inclusive) and the <code>valid_to</code> column is the timestamp that the record became invalid (exclusive). The <code>valid_to</code> column is set to <code>NULL</code> for the latest record.</p> <p>Therefore, you can use these models to not only tell you what the latest value is for a given record but also what the values were anytime in the past. Note that maintaining this history does come at a cost of increased storage and compute and this may not be a good fit for sources that change frequently since the history could get very large.</p> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss, so data restatement is disabled for models of this kind by default.</p> <p>Vulcan supports two ways to detect changes: By Time (recommended) or By Column. Let's look at both:</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-recommended","title":"SCD Type 2 By Time (Recommended)","text":"<p>By Time is the recommended approach. It works with source tables that have an \"Updated At\" timestamp column (like <code>updated_at</code>, <code>modified_at</code>, <code>last_changed</code>).</p> <p>Why it's recommended: The timestamp tells you exactly when a record changed, which makes your SCD Type 2 table more accurate. You get precise <code>valid_from</code> times based on when the source system actually updated the record.</p> <p>If your source table has an <code>updated_at</code> column, use this approach!</p> <p>This example specifies a <code>SCD_TYPE_2_BY_TIME</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_time,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key dt\n  ),\n  grains (dt)\n);\n\nSELECT\n  dd.dt,\n  dd.year,\n  dd.month,\n  dd.day_of_week,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity) AS total_quantity_sold,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  CURRENT_TIMESTAMP AS updated_at\nFROM vulcan_demo.dim_dates AS dd\nLEFT JOIN vulcan_demo.orders AS o\n  ON dd.dt = o.order_date\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY dd.dt, dd.year, dd.month, dd.day_of_week\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_time_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"region_name\": \"string\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.regions\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id, c.name as customer_name, c.email, r.region_name\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.regions r ON c.region_id = r.region_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p> <p>The <code>updated_at</code> column name can also be changed by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    updated_at_name my_updated_at -- Name for `updated_at` column\n  )\n);\n\nSELECT\n  id,\n  name,\n  price,\n  my_updated_at\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  my_updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>By Column works when your source table doesn't have an \"Updated At\" timestamp. Instead, Vulcan compares the values in specific columns between runs and detects changes.</p> <p>How it works: You specify which columns to watch (or use <code>*</code> to watch all columns). When Vulcan detects a change in any of those columns, it records <code>valid_from</code> as the execution time when the change was detected.</p> <p>Use this when: Your source system doesn't track update timestamps, but you still want to maintain history. The trade-off is that <code>valid_from</code> times are based on when Vulcan detected the change, not when the source system actually changed it.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_COLUMN</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_column,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key ARRAY[product_id],\n    columns ARRAY[product_name, category, price]\n  ),\n  grains (product_id)\n);\n\nSELECT\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  p.price,\n  s.name AS supplier_name,\n  r.region_name\nFROM vulcan_demo.products AS p\nLEFT JOIN vulcan_demo.suppliers AS s\n  ON p.supplier_id = s.supplier_id\nLEFT JOIN vulcan_demo.regions AS r\n  ON s.region_id = r.region_id\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_column_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"price\": \"decimal(10,2)\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"product_name\", \"category\", \"price\"],\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT product_id, name as product_name, category, price\n    FROM vulcan_demo.products\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#change-column-names","title":"Change Column Names","text":"<p>Vulcan automatically adds <code>valid_from</code> and <code>valid_to</code> columns to your table. If you want different names (maybe to match your existing schema conventions), you can customize them: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    valid_from_name my_valid_from, -- Name for `valid_from` column\n    valid_to_name my_valid_to -- Name for `valid_to` column\n  )\n);\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  my_valid_from TIMESTAMP,\n  my_valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#deletes","title":"Deletes","text":"<p>A \"hard delete\" is when a record disappears from your source table entirely. How should SCD Type 2 handle this?</p> <p>Default behavior (<code>invalidate_hard_deletes: false</code>):</p> <ul> <li><code>valid_to</code> column will continue to be set to <code>NULL</code> (therefore still considered \"valid\")</li> <li>If the record is added back, then the <code>valid_to</code> column will be set to the <code>valid_from</code> of the new record.</li> </ul> <p>When a record is added back, the new record will be inserted into the table with <code>valid_from</code> set to:</p> <ul> <li>SCD_TYPE_2_BY_TIME: the largest of either the <code>updated_at</code> timestamp of the new record or the <code>valid_from</code> timestamp of the deleted record in the SCD Type 2 table</li> <li>SCD_TYPE_2_BY_COLUMN: the <code>execution_time</code> when the record was detected again</li> </ul> <p>With <code>invalidate_hard_deletes: true</code>:</p> <ul> <li><code>valid_to</code> is set to the execution time when Vulcan detected the missing record</li> <li>If the record comes back later, <code>valid_to</code> stays unchanged (you'll have a gap in history)</li> </ul> <p>Which should you use?</p> <ul> <li> <p><code>false</code> (default): Missing records are still considered \"valid\" (no gaps in history). Use this if you think missing records might be temporary or if you prefer continuous history.</p> </li> <li> <p><code>true</code>: Deletes are accurately tracked with precise timestamps. Use this if you want to know exactly when records were deleted, even if it creates gaps in history.</p> </li> </ul> <p>Think of it this way: <code>false</code> assumes \"missing = still valid\", while <code>true</code> assumes \"missing = deleted at this time\".</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-time-in-action","title":"Example of SCD Type 2 By Time in Action","text":"<p>Let's walk through a real example. Say you're tracking a restaurant menu, and you start with this source data (with <code>invalidate_hard_deletes: true</code>):</p> ID Name Price Updated At 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 2 Cheeseburger 8.99 2020-01-01 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Chocolate Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 4 Chocolate Milkshake 3.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL <p>Notice: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 00:00:00</code>. If you query the table for that time range, you won't see it, which accurately reflects that it wasn't on the menu during that period.</p> <p>This is the most accurate representation based on your source data. If <code>Cheeseburger</code> had been added back with its original <code>updated_at</code> timestamp (<code>2020-01-01</code>), Vulcan would have set the new record's <code>valid_from</code> to <code>2020-01-02 11:00:00</code> (when it was detected again), filling the gap. But since the timestamp didn't change, it's likely the item was removed in error, and the gap accurately represents that.</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-column-in-action","title":"Example of SCD Type 2 By Column in Action","text":"<p>Now let's see how By Column works. Same restaurant menu example, but this time the source table doesn't have an <code>updated_at</code> column. We'll configure the model to watch <code>Name</code> and <code>Price</code> for changes.</p> <p>Starting data:</p> ID Name Price 1 Chicken Sandwich 10.99 2 Cheeseburger 8.99 3 French Fries 4.99 <p>After the first run, your SCD Type 2 table looks like this:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 12.99 3 French Fries 4.99 4 Milkshake 3.99 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your pipeline ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 14.99 2 Cheeseburger 8.99 3 French Fries 4.99 4 Chocolate Milkshake 3.99 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>After running at <code>2020-01-03 11:00:00</code>, your final SCD Type 2 table:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 2020-01-03 11:00:00 1 Chicken Sandwich 14.99 2020-01-03 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 11:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 2020-01-03 11:00:00 4 Chocolate Milkshake 3.99 2020-01-03 11:00:00 NULL <p>Notice: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 11:00:00</code>. Query the table for that time range, and you won't see it, which accurately reflects that it wasn't on the menu during that period.</p>"},{"location":"components/model/model_kinds/#shared-configuration-options","title":"Shared Configuration Options","text":"Name Description Type unique_key Unique key used for identifying rows between source and target List of strings or string valid_from_name The name of the <code>valid_from</code> column to create in the target table. Default: <code>valid_from</code> string valid_to_name The name of the <code>valid_to</code> column to create in the target table. Default: <code>valid_to</code> string invalidate_hard_deletes If set to <code>true</code>, when a record is missing from the source table it will be marked as invalid. Default: <code>false</code> bool batch_size The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int <p>BigQuery Data Types</p> <p>On BigQuery, <code>valid_from</code> and <code>valid_to</code> columns default to <code>DATETIME</code>. If you want <code>TIMESTAMP</code> instead, specify it in your model definition:</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    time_data_type TIMESTAMP\n  )\n);\n</code></pre> <p>This might work on other engines too, but it's only been tested on BigQuery.</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-configuration-options","title":"SCD Type 2 By Time Configuration Options","text":"Name Description Type updated_at_name The name of the column containing a timestamp to check for new or updated records. Default: <code>updated_at</code> string updated_at_as_valid_from By default, for new rows <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code>. This changes the behavior to set it to the valid of <code>updated_at</code> when the row is inserted. Default: <code>false</code> bool"},{"location":"components/model/model_kinds/#scd-type-2-by-column-configuration-options","title":"SCD Type 2 By Column Configuration Options","text":"Name Description Type columns The name of the columns to check for changes. <code>*</code> to represent that all columns should be checked. List of strings or string execution_time_as_valid_from By default, when the model is first loaded <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code> and future new rows will have <code>execution_time</code> of when the pipeline ran. This changes the behavior to always use <code>execution_time</code>. Default: <code>false</code> bool updated_at_name If sourcing from a table that includes as timestamp to use as valid_from, set this property to that column. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int"},{"location":"components/model/model_kinds/#processing-source-table-with-historical-data","title":"Processing Source Table with Historical Data","text":"<p>Most of the time, you're creating history for a table that doesn't have it. Like the restaurant menu, it just shows what's available now, but you want to track what was available over time. For this use case, leave <code>batch_size</code> as <code>None</code> (the default).</p> <p>But what if your source already has history? Some systems create \"daily snapshot\" tables that contain historical records. If you're sourcing from one of these, set <code>batch_size</code> to <code>1</code> to process each interval sequentially (one day at a time if you're using <code>@daily</code> cron).</p> <p>Why sequential? SCD Type 2 needs to compare each day's snapshot to the previous day to detect changes. Processing them in order ensures the history is captured correctly.</p>"},{"location":"components/model/model_kinds/#example-source-from-daily-snapshot-table","title":"Example - Source from Daily Snapshot Table","text":"<pre><code>MODEL (\n    name db.table,\n    kind SCD_TYPE_2_BY_COLUMN (\n        unique_key id,\n        columns [some_value],\n        updated_at_name ds,\n        batch_size 1\n    ),\n    start '2025-01-01',\n    cron '@daily'\n);\nSELECT\n    id,\n    some_value,\n    ds\nFROM\n    source_table\nWHERE\n    ds between @start_ds and @end_ds\n</code></pre> <p>This processes each day sequentially, checking if <code>some_value</code> changed. When a change is detected, <code>valid_from</code> is set to match the <code>ds</code> column value (except for the very first record, which gets <code>1970-01-01 00:00:00</code>).</p> <p>If the source data was the following:</p> id some_value ds 1 1 2025-01-01 1 2 2025-01-02 1 3 2025-01-03 1 3 2025-01-04 <p>Then the resulting SCD Type 2 table would be:</p> id some_value ds valid_from valid_to 1 1 2025-01-01 1970-01-01 00:00:00 2025-01-02 00:00:00 1 2 2025-01-02 2025-01-02 00:00:00 2025-01-03 00:00:00 1 3 2025-01-03 2025-01-03 00:00:00 NULL"},{"location":"components/model/model_kinds/#querying-scd-type-2-models","title":"Querying SCD Type 2 Models","text":"<p>Even though SCD Type 2 models track history, querying the current version is still simple. Here are some common patterns:</p>"},{"location":"components/model/model_kinds/#querying-the-current-version","title":"Querying the Current Version","text":"<p>Want just the latest version of each record? Filter for <code>valid_to IS NULL</code>:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  valid_to IS NULL;\n</code></pre> <p>You can also create a view that adds an <code>is_current</code> flag to make it even easier for downstream consumers:</p> <pre><code>SELECT\n  *,\n  valid_to IS NULL AS is_current\nFROM\n  menu_items;\n</code></pre>"},{"location":"components/model/model_kinds/#querying-for-a-specific-point-in-time","title":"Querying for a Specific Point in Time","text":"<p>Want to see what the menu looked like on a specific date? Filter by <code>valid_from</code> and <code>valid_to</code>:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  id = 1\n  AND '2020-01-02 01:00:00' &gt;= valid_from\n  AND '2020-01-02 01:00:00' &lt; COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>Here's how you'd use it in a join (to get the menu item price that was valid when an order was placed):</p> <pre><code>SELECT\n  *\nFROM\n  orders\nINNER JOIN\n  menu_items\n  ON orders.menu_item_id = menu_items.id\n  AND orders.created_at &gt;= menu_items.valid_from\n  AND orders.created_at &lt; COALESCE(menu_items.valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>You can create a view that handles the <code>COALESCE</code> automatically, making point-in-time queries even easier:</p> <pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP)) AS valid_to\n  valid_to IS NULL AS is_current,\nFROM\n  menu_items;\n</code></pre> <p>Want to make <code>valid_to</code> inclusive so users can use <code>BETWEEN</code>? Adjust it like this: </p><pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2200-01-01 00:00:00+00:00' AS TIMESTAMP)) - INTERVAL 1 SECOND AS valid_to\n  valid_to IS NULL AS is_current,\n</code></pre><p></p> <p>Timestamp Precision</p> <p>This example uses second precision, so we subtract 1 second. Adjust the subtraction based on your timestamp precision (milliseconds, microseconds, etc.).</p>"},{"location":"components/model/model_kinds/#querying-for-deleted-records","title":"Querying for Deleted Records","text":"<p>To find records that were deleted, query for IDs that don't have a current version (<code>valid_to IS NULL</code>). Here's how:</p> <pre><code>SELECT\n  id,\n  MAX(CASE WHEN valid_to IS NULL THEN 0 ELSE 1 END) AS is_deleted\nFROM\n  menu_items\nGROUP BY\n  id\n</code></pre>"},{"location":"components/model/model_kinds/#reset-scd-type-2-model-clearing-history","title":"Reset SCD Type 2 Model (Clearing History)","text":"<p>By default, SCD Type 2 models protect your history, once it's gone, you can't recreate it. But sometimes you need to start fresh (maybe you're fixing a bug, or the history got corrupted).</p> <p>Warning: This will delete all historical data. Make sure you really want to do this!</p> <p>To reset history, follow these steps:</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    disable_restatement false\n  )\n);\n</code></pre> <p>Plan/apply this change to production. Then you will want to restate the model.</p> <p>Data Loss Warning</p> <p>This will permanently remove all historical data. In most cases, you cannot recover it. Make absolutely sure this is what you want!</p> <ol> <li>Once complete, remove <code>disable_restatement</code> from your model definition (sets it back to <code>true</code>) to prevent accidental data loss in the future</li> </ol> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n  )\n);\n</code></pre> <ol> <li>Plan and apply this change to production</li> </ol>"},{"location":"components/model/model_kinds/#external","title":"EXTERNAL","text":"<p>The EXTERNAL model kind is used to specify external models that store metadata about external tables. External models are special; they are not specified in .sql files like the other model kinds. They are optional but useful for propagating column and type information for external tables queried in your Vulcan project.</p>"},{"location":"components/model/model_kinds/#managed","title":"MANAGED","text":"<p>Warning</p> <p>Managed models are still under development and the API / semantics may change as support for more engines is added</p> <p>Note: Python models do not support the <code>MANAGED</code> model kind - use a SQL model instead.</p> <p>The <code>MANAGED</code> model kind is used to create models where the underlying database engine manages the data lifecycle.</p> <p>These models don't get updated with new intervals or refreshed when <code>vulcan run</code> is called. Responsibility for keeping the data up to date falls on the engine.</p> <p>You can control how the engine creates the managed model by using the <code>physical_properties</code> to pass engine-specific parameters for adapter to use when issuing commands to the underlying database.</p> <p>Due to there being no standard, each vendor has a different implementation with different semantics and different configuration parameters. Therefore, <code>MANAGED</code> models are not as portable between database engines as other Vulcan model types. In addition, due to their black-box nature, Vulcan has limited visibility into the integrity and state of the model.</p> <p>We would recommend using standard Vulcan model types in the first instance. However, if you do need to use Managed models, you still gain other Vulcan benefits like the ability to use them in virtual environments.</p> <p>See Managed Models for more information on which engines are supported and which properties are available.</p>"},{"location":"components/model/model_kinds/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Models of the <code>INCREMENTAL_BY_PARTITION</code> kind are computed incrementally based on partition. A set of columns defines the model's partitioning key, and a partition is the group of rows with the same partitioning key value.</p> <p>Should you use this model kind?</p> <p>Any model kind can use a partitioned table by specifying the <code>partitioned_by</code> key in the <code>MODEL</code> DDL.</p> <p>The \"partition\" in <code>INCREMENTAL_BY_PARTITION</code> is about how the data is loaded when the model runs.</p> <p><code>INCREMENTAL_BY_PARTITION</code> models are inherently non-idempotent, so restatements and other actions can cause data loss. This makes them more complex to manage than other model kinds.</p> <p>In most scenarios, an <code>INCREMENTAL_BY_TIME_RANGE</code> model can meet your needs and will be easier to manage. The <code>INCREMENTAL_BY_PARTITION</code> model kind should only be used when the data must be loaded by partition (usually for performance reasons).</p> <p>This model kind is designed for the scenario where data rows should be loaded and updated as a group based on their shared value for the partitioning key.</p> <p>It may be used with any SQL engine. Vulcan will automatically create partitioned tables on engines that support explicit table partitioning (e.g., BigQuery, Databricks).</p> <p>New rows are loaded based on their partitioning key value:</p> <ul> <li>If a partitioning key in newly loaded data is not present in the model table, the new partitioning key and its data rows are inserted.</li> <li>If a partitioning key in newly loaded data is already present in the model table, all the partitioning key's existing data rows in the model table are replaced with the partitioning key's data rows in the newly loaded data.</li> <li>If a partitioning key is present in the model table but not present in the newly loaded data, the partitioning key's existing data rows are not modified and remain in the model table.</li> </ul> <p>This kind should only be used for datasets that have the following traits:</p> <ul> <li>The dataset's records can be grouped by a partitioning key.</li> <li>Each record has a partitioning key associated with it.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their partitioning keys match.</li> <li>All existing records associated with a given partitioning key can be removed or overwritten when any new record has the partitioning key value.</li> </ul> <p>The column defining the partitioning key is specified in the model's <code>MODEL</code> DDL <code>partitioned_by</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_PARTITION</code> model:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.partition,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by ARRAY[warehouse_id, category],\n  grains (partitioned_analysis_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  p.category,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', p.category, '_', o.order_date::TEXT) AS partitioned_analysis_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p ON oi.product_id = p.product_id\nJOIN vulcan_demo.warehouses AS w ON o.warehouse_id = w.warehouse_id\nGROUP BY w.warehouse_id, w.name, p.category, o.order_date\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.partition_py\",\n    columns={\n        \"warehouse_id\": \"int\",\n        \"order_date\": \"date\",\n        \"daily_revenue\": \"decimal(10,2)\",\n    },\n    partitioned_by=[\"warehouse_id\"],\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_PARTITION,\n    ),\n    grains=[\"warehouse_id\", \"order_date\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT o.warehouse_id, o.order_date,\n           SUM(oi.quantity * oi.unit_price) as daily_revenue\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY o.warehouse_id, o.order_date\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>You can use multiple columns for composite partition keys:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by (warehouse_id, category)\n);\n</code></pre> <p>Some engines support expression-based partitioning. Here's a BigQuery example that partitions by month:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by DATETIME_TRUNC(order_date, MONTH)\n);\n</code></pre> <p>Only Full Restatements Supported</p> <p>Partial data restatements are used to reprocess part of a table's data (usually a limited time range).</p> <p>Partial data restatement is not supported for <code>INCREMENTAL_BY_PARTITION</code> models. If you restate an <code>INCREMENTAL_BY_PARTITION</code> model, its entire table will be recreated from scratch.</p> <p>Restating <code>INCREMENTAL_BY_PARTITION</code> models may lead to data loss and should be performed with care.</p>"},{"location":"components/model/model_kinds/#example","title":"Example","text":"<p>Here's a practical example that shows how to limit which partitions get updated using a CTE. This is a common pattern to avoid full restatements:</p> <pre><code>MODEL (\n  name demo.incremental_by_partition_demo,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by user_segment,\n);\n\n-- This is the source of truth for what partitions need to be updated and will join to the product usage data\n-- This could be an INCREMENTAL_BY_TIME_RANGE model that reads in the user_segment values last updated in the past 30 days to reduce scope\n-- Use this strategy to reduce full restatements\nWITH partitions_to_update AS (\n  SELECT DISTINCT\n    user_segment\n  FROM demo.incremental_by_time_range_demo  -- upstream table tracking which user segments to update\n  WHERE last_updated_at BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage\n  WHERE user_segment IN (SELECT user_segment FROM partitions_to_update) -- partition filter applied here\n)\n\nSELECT\n  product_id,\n  customer_id,\n  last_usage_date,\n  usage_count,\n  feature_utilization_score,\n  user_segment,\n  CASE\n    WHEN usage_count &gt; 100 AND feature_utilization_score &gt; 0.7 THEN 'Power User'\n    WHEN usage_count &gt; 50 THEN 'Regular User'\n    WHEN usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type\nFROM product_usage\n</code></pre> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss.</p>"},{"location":"components/model/model_kinds/#materialization-strategy_3","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_PARTITION</code> kind are materialized using the following strategies:</p> Engine Strategy Databricks REPLACE WHERE by partitioning key Spark INSERT OVERWRITE by partitioning key Snowflake DELETE by partitioning key, then INSERT BigQuery DELETE by partitioning key, then INSERT Redshift DELETE by partitioning key, then INSERT Postgres DELETE by partitioning key, then INSERT DuckDB DELETE by partitioning key, then INSERT"},{"location":"components/model/model_kinds/#incremental_unmanaged","title":"INCREMENTAL_UNMANAGED","text":"<p><code>INCREMENTAL_UNMANAGED</code> models are for append-only tables. They're \"unmanaged\" because Vulcan doesn't try to deduplicate or manage the data, it just runs your query and appends whatever it gets to the table.</p> <p>How it works: Every time the model runs, Vulcan executes your query and appends the results to the table. No deduplication, no updates, no deletes, just append, append, append.</p> <p>Should You Use This?</p> <p>Use it for: Data Vault patterns, event logs, audit trails, or any scenario where you need true append-only behavior.</p> <p>Don't use it for: Most other cases. <code>INCREMENTAL_BY_TIME_RANGE</code> or <code>INCREMENTAL_BY_UNIQUE_KEY</code> give you much more control and are usually better choices.</p> <p>When to use: - Data Vault hubs, links, or satellites - Event logs where every event should be preserved - Audit trails - Any pattern that requires true append-only semantics</p> <p>Here's how you'd set one up:</p> <pre><code>MODEL (\n  name vulcan_demo.incremental_unmanaged,\n  kind INCREMENTAL_UNMANAGED,\n  cron '@daily',\n  start '2025-01-01',\n  grains (shipment_id)\n);\n\n/* Append-only shipment event log */\nSELECT\n  s.shipment_id,\n  s.order_id,\n  s.shipped_date,\n  s.carrier,\n  o.customer_id,\n  c.name AS customer_name,\n  o.order_date,\n  (s.shipped_date - o.order_date::DATE)::INT AS days_to_ship,\n  CURRENT_TIMESTAMP AS shipment_event_timestamp\nFROM vulcan_demo.shipments AS s\nJOIN vulcan_demo.orders AS o ON s.order_id = o.order_id\nJOIN vulcan_demo.customers AS c ON o.customer_id = c.customer_id\nORDER BY s.shipped_date DESC\n</code></pre> <p>Note: Since it's unmanaged, <code>INCREMENTAL_UNMANAGED</code> doesn't support <code>batch_size</code> or <code>batch_concurrency</code> properties. Vulcan just runs your query and appends the results, no batching or concurrency control.</p> <p>Only Full Restatements Supported</p> <p>Similar to <code>INCREMENTAL_BY_PARTITION</code>, attempting to restate an <code>INCREMENTAL_UNMANAGED</code> model will trigger a full restatement. That is, the model will be rebuilt from scratch rather than from a time slice you specify.</p> <p>Be very careful when restating these models!</p>"},{"location":"components/model/overview/","title":"Overview","text":""},{"location":"components/model/overview/#overview","title":"Overview","text":"<p>Models are the heart of Vulcan, they're how you transform raw data into useful tables and views. Think of a model as a recipe: you define what you want (the metadata) and how to make it (the SQL query), and Vulcan handles the rest.</p> <p>Models live in <code>.sql</code> files in the <code>models/</code> directory of your project. The cool thing is that Vulcan automatically figures out how your models relate to each other by parsing your SQL, so you don't have to manually configure dependencies. Just write your SQL, and Vulcan handles the lineage.</p> <p>Every model has two parts:</p> <ul> <li>DDL (Data Definition Language) - The <code>MODEL</code> block that tells Vulcan what this model is (name, schedule, how to materialize it, etc.)</li> <li>DML (Data Manipulation Language) - The <code>SELECT</code> query that does the actual transformation work</li> </ul> <p>It's like filling out a form (DDL) and then writing the actual code (DML). Simple!</p>"},{"location":"components/model/overview/#model-structure","title":"Model Structure","text":"<p>You can write models in SQL or Python. Both work the same way conceptually, they just look different. Let's see both:</p> SQL ModelPython Model <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>Breaking it down: - Lines 1-6: The DDL (<code>MODEL</code> block) - tells Vulcan this is a daily sales model that runs every day - Lines 8-17: The DML (<code>SELECT</code> query) - the actual transformation that aggregates orders by date</p> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n  \"sales.daily_sales_py\",\n  columns={\n    \"order_date\": \"timestamp\",\n    \"total_orders\": \"int\",\n    \"total_revenue\": \"decimal(18,2)\",\n    \"last_order_id\": \"string\",\n  },\n  kind=dict(name=ModelKindName.FULL),\n  grains=[\"order_date\"],\n  depends_on=[\"raw.raw_orders\"],\n  cron='@daily',\n)\ndef execute(\n  context: ExecutionContext,\n  start: datetime,\n  end: datetime,\n  execution_time: datetime,\n  **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n  query = \"\"\"\n  SELECT\n    CAST(order_date AS TIMESTAMP) AS order_date,\n    COUNT(order_id)::INTEGER AS total_orders,\n    SUM(total_amount)::NUMERIC(18,2) AS total_revenue,\n    MAX(order_id)::VARCHAR AS last_order_id\n  FROM raw.raw_orders\n  GROUP BY order_date\n  ORDER BY order_date\n  \"\"\"\n\n  return context.fetchdf(query)\n</code></pre> <p>Breaking it down: - Lines 7-20: The DDL (<code>@model</code> decorator) - same metadata as SQL, just Python syntax - Lines 21-40: The DML (function body) - runs the SQL and returns a DataFrame</p> <p>Both formats do the same thing, pick whichever you're more comfortable with!</p>"},{"location":"components/model/overview/#ddl-the-model-block","title":"DDL: The MODEL Block","text":"<p>The <code>MODEL</code> block is where you tell Vulcan about your model. It's the first thing in your file (after any comments) and uses a simple, declarative syntax.</p>"},{"location":"components/model/overview/#basic-syntax","title":"Basic Syntax","text":"<p>Here's what a <code>MODEL</code> block looks like:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n</code></pre> <p>This tells Vulcan: - <code>name</code> - What to call this model (schema.table format) - <code>kind</code> - How to materialize it (FULL rebuilds everything, INCREMENTAL only updates changes, etc.) - <code>cron</code> - When to run it (<code>@daily</code> means every day) - <code>grain</code> - What makes each row unique (in this case, <code>order_date</code>)</p>"},{"location":"components/model/overview/#common-properties","title":"Common Properties","text":"<p>Here are the properties you'll use most often:</p> Property What It Does Example <code>name</code> Fully qualified model name (schema.table) <code>sales.daily_sales</code> <code>kind</code> Materialization strategy <code>FULL</code>, <code>INCREMENTAL</code>, <code>VIEW</code> <code>cron</code> When to run (scheduling) <code>'@daily'</code>, <code>'0 0 * * *'</code> <code>grain</code> Column(s) that make rows unique <code>order_date</code> or <code>(customer_id, order_date)</code> <code>owner</code> Who owns this model (for governance) <code>analytics_team</code> <code>description</code> Human-readable description <code>'Daily sales aggregates'</code> <p>More DDL Properties</p> <p>There are more properties available beyond these common ones. Check out the Model Properties reference for the complete list of all available model properties and their configurations.</p>"},{"location":"components/model/overview/#dml-the-select-query","title":"DML: The SELECT Query","text":"<p>The <code>SELECT</code> query is where the magic happens. This is your transformation logic, the SQL that actually does the work.</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>This query: - Reads from <code>raw.raw_orders</code> - Groups by <code>order_date</code> - Counts orders, sums revenue, finds the latest order ID - Returns the results ordered by date</p> <p>Pretty standard SQL! Vulcan will automatically figure out that this model depends on <code>raw.raw_orders</code> and build the dependency graph for you.</p>"},{"location":"components/model/overview/#conventions","title":"Conventions","text":"<p>Vulcan tries to be smart and infer as much as possible from your SQL. This means you don't have to write a bunch of YAML config files, just write SQL and Vulcan figures it out. But to do this, your SQL needs to follow some conventions.</p>"},{"location":"components/model/overview/#sql-model-conventions","title":"SQL Model Conventions","text":""},{"location":"components/model/overview/#unique-column-names","title":"Unique Column Names","text":"<p>Your final <code>SELECT</code> needs unique column names. No duplicates allowed!</p> <pre><code>-- \u2713 Good: Each column has a unique name\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>If you have duplicate column names, Vulcan won't know which one you mean, and that causes problems.</p>"},{"location":"components/model/overview/#explicit-types","title":"Explicit Types","text":"<p>Cast your types explicitly. This prevents surprises and ensures your schema is consistent:</p> <pre><code>-- Explicit casting ensures consistent schema\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,  -- explicit timestamp\n  COUNT(order_id)::INTEGER AS total_orders,                 -- explicit integer\n  SUM(total_amount)::FLOAT AS total_revenue,                -- explicit float\n  MAX(order_id)::VARCHAR AS last_order_id                   -- explicit varchar\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan uses PostgreSQL-style casting (<code>x::int</code>), but don't worry, it automatically converts this to whatever your execution engine needs. So you write <code>::INTEGER</code> and Vulcan handles the rest.</p> <p>Why this matters: Without explicit types, you might get <code>FLOAT</code> when you expected <code>INTEGER</code>, or <code>VARCHAR</code> when you wanted <code>TIMESTAMP</code>. Explicit casting prevents these surprises.</p>"},{"location":"components/model/overview/#inferrable-names","title":"Inferrable Names","text":"<p>Your columns need names that Vulcan can figure out. If Vulcan can't infer a name, you need to add an alias:</p> <pre><code>SELECT\n  1,                              -- \u274c not inferrable (what do you call this?)\n  total_amount + 1,               -- \u274c not inferrable (needs an alias)\n  SUM(total_amount),              -- \u274c not inferrable (needs an alias)\n  order_date,                     -- \u2713 inferrable as order_date\n  order_date::TIMESTAMP,          -- \u2713 inferrable as order_date\n  total_amount + 1 AS adjusted,   -- \u2713 explicitly named\n  SUM(total_amount) AS revenue    -- \u2713 explicitly named\n</code></pre> <p>If you forget an alias, Vulcan's formatter will add one automatically when it renders your SQL. But it's better to be explicit, you'll know what the column is called!</p>"},{"location":"components/model/overview/#column-descriptions","title":"Column Descriptions","text":"<p>Document your columns! There are two ways to do this:</p> <p>Option 1: In the DDL (Recommended)</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Aggregated daily sales metrics',\n  column_descriptions (\n    order_date = 'The date of the sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts for this date',\n    last_order_id = 'The most recent order ID for this date'\n  )\n);\n</code></pre> <p>This is the cleanest way, all your documentation is in one place, right in the MODEL block.</p> <p>Priority</p> <p>If you use <code>column_descriptions</code> in the DDL, Vulcan will use those and ignore any inline comments in your query. DDL descriptions take priority, so if you define descriptions in both places, the DDL version wins.</p> <p>Option 2: Inline Comments</p> <p>If you don't specify <code>column_descriptions</code> in the DDL, Vulcan will automatically pick up comments from your query:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  order_date::TIMESTAMP AS order_date,           -- The date of sales transactions\n  COUNT(order_id)::INTEGER AS total_orders,      -- Number of orders placed\n  SUM(total_amount)::FLOAT AS total_revenue,     -- Total revenue for the day\n  MAX(order_id)::VARCHAR AS last_order_id        -- Most recent order ID\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan will register these comments as column descriptions in your database. Pretty handy!</p> <p>Table comments: If you put a comment before the <code>MODEL</code> block, Vulcan will use it as the table description. But if you also specify <code>description</code> in the MODEL block, that takes priority.</p>"},{"location":"components/model/overview/#python-model-conventions","title":"Python Model Conventions","text":"<p>Python models work a bit differently because Python doesn't have the same type inference capabilities as SQL.</p>"},{"location":"components/model/overview/#explicit-column-definitions","title":"Explicit Column Definitions","text":"<p>You must define your columns explicitly in the <code>@model</code> decorator:</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(name=ModelKindName.FULL),\n)\n</code></pre> <p>Vulcan can't infer types from Python code the way it can from SQL, so you need to tell it explicitly.</p>"},{"location":"components/model/overview/#explicit-dependencies","title":"Explicit Dependencies","text":"<p>Unlike SQL models (where Vulcan figures out dependencies automatically), Python models need you to list them:</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={...},\n    depends_on=[\"raw.raw_orders\"],  # Must explicitly list upstream models\n)\n</code></pre> <p>This is because Vulcan can't parse your Python code to find <code>FROM</code> clauses and joins. You need to tell it what this model depends on.</p>"},{"location":"components/model/overview/#column-descriptions_1","title":"Column Descriptions","text":"<p>Python models can't use inline comments for column descriptions. Instead, specify them in the decorator:</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n    },\n    column_descriptions={\n        \"order_date\": \"The date of sales transactions\",\n        \"total_orders\": \"Number of orders placed on this date\",\n        \"total_revenue\": \"Total revenue for the day\",\n    },\n)\n</code></pre> <p>Column name validation</p> <p>Vulcan will error if you put a column name in <code>column_descriptions</code> that doesn't exist in <code>columns</code>. This prevents typos and keeps things consistent, if you describe a column, it better exist!</p>"},{"location":"components/model/overview/#return-type","title":"Return Type","text":"<p>Your <code>execute</code> function must return a pandas DataFrame, and the columns must match what you defined in <code>columns</code>:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:  # Must return a DataFrame\n    query = \"SELECT ...\"\n    return context.fetchdf(query)\n</code></pre> <p>The DataFrame columns need to match your <code>columns</code> definition exactly, same names, compatible types.</p> <p>Learn more</p> <p>Want to dive deeper into Python models? Check out the Python Models documentation for detailed information, advanced patterns, and more examples.</p>"},{"location":"components/model/overview/#comment-registration","title":"Comment Registration","text":"<p>Vulcan can register comments (descriptions) in your database so they show up in your BI tools and data catalogs. This is super useful for documentation!</p>"},{"location":"components/model/overview/#how-comments-get-registered","title":"How Comments Get Registered","text":"<p>Model-level comments: - If you put a comment before the <code>MODEL</code> block, Vulcan uses it as the table comment - If you also specify <code>description</code> in the MODEL block, that takes priority</p> <p>Column-level comments: - Use <code>column_descriptions</code> in the DDL (recommended) - Or use inline comments in your SELECT query (if <code>column_descriptions</code> isn't specified)</p>"},{"location":"components/model/overview/#what-gets-registered","title":"What Gets Registered","text":"<p>Not everything gets comments registered:</p> <ul> <li>\u2705 Physical tables - Comments are registered (tables in the <code>vulcan__[project schema]</code> schema)</li> <li>\u2705 Production views - Comments are registered</li> <li>\u274c Temporary tables - No comments (they're temporary!)</li> <li>\u274c Non-production views - No comments (keeps things clean)</li> </ul> <p>Note: Some engines automatically pass comments from physical tables to views that select from them. So even if Vulcan didn't explicitly register a comment on a view, it might still show up if the engine does this automatically.</p>"},{"location":"components/model/overview/#engine-support","title":"Engine Support","text":"<p>Different databases support comments differently. Some can register comments in the <code>CREATE</code> statement (one command), others need separate commands for each comment.</p> <p>Here's what each engine supports:</p> Engine <code>TABLE</code> comments <code>VIEW</code> comments Athena N N BigQuery Y Y ClickHouse Y Y Databricks Y Y DuckDB &lt;=0.9 N N DuckDB &gt;=0.10 Y Y MySQL Y Y MSSQL N N Postgres Y Y GCP Postgres Y Y Redshift Y N Snowflake Y Y Spark Y Y Trino Y Y <p>If your engine doesn't support comments, Vulcan will skip registration (no errors, it just won't register them).</p>"},{"location":"components/model/overview/#macros","title":"Macros","text":"<p>Macros are like variables for your SQL. They let you parameterize queries and avoid repetition. Vulcan provides several built-in macros (like <code>@start_ds</code> and <code>@end_ds</code> for incremental models), and you can define your own.</p> <p>Macros use the <code>@</code> prefix. For example, <code>@this_model</code> refers to the current model being processed, and <code>@start_ds</code> is the start date for incremental processing.</p> <p>Want to learn more? Check out the macros documentation for all the details.</p>"},{"location":"components/model/properties/","title":"Properties","text":""},{"location":"components/model/properties/#properties","title":"Properties","text":"<p>The <code>MODEL</code> DDL statement has a bunch of properties you can use to control how your model behaves. Think of them as knobs and switches, you can configure scheduling, storage, validation, and more.</p> <p>This page is your complete reference for all available properties. We'll cover what each one does, when to use it, and show you examples.</p>"},{"location":"components/model/properties/#quick-reference","title":"Quick Reference","text":"Property Description Type Required <code>name</code> Fully qualified model name (<code>schema.model</code> or <code>catalog.schema.model</code>) <code>str</code> N* <code>project</code> Project name for multi-repo deployments <code>str</code> N <code>kind</code> Model kind (VIEW, FULL, INCREMENTAL, etc.) <code>str</code> | <code>dict</code> N <code>cron</code> Schedule expression for model refresh <code>str</code> N <code>cron_tz</code> Timezone for the cron schedule <code>str</code> N <code>interval_unit</code> Temporal granularity of data intervals <code>str</code> N <code>start</code> Earliest date/time to process <code>str</code> | <code>int</code> N <code>end</code> Latest date/time to process <code>str</code> | <code>int</code> N <code>grain</code> Column(s) defining row uniqueness <code>str</code> | <code>array</code> N <code>grains</code> Multiple unique key definitions <code>array</code> N <code>owner</code> Model owner for governance <code>str</code> N <code>description</code> Model description (registered as table comment) <code>str</code> N <code>column_descriptions</code> Column-level comments <code>dict</code> N <code>columns</code> Explicit column names and types <code>array</code> N <code>dialect</code> SQL dialect of the model <code>str</code> N <code>tags</code> Labels for organizing models <code>array[str]</code> N <code>assertions</code> Audits to run after model evaluation <code>array</code> N <code>profiles</code> Columns to track statistical metrics over time <code>array</code> N <code>depends_on</code> Explicit model dependencies <code>array[str]</code> N <code>references</code> Non-unique join relationship columns <code>array</code> N <code>partitioned_by</code> Partition key column(s) <code>str</code> | <code>array</code> N <code>clustered_by</code> Clustering column(s) <code>str</code> N <code>table_format</code> Table format (iceberg, hive, delta) <code>str</code> N <code>storage_format</code> Storage format (parquet, orc) <code>str</code> N <code>physical_properties</code> Engine-specific table/view properties <code>dict</code> N <code>virtual_properties</code> Engine-specific view layer properties <code>dict</code> N <code>session_properties</code> Engine session properties <code>dict</code> N <code>stamp</code> Arbitrary version string <code>str</code> N <code>enabled</code> Whether model is enabled <code>bool</code> N <code>allow_partials</code> Allow partial data intervals <code>bool</code> N <code>gateway</code> Specific gateway for execution <code>str</code> N <code>optimize_query</code> Enable query optimization <code>bool</code> N <code>formatting</code> Enable model formatting <code>bool</code> N <code>ignored_rules</code> Linter rules to ignore <code>str</code> | <code>array</code> N <p>*Required unless name inference is enabled.</p>"},{"location":"components/model/properties/#general-properties","title":"General Properties","text":""},{"location":"components/model/properties/#name","title":"name","text":"<p>Your model's name is how it's identified in the data warehouse. It needs at least a schema (<code>schema.model</code>), and you can optionally include a catalog (<code>catalog.schema.model</code>).</p> <p>Format: <code>schema.model</code> or <code>catalog.schema.model</code></p> <p>This becomes the production table/view name that other models and users will reference.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,        -- schema.model format\n);\n\n-- Or with catalog\nMODEL (\n  name catalog.sales.daily_sales -- catalog.schema.model format\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",  # schema.model format\n)\ndef execute(context, **kwargs):\n    ...\n\n# Or with catalog\n@model(\n    \"catalog.sales.daily_sales\",  # catalog.schema.model format\n)\n</code></pre> <p>Environment Prefixing</p> <p>In non-production environments, Vulcan automatically prefixes your model names. So <code>sales.daily_sales</code> becomes <code>sales__dev.daily_sales</code> in the dev environment. This keeps your dev and prod data separate without you having to think about it.</p>"},{"location":"components/model/properties/#project","title":"project","text":"<p>If you're running multiple Vulcan projects in the same repository (multi-repo setup), use <code>project</code> to specify which project this model belongs to. This helps Vulcan organize and isolate models from different projects.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  project 'analytics_project',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    project=\"analytics_project\",\n)\n</code></pre>"},{"location":"components/model/properties/#kind","title":"kind","text":"<p>The <code>kind</code> property determines how your model is computed and stored. Do you want to rebuild everything each run? Update incrementally? Create a view? This is where you decide.</p> <p>For all the details on each kind and when to use them, check out the Model Kinds documentation.</p> SQLPython <pre><code>-- VIEW (default for SQL)\nMODEL (\n  name sales.daily_sales,\n  kind VIEW,\n);\n\n-- FULL\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n);\n\n-- Incremental with properties\nMODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n  ),\n);\n\n-- SEED\nMODEL (\n  name raw.holidays,\n  kind SEED (\n    path 'seeds/holidays.csv',\n  ),\n);\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n# FULL (default for Python)\n@model(\n    \"sales.daily_sales\",\n    kind=dict(name=ModelKindName.FULL),\n)\n\n# Incremental\n@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n    ),\n)\n\n# SCD Type 2\n@model(\n    \"dim.customers\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#cron","title":"cron","text":"<p>Controls when your model runs. You can use standard cron expressions or Vulcan's shortcuts for common schedules.</p> <p>Why this matters: Without a schedule, your model only runs when you manually trigger it. Set a cron, and Vulcan will automatically process new data on schedule.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',          -- Daily at midnight UTC\n);\n\nMODEL (\n  name sales.hourly_metrics,\n  cron '@hourly',         -- Every hour\n);\n\nMODEL (\n  name sales.custom_schedule,\n  cron '0 6 * * *',       -- Custom: every day at 6 AM UTC\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n)\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"@hourly\",\n)\n\n@model(\n    \"sales.custom_schedule\",\n    cron=\"0 6 * * *\",  # Every day at 6 AM UTC\n)\n</code></pre> <p>Cron shortcuts: Vulcan provides convenient shortcuts: - <code>@hourly</code> - Every hour - <code>@daily</code> - Every day at midnight UTC - <code>@weekly</code> - Once per week - <code>@monthly</code> - Once per month</p> <p>These are much easier than writing <code>0 * * * *</code>!</p>"},{"location":"components/model/properties/#cron_tz","title":"cron_tz","text":"<p>Sets the timezone for your cron schedule. This only affects when the model runs, not how time intervals are calculated (those are always UTC).</p> <p>Example: If you set <code>cron '@daily'</code> and <code>cron_tz 'America/Los_Angeles'</code>, your model runs at midnight Pacific time, but the time intervals it processes are still in UTC.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',\n  cron_tz 'America/Los_Angeles',  -- Runs at midnight Pacific time\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n    cron_tz=\"America/Los_Angeles\",\n)\n</code></pre>"},{"location":"components/model/properties/#interval_unit","title":"interval_unit","text":"<p>Controls the granularity of time intervals for incremental models. By default, Vulcan figures this out from your <code>cron</code> expression, but you can override it if needed.</p> <p>Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code></p> <p>When to override: If your cron runs daily but you want to process hourly intervals, set <code>interval_unit 'hour'</code>. This is useful when you want finer-grained control over incremental processing.</p> SQLPython <pre><code>MODEL (\n  name sales.hourly_metrics,\n  cron '30 7 * * *',      -- Run daily at 7:30 AM\n  interval_unit 'hour',   -- Process hourly intervals (not daily)\n  );\n</code></pre> <pre><code>from vulcan import IntervalUnit\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"30 7 * * *\",\n    interval_unit=IntervalUnit.HOUR,\n)\n</code></pre>"},{"location":"components/model/properties/#start","title":"start","text":"<p>Sets the earliest date/time your model should process. This is useful for limiting backfills or defining when your model's data begins.</p> <p>You can use: - Absolute dates: <code>'2024-01-01'</code> - Relative expressions: <code>'1 year ago'</code> - Epoch milliseconds: <code>1704067200000</code></p> SQLPython <pre><code>-- Absolute date\nMODEL (\n  name sales.daily_sales,\n  start '2024-01-01',\n);\n\n-- Relative expression\nMODEL (\n  name sales.recent_sales,\n  start '1 year ago',\n);\n\n-- Epoch milliseconds\nMODEL (\n  name sales.events,\n  start 1704067200000,\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    start=\"2024-01-01\",\n)\n\n@model(\n    \"sales.recent_sales\",\n    start=\"1 year ago\",\n)\n</code></pre>"},{"location":"components/model/properties/#end","title":"end","text":"<p>Sets the latest date/time your model should process. Uses the same format as <code>start</code>. This is handy for historical models or limiting processing to a specific time range.</p> SQLPython <pre><code>MODEL (\n  name sales.historical_sales,\n  start '2020-01-01',\n  end '2023-12-31',\n  );\n</code></pre> <pre><code>@model(\n    \"sales.historical_sales\",\n    start=\"2020-01-01\",\n    end=\"2023-12-31\",\n)\n</code></pre>"},{"location":"components/model/properties/#grain-grains","title":"grain / grains","text":"<p>Defines the column(s) that make each row unique. This is like a primary key, it tells Vulcan what identifies a single row in your table.</p> <p>Why this matters: Tools like <code>table_diff</code> use grains to compare tables. It also helps Vulcan understand your data structure for better optimization and validation.</p> <p>You can specify a single grain (<code>grain order_id</code>) or multiple grains (<code>grains (order_id, (customer_id, order_date))</code>).</p> SQLPython <pre><code>-- Single column grain\nMODEL (\n  name sales.daily_sales,\n  grain order_date,\n);\n\n-- Composite grain\nMODEL (\n  name sales.customer_daily,\n  grain (customer_id, order_date),\n);\n\n-- Multiple grains\nMODEL (\n  name sales.orders,\n  grains (\n    order_id,\n    (customer_id, order_date)\n  ),\n);\n</code></pre> <pre><code># Single grain\n@model(\n    \"sales.daily_sales\",\n    grains=[\"order_date\"],\n)\n\n# Composite grain\n@model(\n    \"sales.customer_daily\",\n    grains=[(\"customer_id\", \"order_date\")],\n)\n\n# Multiple grains\n@model(\n    \"sales.orders\",\n    grains=[\n        \"order_id\",\n        (\"customer_id\", \"order_date\"),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#owner","title":"owner","text":"<p>Sets the owner of the model, usually a team name or individual. This is used for governance, notifications, and knowing who to contact when something breaks.</p> <p>Example: <code>owner 'analytics_team'</code> or <code>owner 'data_engineers'</code></p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  owner 'analytics_team',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    owner=\"analytics_team\",\n)\n</code></pre>"},{"location":"components/model/properties/#description","title":"description","text":"<p>A human-readable description of what your model does. Vulcan automatically registers this as a table comment in your SQL engine (if it supports comments), so it shows up in your BI tools and data catalogs.</p> <p>Pro tip: Write descriptions that explain the business purpose, not just the technical details. Future you (and your teammates) will thank you!</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  description 'Aggregated daily sales metrics including total orders and revenue',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    description=\"Aggregated daily sales metrics including total orders and revenue\",\n)\n</code></pre>"},{"location":"components/model/properties/#column_descriptions","title":"column_descriptions","text":"<p>Document your columns! This property lets you add descriptions for each column, which get registered as column comments in your database.</p> <p>Why document columns? When someone queries your table in a BI tool, they'll see what each column means. It's like inline documentation that travels with your data.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  column_descriptions (\n    order_date = 'The date of sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts',\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n    },\n    column_descriptions={\n        \"order_date\": \"The date of sales transactions\",\n        \"total_orders\": \"Count of orders placed on this date\",\n        \"total_revenue\": \"Sum of all order amounts\",\n    },\n)\n</code></pre> <p>Priority</p> <p>If <code>column_descriptions</code> is present, inline column comments will not be registered.</p>"},{"location":"components/model/properties/#columns","title":"columns","text":"<p>Explicitly defines your model's column names and data types. When you use this, Vulcan won't try to infer types from your query, it'll use exactly what you specify.</p> <p>When to use: - Python models (required\u2014Vulcan can't infer types from Python code) - Seed models (you need to define the CSV schema) - When you want strict type control</p> SQLPython <pre><code>MODEL (\n  name sales.national_holidays,\n  kind SEED (path 'holidays.csv'),\n  columns (\n    holiday_name VARCHAR,\n    holiday_date DATE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n)\ndef execute(context, **kwargs) -&gt; pd.DataFrame:\n    ...\n</code></pre> <p>Python Models</p> <p>This is required for Python models since Vulcan can't infer column types from Python code. You must explicitly define your schema.</p>"},{"location":"components/model/properties/#dialect","title":"dialect","text":"<p>Specifies the SQL dialect your model uses. Defaults to whatever you set in <code>model_defaults</code>, but you can override it per-model if needed.</p> <p>Why this matters: Vulcan uses SQLGlot to parse and transpile SQL. You can write in one dialect (like PostgreSQL) and Vulcan will convert it to whatever your engine needs (like BigQuery). Pretty neat!</p> <p>Supports all SQLGlot dialects.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  dialect 'snowflake',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    dialect=\"snowflake\",\n)\n</code></pre>"},{"location":"components/model/properties/#tags","title":"tags","text":"<p>Labels for organizing and filtering models.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  tags ['sales', 'daily', 'core'],\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    tags=[\"sales\", \"daily\", \"core\"],\n)\n</code></pre>"},{"location":"components/model/properties/#assertions","title":"assertions","text":"<p>Attach audits directly to your model. These validations run after each model evaluation and will block the pipeline if they fail.</p> <p>Why use assertions? They're your safety net, they catch bad data before it flows downstream. If revenue can't be negative, assert it. If customer IDs must be unique, assert it. Fail fast, fix fast.</p> <p>Think of assertions as \"this data must be true\" validations that run automatically.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    not_null(columns := (order_date, customer_id)),\n    unique_values(columns := (order_id)),\n    accepted_range(column := price, min_v := 0, max_v := 1000),\n    forall(criteria := (price &gt; 0, quantity &gt;= 1))\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    assertions=[\n        (\"not_null\", {\"columns\": [\"order_date\", \"customer_id\"]}),\n        (\"unique_values\", {\"columns\": [\"order_id\"]}),\n        (\"accepted_range\", {\"column\": \"price\", \"min_v\": 0, \"max_v\": 1000}),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#profiles","title":"profiles","text":"<p>Enable automatic data profiling for specific columns. Profiles track statistical metrics over time (like null percentages, distinct counts, distributions) without blocking your pipeline.</p> <p>How it works: Vulcan collects metrics each run and stores them in the <code>_check_profiles</code> table. You can query this to see how your data changes over time, detect data drift, understand patterns, and decide which checks or audits to add.</p> <p>Use cases: - Track null percentages over time - Monitor distinct value counts - Detect data drift - Understand column distributions - Inform which checks/audits to create</p> <p>Think of profiles as your data observability layer, they watch and learn, but don't block.</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  grains (customer_id),\n  profiles (customer_id, customer_name, email, total_orders, total_spent, avg_order_value)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"avg_order_value\": \"decimal(10,2)\",\n    },\n    kind=\"full\",\n    grains=[\"customer_id\"],\n    profiles=[\"customer_id\", \"customer_name\", \"email\", \"total_orders\", \"total_spent\", \"avg_order_value\"],\n)\ndef execute(context, **kwargs):\n    ...\n</code></pre>"},{"location":"components/model/properties/#depends_on","title":"depends_on","text":"<p>Explicitly declare model dependencies. Vulcan automatically infers dependencies from SQL queries, but sometimes you need to add extra ones.</p> <p>When to use: - Python models (required\u2014Vulcan can't parse Python to find dependencies) - Hidden dependencies (like a macro that references another model) - External dependencies that aren't in your SQL</p> <p>Note: Dependencies you declare here are added to the ones Vulcan infers, they don't replace them.</p> SQLPython <pre><code>MODEL (\n  name sales.summary,\n  depends_on ['sales.daily_sales', 'sales.products'],\n);\n</code></pre> <pre><code>@model(\n    \"sales.summary\",\n    depends_on=[\"sales.daily_sales\", \"sales.products\"],\n)\n</code></pre> <p>Python Models</p> <p>Python models require <code>depends_on</code> since Vulcan can't automatically infer dependencies from Python code. You need to tell it explicitly what your model depends on.</p>"},{"location":"components/model/properties/#references","title":"references","text":"<p>Declare non-unique join relationships to other models. These help Vulcan understand how models relate to each other for better lineage and optimization.</p> <p>Example: If your <code>orders</code> table has a <code>customer_id</code> that joins to <code>customers.customer_id</code>, you'd add <code>customer_id</code> to references. This tells Vulcan about the relationship even though <code>customer_id</code> isn't unique in the orders table.</p> SQLPython <pre><code>MODEL (\n  name sales.orders,\n  references (\n    customer_id,\n    guest_id AS account_id,  -- Alias for joining to account_id grain\n  ),\n);\n</code></pre> <pre><code>@model(\n    \"sales.orders\",\n    references=[\n        \"customer_id\",\n        (\"guest_id\", \"account_id\"),  # Alias\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#storage-properties","title":"Storage Properties","text":"<p>These properties control how your data is physically stored in the database. They're engine-specific, so check your engine's documentation for what's supported.</p>"},{"location":"components/model/properties/#partitioned_by","title":"partitioned_by","text":"<p>Defines the partition key for your table. Partitioning splits your table into chunks based on column values, which makes queries faster (the engine can skip irrelevant partitions).</p> <p>Supported engines: Spark, BigQuery, Databricks, and others that support table partitioning.</p> <p>Why partition? If you're querying data from the last 7 days and your table is partitioned by date, the engine only scans 7 partitions instead of scanning the entire table. That's a huge performance win!</p> SQLPython <pre><code>-- Single column partition\nMODEL (\n  name sales.events,\n  partitioned_by event_date,\n);\n\n-- Partition with transformation (BigQuery)\nMODEL (\n  name sales.events,\n  partitioned_by TIMESTAMP_TRUNC(event_ts, DAY),\n);\n\n-- Multi-column partition\nMODEL (\n  name sales.events,\n  partitioned_by (year, month, day),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n)\n\n# Multi-column\n@model(\n    \"sales.events\",\n    partitioned_by=[\"year\", \"month\", \"day\"],\n)\n</code></pre>"},{"location":"components/model/properties/#clustered_by","title":"clustered_by","text":"<p>Sets clustering columns for engines that support it (like BigQuery). Clustering organizes data within partitions based on column values, which makes range queries and filters faster.</p> <p>How it works: Data is physically stored sorted by the clustering columns. When you filter on those columns, the engine can skip reading irrelevant data blocks.</p> <p>Example: If you cluster by <code>customer_id</code>, queries filtering by customer will be faster because related data is stored together.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  partitioned_by event_date,\n  clustered_by (customer_id, product_id),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n    clustered_by=[\"customer_id\", \"product_id\"],\n)\n</code></pre>"},{"location":"components/model/properties/#table_format","title":"table_format","text":"<p>Specifies the table format for engines that support multiple formats. Different formats have different features and performance characteristics.</p> <p>Supported formats: <code>iceberg</code>, <code>hive</code>, <code>delta</code></p> <p>When to use: If your engine supports multiple formats, choose based on your needs: - Iceberg: Great for time travel and schema evolution - Delta: Good for ACID transactions and time travel - Hive: Traditional format, widely supported</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  table_format 'iceberg',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    table_format=\"iceberg\",\n)\n</code></pre>"},{"location":"components/model/properties/#storage_format","title":"storage_format","text":"<p>Sets the physical file format for your table's data files. This affects compression, query performance, and storage costs.</p> <p>Common formats: <code>parquet</code>, <code>orc</code></p> <p>Parquet is usually the best choice, it's columnar (great for analytics), has good compression, and is widely supported. ORC is another option, especially if you're using Hive.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  storage_format 'parquet',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    storage_format=\"parquet\",\n)\n</code></pre>"},{"location":"components/model/properties/#engine-properties","title":"Engine Properties","text":"<p>These properties let you pass engine-specific settings to Vulcan. Each engine has different capabilities, so these properties vary by engine.</p>"},{"location":"components/model/properties/#physical_properties","title":"physical_properties","text":"<p>Pass engine-specific properties directly to the physical table/view creation. This is where you set things like retention policies, labels, or other engine-specific features.</p> <p>Use cases: - Set table retention (BigQuery: <code>partition_expiration_days</code>) - Add labels or tags (BigQuery, Snowflake) - Configure table type (Snowflake: <code>TRANSIENT</code> tables) - Any other engine-specific table settings</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  physical_properties (\n    partition_expiration_days = 7,\n    require_partition_filter = true,\n    creatable_type = TRANSIENT,  -- Creates TRANSIENT TABLE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    physical_properties={\n        \"partition_expiration_days\": 7,\n        \"require_partition_filter\": True,\n        \"creatable_type\": \"TRANSIENT\",\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#virtual_properties","title":"virtual_properties","text":"<p>Pass engine-specific properties to the virtual layer view. This is useful for things like view-level security, labels, or other view-specific settings.</p> <p>Use cases: - Create secure views (Snowflake: <code>SECURE</code> views) - Add labels to views - Set view-level permissions - Configure view-specific engine features</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  virtual_properties (\n    creatable_type = SECURE,  -- Creates SECURE VIEW\n    labels = [('team', 'analytics')]\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    virtual_properties={\n        \"creatable_type\": \"SECURE\",\n        \"labels\": [(\"team\", \"analytics\")],\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#session_properties","title":"session_properties","text":"<p>Set session-level properties that apply when Vulcan executes your model. These affect how queries run but don't change the table structure.</p> <p>Use cases: - Set query timeouts - Configure parallelism - Adjust memory limits - Set engine-specific session variables</p> <p>Example: If you have a large query that needs more time, set <code>query_timeout: 3600</code> to give it an hour instead of the default timeout.</p> SQLPython <pre><code>MODEL (\n  name sales.large_query,\n  session_properties (\n    query_timeout = 3600,\n    max_parallel_workers = 8,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.large_query\",\n    session_properties={\n        \"query_timeout\": 3600,\n        \"max_parallel_workers\": 8,\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#gateway","title":"gateway","text":"<p>Specifies which gateway to use for executing this model. Useful when you have multiple database connections and want to route specific models to specific databases.</p> <p>When to use: Multi-warehouse setups, isolated environments, or when you need to run certain models on a different database than the default.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  gateway 'warehouse_gateway',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    gateway=\"warehouse_gateway\",\n)\n</code></pre>"},{"location":"components/model/properties/#behavior-properties","title":"Behavior Properties","text":"<p>These properties control how Vulcan behaves when processing your model.</p>"},{"location":"components/model/properties/#stamp","title":"stamp","text":"<p>Force a new model version without changing the definition. This is like a version tag, useful for tracking deployments or forcing a refresh.</p> <p>When to use: When you want to create a new version for tracking purposes, or when you need to force downstream models to rebuild even though this model's definition hasn't changed.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  stamp 'v2.1.0',  -- Force new version\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    stamp=\"v2.1.0\",\n)\n</code></pre>"},{"location":"components/model/properties/#enabled","title":"enabled","text":"<p>Control whether the model is active. Set to <code>false</code> to disable a model without deleting it.</p> <p>When to use: - Temporarily disable a model while debugging - Deprecate a model but keep it for reference - Skip models during development</p> <p>Default: <code>true</code> (models are enabled by default)</p> SQLPython <pre><code>MODEL (\n  name sales.deprecated_model,\n  enabled false,  -- Model will be ignored\n);\n</code></pre> <pre><code>@model(\n    \"sales.deprecated_model\",\n    enabled=False,\n)\n</code></pre>"},{"location":"components/model/properties/#allow_partials","title":"allow_partials","text":"<p>Allow processing of incomplete data intervals. By default, Vulcan waits for complete intervals before processing (keeps data quality high). Set this to <code>true</code> if you need to process partial intervals.</p> <p>When to use: - Real-time or near-real-time pipelines - When you need data ASAP, even if it's incomplete - Streaming data scenarios</p> <p>Trade-off: You lose the ability to distinguish between \"missing data\" (pipeline issue) and \"partial interval\" (expected). Use with caution!</p> <p>Default: <code>false</code> (wait for complete intervals)</p>"},{"location":"components/model/properties/#optimize_query","title":"optimize_query","text":"<p>Enable or disable query optimization. Vulcan optimizes queries by default (rewrites them for better performance), but sometimes you want to disable this.</p> <p>When to disable: - The optimizer is breaking your query - You have engine-specific optimizations you want to preserve - Debugging query issues</p> <p>Default: <code>true</code> (optimize queries)</p> SQLPython <pre><code>MODEL (\n  name sales.complex_query,\n  optimize_query false,  -- Disable optimization\n);\n</code></pre> <pre><code>@model(\n    \"sales.complex_query\",\n    optimize_query=False,\n)\n</code></pre>"},{"location":"components/model/properties/#formatting","title":"formatting","text":"<p>Control whether Vulcan formats this model when you run <code>vulcan format</code>. Set to <code>false</code> if you want to preserve custom formatting.</p> <p>When to disable: - Legacy models with specific formatting requirements - Models where formatting breaks something - When you prefer manual formatting control</p> <p>Default: <code>true</code> (format models automatically)</p> SQLPython <pre><code>MODEL (\n  name sales.legacy_model,\n  formatting false,  -- Skip formatting\n);\n</code></pre> <pre><code>@model(\n    \"sales.legacy_model\",\n    formatting=False,\n)\n</code></pre>"},{"location":"components/model/properties/#ignored_rules","title":"ignored_rules","text":"<p>Tell Vulcan's linter to ignore specific rules for this model. Useful when you have a legitimate reason to break a rule, or when a rule doesn't apply to your use case.</p> <p>You can ignore specific rules (<code>['rule_name', 'another_rule']</code>) or all rules (<code>'ALL'</code>).</p> <p>Use sparingly: If you're ignoring lots of rules, maybe the rules need updating, or maybe the model needs refactoring.</p> SQLPython <pre><code>-- Ignore specific rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules ['rule_name', 'another_rule'],\n);\n\n-- Ignore all rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules 'ALL',\n);\n</code></pre> <pre><code># Ignore specific rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=[\"rule_name\", \"another_rule\"],\n)\n\n# Ignore all rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=\"ALL\",\n)\n</code></pre>"},{"location":"components/model/properties/#incremental-model-properties","title":"Incremental Model Properties","text":"<p>These properties are specified inside the <code>kind</code> definition for incremental models. They control how incremental models behave, things like handling schema changes, restatements, and batch processing.</p> <p>For the full picture on incremental models, check out the Model Kinds documentation.</p>"},{"location":"components/model/properties/#common-incremental-properties","title":"Common Incremental Properties","text":"<p>These properties work with all incremental model kinds. They're your toolkit for controlling incremental behavior:</p> Property Description Type Default <code>forward_only</code> All changes should be forward-only <code>bool</code> <code>false</code> <code>on_destructive_change</code> Behavior for destructive schema changes <code>str</code> <code>error</code> <code>on_additive_change</code> Behavior for additive schema changes <code>str</code> <code>allow</code> <code>disable_restatement</code> Disable data restatement <code>bool</code> <code>false</code> <code>auto_restatement_cron</code> Cron expression for automatic restatement <code>str</code> - <p>Values for <code>on_destructive_change</code> / <code>on_additive_change</code>: - <code>allow</code> - Let the change happen (default for additive) - <code>warn</code> - Allow but warn about it - <code>error</code> - Block the change (default for destructive) - <code>ignore</code> - Pretend it didn't happen</p> <p>Why this matters: Schema changes can break downstream models. These settings let you control how strict Vulcan should be when your schema evolves.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    forward_only true,\n    on_destructive_change 'warn',\n    on_additive_change 'allow',\n    disable_restatement false,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        forward_only=True,\n        on_destructive_change=\"warn\",\n        on_additive_change=\"allow\",\n        disable_restatement=False,\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p>Properties for models that update incrementally based on a time column. These control how time-based incremental processing works.</p> <p>For the full guide on <code>INCREMENTAL_BY_TIME_RANGE</code> models, see the Model Kinds documentation.</p> Property Description Type Required <code>time_column</code> Column containing each row's timestamp (should be UTC) <code>str</code> Y <code>format</code> Format of the time column's data <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>batch_concurrency</code> Maximum concurrent batches <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N <code>auto_restatement_intervals</code> Number of last intervals to auto-restate <code>int</code> N SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    time_column (event_ts, '%Y-%m-%d'),  -- With format\n    batch_size 12,\n    batch_concurrency 4,\n    lookback 7,\n    auto_restatement_cron '@weekly',\n    auto_restatement_intervals 7,\n  )\n);\n\nSELECT\n  event_ts::TIMESTAMP AS event_ts,\n  event_type::VARCHAR AS event_type,\n  user_id::INTEGER AS user_id\nFROM raw.events\nWHERE event_ts BETWEEN @start_ts AND @end_ts;\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n@model(\n    \"sales.events\",\n    columns={\n        \"event_ts\": \"timestamp\",\n        \"event_type\": \"varchar\",\n        \"user_id\": \"int\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        batch_size=12,\n        batch_concurrency=4,\n        lookback=7,\n    ),\n    depends_on=[\"raw.events\"],\n)\ndef execute(context, start, end, **kwargs) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT event_ts, event_type, user_id\n    FROM raw.events\n    WHERE event_ts BETWEEN '{start}' AND '{end}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Important: UTC Timezone</p> <p>Your <code>time_column</code> should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.</p>"},{"location":"components/model/properties/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p>Properties for models that update based on unique keys (upsert operations). These control MERGE behavior and key handling.</p> <p>For details on <code>INCREMENTAL_BY_UNIQUE_KEY</code> models, see the Model Kinds documentation.</p> Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>str</code> | <code>array</code> Y <code>when_matched</code> SQL logic to update columns on match (MERGE engines only) <code>str</code> N <code>merge_filter</code> Predicates for ON clause of MERGE operation <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N SQLPython <pre><code>-- Single unique key\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n  )\n);\n\n-- Composite unique key\nMODEL (\n  name sales.order_items,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, item_id),\n  )\n);\n\n-- With MERGE options\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched WHEN MATCHED THEN UPDATE SET \n      name = source.name,\n      updated_at = source.updated_at,\n    auto_restatement_cron '@weekly',\n  )\n);\n</code></pre> <pre><code># Single unique key\n@model(\n    \"sales.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=\"customer_id\",\n    ),\n    depends_on=[\"raw.customers\"],\n)\n\n# Composite unique key\n@model(\n    \"sales.order_items\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"order_id\", \"item_id\"],\n    ),\n)\n</code></pre> <p>Batch Concurrency</p> <p><code>batch_concurrency</code> isn't supported for this kind because MERGE operations can't safely run in parallel. Vulcan processes these models sequentially to avoid conflicts.</p>"},{"location":"components/model/properties/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Properties for models that update by partition. This kind uses the <code>partitioned_by</code> property (from the General Properties section) as its partition key.</p> <p>Note: There are no additional kind-specific properties, just use <code>partitioned_by</code> to define your partition columns.</p> <p>For details on <code>INCREMENTAL_BY_PARTITION</code> models, see the Model Kinds documentation.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by event_date,\n);\n\nSELECT\n  event_date::DATE AS event_date,\n  event_type::VARCHAR AS event_type,\n  COUNT(*)::INTEGER AS event_count\nFROM raw.events\nGROUP BY event_date, event_type;\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    columns={\n        \"event_date\": \"date\",\n        \"event_type\": \"varchar\",\n        \"event_count\": \"int\",\n    },\n    kind=dict(name=ModelKindName.INCREMENTAL_BY_PARTITION),\n    partitioned_by=[\"event_date\"],\n    depends_on=[\"raw.events\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2","title":"SCD_TYPE_2","text":"<p>Properties for Slowly Changing Dimension Type 2 models, which track historical changes to your data.</p> <p>For the complete guide on SCD Type 2 models, see the Model Kinds documentation.</p>"},{"location":"components/model/properties/#common-scd-type-2-properties","title":"Common SCD Type 2 Properties","text":"Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>array</code> Y <code>valid_from_name</code> Column for valid from date <code>str</code> N (default: <code>valid_from</code>) <code>valid_to_name</code> Column for valid to date <code>str</code> N (default: <code>valid_to</code>) <code>invalidate_hard_deletes</code> Mark missing records as invalid <code>bool</code> N (default: <code>true</code>)"},{"location":"components/model/properties/#scd_type_2_by_time","title":"SCD_TYPE_2_BY_TIME","text":"<p>Properties for SCD Type 2 models that detect changes using an <code>updated_at</code> timestamp column. This is the recommended approach when your source table has update timestamps.</p> Property Description Type Required <code>updated_at_name</code> Column containing updated at date <code>str</code> N (default: <code>updated_at</code>) <code>updated_at_as_valid_from</code> Use <code>updated_at</code> value as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>MODEL (\n  name dim.customers,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key customer_id,\n    updated_at_name last_modified,\n    valid_from_name effective_from,\n    valid_to_name effective_to,\n    invalidate_hard_deletes true,\n    updated_at_as_valid_from true,\n  )\n);\n\nSELECT\n  customer_id::INTEGER AS customer_id,\n  name::VARCHAR AS name,\n  email::VARCHAR AS email,\n  last_modified::TIMESTAMP AS last_modified\nFROM raw.customers;\n</code></pre> <pre><code>@model(\n    \"dim.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"last_modified\": \"timestamp\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n        updated_at_name=\"last_modified\",\n        valid_from_name=\"effective_from\",\n        valid_to_name=\"effective_to\",\n        invalidate_hard_deletes=True,\n    ),\n    depends_on=[\"raw.customers\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2_by_column","title":"SCD_TYPE_2_BY_COLUMN","text":"<p>Properties for SCD Type 2 models that detect changes by comparing column values. Use this when your source table doesn't have an <code>updated_at</code> column.</p> Property Description Type Required <code>columns</code> Columns to check for changes (<code>*</code> for all) <code>str</code> | <code>array</code> Y <code>execution_time_as_valid_from</code> Use execution time as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>-- Track specific columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns (name, price, category),\n    execution_time_as_valid_from true,\n  )\n);\n\n-- Track all columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns '*',\n  )\n);\n</code></pre> <pre><code># Track specific columns\n@model(\n    \"dim.products\",\n    columns={\n        \"product_id\": \"int\",\n        \"name\": \"varchar\",\n        \"price\": \"decimal(10,2)\",\n        \"category\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"name\", \"price\", \"category\"],\n        execution_time_as_valid_from=True,\n    ),\n    depends_on=[\"raw.products\"],\n)\n\n# Track all columns\n@model(\n    \"dim.products\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=\"*\",\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#model-naming","title":"Model Naming","text":"<p>By default, you need to specify the <code>name</code> property in every model. But if you organize your models in a directory structure that matches your schema names, you can enable automatic name inference.</p> <p>How it works: With <code>infer_names: true</code>, a model at <code>models/sales/daily_sales.sql</code> automatically gets the name <code>sales.daily_sales</code>. The directory structure becomes your schema, and the filename becomes your model name.</p> <p>Enable it in your config:</p> <pre><code>model_defaults:\n  dialect: snowflake\n\n# Enable name inference\ninfer_names: true\n</code></pre> <p>When to use: If your project structure matches your schema structure, this saves you from typing <code>name</code> in every model. Pretty convenient!</p> <p>Learn more in the configuration guide.</p>"},{"location":"components/model/statements/","title":"Statements","text":""},{"location":"components/model/statements/#statements","title":"Statements","text":"<p>Statements let you run SQL commands at specific points during model execution. Think of them as hooks, you can run code before your query, after it completes, or when views are created.</p> <p>Why use statements? They're perfect for: - Setting session parameters (timeouts, memory limits) - Loading UDFs or creating temporary tables - Creating indexes or clustering - Running data quality checks - Logging anomalies or errors - Granting permissions on views</p> <p>You can define statements at the model level (for specific needs) or at the project level via <code>model_defaults</code> (for consistent behavior across all models).</p> <p>Statement types:</p> <ul> <li>Pre-statements: Run before the main model query executes</li> <li>Post-statements: Run after the main model query completes</li> <li>On-virtual-update statements: Run when views are created or updated in the virtual layer</li> </ul> <p>Concurrency Considerations</p> <p>Pre-statements should generally only prepare the main query. Avoid creating or altering physical tables in pre-statements, if multiple models run concurrently, you could get race conditions or unpredictable behavior. Stick to session settings, UDFs, and temporary objects.</p>"},{"location":"components/model/statements/#model-defaults","title":"Model Defaults","text":"<p>You can define statements at the project level using <code>model_defaults</code> in your configuration. This is great for setting up common behavior across all models, like session timeouts or default permissions.</p> <p>How it works: Default statements run first, then model-specific statements. So if you set a default timeout in <code>model_defaults</code> and a model-specific timeout in a model, the model-specific one runs after (and can override the default).</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  pre_statements:\n    - \"SET query_timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre>"},{"location":"components/model/statements/#pre-statements","title":"Pre-statements","text":"<p>Pre-statements run before your main model query executes. They're perfect for setting up the environment your query needs.</p> <p>Common use cases: - Loading JARs or UDFs that your query uses - Creating temporary tables or caching data - Setting session parameters (timeouts, memory, etc.) - Initializing variables or settings</p> <p>Think of it as: The \"setup\" phase before your main query runs.</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily'\n);\n\n/* Pre-statement: Create table for anomaly tracking */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* Pre-statement: Set session variables using Jinja */\nJINJA_STATEMENT_BEGIN;\n{% if start_date is none or end_date is none %}\n  SET start_date = DATE '{{ start }}';\n  SET end_date = CURRENT_DATE;\n{% endif %}\nJINJA_END;\n\n/* Main model query */\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    pre_statements=[\n        \"SET query_timeout = 300000\",\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR\n        )\"\"\",\n        exp.Cache(this=exp.table_(\"orders_cache\"), expression=exp.select(\"*\").from_(\"demo.raw_data.orders\")),\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#post-statements","title":"Post-statements","text":"<p>Post-statements run after your model query completes. They're great for cleanup, optimization, or validation tasks.</p> <p>Important: When you use post-statements in SQL models, your main query must end with a semicolon. This tells Vulcan where the query ends and the statements begin.</p> <p>Common use cases: - Creating indexes or clustering (for query performance) - Running data quality checks or validations - Logging anomalies or errors to tracking tables - Conditional table alterations (like setting retention policies)</p> <p>Think of it as: The \"cleanup and optimization\" phase after your data is loaded.</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  )\n);\n\nSELECT\n  order_id,\n  order_date,\n  customer_id,\n  quantity,\n  unit_price,\n  total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* Post-statement: Conditional retention policy (only on table creation) */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Post-statement: Add clustering for query performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Post-statement: Capture data anomalies - negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'NEGATIVE_QUANTITY',\n  CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Post-statement: Capture data anomalies - total mismatch */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calculated=', ROUND(unit_price * quantity, 2),\n    '; actual=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS((unit_price * quantity) - total_amount) &gt; 0.01;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#on-virtual-update-statements","title":"On-virtual-update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.</p> <p>Common use cases: - Granting permissions on views (so users can query them) - Setting up access controls or row-level security - Applying column masking policies - Any view-level configuration</p> <p>Think of it as: The \"access control\" phase\u2014setting up who can see what.</p> <p>Note: These statements run at the virtual layer, so table names (including <code>@this_model</code>) resolve to view names, not physical table names.</p> SQLPython <p>Use <code>ON_VIRTUAL_UPDATE_BEGIN</code> and <code>ON_VIRTUAL_UPDATE_END</code> to define these statements:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  )\n);\n\nSELECT\n  customer_id,\n  full_name,\n  email,\n  customer_segment\nFROM demo.raw_data.customers;\n\n/* Post-statement: Apply masking policy */\nJINJA_STATEMENT_BEGIN;\nALTER TABLE {{ this_model }} MODIFY COLUMN email SET MASKING POLICY demo.security.mask_email_policy;\nJINJA_END;\n\n/* On-virtual-update: Grant permissions when view is created/updated */\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Use the <code>on_virtual_update</code> argument in the <code>@model</code> decorator:</p> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.customers_py\",\n    columns={\n        \"customer_id\": \"varchar\",\n        \"full_name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"customer_segment\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 7)\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT customer_id, CONCAT(first_name, ' ', last_name) AS full_name,\n           email, customer_segment\n    FROM demo.raw_data.customers\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#complete-example","title":"Complete example","text":"<p>Here's a complete example showing all statement types:</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grains (order_id),\n  description 'Orders fact table with incremental loading'\n);\n\n/* ============ PRE-STATEMENTS ============ */\n\n/* Create anomaly tracking table */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* ============ MAIN QUERY ============ */\n\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  product_id::VARCHAR AS product_id,\n  quantity::INT AS quantity,\n  unit_price::FLOAT AS unit_price,\n  discount::FLOAT AS discount,\n  tax::FLOAT AS tax,\n  shipping_cost::FLOAT AS shipping_cost,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* ============ POST-STATEMENTS ============ */\n\n/* Conditional: Set retention only on table creation */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Add clustering for performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Data quality: Log negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Data quality: Log total mismatches */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calc=', ROUND(unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0), 2),\n    '; total=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS(\n  (unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0))\n  - total_amount\n) &gt; 0.01;\n\n/* ============ ON-VIRTUAL-UPDATE ============ */\n\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"product_id\": \"varchar\",\n        \"quantity\": \"int\",\n        \"unit_price\": \"float\",\n        \"discount\": \"float\",\n        \"tax\": \"float\",\n        \"shipping_cost\": \"float\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_id\"],\n    depends_on=[\"demo.raw_data.orders\"],\n    description=\"Orders fact table with incremental loading\",\n    pre_statements=[\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR,\n            captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\"\"\",\n    ],\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT\n        order_id, order_date, customer_id, product_id,\n        quantity, unit_price, discount, tax, shipping_cost, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#useful-macros-and-variables","title":"Useful macros and variables","text":"Macro/Variable Description <code>@this_model</code> References the current model's table/view <code>@runtime_stage</code> Current execution stage: <code>'creating'</code>, <code>'evaluating'</code>, or <code>'testing'</code> <code>@IF(condition, statement)</code> Conditionally execute a statement <code>@start_date</code>, <code>@end_date</code> Time range macros for incremental models <code>{{ this_model }}</code> Jinja equivalent of <code>@this_model</code> <p>For more information on macros, see the Macro Variables documentation.</p>"},{"location":"components/model/types/external_models/","title":"External","text":""},{"location":"components/model/types/external_models/#external","title":"External","text":"<p>Sometimes your models need to query tables that exist outside your Vulcan project, maybe a third-party data source, a table managed by another system, or a read-only database. These are \"external\" tables.</p> <p>Vulcan doesn't manage external tables (you can't create or update them), but it can use metadata about them to make your life easier. By defining external models, you give Vulcan information about column names and types, which enables better column-level lineage and query optimization.</p> <p>Why define external models? Even though Vulcan can't manage them, knowing their schema helps with: - Column-level lineage (see how data flows through external tables) - Query optimization (Vulcan can make better decisions) - Documentation (your data catalog knows what's in those tables)</p> <p>Vulcan stores this metadata as <code>EXTERNAL</code> models.</p>"},{"location":"components/model/types/external_models/#how-external-models-work","title":"How External Models Work","text":"<p><code>EXTERNAL</code> models are metadata-only, they just describe a table's schema (column names and types). There's no query for Vulcan to run, and Vulcan doesn't manage the data.</p> <p>Important limitations: - Vulcan doesn't know what data is in the table (or if it even exists) - If someone alters the external table, Vulcan won't detect it - If all data is deleted, Vulcan won't know - Vulcan never modifies external tables</p> <p>The querying model's <code>kind</code>, <code>cron</code>, and previously loaded time intervals determine when Vulcan will query the <code>EXTERNAL</code> model.</p> <p>When external tables get queried: Only when a Vulcan model references them. The querying model's <code>kind</code>, <code>cron</code>, and time intervals determine when the external table is actually queried. Vulcan doesn't proactively query external tables, it only queries them as part of executing your models.</p>"},{"location":"components/model/types/external_models/#creating-external-models","title":"Creating External Models","text":"<p>External models are defined in YAML files. You have two options: 1. Let Vulcan generate it (easiest) - Use the <code>create_external_models</code> CLI command 2. Write it yourself - Hand-craft the YAML if you need more control</p> <p>The main file is <code>external_models.yaml</code> (or <code>schema.yaml</code>) in your project root. You can also add more files in the <code>external_models/</code> directory.</p> <p>Let's say you have a model that queries external tables. Here's an example:</p> <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <p>The following sections show you how to create external models for these tables. You can define all external models in <code>external_models.yaml</code>, or split them across multiple files in the <code>external_models/</code> directory (useful for organization or when Vulcan regenerates the main file).</p>"},{"location":"components/model/types/external_models/#using-cli","title":"Using CLI","text":"<p>Instead of creating the <code>external_models.yaml</code> file manually, Vulcan can generate it for you with the create_external_models CLI command.</p> <p>The command identifies all external tables referenced in your Vulcan project, fetches their column information from the SQL engine's metadata, and then stores the information in the <code>external_models.yaml</code> file.</p> <p>If Vulcan does not have access to an external table's metadata, the table will be omitted from the file and Vulcan will issue a warning.</p> <p><code>create_external_models</code> solely queries SQL engine metadata and does not query external tables themselves.</p>"},{"location":"components/model/types/external_models/#gateway-specific-external-models","title":"Gateway-specific external models","text":"<p>In some use-cases such as isolated systems with multiple gateways, there are external models that only exist on a certain gateway.</p> <p>Gateway names are case-insensitive in external model configurations. You can specify the gateway name using any case (e.g., <code>gateway: dev</code>, <code>gateway: DEV</code>, <code>gateway: Dev</code>) and Vulcan will handle the matching correctly.</p> <p>Consider the following model that queries an external table with a dynamic database based on the current gateway:</p> <pre><code>vulcan create_external_models\n</code></pre> <p>What it does: - Scans your project for references to external tables - Fetches column information from your SQL engine's metadata - Writes everything to <code>external_models.yaml</code></p> <p>Important: This command only queries metadata (table schemas), not the actual data. It's fast and safe.</p> <p>If Vulcan can't access a table's metadata: That table gets skipped and Vulcan warns you. You'll need to define it manually (see the \"Writing YAML by hand\" section below).</p>"},{"location":"components/model/types/external_models/#gateway-specific-external-models_1","title":"Gateway-Specific External Models","text":"<p>If you're using isolated systems with multiple gateways, you might have external tables that only exist on specific gateways.</p> <p>Example: Your model uses a gateway variable to select different databases:</p> <pre><code>MODEL (\n  name vulcan_demo.customer_summary,\n  kind FULL\n);\n\nSELECT * FROM @{gateway}_db.customers;\n</code></pre> <p>When you run with <code>--gateway dev</code>, it queries <code>dev_db.customers</code>. When you run with <code>--gateway prod</code>, it queries <code>prod_db.customers</code>. These are different tables with potentially different schemas!</p> <p>Solution: Run <code>create_external_models</code> with the <code>--gateway</code> flag:</p> <pre><code>vulcan --gateway dev create_external_models\n</code></pre> <p>This sets <code>gateway: dev</code> on the external model, so it only loads when that gateway is active. Do this for each gateway that has different external tables.</p> <p>Case-Insensitive Gateway Names</p> <p>Gateway names are case-insensitive in external model configs. <code>gateway: dev</code>, <code>gateway: DEV</code>, and <code>gateway: Dev</code> all work the same.</p>"},{"location":"components/model/types/external_models/#writing-yaml-by-hand","title":"Writing YAML by Hand","text":"<p>Sometimes you need to define external models manually, maybe Vulcan can't access the metadata, or you want more control. Here's the structure:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  description: \"Customer dimension table from external system\"\n  gateway: dev  # Optional: only load for this gateway\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n</code></pre> <p>What you need: - <code>name</code>: Fully qualified table name (with quotes if needed for case sensitivity) - <code>columns</code>: Dictionary of column names to data types</p> <p>Optional fields: - <code>description</code>: Human-readable description - <code>gateway</code>: Gateway name (for gateway-specific tables)</p> <p>Pro tip: Use triple-quoted names if your table names have special characters or need case sensitivity. The exact format depends on your SQL engine.</p>"},{"location":"components/model/types/external_models/#using-the-external_models-directory","title":"Using the <code>external_models</code> Directory","text":"<p>Here's a common problem: You run <code>vulcan create_external_models</code> and it generates <code>external_models.yaml</code>. But some tables need manual definitions (maybe Vulcan can't access their metadata). If you add them to <code>external_models.yaml</code> and run the command again, your manual changes get overwritten!</p> <p>Solution: Put manual definitions in the <code>external_models/</code> directory:</p> <pre><code>external_models.yaml              # Auto-generated by Vulcan\nexternal_models/manual_tables.yaml # Your manual definitions\nexternal_models/legacy_tables.yaml # More manual definitions\n</code></pre> <p>How it works: - Vulcan loads <code>external_models.yaml</code> first (or <code>schema.yaml</code>) - Then it loads all <code>.yaml</code> files from <code>external_models/</code> - Everything gets merged together</p> <p>Best practice: Use <code>create_external_models</code> to manage the main file, and put any tables that need manual definitions in the <code>external_models/</code> directory. That way you can regenerate the main file without losing your manual work!</p>"},{"location":"components/model/types/external_models/#external-assertions","title":"External Assertions","text":"<p>You can define assertions on external models! This is super useful for validating upstream data quality before your internal models run.</p> <p>Why this matters: If your external data source has quality issues, you want to catch them early, before they flow into your models and cause bigger problems downstream.</p> <p>Here's how you'd add assertions to an external model:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  description: Table containing customer information\n  assertions:\n    - name: not_null\n      columns: \"[customer_id, email]\"\n    - name: unique_values\n      columns: \"[customer_id]\"\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  description: Table containing order transactions\n  assertions:\n    - name: not_null\n      columns: \"[order_id, customer_id, order_date]\"\n    - name: accepted_range\n      column: order_id\n      min_v: \"1\"\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n</code></pre>"},{"location":"components/model/types/managed_models/","title":"Managed","text":""},{"location":"components/model/types/managed_models/#managed","title":"Managed","text":"<p>Most Vulcan models manage their own data, you run <code>vulcan run</code>, and Vulcan updates the tables. Managed models are different: the database engine handles data updates automatically in the background.</p> <p>How it works: You define a query, and the engine monitors upstream tables. When source data changes, the engine automatically refreshes your managed table. No manual <code>REFRESH</code> commands needed, it just happens.</p> <p>Why use this? Perfect for scenarios where you need always-fresh data without managing refresh schedules yourself. The engine handles the complexity of incremental updates, change detection, and refresh timing.</p> <p>Best use case: Managed models are typically built on External Models rather than other Vulcan models. Since Vulcan already keeps its models up to date, the main benefit comes when you're reading from external tables that aren't tracked by Vulcan. The engine keeps your managed table in sync with those external sources automatically.</p> <p>Python Models Not Supported</p> <p>Python models don't support the <code>MANAGED</code> model kind. You'll need to use a SQL model instead.</p>"},{"location":"components/model/types/managed_models/#difference-from-materialized-views","title":"Difference from Materialized Views","text":"<p>You might be wondering: \"What's the difference between a managed model and a materialized view?\" Good question!</p> <p>Vulcan already supports materialized views, but they have limitations: - Some engines only allow materialized views from a single base table - Materialized views aren't automatically refreshed, you need to run <code>REFRESH MATERIALIZED VIEW</code> manually - You're responsible for scheduling refreshes</p> <p>Managed models are different: - \u2705 Automatic updates - The engine refreshes data when source tables change - \u2705 Smart refresh - The engine understands your query and can do incremental or full refreshes as needed - \u2705 No manual commands - Everything happens in the background</p> <p>In some engines, there's no difference (they're the same thing). In others, managed models give you more automation and flexibility.</p>"},{"location":"components/model/types/managed_models/#lifecycle-in-vulcan","title":"Lifecycle in Vulcan","text":"<p>Managed models follow the same lifecycle as other Vulcan models: - Virtual environments create pointers to model snapshots - Model changes create new snapshots - Upstream changes trigger new snapshots - You can deploy and rollback like any other model - Snapshots get cleaned up when TTL expires</p> <p>Cost consideration: Managed models usually cost more than regular tables. For example, Snowflake charges extra for Dynamic Tables. To save money, Vulcan uses regular tables for dev previews (in forward-only plans) and only creates managed tables when deploying to production.</p> <p>Dev vs Prod Differences</p> <p>Since dev uses regular tables and prod uses managed tables, it's possible to write a query that works in dev but fails in prod. This happens if you use features available to regular tables but not managed tables.</p> <p>We think the cost savings are worth it, but if this causes issues, let us know!</p>"},{"location":"components/model/types/managed_models/#supported-engines","title":"Supported Engines","text":"<p>Currently, Vulcan supports managed models on:</p> Engine Implementation Snowflake Dynamic Tables <p>To create a managed model, use the <code>MANAGED</code> model kind. More engines are coming soon!</p>"},{"location":"components/model/types/managed_models/#snowflake","title":"Snowflake","text":"<p>On Snowflake, managed models are implemented as Dynamic Tables. Dynamic Tables automatically refresh when their source data changes, which is exactly what managed models need.</p> <p>Here's how you'd create one:</p> <pre><code>MODEL (\n  name db.events,\n  kind MANAGED,\n  physical_properties (\n    warehouse = datalake,\n    target_lag = '2 minutes',\n    data_retention_time_in_days = 2\n  )\n);\n\nSELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>results in:</p> <pre><code>CREATE OR REPLACE DYNAMIC TABLE db.events\n  WAREHOUSE = \"datalake\",\n  TARGET_LAG = '2 minutes'\n  DATA_RETENTION_TIME_IN_DAYS = 2\nAS SELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>No Intervals</p> <p>Vulcan doesn't create intervals or run this model on a schedule. You don't need <code>WHERE</code> clauses with date filters like you would for incremental models. Snowflake handles all the refreshing automatically, you just define the query and let Snowflake do its thing.</p>"},{"location":"components/model/types/managed_models/#table-properties","title":"Table Properties","text":"<p>Dynamic Tables have properties that control refresh frequency, initial data population, retention, and more. You can find the complete list in the Snowflake documentation.</p> <p>In Vulcan, you set these properties using <code>physical_properties</code> in your model definition. Here are the key ones:</p> Snowflake Property Required Notes target_lag Y warehouse N In Snowflake, this is a required property. However, if not specified, then Vulcan will use the result of <code>select current_warehouse()</code>. refresh_mode N initialize N data_retention_time_in_days N max_data_extension_time_in_days N <p>The following Dynamic Table properties can be set directly on the model:</p> Snowflake Property Required Notes cluster by N <code>clustered_by</code> is a standard model property, so set <code>clustered_by</code> on the model to add a <code>CLUSTER BY</code> clause to the Dynamic Table"},{"location":"components/model/types/python_models/","title":"Python","text":""},{"location":"components/model/types/python_models/#python","title":"Python","text":"<p>SQL is great, but sometimes you need Python. Maybe you're doing machine learning, calling external APIs, or implementing complex business logic that's painful to express in SQL.</p> <p>Vulcan has first-class support for Python models. As long as your function returns a Pandas, Spark, Bigframe, or Snowpark DataFrame, you can do whatever you want in Python. No restrictions!</p> <p>When to use Python models: - Machine learning pipelines - API integrations - Complex transformations that are easier in Python - Data processing that benefits from Python libraries</p> <p>Unsupported Model Kinds</p> <p>Python models don't support these model kinds. If you need one of these, use a SQL model instead:</p> <pre><code>- `VIEW` - Views need to be SQL\n- `SEED` - Seed models load CSV files (SQL only)\n- `MANAGED` - Managed models require SQL\n- `EMBEDDED` - Embedded models inject SQL subqueries\n</code></pre>"},{"location":"components/model/types/python_models/#definition","title":"Definition","text":"<p>Creating a Python model is straightforward: add a <code>.py</code> file to your <code>models/</code> directory and define an <code>execute</code> function. That's it!</p> <p>Here's what a basic Python model looks like:</p> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"order_date\"],\n    depends_on=[\"raw.raw_orders\"],\n    cron='@daily',\n    description=\"Daily sales aggregated by order_date.\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"FULL model - rebuilds entire daily_sales table each run\"\"\"\n\n    query = \"\"\"\n    SELECT\n      CAST(order_date AS TIMESTAMP) AS order_date,\n      COUNT(order_id) AS total_orders,\n      SUM(total_amount) AS total_revenue,\n      MAX(order_id) AS last_order_id\n    FROM raw.raw_orders\n    GROUP BY order_date\n    ORDER BY order_date\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>How it works:</p> <p>The <code>@model</code> decorator captures your model's metadata (just like the <code>MODEL</code> DDL in SQL models). You specify column names and types in the <code>columns</code> argument, this is required because Vulcan needs to create the table before your function runs.</p> <p>Function signature: Your <code>execute</code> function receives: - <code>context: ExecutionContext</code> - For running queries and getting time intervals - <code>start</code>, <code>end</code> - Time range for incremental models - <code>execution_time</code> - When the model is running - <code>**kwargs</code> - Any other runtime variables</p> <p>Return types: You can return Pandas, PySpark, Bigframe, or Snowpark DataFrames. If your output is huge, you can also use Python generators to return data in chunks (great for memory management).</p>"},{"location":"components/model/types/python_models/#model-specification","title":"<code>@model</code> Specification","text":"<p>The <code>@model</code> decorator accepts the same properties as SQL models, just use Python syntax instead of SQL DDL. <code>name</code>, <code>kind</code>, <code>cron</code>, <code>grains</code>, etc.\u2014they all work the same way.</p> <p>Python model <code>kind</code>s are specified with a Python dictionary containing the kind's name and arguments. All model kind arguments are listed in the models configuration reference page.</p> <pre><code>from vulcan import ModelKindName\n\n@model(\n    \"sales.daily_sales\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n)\n</code></pre> <p>All model kind properties are documented in the model configuration reference.</p> <p>Supported <code>kind</code> dictionary <code>name</code> values are:</p> <ul> <li><code>ModelKindName.VIEW</code></li> <li><code>ModelKindName.FULL</code></li> <li><code>ModelKindName.SEED</code></li> <li><code>ModelKindName.INCREMENTAL_BY_TIME_RANGE</code></li> <li><code>ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</code></li> <li><code>ModelKindName.INCREMENTAL_BY_PARTITION</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_TIME</code></li> <li><code>ModelKindName.SCD_TYPE_2_BY_COLUMN</code></li> <li><code>ModelKindName.EMBEDDED</code></li> <li><code>ModelKindName.CUSTOM</code></li> <li><code>ModelKindName.MANAGED</code></li> <li><code>ModelKindName.EXTERNAL</code></li> </ul>"},{"location":"components/model/types/python_models/#execution-context","title":"Execution Context","text":"<p>Python models can do anything you want, but it is strongly recommended for all models to be idempotent. Python models can fetch data from upstream models or even data outside of Vulcan.</p> <p>Fetching data: Use <code>context.fetchdf()</code> to run SQL queries and get DataFrames:</p> <pre><code>df = context.fetchdf(\"SELECT * FROM vulcan_demo.products\")\n</code></pre> <p>Resolving table names: Use <code>context.resolve_table()</code> to get the correct table name for the current environment (handles dev/prod prefixes automatically):</p> <pre><code>table = context.resolve_table(\"vulcan_demo.products\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>Best practice: Make your models idempotent, running them multiple times should produce the same result. This makes debugging and restatements much easier.</p> <pre><code>df = context.fetchdf(\"SELECT * FROM vulcan_demo.products\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-prepost-statements","title":"Optional Pre/Post-Statements","text":"<p>You can run SQL commands before and after your Python model executes. This is useful for setting session parameters, creating indexes, or running data quality checks.</p> <p>Pre-statements: Run before your <code>execute</code> function Post-statements: Run after your <code>execute</code> function completes</p> <p>You can pass SQL strings, SQLGlot expressions, or macro calls as lists to <code>pre_statements</code> and <code>post_statements</code>.</p> <p>Concurrency</p> <p>Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings, UDFs, and temporary objects in pre-statements.</p> <p>Project-level defaults: You can also define pre/post-statements in <code>model_defaults</code> for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_statements\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    pre_statements=[\n        \"SET GLOBAL parameter = 'value';\",\n        exp.Cache(this=exp.table_(\"x\"), expression=exp.select(\"1\")),\n    ],\n    post_statements=[\"@CREATE_INDEX(@this_model, id)\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>The previous example's <code>post_statements</code> called user-defined Vulcan macro <code>@CREATE_INDEX(@this_model, id)</code>.</p> <p>We could define the <code>CREATE_INDEX</code> macro in the project's <code>macros</code> directory like this. The macro creates a table index on a single column, conditional on the runtime stage being <code>creating</code> (table creation time).</p> <pre><code>@macro()\ndef create_index(\n    evaluator: MacroEvaluator,\n    model_name: str,\n    column: str,\n):\n    if evaluator.runtime_stage == \"creating\":\n        return f\"CREATE INDEX idx ON {model_name}({column});\"\n    return None\n</code></pre> <p>Alternative approach: Instead of using the <code>@model</code> decorator's <code>pre_statements</code> and <code>post_statements</code>, you can execute SQL directly in your function using <code>context.engine_adapter.execute()</code>.</p> <p>Important: If you want post-statements to run after your function completes, you need to use <code>yield</code> instead of <code>return</code>. Post-statements specified after a <code>yield</code> will execute after the function finishes.</p> <p>This example function includes both pre- and post-statements:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    # pre-statement\n    context.engine_adapter.execute(\"SET GLOBAL parameter = 'value';\")\n\n    # post-statement requires using `yield` instead of `return`\n    yield pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n\n    # post-statement\n    context.engine_adapter.execute(\"CREATE INDEX idx ON vulcan_demo.model_with_statements (id);\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-on-virtual-update-statements","title":"Optional On-Virtual-Update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.</p> <p>Common use case: Granting permissions on views so users can query them.</p> <p>You can set <code>on_virtual_update</code> in the <code>@model</code> decorator to a list of SQL strings, SQLGlot expressions, or macro calls.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project (including Python models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_grants\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    on_virtual_update=[\"GRANT SELECT ON VIEW @this_model TO ROLE dev_role\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>Virtual Layer Resolution</p> <p>These statements run at the virtual layer, so table names resolve to view names, not physical table names. For example, in a <code>dev</code> environment, <code>vulcan_demo.model_with_grants</code> and <code>@this_model</code> resolve to <code>vulcan_demo__dev.model_with_grants</code> (the view), not the physical table.</p>"},{"location":"components/model/types/python_models/#dependencies","title":"Dependencies","text":"<p>In order to fetch data from an upstream model, you first get the table name using <code>context</code>'s <code>resolve_table</code> method. This returns the appropriate table name for the current runtime environment:</p> <pre><code>table = context.resolve_table(\"vulcan_demo.products\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>The <code>resolve_table</code> method will automatically add the referenced model to the Python model's dependencies.</p> <p>The only other way to set dependencies of models in Python models is to define them explicitly in the <code>@model</code> decorator using the keyword <code>depends_on</code>. The dependencies defined in the model decorator take precedence over any dynamic references inside the function.</p> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # Dependencies are explicitly declared above\n    query = \"\"\"\n    SELECT \n        p.product_id,\n        p.name AS product_name,\n        p.category,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>You can use global variables or blueprint variables in <code>resolve_table</code> calls. Here's how:</p> <pre><code>@model(\n    \"@schema_name.test_model2\",\n    kind=\"FULL\",\n    columns={\"id\": \"INT\"},\n)\ndef execute(context, **kwargs):\n    table = context.resolve_table(f\"{context.var('schema_name')}.test_model1\")\n    select_query = exp.select(\"*\").from_(table)\n    return context.fetchdf(select_query)\n</code></pre>"},{"location":"components/model/types/python_models/#returning-empty-dataframes","title":"Returning Empty DataFrames","text":"<p>Python models can't return empty DataFrames directly. If your model might return empty data, use <code>yield</code> instead of <code>return</code>:</p> <p>Why? This allows Vulcan to handle the empty case properly. If you <code>return</code> an empty DataFrame, Vulcan will error. If you <code>yield</code> an empty generator or conditionally yield, it works fine.</p> <pre><code>@model(\n    \"vulcan_demo.empty_df_model\"\n)\ndef execute(\n    context: ExecutionContext,\n) -&gt; pd.DataFrame:\n\n    [...code creating df...]\n\n    if df.empty:\n        yield from ()\n    else:\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#user-defined-variables","title":"User-defined variables","text":"<p>User-defined global variables can be accessed from within the Python model with the <code>context.var</code> method.</p> <p>For example, this model access the user-defined variables <code>var</code> and <code>var_with_default</code>. It specifies a default value of <code>default_value</code> if <code>variable_with_default</code> resolves to a missing value.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    var_value = context.var(\"var\")\n    var_with_default_value = context.var(\"var_with_default\", \"default_value\")\n    ...\n</code></pre> <p>Alternatively, you can access global variables via <code>execute</code> function arguments, where the name of the argument corresponds to the name of a variable key.</p> <p>For example, this model specifies <code>my_var</code> as an argument to the <code>execute</code> method. The model code can reference the <code>my_var</code> object directly:</p> <pre><code>@model(\n    \"vulcan_demo.model_with_arg_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    my_var: Optional[str] = None,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    my_var_plus1 = my_var + 1\n    ...\n</code></pre> <p>Make sure the argument has a default value if it's possible for the variable to be missing.</p> <p>Note that arguments must be specified explicitly - variables cannot be accessed using <code>kwargs</code>.</p>"},{"location":"components/model/types/python_models/#python-model-blueprinting","title":"Python Model Blueprinting","text":"<p>Python models can serve as templates for creating multiple models. This is called \"blueprinting\", you define one model template, and Vulcan creates multiple models from it.</p> <p>How it works: You parameterize the model name with a variable (using <code>@{variable}</code> syntax) and provide a list of mappings in <code>blueprints</code>. Vulcan creates one model for each mapping.</p> <p>Use case: When you have similar models that differ only by a few parameters (like different schemas, regions, or customers).</p> <p>Here's an example that creates two models:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"@{customer}.some_table\",\n    kind=\"FULL\",\n    blueprints=[\n        {\"customer\": \"customer1\", \"field_a\": \"x\", \"field_b\": \"y\"},\n        {\"customer\": \"customer2\", \"field_a\": \"z\", \"field_b\": \"w\"},\n    ],\n    columns={\n        \"field_a\": \"text\",\n        \"field_b\": \"text\",\n        \"customer\": \"text\",\n    },\n)\ndef entrypoint(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        {\n            \"field_a\": [context.blueprint_var(\"field_a\")],\n            \"field_b\": [context.blueprint_var(\"field_b\")],\n            \"customer\": [context.blueprint_var(\"customer\")],\n        }\n    )\n</code></pre> <p>Important: Notice the <code>@{customer}</code> syntax in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal). Learn more about this syntax here.</p> <p>Dynamic blueprints: You can generate blueprints dynamically using macros. This is handy when your blueprint list comes from external sources (like CSV files or API calls):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@gen_blueprints()\",  # Macro generates the list\n    ...\n)\n</code></pre> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@EACH(@values, x -&gt; (customer := schema_@x))\",\n    ...\n)\n...\n</code></pre>"},{"location":"components/model/types/python_models/#using-macros-in-model-properties","title":"Using Macros in Model Properties","text":"<p>Python models support macro variables in model properties, but there's a gotcha when macros appear inside strings.</p> <p>The issue: Cron expressions often use <code>@</code> (like <code>@daily</code>, <code>@hourly</code>), which conflicts with Vulcan's macro syntax.</p> <p>The solution: Wrap the entire expression in quotes and prefix with <code>@</code>:</p> <pre><code># Correct: Wrap the cron expression containing a macro variable\n@model(\n    \"vulcan_demo.scheduled_model\",\n    cron=\"@'*/@{mins} * * * *'\",  # Note the @'...' syntax\n    ...\n)\n\n# This also works with blueprint variables\n@model(\n    \"@{customer}.scheduled_model\",\n    cron=\"@'0 @{hour} * * *'\",\n    blueprints=[\n        {\"customer\": \"customer_1\", \"hour\": 2}, # Runs at 2 AM\n        {\"customer\": \"customer_2\", \"hour\": 8}, # Runs at 8 AM\n    ],\n    ...\n)\n</code></pre> <p>This is necessary because cron expressions often use <code>@</code> for aliases (like <code>@daily</code>, <code>@hourly</code>), which can conflict with Vulcan's macro syntax.</p>"},{"location":"components/model/types/python_models/#examples","title":"Examples","text":"<p>Here are some practical examples showing different ways to use Python models.</p>"},{"location":"components/model/types/python_models/#basic","title":"Basic","text":"<p>A simple Python model that returns a static Pandas DataFrame. All the metadata properties work the same as SQL models, just use Python syntax.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom sqlglot.expressions import to_column\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.basic_model\",\n    owner=\"data_team\",\n    cron=\"@daily\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    column_descriptions={\n        \"id\": \"Unique ID\",\n        \"name\": \"Name corresponding to the ID\",\n    },\n    audits=[\n        (\"not_null\", {\"columns\": [to_column(\"id\")]}),\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre>"},{"location":"components/model/types/python_models/#sql-query-and-pandas","title":"SQL Query and Pandas","text":"<p>A more realistic example: query upstream models, do some pandas processing, and return the result. This shows how you'd typically use Python models in practice:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.sql_pandas_model\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"text\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's name and register it as a dependency\n    products_table = context.resolve_table(\"vulcan_demo.products\")\n    order_items_table = context.resolve_table(\"vulcan_demo.order_items\")\n\n    # fetch data from the model as a pandas DataFrame\n    df = context.fetchdf(f\"\"\"\n        SELECT \n            p.product_id,\n            p.name AS product_name,\n            SUM(oi.quantity * oi.unit_price) as total_sales\n        FROM {products_table} p\n        LEFT JOIN {order_items_table} oi ON p.product_id = oi.product_id\n        GROUP BY p.product_id, p.name\n    \"\"\")\n\n    # do some pandas stuff\n    df['total_sales'] = df['total_sales'].fillna(0)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#pyspark","title":"PySpark","text":"<p>If you're using Spark, use the PySpark DataFrame API instead of Pandas. PySpark DataFrames compute in a distributed fashion (across your Spark cluster), which is much faster for large datasets.</p> <p>Why PySpark over Pandas: Pandas loads everything into memory on a single machine. PySpark distributes the work across your cluster, so you can handle much larger datasets.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom pyspark.sql import DataFrame, functions\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.pyspark_model\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"text\",\n        \"region\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # get the upstream model's name and register it as a dependency\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    # use the spark DataFrame api to add the region column\n    df = context.spark.table(table).withColumn(\"region\", functions.lit(\"North\"))\n\n    # returns the pyspark DataFrame directly, so no data is computed locally\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#snowpark","title":"Snowpark","text":"<p>If you're using Snowflake, use the Snowpark DataFrame API. Like PySpark, Snowpark DataFrames compute on Snowflake's servers (not locally), which is much more efficient.</p> <p>Why Snowpark over Pandas: All computation happens in Snowflake, so you're not moving data to your local machine. Faster, cheaper, and can handle huge datasets.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom snowflake.snowpark.dataframe import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.snowpark_model\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n        \"country\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # returns the snowpark DataFrame directly, so no data is computed locally\n    df = context.snowpark.create_dataframe([[1, \"a\", \"usa\"], [2, \"b\", \"cad\"]], schema=[\"id\", \"name\", \"country\"])\n    df = df.filter(df.id &gt; 1)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#bigframe","title":"Bigframe","text":"<p>If you're using BigQuery, use the Bigframe DataFrame API. Bigframe looks like Pandas but runs everything in BigQuery.</p> <p>Why Bigframe over Pandas: All computation happens in BigQuery, so you get BigQuery's scale and performance. Plus, you can use BigQuery remote functions (like in the example below) for custom Python logic.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nfrom bigframes.pandas import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n\ndef get_bucket(num: int):\n    if not num:\n        return \"NA\"\n    boundary = 10\n    return \"at_or_above_10\" if num &gt;= boundary else \"below_10\"\n\n\n@model(\n    \"vulcan_demo.bigframe_model\",\n    columns={\n        \"title\": \"text\",\n        \"views\": \"int\",\n        \"bucket\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # Create a remote function to be used in the Bigframe DataFrame\n    remote_get_bucket = context.bigframe.remote_function([int], str)(get_bucket)\n\n    # Returns the Bigframe DataFrame handle, no data is computed locally\n    df = context.bigframe.read_gbq(\"bigquery-samples.wikipedia_pageviews.200809h\")\n\n    df = (\n        # This runs entirely on the BigQuery engine lazily\n        df[df.title.str.contains(r\"[Gg]oogle\")]\n        .groupby([\"title\"], as_index=False)[\"views\"]\n        .sum(numeric_only=True)\n        .sort_values(\"views\", ascending=False)\n    )\n\n    return df.assign(bucket=df[\"views\"].apply(remote_get_bucket))\n</code></pre>"},{"location":"components/model/types/python_models/#batching","title":"Batching","text":"<p>If your Python model outputs a huge DataFrame and you can't use Spark/Bigframe/Snowpark, you can batch the output using Python generators.</p> <p>The problem: With Pandas, everything loads into memory. If your output is too large, you'll run out of memory.</p> <p>The solution: Use <code>yield</code> to return DataFrames in chunks. Vulcan processes them one at a time, so you never have more than one chunk in memory at once.</p> <p>Here's how you'd do it:</p> <pre><code>@model(\n    \"vulcan_demo.batching_model\",\n    columns={\n        \"customer_id\": \"int\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's table name\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    for i in range(3):\n        # run 3 queries to get chunks of data and not run out of memory\n        df = context.fetchdf(f\"SELECT customer_id from {table} WHERE customer_id = {i}\")\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#serialization","title":"Serialization","text":"<p>Vulcan executes Python models locally (wherever Vulcan is running) using a custom serialization framework. This means your Python code runs on your machine or CI/CD environment, not in the database.</p> <p>Why this matters: You have full access to Python libraries, can make API calls, do ML processing, etc. The database just receives the final DataFrame.</p>"},{"location":"components/model/types/sql_models/","title":"SQL","text":""},{"location":"components/model/types/sql_models/#sql","title":"SQL","text":"<p>SQL models are Vulcan's bread and butter, they're the most common type of model you'll write. You can define them using SQL directly, or use Python to generate SQL dynamically.</p> <p>Why SQL models? They're simple, powerful, and work with any SQL database. Most of your data transformations will probably be SQL models.</p>"},{"location":"components/model/types/sql_models/#sql-based-definition","title":"SQL-Based Definition","text":"<p>SQL-based models are the most common type. They're designed to feel like regular SQL, but with superpowers.</p> <p>Structure: A SQL model file has these parts (in order): 1. The <code>MODEL</code> DDL (metadata and configuration) 2. Optional pre-statements (setup SQL) 3. A single query (your transformation logic) 4. Optional post-statements (cleanup/optimization SQL) 5. Optional on-virtual-update statements (view permissions, etc.)</p> <p>Creating a SQL model: Add a <code>.sql</code> file to your <code>models/</code> directory (or a subdirectory). The filename doesn't matter to Vulcan, but it's conventional to name it after your model. For example, <code>sales.daily_sales</code> \u2192 <code>daily_sales.sql</code>.</p>"},{"location":"components/model/types/sql_models/#example","title":"Example","text":"<p>Here's a simple SQL model to get you started:</p> <pre><code>-- This is the MODEL DDL, where you specify model metadata and configuration information.\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\n/*\n  This is the single query that defines the model's logic.\n  Although it is not required, it is considered best practice to explicitly\n  specify the type for each one of the model's columns through casting.\n*/\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"components/model/types/sql_models/#model-ddl","title":"<code>MODEL</code> DDL","text":"<p>The <code>MODEL</code> DDL is where you define your model's metadata, name, kind, schedule, owner, and more. It must be the first statement in your SQL file.</p> <p>Think of it as the \"header\" that tells Vulcan everything it needs to know about your model. For a complete list of all available properties, check out the Model Properties documentation.</p>"},{"location":"components/model/types/sql_models/#optional-prepost-statements","title":"Optional Pre/Post-Statements","text":"<p>Pre-statements run before your query, post-statements run after. They're perfect for setup, cleanup, and optimization tasks.</p> <p>Common use cases: - Pre-statements: Set session parameters, load UDFs, cache tables - Post-statements: Create indexes, run data quality checks, set retention policies</p> <p>Important: Pre/post-statements must end with semicolons. If you have post-statements, your main query must also end with a semicolon (so Vulcan knows where the query ends).</p> <p>Concurrency</p> <p>Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings and temporary objects.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\n-- Pre-statement: Cache a table for use in the query\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\n-- The model query (must end with semi-colon when post-statements are present)\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date;\n\n-- Post-statement: Clean up the cached table\nUNCACHE TABLE countries;\n</code></pre> <p>Project-level defaults: You can define pre/post-statements in <code>model_defaults</code> for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the model configuration reference.</p> <p>Statements Run Twice</p> <p>Pre/post-statements are evaluated twice: when a model's table is created and when its query logic is evaluated. Executing statements more than once can have unintended side-effects, so you can conditionally execute them based on Vulcan's runtime stage.</p> <pre><code>**Solution:** Use conditional execution with `@IF` and `@runtime_stage` to control when statements run. For example, only run a post-statement when the query is actually being evaluated:\n</code></pre> <p>We can condition the post-statement to only run after the model query is evaluated using the <code>@IF</code> macro operator and <code>@runtime_stage</code> macro variable like this:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  UNCACHE TABLE countries\n);\n</code></pre> <p>Important: The SQL command inside <code>@IF()</code> doesn't end with a semicolon. The semicolon goes after the <code>@IF()</code> macro's closing parenthesis.</p>"},{"location":"components/model/types/sql_models/#optional-on-virtual-update-statements","title":"Optional On-Virtual-Update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view is set up.</p> <p>Common use case: Granting permissions on views so users can query them.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>Syntax: Wrap these statements in <code>ON_VIRTUAL_UPDATE_BEGIN;</code> ... <code>ON_VIRTUAL_UPDATE_END;</code> blocks:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nSELECT\n  order_date::TIMESTAMP,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\nON_VIRTUAL_UPDATE_BEGIN;\nGRANT SELECT ON VIEW @this_model TO ROLE role_name;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE admin;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Jinja support: You can use Jinja expressions in these statements. Just wrap them in <code>JINJA_STATEMENT_BEGIN;</code> ... <code>JINJA_END;</code> blocks (as shown in the example above).</p> <p>Virtual Layer Resolution</p> <p>These statements run at the virtual layer, so table names resolve to view names, not physical table names. In a <code>dev</code> environment, <code>sales.daily_sales</code> and <code>@this_model</code> resolve to <code>sales__dev.daily_sales</code> (the view), not the physical table.</p>"},{"location":"components/model/types/sql_models/#the-model-query","title":"The Model Query","text":"<p>Your model must contain a standalone query. This can be: - A single <code>SELECT</code> statement - Multiple <code>SELECT</code> statements combined with <code>UNION</code>, <code>INTERSECT</code>, or <code>EXCEPT</code></p> <p>The result of this query becomes your model's table or view data. Pretty straightforward!</p>"},{"location":"components/model/types/sql_models/#sql-model-blueprinting","title":"SQL Model Blueprinting","text":"<p>SQL models can serve as templates for creating multiple models. This is called \"blueprinting\", define one template, get multiple models.</p> <p>How it works: Parameterize your model name with a variable (using <code>@{variable}</code> syntax) and provide a list of mappings in <code>blueprints</code>. Vulcan creates one model for each mapping.</p> <p>Use case: When you have similar models that differ only by parameters (like different regions, schemas, or customers).</p> <p>Here's an example that creates four models from one template:</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints (\n    (region := 'north'),\n    (region := 'south'),\n    (region := 'east'),\n    (region := 'west')\n  ),\n  grains region_id\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre> <p>Vulcan creates these four models from that template:</p> <pre><code>-- This uses the first variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__north,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('north')\n\n-- This uses the second variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__south,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('south')\n</code></pre> <p>Important syntax: Notice <code>@{region}</code> in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal).</p> <p>You can see the different behavior in the WHERE clause. <code>@region</code> (without braces) is resolved to the string literal <code>'north'</code> (with single quotes) because the blueprint value is quoted. Learn more about the curly brace syntax here.</p> <p>Learn more about this syntax here.</p> <p>Dynamic blueprints: You can generate blueprints using macros. This is handy when your blueprint list comes from external sources (CSV files, APIs, etc.):</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  blueprints @gen_blueprints(),  -- Macro generates the list\n  ...\n);\n</code></pre> <p>Here's how you might define the macro:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((region := 'north'),\"\n        \" (region := 'south'),\"\n        \" (region := 'east'),\"\n        \" (region := 'west'))\"\n    )\n</code></pre> <p>You can also use the <code>@EACH</code> macro with a global list variable:</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints @EACH(@values, x -&gt; (region := @x)),\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre>"},{"location":"components/model/types/sql_models/#python-based-definition","title":"Python-Based Definition","text":"<p>You can also define SQL models using Python! This is useful when: - Your query is too complex for clean SQL - You need heavy dynamic logic (would require lots of macros) - You want to generate SQL programmatically</p> <p>How it works: You write Python code that generates SQL, and Vulcan executes it. You still get SQL models (they run SQL queries), but you write them in Python.</p> <p>For the complete guide on Python-based SQL models, including the <code>@model</code> decorator, execution context, and examples, see the Python Models page.</p>"},{"location":"components/model/types/sql_models/#automatic-dependencies","title":"Automatic Dependencies","text":"<p>One of Vulcan's superpowers: it parses your SQL and automatically figures out dependencies. No need to manually specify what your model depends on, Vulcan just knows!</p> <p>How it works: Vulcan analyzes your <code>FROM</code> and <code>JOIN</code> clauses and builds a dependency graph. When you run <code>vulcan plan</code>, it ensures upstream models run first.</p> <p>Example: This query automatically depends on <code>raw.raw_orders</code>:</p> <pre><code>SELECT order_date, COUNT(order_id) AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan will make sure <code>raw.raw_orders</code> runs before this model. Pretty neat!</p> <p>External dependencies: If you reference tables that aren't Vulcan models, Vulcan can handle them too, either implicitly (through execution order) or via signals.</p> <p>Manual dependencies: Sometimes you need to add extra dependencies manually (maybe a hidden dependency or a macro that references another model). Use the <code>depends_on</code> property in your <code>MODEL</code> DDL for that.</p>"},{"location":"components/model/types/sql_models/#conventions","title":"Conventions","text":"<p>Vulcan follows some conventions to keep things consistent and reliable. Here are the key ones:</p>"},{"location":"components/model/types/sql_models/#explicit-type-casting","title":"Explicit Type Casting","text":"<p>Vulcan encourages explicit type casting for all columns. This helps Vulcan understand your data types and prevents incorrect inference.</p> <p>Format: Use <code>column_name::data_type</code> syntax (works in any SQL dialect):</p> <pre><code>SELECT\n  order_date::DATE AS order_date,\n  total_orders::INTEGER AS total_orders,\n  revenue::DECIMAL(10,2) AS revenue\n</code></pre> <p>Why this matters: Explicit types make your models more predictable and help Vulcan optimize queries better.</p>"},{"location":"components/model/types/sql_models/#explicit-selects","title":"Explicit SELECTs","text":"<p>Avoid <code>SELECT *</code> when possible. It's convenient, but dangerous\u2014if an upstream source adds or removes columns, your model's output changes unexpectedly.</p> <p>Best practice: List every column you need explicitly. If you're querying external sources, use <code>create_external_models</code> to capture their schema, or define them as external models.</p> <p>Why avoid <code>SELECT *</code> on external sources: It prevents Vulcan from optimizing queries and determining column-level lineage. Define external models instead!</p>"},{"location":"components/model/types/sql_models/#encoding","title":"Encoding","text":"<p>SQL model files must be UTF-8 encoded. Using other encodings can cause parsing errors or unexpected behavior.</p>"},{"location":"components/model/types/sql_models/#transpilation","title":"Transpilation","text":"<p>Vulcan uses SQLGlot to parse and transpile SQL. This gives you some superpowers:</p> <p>Write in any dialect, run on any engine: Write PostgreSQL-style SQL, and Vulcan converts it to BigQuery. Or write Snowflake SQL and run it on Spark. Pretty cool!</p> <p>Use advanced syntax: You can use features from one dialect even if your engine doesn't support them. For example, <code>x::int</code> (PostgreSQL syntax) works even on engines that only support <code>CAST(x AS INT)</code>. SQLGlot handles the conversion.</p> <p>Formatting flexibility: Trailing commas, extra whitespace, minor formatting differences, SQLGlot normalizes them all. Write SQL however you like, and Vulcan makes it consistent.</p>"},{"location":"components/model/types/sql_models/#macros","title":"Macros","text":"<p>Standard SQL is powerful, but real-world data pipelines need dynamic components. Date filters that change each run, conditional logic, reusable query patterns\u2014macros give you all of this.</p> <p>Macro variables: Vulcan provides automatic date/time variables for incremental models. Use <code>@start_date</code>, <code>@end_date</code>, <code>@start_ds</code>, <code>@end_ds</code> and Vulcan fills them in with the current time range. No more hardcoding dates!</p> <p>Custom macros: For complex logic or reusable patterns, Vulcan supports a powerful macro syntax and Jinja templates. Write macros once, use them everywhere.</p> <p>Why macros matter: They make your SQL more maintainable. Instead of copy-pasting complex logic, define it once as a macro and reuse it. Your queries stay clean and readable.</p> <p>Learn more about macros in the Macros documentation.</p>"},{"location":"components/semantics/business_metrics/","title":"Business Metrics","text":""},{"location":"components/semantics/business_metrics/#business-metrics","title":"Business Metrics","text":"<p>Business metrics are where your semantic layer really shines. They combine measures (the calculations) with dimensions (the attributes) and time (the when) to create complete analytical definitions that are ready for time-series analysis.</p> <p>Think of it this way: semantic models give you the building blocks (measures, dimensions, joins), and business metrics combine those blocks into something you can actually analyze over time. They're pre-configured for dashboards, reports, and APIs\u2014no SQL required.</p>"},{"location":"components/semantics/business_metrics/#what-are-business-metrics","title":"What are business metrics?","text":"<p>Business metrics are complete analytical definitions that:</p> <ul> <li>Combine measures with time: Let you analyze trends at different time granularities (daily, weekly, monthly, etc.)</li> <li>Include dimensions: Enable slicing and dicing by business attributes (customer tier, region, product category, etc.)</li> <li>Ready for analysis: Pre-configured so they can power dashboards, reports, and APIs directly</li> <li>Examples: <code>monthly_revenue_by_tier</code>, <code>daily_active_users</code>, <code>customer_acquisition_trend</code></li> </ul> <p>They're the bridge between your technical data models and the business questions people actually want to answer.</p>"},{"location":"components/semantics/business_metrics/#basic-structure","title":"Basic structure","text":"<p>A business metric brings together three things:</p> <ul> <li>Measure: The calculation you want to perform (like <code>orders.total_revenue</code>)</li> <li>Time: The time dimension for your analysis (like <code>orders.order_date</code>)</li> <li>Dimensions: Optional attributes for grouping and filtering (like <code>customers.customer_tier</code>)</li> </ul> <p>Here's the simplest possible metric:</p> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue      # Which measure to calculate\n    time: orders.order_date            # Time dimension for analysis\n    description: \"Monthly revenue trends\"\n</code></pre> <p>That's it! This metric is now ready to be queried at any time granularity you want.</p>"},{"location":"components/semantics/business_metrics/#simple-metric","title":"Simple metric","text":"<p>Let's start with the basics, a metric that just has a measure and time:</p> <pre><code>metrics:\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n</code></pre> <p>Even though it's called <code>daily_revenue</code>, you're not locked into daily granularity. You can query this same metric at different time intervals (day, week, month, quarter, year) without redefining it. The metric definition stays the same; you just change the granularity when you query it.</p>"},{"location":"components/semantics/business_metrics/#metric-with-dimensions","title":"Metric with dimensions","text":"<p>Add dimensions to enable slicing and grouping:</p> <pre><code>metrics:\n  revenue_by_tier:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier      # Group by tier\n      - customers.country            # And country\n    description: \"Revenue trends by customer tier and country\"\n</code></pre> <p>Now you can answer questions like: - What's our revenue by tier over time? - How does revenue vary by country? - What's the revenue breakdown by tier and country together?</p> <p>The dimensions give you flexibility to analyze the metric from different angles.</p>"},{"location":"components/semantics/business_metrics/#cross-model-metrics","title":"Cross-model metrics","text":"<p>You're not limited to one model. Combine measures and dimensions from multiple models:</p> <pre><code>metrics:\n  product_revenue_by_customer_segment:\n    measure: orders.total_revenue      # From orders\n    time: orders.order_date            # From orders\n    dimensions:\n      - products.category              # From products\n      - products.brand\n      - customers.customer_tier        # From customers\n      - customers.region\n    description: \"Product revenue segmented by customer demographics\"\n</code></pre> <p>This metric pulls the measure from <code>orders</code>, time from <code>orders</code>, product dimensions from <code>products</code>, and customer dimensions from <code>customers</code>. As long as you've defined the proper joins between these semantic models, Vulcan will handle the cross-model logic for you.</p> <p>Important: Make sure your semantic models have the right joins defined, or cross-model metrics won't work.</p>"},{"location":"components/semantics/business_metrics/#reference-format","title":"Reference format","text":"<p>Always use dot notation with semantic model aliases when referencing measures, dimensions, and time:</p> <pre><code># \u2705 Good: Use aliases\nmeasure: orders.total_revenue     # alias.measure_name\ntime: orders.order_date           # alias.column_name\ndimensions:\n  - customers.customer_tier       # alias.column_name\n\n# \u274c Bad: Don't use physical names\nmeasure: analytics.fact_orders.revenue\ntime: order_date  # Missing alias\n</code></pre> <p>The dot notation (<code>orders.total_revenue</code>) tells Vulcan which semantic model to look in and what to reference. Physical table names won't work here, you need the semantic aliases.</p>"},{"location":"components/semantics/business_metrics/#time-granularity","title":"Time granularity","text":"<p>One of the coolest things about metrics is that you define them once, then query them at any time granularity you want:</p> <pre><code>metrics:\n  revenue_trends:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Revenue at any time granularity\"\n</code></pre> <p>The same metric can be queried with different granularities: - Daily: <code>granularity=day</code> - Weekly: <code>granularity=week</code> - Monthly: <code>granularity=month</code> - Quarterly: <code>granularity=quarter</code> - Yearly: <code>granularity=year</code></p> <p>You don't need separate metric definitions for each granularity, just change the query parameter.</p>"},{"location":"components/semantics/business_metrics/#complete-example","title":"Complete example","text":"<p>Here's a more complete example showing different types of metrics:</p> <pre><code>metrics:\n  # Simple revenue metric\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n    tags: [revenue, financial, kpi]\n\n  # Customer acquisition\n  customer_acquisition_trend:\n    measure: customers.new_signups\n    time: customers.signup_date\n    dimensions:\n      - customers.signup_channel\n      - customers.customer_tier\n      - customers.country\n    description: \"Customer acquisition by channel, tier, and geography\"\n    tags: [acquisition, growth, customer]\n\n  # Cross-model metric\n  product_performance:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - products.category\n      - products.brand\n      - customers.customer_tier\n    description: \"Product revenue by category, brand, and customer segment\"\n    tags: [revenue, products, segmentation]\n</code></pre> <p>Notice how each metric has a clear purpose, good description, and relevant tags. The tags help organize and discover metrics later.</p>"},{"location":"components/semantics/business_metrics/#benefits","title":"Benefits","text":""},{"location":"components/semantics/business_metrics/#time-series-analysis","title":"Time-series analysis","text":"<p>Metrics are built for analyzing trends over time:</p> <ul> <li>Flexible granularity: Query the same metric at different time intervals without redefinition</li> <li>Consistent definitions: Same calculation logic applies across all time periods</li> <li>Trend analysis: Built-in support for comparing periods (month-over-month, year-over-year, etc.)</li> </ul>"},{"location":"components/semantics/business_metrics/#self-service-analytics","title":"Self-service analytics","text":"<p>Business users can query metrics without writing SQL:</p> <ul> <li>Simple API: Query metrics by name with a time range and dimensions</li> <li>Consistent results: Same metric definition is used everywhere, so everyone gets the same answer</li> <li>No SQL required: Complex joins and aggregations are abstracted away</li> </ul>"},{"location":"components/semantics/business_metrics/#single-source-of-truth","title":"Single source of truth","text":"<p>Centralized metric definitions mean:</p> <ul> <li>Define once: Create metric definitions in YAML files</li> <li>Use everywhere: Same metrics power dashboards, reports, and APIs</li> <li>Version controlled: Metric definitions live alongside your code, so changes are tracked</li> </ul>"},{"location":"components/semantics/business_metrics/#best-practices","title":"Best practices","text":""},{"location":"components/semantics/business_metrics/#descriptive-names","title":"Descriptive names","text":"<p>Make your metric names self-explanatory:</p> <pre><code># \u2705 Good: Self-explanatory\nmetrics:\n  monthly_revenue_by_tier: ...\n  daily_active_users: ...\n\n# \u274c Bad: Vague\nmetrics:\n  metric_1: ...\n  rev: ...\n</code></pre> <p>Good names make it obvious what the metric measures and how it's broken down.</p>"},{"location":"components/semantics/business_metrics/#include-essential-dimensions","title":"Include essential dimensions","text":"<p>Think about what business questions people will want to answer, and include those dimensions:</p> <pre><code># \u2705 Good: Key business dimensions\nmetrics:\n  revenue_analysis:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n      - customers.region\n      - products.category\n\n# \u274c Too few: Limited analysis\nmetrics:\n  revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    # Missing dimensions - can't slice and dice!\n</code></pre> <p>Dimensions are what make metrics useful. Without them, you can only see the overall trend, not the breakdowns that drive business decisions.</p>"},{"location":"components/semantics/business_metrics/#document-business-context","title":"Document business context","text":"<p>Add descriptions and metadata to help people understand what the metric means:</p> <pre><code>metrics:\n  net_revenue_retention:\n    measure: subscriptions.nrr\n    time: subscriptions.cohort_month\n    description: \"Net Revenue Retention: expansion minus churn\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation: \"(Starting MRR + Expansion - Churn) / Starting MRR\"\n      benchmark: \"&gt;110% is good for SaaS\"\n</code></pre> <p>The <code>meta</code> section is perfect for business context, calculation details, benchmarks, and ownership information. This helps people understand not just what the metric is, but what it means and how to interpret it.</p>"},{"location":"components/semantics/business_metrics/#integration-with-semantic-models","title":"Integration with semantic models","text":"<p>Metrics build on top of semantic models:</p> <ol> <li>Semantic models define measures, dimensions, and joins</li> <li>Metrics combine these components with time for analysis</li> <li>APIs expose metrics for querying and visualization</li> </ol> <p>The semantic layer provides the foundation (the building blocks), and metrics add the time-series analytical capabilities (the finished product).</p>"},{"location":"components/semantics/business_metrics/#next-steps","title":"Next steps","text":"<ul> <li>Learn about Semantic Models that provide the foundation for metrics</li> <li>See the Semantics Overview for the complete picture</li> <li>Explore metric definitions in your project's <code>semantics/</code> directory</li> </ul>"},{"location":"components/semantics/models/","title":"Semantic Models","text":""},{"location":"components/semantics/models/#semantic-models","title":"Semantic Models","text":"<p>Semantic models are your bridge between technical data structures and business understanding. They take your physical Vulcan models (the tables and columns in your database) and map them to business concepts that make sense to analysts, product managers, and other non-technical users.</p> <p>Think of semantic models as a translation layer. Your database might have tables named <code>dim_customers</code> or <code>fact_orders</code> (technical naming), but your semantic layer can expose them as <code>customers</code> and <code>orders</code> (business-friendly naming). More importantly, semantic models define what you can actually do with the data\u2014dimensions for grouping, measures for calculations, segments for filtering, and joins for combining models.</p>"},{"location":"components/semantics/models/#what-are-semantic-models","title":"What are semantic models?","text":"<p>Semantic models bridge the gap between technical table structures and business understanding:</p> <ul> <li>Reference physical models: Each semantic model references a Vulcan model defined in your <code>models/</code> directory</li> <li>Provide business aliases: Hide technical naming (like <code>dim_customers</code> or <code>fact_orders</code>) behind consumer-friendly names</li> <li>Expose analytical capabilities: Define dimensions, measures, segments, and joins for each model</li> </ul> <p>They're the foundation of your semantic layer, everything else (business metrics, semantic queries) builds on top of semantic models.</p>"},{"location":"components/semantics/models/#basic-structure","title":"Basic structure","text":"<p>A semantic model maps a physical Vulcan model to a semantic representation. Here's the basic structure:</p> <pre><code>models:\n  analytics.customers:  # Physical model name (dictionary key)\n    alias: customers     # Business-friendly semantic alias\n    description: \"Customer master data\"\n    dimensions: {...}    # Optional: control which columns are exposed\n    measures: {...}      # Optional: aggregated calculations\n    segments: {...}      # Optional: reusable filter conditions\n    joins: {...}         # Optional: relationships to other models\n</code></pre> <p>The physical model name (<code>analytics.customers</code>) is the key, and everything else defines how it appears in the semantic layer.</p>"},{"location":"components/semantics/models/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes you use for grouping and filtering. They answer \"by what?\" questions, like \"revenue by customer tier\" or \"orders by country.\"</p> <p>The good news: All columns from your Vulcan model automatically become dimensions. You don't have to define them manually unless you want to control which ones are exposed or add enhancements.</p> <p>Here's how you can control dimensions:</p> <pre><code># All columns from analytics.customers automatically become dimensions:\n# - customers.customer_id\n# - customers.customer_tier\n# - customers.signup_date\n# - customers.country\n\n# You can control which columns are exposed:\ndimensions:\n  excludes:\n    - password_hash       # Hide sensitive data\n    - internal_notes\n\n  # Enhance dimensions with additional capabilities:\n  enhancements:\n    - name: start_date\n      granularities:\n        - name: monthly\n          interval: \"1 month\"\n          description: \"Monthly subscription cohorts\"\n        - name: quarterly\n          interval: \"3 months\"\n          description: \"Quarterly cohorts\"\n</code></pre> <p>Use <code>excludes</code> to hide sensitive or internal columns. Use <code>enhancements</code> to add time granularities for cohort analysis, super useful for subscription or signup dates.</p>"},{"location":"components/semantics/models/#measures","title":"Measures","text":"<p>Measures are aggregated calculations that answer \"how much?\" or \"how many?\" questions. They're what you use to calculate totals, averages, counts, and other aggregations.</p> <p>You define measures using SQL expressions with aggregations like <code>SUM()</code>, <code>COUNT()</code>, <code>AVG()</code>, etc.:</p> <pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{customers.amount}\"\n    description: \"Total revenue from all orders\"\n    format: currency\n\n  avg_order_value:\n    type: number\n    expression: \"SUM({customers.total_revenue}) / NULLIF(COUNT(*), 0)\"\n    format: currency\n    description: \"Average order value\"\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"\n    filters:\n      - \"{customers.status} = 'active'\"\n    description: \"Number of active customers\"\n</code></pre> <p>Notice the curly braces around column references like <code>{customers.amount}</code>? That's the semantic reference syntax. We'll talk more about that in the best practices section.</p> <p>Measures can have filters (like <code>active_customers</code> above), which let you calculate metrics on subsets of data. They can also have formatting hints (like <code>currency</code>) to help visualization tools display them correctly.</p>"},{"location":"components/semantics/models/#segments","title":"Segments","text":"<p>Segments are reusable filter conditions that answer \"which ones?\" questions. They define meaningful subsets of your data that you can use across multiple queries and metrics.</p> <p>Think of segments as saved filters. Instead of writing <code>WHERE status = 'active'</code> every time, you define an <code>active_customers</code> segment once and reuse it:</p> <pre><code>segments:\n  active_customers:\n    expression: \"{customers.status} = 'active'\"\n    description: \"Customers with active subscriptions\"\n\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"\n    description: \"Customers who spent over $10K\"\n\n  recent_signups:\n    expression: \"{customers.signup_date} &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    description: \"Customers who signed up in last 30 days\"\n</code></pre> <p>Segments make your semantic layer more consistent, everyone uses the same definition of \"active customers\" or \"high value,\" so there's no confusion about what those terms mean.</p>"},{"location":"components/semantics/models/#joins","title":"Joins","text":"<p>Joins define relationships between semantic models. They're what enable cross-model analysis, like combining order data with customer data or product data.</p> <p>You define the relationship type (<code>one_to_one</code>, <code>one_to_many</code>, <code>many_to_one</code>) and the join expression:</p> <pre><code>joins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"\n    description: \"Order's customer\"\n\n  products:\n    type: many_to_one\n    expression: \"{orders.product_id} = {products.product_id}\"\n    description: \"Ordered product\"\n</code></pre> <p>The relationship type helps Vulcan understand the cardinality, which is important for aggregations and preventing double-counting. The expression is the actual SQL join condition, using semantic references with curly braces.</p>"},{"location":"components/semantics/models/#cross-model-analysis","title":"Cross-model analysis","text":"<p>Once you've defined joins, you can reference columns and measures from other models in your current model's definitions. This is where semantic models really shine, you can build complex analytical definitions that span multiple models.</p>"},{"location":"components/semantics/models/#referencing-joined-model-fields","title":"Referencing joined model fields","text":"<p>You can use columns from joined models in measure expressions and filters:</p> <pre><code>measures:\n  enterprise_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    filters:\n      - \"{customers.customer_tier} = 'Enterprise'\"\n    description: \"Revenue from Enterprise customers\"\n</code></pre> <p>Even though <code>enterprise_revenue</code> is defined on the <code>orders</code> model, it filters by <code>customers.customer_tier</code> from the joined <code>customers</code> model. Vulcan handles the join logic automatically.</p>"},{"location":"components/semantics/models/#proxy-dimensions","title":"Proxy dimensions","text":"<p>Proxy dimensions let you expose measures from joined models as dimensions on the current model. This is useful when you want to filter or group by aggregated values from other models:</p> <pre><code>dimensions:\n  proxies:\n    - name: plan_average_monthly_price\n      measure: subscription_plans.avg_monthly_price\n\n    - name: plan_average_annual_price\n      measure: subscription_plans.avg_annual_price\n</code></pre> <p>Requirements: - The referenced model must be joined to the current model - The measure must exist on the target model - Use the format <code>model_alias.measure_name</code></p> <p>Proxy dimensions are powerful, they let you analyze one model using aggregated values from another model, all without writing complex SQL.</p>"},{"location":"components/semantics/models/#complete-example","title":"Complete example","text":"<p>Here's a complete semantic model definition that brings it all together:</p> <pre><code>models:\n  analytics.customers:\n    alias: customers\n\n    dimensions:\n      excludes:\n        - password_hash\n        - internal_notes\n      enhancements:\n        - name: signup_date\n          granularities:\n            - name: monthly\n              interval: \"1 month\"\n              description: \"Monthly signup cohorts\"\n            - name: quarterly\n              interval: \"3 months\"\n              description: \"Quarterly signup cohorts\"\n\n    measures:\n      total_customers:\n        type: count\n        expression: \"{customers.customer_id}\"\n        description: \"Total number of customers\"\n\n      active_customers:\n        type: count_distinct\n        expression: \"{customers.customer_id}\"\n        filters:\n          - \"{customers.status} = 'active'\"\n        description: \"Number of active customers\"\n\n    segments:\n      active:\n        expression: \"{customers.status} = 'active'\"\n        description: \"Active customers\"\n\n      high_value:\n        expression: \"{customers.total_spent} &gt; 10000\"\n        description: \"High-value customers\"\n\n    joins:\n      orders:\n        type: one_to_many\n        expression: \"{customers.customer_id} = {orders.customer_id}\"\n        description: \"Customer's orders\"\n</code></pre> <p>This semantic model: - Exposes customer dimensions (with some exclusions and enhancements) - Defines customer measures (total and active counts) - Creates reusable segments (active and high-value customers) - Joins to orders for cross-model analysis</p>"},{"location":"components/semantics/models/#best-practices","title":"Best practices","text":""},{"location":"components/semantics/models/#use-business-friendly-aliases","title":"Use business-friendly aliases","text":"<p>Your aliases should make sense to business users, not just developers:</p> <pre><code># \u2705 Good: Consumer-friendly\nalias: customers\nalias: orders\nalias: subscriptions\n\n# \u274c Bad: Technical naming\nalias: dim_customers\nalias: fact_orders\n</code></pre> <p>The whole point of semantic models is to hide technical complexity. Don't bring it back with technical naming!</p>"},{"location":"components/semantics/models/#design-models-with-semantics-in-mind","title":"Design models with semantics in mind","text":"<p>When you're building your Vulcan models, think about how they'll be used semantically:</p> <pre><code>-- \u2705 Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name\n  signup_date,        -- Good time dimension\n  total_spent         -- Good for segments\nFROM raw.customers;\n</code></pre> <p>Clean, descriptive column names make semantic models easier to build and use. Avoid abbreviations and technical jargon.</p>"},{"location":"components/semantics/models/#document-business-logic","title":"Document business logic","text":"<p>Add descriptions and metadata to help people understand what measures and segments mean:</p> <pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    description: \"Total revenue from all completed orders\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation_method: \"Sum of order amounts excluding refunds\"\n</code></pre> <p>The <code>meta</code> section is perfect for business context, ownership, calculation details, and other information that helps people understand and trust the metric.</p>"},{"location":"components/semantics/models/#use-curly-braces-for-references","title":"Use curly braces for references","text":"<p>When referencing any column or measure anywhere in your semantic model definitions, always use curly braces <code>{}</code>:</p> <pre><code># \u2705 Good: Use curly braces for all references\nmeasures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"  # Column reference with curly braces\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"  # Column reference with curly braces\n    filters:\n      - \"{customers.status} = 'active'\"  # Column reference in filter\n\nsegments:\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"  # Column reference with curly braces\n\njoins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"  # Both references use curly braces\n</code></pre> <p>Why use curly braces? - Clear distinction between semantic references and SQL functions - Consistent syntax across all semantic model definitions - Prevents ambiguity in complex expressions - Required for cross-model references (e.g., <code>{customers.customer_tier}</code>)</p> <p>It's a best practice that makes your semantic models more maintainable and less error-prone.</p>"},{"location":"components/semantics/models/#validation","title":"Validation","text":"<p>Vulcan automatically validates semantic model definitions when you create a plan. It checks:</p> <ul> <li>All column references in measures exist</li> <li>All column references in segments exist</li> <li>Join expressions reference valid columns</li> <li>Cross-model references have valid join paths</li> <li>Semantic aliases are properly defined</li> </ul> <p>If something's wrong, you'll know about it before you try to use the semantic layer. This catches errors early and keeps your semantic models reliable.</p>"},{"location":"components/semantics/models/#next-steps","title":"Next steps","text":"<ul> <li>Learn about Business Metrics that combine measures with time and dimensions</li> <li>Explore semantic model examples in your project's <code>semantics/</code> directory</li> <li>See the Semantics Overview for the complete picture</li> </ul>"},{"location":"components/semantics/overview/","title":"Overview","text":""},{"location":"components/semantics/overview/#overview","title":"Overview","text":"<p>The semantic layer is like a translator between your technical data models and your business users. It takes your SQL tables and makes them understandable to people who don't write SQL, turning <code>analytics.daily_revenue_metrics</code> into \"Monthly Revenue by Customer Tier\" that anyone can query.</p> <p>Think of it this way: your models are the engine (they do the work), and the semantic layer is the dashboard (it makes things usable). It adds business context, consistent definitions, and a friendly interface so people can actually use your data.</p>"},{"location":"components/semantics/overview/#what-is-the-semantic-layer","title":"What is the Semantic Layer?","text":"<p>The semantic layer bridges the gap between \"here's a table with columns\" and \"here's what this means for the business.\" It provides a consistent, business-friendly interface to your data that enables self-service analytics while keeping a single source of truth for your business logic.</p> <p>Without a semantic layer, every time someone wants to analyze revenue, they have to remember which table has it, what the column is called, how to join it with other tables, and how to calculate it correctly. With a semantic layer, they just ask for \"revenue\" and it works. Pretty cool, right?</p>"},{"location":"components/semantics/overview/#key-benefits","title":"Key Benefits","text":"<p>The semantic layer helps everyone in your organization work with data more effectively:</p> <p>For Developers:</p> <ul> <li>\u2705 Define metrics once, use everywhere - Write the calculation once, use it in dashboards, APIs, and reports</li> <li>\u2705 Version-controlled business logic - Your metric definitions live in code, so changes are tracked and reviewable</li> <li>\u2705 Consistent calculations - No more \"which revenue calculation should I use?\", there's one definition</li> </ul> <p>For Business Users:</p> <ul> <li>\u2705 Self-service analytics - Query data without writing SQL (or even knowing what SQL is)</li> <li>\u2705 Consistent metric definitions - Everyone uses the same definition of \"revenue\" or \"active users\"</li> <li>\u2705 Trusted, validated data - Metrics are defined by the data team, so you know they're correct</li> <li>\u2705 Works everywhere - Use the same metrics in Tableau, Power BI, Python, or APIs</li> </ul> <p>For Organizations:</p> <ul> <li>\u2705 Single source of truth - One place where \"revenue\" is defined, not scattered across 20 different dashboards</li> <li>\u2705 Faster time to insights - Business users can answer questions themselves instead of waiting for the data team</li> <li>\u2705 Reduced data team bottleneck - Less \"can you build me a dashboard?\" requests</li> <li>\u2705 Better data governance - Centralized definitions make it easier to audit and maintain data quality</li> </ul>"},{"location":"components/semantics/overview/#core-components","title":"Core Components","text":"<p>The semantic layer has two main pieces that work together. Think of them as building blocks, you start with semantic models, then build metrics on top.</p>"},{"location":"components/semantics/overview/#semantic-models","title":"Semantic Models","text":"<p>Semantic models are like wrappers around your Vulcan models. They take your technical tables and expose them in a business-friendly way. For detailed information, check out the Semantic Models documentation.</p> <p>Here's what semantic models do:</p> <ul> <li>Map physical models - Reference your Vulcan models from the <code>models/</code> directory</li> <li>Expose dimensions - All model columns automatically become dimensions (things you can filter and group by)</li> <li>Define measures - Aggregated calculations like <code>SUM(amount)</code> or <code>COUNT(*)</code></li> <li>Create segments - Reusable filter conditions (like \"high-value customers\" or \"active users\")</li> <li>Establish joins - Relationships between models so you can analyze across tables</li> </ul> <p>Here's a simple example:</p> <pre><code>models:\n  analytics.customers:\n    alias: customers\n    measures:\n      total_customers:\n        type: count\n        expression: \"COUNT(*)\"\n</code></pre> <p>This takes your <code>analytics.customers</code> model and exposes a <code>total_customers</code> measure that anyone can use. Pretty straightforward! Now business users can query \"total customers\" without knowing which table it comes from or how to write the SQL.</p>"},{"location":"components/semantics/overview/#business-metrics","title":"Business Metrics","text":"<p>Business metrics combine measures with dimensions and time to create complete analytical definitions. They're like pre-built queries that are ready to use. Learn more in the Business Metrics guide.</p> <p>Here's what makes metrics powerful:</p> <ul> <li>Time-series analysis - Metrics include time dimensions so you can see trends over time</li> <li>Flexible granularity - Query the same metric at different time intervals (day, week, month, etc.)</li> <li>Multi-dimensional - Slice and dice by business attributes (customer tier, region, product category, etc.)</li> <li>Ready for dashboards - Pre-configured for visualization tools</li> </ul> <p>Here's what a metric looks like:</p> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n</code></pre> <p>This creates a <code>monthly_revenue</code> metric that: - Uses the <code>total_revenue</code> measure from the orders model - Groups by <code>order_date</code> (time dimension) - Can be sliced by <code>customer_tier</code> (business dimension)</p> <p>Now anyone can query \"monthly revenue by customer tier\" without writing SQL! They just reference the metric name, and Vulcan handles all the complexity behind the scenes.</p>"},{"location":"components/semantics/overview/#how-it-works","title":"How It Works","text":"<p>Setting up your semantic layer is straightforward. Here's the workflow:</p> <ol> <li>Define semantic models - Create YAML files that reference your Vulcan models</li> <li>Add measures and dimensions - Define what can be calculated and filtered</li> <li>Create joins - Connect models so you can analyze across tables</li> <li>Define metrics - Combine measures with time and dimensions for analysis</li> <li>Validate - Vulcan automatically validates your semantic definitions when you create a plan</li> <li>Query - Use the semantic layer via APIs or export to BI tools</li> </ol> <p>The validation step is important, Vulcan checks that your measures reference real columns, joins are valid, and metrics make sense. It'll catch errors before you try to use them, which saves you from debugging issues later.</p>"},{"location":"components/semantics/overview/#file-organization","title":"File Organization","text":"<p>Semantic layer definitions are YAML files in the <code>semantics/</code> directory. You can organize them however makes sense for your team:</p> <pre><code>project/\n\u251c\u2500\u2500 models/           # Vulcan data models (.sql files)\n\u2502   \u251c\u2500\u2500 customers.sql\n\u2502   \u251c\u2500\u2500 orders.sql\n\u2502   \u2514\u2500\u2500 events.sql\n\u2502\n\u251c\u2500\u2500 semantics/        # Semantic layer definitions (YAML)\n\u2502   \u251c\u2500\u2500 customers.yml\n\u2502   \u251c\u2500\u2500 orders.yml\n\u2502   \u2514\u2500\u2500 metrics.yml\n\u2502\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming: The filename doesn't matter\u2014Vulcan automatically merges all YAML files in the <code>semantics/</code> directory. Organize by domain (like <code>customers.yml</code>, <code>orders.yml</code>) or by model (like <code>revenue_metrics.yml</code>)\u2014whatever makes sense for your team.</p>"},{"location":"components/semantics/overview/#integration-with-models","title":"Integration with Models","text":"<p>Here's the key insight: Model columns automatically become dimensions. The semantic layer adds measures, segments, joins, and metrics on top of your existing models. It builds on what you already have\u2014it doesn't replace anything.</p> <p>When you're designing Vulcan models, keep the semantic layer in mind:</p> <pre><code>-- \u2705 Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name (can filter/group by this)\n  signup_date,        -- Good time dimension (can analyze trends)\n  total_spent         -- Good for segments (can create \"high-value\" segment)\nFROM raw.customers;\n</code></pre> <p>This model will automatically expose: - <code>customer_tier</code> as a dimension (filter by tier, group by tier) - <code>signup_date</code> as a time dimension (analyze trends over time) - <code>total_spent</code> as a dimension (create segments like \"high-value customers\")</p> <p>Then you can add measures and metrics on top. The semantic layer builds on your models\u2014it doesn't replace them. Your models stay exactly as they are; the semantic layer just makes them more accessible.</p>"},{"location":"components/semantics/overview/#next-steps","title":"Next Steps","text":"<p>Ready to dive in? Here's where to go next:</p> <ul> <li>Semantic Models - Learn how to map physical models to business concepts</li> <li>Business Metrics - Create time-series analytical definitions</li> <li>Transpiling Semantic Queries - See how semantic queries get converted to SQL</li> <li>Check your project - Look at the <code>semantics/</code> directory in your Vulcan project for examples</li> </ul> <p>The semantic layer makes your data accessible to everyone, not just SQL experts. Start with semantic models, add some measures, then build metrics. Before you know it, your business users will be answering their own questions!</p>"},{"location":"components/tests/tests/","title":"Tests","text":""},{"location":"components/tests/tests/#tests","title":"Tests","text":"<p>Tests are your safety net for data transformations. Just like software engineers write unit tests to catch bugs before they ship, you can write tests to verify that your models transform data correctly\u2014catching problems before they reach production and cause headaches.</p> <p>Think of tests as executable documentation. They show exactly how your model should behave with specific inputs, and they'll yell at you if something changes unexpectedly. Unlike audits (which check data quality at runtime), tests verify the logic of your models against predefined inputs and expected outputs.</p>"},{"location":"components/tests/tests/#why-testing-matters","title":"Why Testing Matters","text":"<p>Data pipelines are tricky beasts. Small errors can snowball into significant business impacts. A small change in one model can cascade into big problems downstream. Here's why testing is worth your time:</p> <ul> <li>Catch breaking changes - Refactor with confidence knowing tests will flag unintended behavior changes</li> <li>Document expected behavior - Tests serve as executable specifications (better than comments that get outdated!)</li> <li>Faster debugging - When something breaks, tests pinpoint exactly which transformation failed</li> <li>Data quality assurance - Verify that aggregations, joins, and calculations produce correct results</li> <li>Confidence in changes - Make updates knowing you'll catch regressions before they hit production</li> </ul> <p>Tests run either on demand (like in CI/CD pipelines) or automatically when you create a new plan. Either way, they're there to help you sleep better at night.</p>"},{"location":"components/tests/tests/#creating-tests","title":"Creating Tests","text":"<p>Tests live in YAML files in the <code>tests/</code> folder of your project. The filename must start with <code>test</code> and end with <code>.yaml</code> or <code>.yml</code>. You can put multiple tests in one file (organize them however makes sense).</p> <p>At minimum, a test needs three things:</p> <ul> <li>model - Which model you're testing</li> <li>inputs - Mock data for upstream dependencies (what goes in)</li> <li>outputs - Expected results from the model's query (what should come out)</li> </ul> <p>Let's start with a simple example.</p>"},{"location":"components/tests/tests/#your-first-test","title":"Your First Test","text":"<p>Here's a model that aggregates orders by date:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP) AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>Now let's write a test to verify it works correctly:</p> <pre><code>test_daily_sales_aggregation:\n  model: sales.daily_sales\n  description: &gt;\n    Test that daily_sales correctly aggregates orders by date.\n\n  inputs:\n    raw.raw_orders:\n      rows:\n        - order_id: O001\n          order_date: '2024-03-15'\n          customer_id: C001\n          product_id: P001\n          total_amount: 50.00\n        - order_id: O002\n          order_date: '2024-03-15'\n          customer_id: C002\n          product_id: P002\n          total_amount: 75.00\n        - order_id: O003\n          order_date: '2024-03-16'\n          customer_id: C001\n          product_id: P003\n          total_amount: 100.00\n\n  outputs:\n    query:\n      rows:\n        - order_date: \"2024-03-15\"\n          total_orders: 2\n          total_revenue: 125.00\n          last_order_id: \"O002\"\n        - order_date: \"2024-03-16\"\n          total_orders: 1\n          total_revenue: 100.00\n          last_order_id: \"O003\"\n</code></pre> <p>This test gives the model three orders (two on March 15, one on March 16) and checks that:</p> <ul> <li>Orders are correctly grouped by date</li> <li><code>total_orders</code> counts distinct orders per day (should be 2 for March 15, 1 for March 16)</li> <li><code>total_revenue</code> sums the amounts correctly (50 + 75 = 125 for March 15)</li> <li><code>last_order_id</code> returns the maximum order ID per day (O002 for March 15, O003 for March 16)</li> </ul> <p>Pretty straightforward! If any of these expectations don't match, the test fails and tells you what went wrong.</p>"},{"location":"components/tests/tests/#testing-models-with-multiple-dependencies","title":"Testing Models with Multiple Dependencies","text":"<p>Real-world models often join multiple tables. Here's how you'd test a more complex model that joins customers, orders, and order items:</p> <pre><code>test_full_model_basic:\n  model: vulcan_demo.full_model\n  description: |\n    Validates aggregates and averages:\n    - DISTINCT order counting\n    - SUM(quantity * unit_price)\n    - avg_order_value = total_spent / total_orders, or NULL when total_orders = 0\n\n  inputs:\n    vulcan_demo.customers:\n      - customer_id: 1\n        name: Alice\n        email: alice@example.com\n      - customer_id: 2\n        name: Bob\n        email: bob@example.com\n      - customer_id: 3\n        name: Charlie\n        email: charlie@example.com\n\n    vulcan_demo.orders:\n      # Alice has 2 orders\n      - order_id: 1001\n        customer_id: 1\n      - order_id: 1002\n        customer_id: 1\n      # Bob has 1 order\n      - order_id: 2001\n        customer_id: 2\n      # Charlie has 0 orders (no rows)\n\n    vulcan_demo.order_items:\n      # Order 1001: 2*50 + 1*25 = 125\n      - order_id: 1001\n        product_id: 501\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 502\n        quantity: 1\n        unit_price: 25\n      # Order 1002: 1*200 = 200 \u2192 Alice total = 325\n      - order_id: 1002\n        product_id: 503\n        quantity: 1\n        unit_price: 200\n      # Order 2001: 2*5 = 10 \u2192 Bob total = 10\n      - order_id: 2001\n        product_id: 504\n        quantity: 2\n        unit_price: 5\n\n  outputs:\n    query:\n      rows:\n        - customer_id: 1\n          customer_name: Alice\n          email: alice@example.com\n          total_orders: 2\n          total_spent: 325\n          avg_order_value: 162.5\n        - customer_id: 2\n          customer_name: Bob\n          email: bob@example.com\n          total_orders: 1\n          total_spent: 10\n          avg_order_value: 10.0\n        - customer_id: 3\n          customer_name: Charlie\n          email: charlie@example.com\n          total_orders: 0\n          total_spent: 0\n          avg_order_value: null  # Division by zero handled\n</code></pre> <p>Notice how we're providing mock data for all three upstream tables. The test verifies that the model correctly: - Joins customers with orders and order items - Counts distinct orders per customer - Calculates total spent (quantity \u00d7 unit_price summed across all items) - Handles division by zero (Charlie has no orders, so avg_order_value should be NULL)</p> <p>The comments in the YAML help explain the test data, which makes it easier to understand what's being tested.</p>"},{"location":"components/tests/tests/#testing-incremental-models","title":"Testing Incremental Models","text":"<p>Incremental models are a bit special because they filter data by time range. You'll need to set <code>start</code> and <code>end</code> dates using the <code>vars</code> attribute:</p> <pre><code>test_incremental_by_time_range_basic:\n  model: vulcan_demo.incremental_by_time_range\n  description: |\n    Validates per-(order_date, product_id) aggregates over a fixed two-day window.\n    Checks DISTINCT order counts, quantity and revenue sums, and AVG(unit_price).\n  vars:\n    start: '2025-01-01'\n    end: '2025-01-02'\n\n  inputs:\n    vulcan_demo.products:\n      - product_id: 10\n        name: Widget\n        category: Electronics\n      - product_id: 20\n        name: Gizmo\n        category: Home\n\n    vulcan_demo.orders:\n      - order_id: 1001\n        customer_id: 9001\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1002\n        customer_id: 9002\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1003\n        customer_id: 9003\n        warehouse_id: 2\n        order_date: '2025-01-02'\n\n    vulcan_demo.order_items:\n      # 2025-01-01\n      - order_id: 1001\n        product_id: 10\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 20\n        quantity: 1\n        unit_price: 200\n      - order_id: 1002\n        product_id: 10\n        quantity: 1\n        unit_price: 60\n      # 2025-01-02\n      - order_id: 1003\n        product_id: 10\n        quantity: 5\n        unit_price: 40\n\n  outputs:\n    query:\n      rows:\n        - order_date: '2025-01-01'\n          product_id: 20\n          product_name: Gizmo\n          category: Home\n          order_count: 1\n          total_quantity: 1\n          total_sales_amount: 200\n          avg_unit_price: 200\n        - order_date: '2025-01-01'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 2\n          total_quantity: 3\n          total_sales_amount: 160\n          avg_unit_price: 55\n        - order_date: '2025-01-02'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 1\n          total_quantity: 5\n          total_sales_amount: 200\n          avg_unit_price: 40\n</code></pre> <p>The <code>vars</code> section tells Vulcan what time range to use when running the model. This is important because incremental models filter by <code>@start_ds</code> and <code>@end_ds</code> macros, and you need to control those in your test.</p>"},{"location":"components/tests/tests/#testing-ctes","title":"Testing CTEs","text":"<p>You can also test individual CTEs (Common Table Expressions) within your model. This is super useful for debugging complex queries step by step.</p> <p>Say you have a model with a CTE:</p> <pre><code>WITH filtered_orders_cte AS (\n  SELECT id, item_id\n  FROM vulcan_demo.incremental_model\n  WHERE item_id = 1\n)\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM filtered_orders_cte\nGROUP BY item_id\n</code></pre> <p>You can test both the CTE and the final query:</p> <pre><code>test_model_with_cte:\n  model: vulcan_demo.full_model\n  inputs:\n    vulcan_demo.incremental_model:\n      rows:\n        - id: 1\n          item_id: 1\n        - id: 2\n          item_id: 1\n        - id: 3\n          item_id: 2\n  outputs:\n    ctes:\n      filtered_orders_cte:\n        rows:\n          - id: 1\n            item_id: 1\n          - id: 2\n            item_id: 1\n    query:\n      rows:\n        - item_id: 1\n          num_orders: 2\n</code></pre> <p>This verifies that: 1. The CTE correctly filters to <code>item_id = 1</code> (should return rows with id 1 and 2) 2. The final query correctly counts distinct orders (should be 2)</p> <p>Testing CTEs separately makes it easier to pinpoint where things go wrong in complex queries.</p>"},{"location":"components/tests/tests/#supported-data-formats","title":"Supported Data Formats","text":"<p>Vulcan gives you flexibility in how you define test data. Pick whatever format works best for your situation:</p>"},{"location":"components/tests/tests/#yaml-dictionaries-default","title":"YAML Dictionaries (Default)","text":"<p>The most common format, just list your rows as YAML dictionaries:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    rows:\n      - order_id: 1001\n        customer_id: 1\n        order_date: '2025-01-01'\n</code></pre> <p>This is great for small datasets and when you want everything in one place.</p>"},{"location":"components/tests/tests/#csv-format","title":"CSV Format","text":"<p>If you have lots of data, CSV might be easier to read and write:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    rows: |\n      order_id,customer_id,order_date\n      1001,1,2025-01-01\n      1002,2,2025-01-01\n</code></pre> <p>You can also customize CSV parsing with <code>csv_settings</code> if you need different separators or other options.</p>"},{"location":"components/tests/tests/#sql-queries","title":"SQL Queries","text":"<p>Sometimes you want more control over how data is generated. Use a SQL query:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    query: |\n      SELECT 1001 AS order_id, 1 AS customer_id, '2025-01-01' AS order_date\n      UNION ALL\n      SELECT 1002 AS order_id, 2 AS customer_id, '2025-01-01' AS order_date\n</code></pre> <p>This is useful when you need to generate test data programmatically or when the data structure is complex.</p>"},{"location":"components/tests/tests/#external-files","title":"External Files","text":"<p>For large test datasets, store them in separate files:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    path: fixtures/orders_test_data.csv\n</code></pre> <p>This keeps your test files clean and makes it easy to reuse test data across multiple tests.</p>"},{"location":"components/tests/tests/#omitting-columns","title":"Omitting Columns","text":"<p>For wide tables, you don't need to specify every column. You can omit columns (they'll be treated as <code>NULL</code>) or use partial matching to only test the columns you care about:</p> <pre><code>outputs:\n  query:\n    partial: true  # Only test specified columns\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre> <p>This is super handy when you have a table with 50 columns but only care about testing a few of them.</p> <p>Apply partial matching globally:</p> <pre><code>outputs:\n  partial: true\n  query:\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre> <p>This applies partial matching to all outputs in the test, which is convenient when you're only testing a subset of columns.</p>"},{"location":"components/tests/tests/#freezing-time","title":"Freezing Time","text":"<p>If your model uses <code>CURRENT_TIMESTAMP</code> or similar functions, you'll want to freeze time in your tests to make them deterministic. Otherwise, your tests will fail every time you run them because the timestamp changes!</p> <pre><code>test_with_timestamp:\n  model: vulcan_demo.audit_log\n  outputs:\n    query:\n      - event: \"login\"\n        created_at: \"2023-01-01 12:05:03\"\n  vars:\n    execution_time: \"2023-01-01 12:05:03\"\n</code></pre> <p>Setting <code>execution_time</code> in <code>vars</code> makes <code>CURRENT_TIMESTAMP</code> and <code>CURRENT_DATE</code> return fixed values, so your tests are predictable and repeatable.</p>"},{"location":"components/tests/tests/#running-tests","title":"Running Tests","text":""},{"location":"components/tests/tests/#command-line","title":"Command Line","text":"<p>Run tests from the command line:</p> <pre><code># Run all tests\nvulcan test\n\n# Run specific test file\nvulcan test tests/test_daily_sales.yaml\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_aggregation\n\n# Run tests matching a pattern\nvulcan test tests/test_*\n</code></pre> <p>The <code>::</code> syntax lets you run a specific test from a file, which is handy when you're debugging a single failing test.</p>"},{"location":"components/tests/tests/#example-output","title":"Example Output","text":"<p>When tests pass, you'll see something like:</p> <pre><code>$ vulcan test\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.024s\n\nOK\n</code></pre> <p>The dots (<code>.</code>) indicate passing tests. Simple and clean!</p> <p>When tests fail:</p> <pre><code>$ vulcan test\nF\n======================================================================\nFAIL: test_daily_sales_aggregation (tests/test_daily_sales.yaml)\n----------------------------------------------------------------------\nAssertionError: Data mismatch (exp: expected, act: actual)\n\n  total_orders\n         exp  act\n0        3.0  2.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.012s\n\nFAILED (failures=1)\n</code></pre> <p>The output shows you exactly what didn't match. In this case, <code>total_orders</code> was expected to be 3.0 but was actually 2.0. This tells you exactly what to investigate.</p>"},{"location":"components/tests/tests/#automatic-test-generation","title":"Automatic Test Generation","text":"<p>Writing tests can be tedious, especially when you're just getting started. Vulcan can help by generating tests automatically:</p> <pre><code>vulcan create_test vulcan_demo.daily_sales \\\n  --query raw.raw_orders \"SELECT * FROM raw.raw_orders WHERE order_date BETWEEN '2025-01-01' AND '2025-01-02' LIMIT 10\" \n</code></pre> <p>This creates a test file with actual data from your warehouse, which makes it easy to bootstrap your test suite. You can then tweak the generated test to match your needs.</p> <p>Pro tip: Start with generated tests, then refine them to test edge cases and specific scenarios. It's much faster than writing everything from scratch!</p>"},{"location":"components/tests/tests/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/tests/tests/#preserving-fixtures","title":"Preserving Fixtures","text":"<p>When a test fails, you might want to inspect the actual data that was created. Use <code>--preserve-fixtures</code> to keep test fixtures around:</p> <pre><code>vulcan test --preserve-fixtures\n</code></pre> <p>Fixtures are created as views in a schema named <code>vulcan_test_&lt;random_ID&gt;</code>. You can query these views directly to see what data was actually produced, which is super helpful for debugging.</p>"},{"location":"components/tests/tests/#type-mismatches","title":"Type Mismatches","text":"<p>Sometimes Vulcan can't figure out the correct types for your test data. If you're seeing type errors, specify them explicitly:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    columns:\n      order_id: INT\n      order_date: DATE\n      total_amount: DECIMAL(10,2)\n    rows:\n      - order_id: 1001\n        order_date: '2025-01-01'\n        total_amount: 99.99\n</code></pre> <p>The <code>columns</code> section tells Vulcan exactly what types to use, which helps avoid type inference issues. You can also explicitly cast columns in your model's query to help Vulcan infer types more accurately.</p>"},{"location":"components/tests/tests/#test-not-finding-model","title":"Test Not Finding Model","text":"<p>Problem: Test says it can't find the model.</p> <p>Solution: Make sure the model name in your test matches exactly what's in your <code>models/</code> folder. Model names are case-sensitive and must include the schema (like <code>sales.daily_sales</code>, not just <code>daily_sales</code>).</p>"},{"location":"components/tests/tests/#output-order-matters","title":"Output Order Matters","text":"<p>Problem: Test fails even though the data looks correct.</p> <p>Solution: The columns in your expected output must appear in the same order as they're selected in the model's query. Check the <code>SELECT</code> statement order and make sure your test rows match.</p>"},{"location":"components/tests/tests/#partial-matching-not-working","title":"Partial Matching Not Working","text":"<p>Problem: Partial matching isn't ignoring extra columns.</p> <p>Solution: Make sure you set <code>partial: true</code> at the right level. It needs to be under <code>outputs.query</code> (or <code>outputs.ctes.&lt;cte_name&gt;</code>) for CTE-specific partial matching, or under <code>outputs</code> for global partial matching.</p>"},{"location":"components/tests/tests/#test-structure-reference","title":"Test Structure Reference","text":"<p>Here's a complete reference of all the fields you can use in a test. Most tests only need <code>model</code>, <code>inputs</code>, and <code>outputs</code>, but it's good to know what else is available.</p>"},{"location":"components/tests/tests/#test_name","title":"<code>&lt;test_name&gt;</code>","text":"<p>The unique name of your test. Use descriptive names that explain what you're testing, like <code>test_daily_sales_aggregation</code> or <code>test_customer_revenue_calculation</code>.</p>"},{"location":"components/tests/tests/#test_namemodel","title":"<code>&lt;test_name&gt;.model</code>","text":"<p>The fully qualified name of the model being tested (like <code>sales.daily_sales</code>). This model must exist in your project's <code>models/</code> folder.</p>"},{"location":"components/tests/tests/#test_namedescription","title":"<code>&lt;test_name&gt;.description</code>","text":"<p>An optional description that explains what the test validates. This is helpful for your teammates (and future you) to understand what the test is checking.</p>"},{"location":"components/tests/tests/#test_nameschema","title":"<code>&lt;test_name&gt;.schema</code>","text":"<p>The name of the schema that will contain the test fixtures (the views created for this test). If not specified, Vulcan creates a temporary schema.</p>"},{"location":"components/tests/tests/#test_namegateway","title":"<code>&lt;test_name&gt;.gateway</code>","text":"<p>The gateway whose <code>test_connection</code> will be used to run this test. If not specified, the default gateway is used. Useful when you need to test against a specific database or engine.</p>"},{"location":"components/tests/tests/#test_nameinputs","title":"<code>&lt;test_name&gt;.inputs</code>","text":"<p>Mock data for upstream models that your target model depends on. If your model has no dependencies, you can omit this.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_model","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;</code>","text":"<p>A model that your target model depends on. Provide mock data for each upstream model.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelrows","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>","text":"<p>The rows of test data, defined as an array of dictionaries:</p> <pre><code>    &lt;upstream_model&gt;:\n      rows:\n        - &lt;column_name&gt;: &lt;column_value&gt;\n        ...\n</code></pre> <p>Shortcut: If <code>rows</code> is the only key, you can omit it:</p> <pre><code>    &lt;upstream_model&gt;:\n      - &lt;column_name&gt;: &lt;column_value&gt;\n      ...\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelformat","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.format</code>","text":"<p>The format of the input data. Options: <code>yaml</code> (default) or <code>csv</code>.</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcsv_settings","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.csv_settings</code>","text":"<p>When using CSV format, customize how the CSV is parsed:</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n      csv_settings: \n        sep: \"#\"\n        skip_blank_lines: true\n      rows: |\n        &lt;column1_name&gt;#&lt;column2_name&gt;\n        &lt;row1_value&gt;#&lt;row1_value&gt;\n</code></pre> <p>See pandas read_csv documentation for all supported settings.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelpath","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.path</code>","text":"<p>Load data from an external file:</p> <pre><code>    &lt;upstream_model&gt;:\n      path: filepath/test_data.yaml\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcolumns","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.columns</code>","text":"<p>Explicitly specify column types to help Vulcan interpret your data correctly:</p> <pre><code>    &lt;upstream_model&gt;:\n      columns:\n        &lt;column_name&gt;: &lt;column_type&gt;\n        ...\n</code></pre> <p>This is especially useful when Vulcan can't infer types correctly (like with dates or decimals).</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelquery","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>","text":"<p>Generate input data using a SQL query:</p> <pre><code>    &lt;upstream_model&gt;:\n      query: &lt;sql_query&gt;\n</code></pre> <p>Note: You can't use <code>query</code> together with <code>rows</code>, pick one or the other.</p>"},{"location":"components/tests/tests/#test_nameoutputs","title":"<code>&lt;test_name&gt;.outputs</code>","text":"<p>The expected outputs from your model. This is what you're asserting should be true.</p> <p>Important: Column order matters! The columns in your expected rows must match the order they appear in the model's <code>SELECT</code> statement.</p>"},{"location":"components/tests/tests/#test_nameoutputspartial","title":"<code>&lt;test_name&gt;.outputs.partial</code>","text":"<p>When <code>true</code>, only test the columns you specify. Extra columns in the output are ignored. Useful for wide tables where you only care about a few columns.</p>"},{"location":"components/tests/tests/#test_nameoutputsquery","title":"<code>&lt;test_name&gt;.outputs.query</code>","text":"<p>The expected output of the model's final query. This is optional if you're testing CTEs instead.</p>"},{"location":"components/tests/tests/#test_nameoutputsquerypartial","title":"<code>&lt;test_name&gt;.outputs.query.partial</code>","text":"<p>Same as <code>outputs.partial</code>, but applies only to the query output (not CTEs).</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryrows","title":"<code>&lt;test_name&gt;.outputs.query.rows</code>","text":"<p>The expected rows from the model's query. Same format as input rows.</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryquery","title":"<code>&lt;test_name&gt;.outputs.query.query</code>","text":"<p>Generate expected output using a SQL query. Useful when the expected output is complex or when you want to compute it dynamically.</p>"},{"location":"components/tests/tests/#test_nameoutputsctes","title":"<code>&lt;test_name&gt;.outputs.ctes</code>","text":"<p>Test individual CTEs within your model. This is optional if you're testing the final query output.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_name","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;</code>","text":"<p>The expected output of a specific CTE. Use this to test intermediate steps in complex queries.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namepartial","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.partial</code>","text":"<p>Partial matching for a specific CTE.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namerows","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.rows</code>","text":"<p>Expected rows for a specific CTE.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namequery","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.query</code>","text":"<p>Generate expected CTE output using a SQL query.</p>"},{"location":"components/tests/tests/#test_namevars","title":"<code>&lt;test_name&gt;.vars</code>","text":"<p>Set values for macro variables used in your model:</p> <pre><code>  vars:\n    start: 2022-01-01\n    end: 2022-01-01\n    execution_time: 2022-01-01\n    &lt;macro_variable_name&gt;: &lt;macro_variable_value&gt;\n</code></pre> <p>Special variables: - <code>start</code> - Overrides <code>@start_ds</code> for incremental models - <code>end</code> - Overrides <code>@end_ds</code> for incremental models - <code>execution_time</code> - Overrides <code>@execution_ds</code> and makes <code>CURRENT_TIMESTAMP</code>/<code>CURRENT_DATE</code> return fixed values</p> <p>These are super useful for testing incremental models and making time-dependent tests deterministic.</p>"},{"location":"concepts-old/environments/","title":"Environments","text":""},{"location":"concepts-old/environments/#environments","title":"Environments","text":"<p>Environments are isolated namespaces that allow you to test and preview your changes.</p> <p>Vulcan differentiates between production and development environments. Currently, only the environment with the name <code>prod</code> is treated by Vulcan as the production one. Environments with other names are considered to be development ones.</p> <p>Models in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name <code>db.model_a</code> in the target environment <code>my_dev</code>, the <code>db__my_dev.model_a</code> table name should be used in a query. Models in the production environment are referred to by their original names.</p>"},{"location":"concepts-old/environments/#why-use-environments","title":"Why use environments","text":"<p>Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.</p> <p>Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment do not impact other environments. At the same time, any computation that was done in this environment can be safely reused in other environments.</p>"},{"location":"concepts-old/environments/#how-to-use-environments","title":"How to use environments","text":"<p>When running the plan command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is <code>prod</code>, which refers to the production environment. Environment with names other than <code>prod</code> are considered to be development environments.</p> <p>By default, the <code>vulcan plan</code> command targets the production (<code>prod</code>) environment.</p>"},{"location":"concepts-old/environments/#example","title":"Example","text":"<p>A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name <code>my_dev</code>, run:</p> <p></p><pre><code>vulcan plan my_dev\n</code></pre> A new environment is created automatically the first time a plan is applied to it.<p></p>"},{"location":"concepts-old/environments/#how-environments-work","title":"How environments work","text":"<p>Whenever a model definition changes, a new model snapshot is created with a unique fingerprint. This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model snapshots.</p> <p>Refer to plans for additional details.</p>"},{"location":"concepts-old/environments/#date-range","title":"Date range","text":"<p>A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the plan creation.</p>"},{"location":"concepts-old/glossary/","title":"Glossary","text":""},{"location":"concepts-old/glossary/#glossary","title":"Glossary","text":""},{"location":"concepts-old/glossary/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.</p>"},{"location":"concepts-old/glossary/#backfill","title":"Backfill","text":"<p>Load or refresh model data, triggered by a vulcan plan command.</p>"},{"location":"concepts-old/glossary/#catalog","title":"Catalog","text":"<p>A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.</p>"},{"location":"concepts-old/glossary/#cicd","title":"CI/CD","text":"<p>An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with tests and audits.</p>"},{"location":"concepts-old/glossary/#cte","title":"CTE","text":"<p>A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to tests.</p>"},{"location":"concepts-old/glossary/#dag","title":"DAG","text":"<p>Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.</p>"},{"location":"concepts-old/glossary/#data-modeling","title":"Data modeling","text":"<p>Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.</p>"},{"location":"concepts-old/glossary/#data-pipeline","title":"Data pipeline","text":"<p>The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.</p>"},{"location":"concepts-old/glossary/#data-transformation","title":"Data transformation","text":"<p>Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.</p>"},{"location":"concepts-old/glossary/#data-warehouse","title":"Data warehouse","text":"<p>The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.</p>"},{"location":"concepts-old/glossary/#direct-modification","title":"Direct Modification","text":"<p>A change to a model's definition from the user instead of being inherited from an upstream dependency like Indirect Modification.</p>"},{"location":"concepts-old/glossary/#elt","title":"ELT","text":"<p>Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.</p>"},{"location":"concepts-old/glossary/#etl","title":"ETL","text":"<p>Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.</p>"},{"location":"concepts-old/glossary/#full-refresh","title":"Full refresh","text":"<p>In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.</p>"},{"location":"concepts-old/glossary/#idempotency","title":"Idempotency","text":"<p>The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.</p>"},{"location":"concepts-old/glossary/#incremental-loads","title":"Incremental Loads","text":"<p>Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See Model Kinds for more information.</p>"},{"location":"concepts-old/glossary/#indirect-modification","title":"Indirect Modification","text":"<p>A change to model's upstream dependency and not to the model itself like a Direct Modification.</p>"},{"location":"concepts-old/glossary/#integration","title":"Integration","text":"<p>Combining data from various sources (such as from a data warehouse) into one unified view.</p>"},{"location":"concepts-old/glossary/#lineage","title":"Lineage","text":"<p>The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.</p>"},{"location":"concepts-old/glossary/#physical-layer","title":"Physical Layer","text":"<p>The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the Vulcan virtual layer's views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.</p>"},{"location":"concepts-old/glossary/#plan-summaries","title":"Plan Summaries","text":"<p>An upcoming feature that allows users to see a summary of changes applied to a given environment.</p>"},{"location":"concepts-old/glossary/#semantic-understanding","title":"Semantic Understanding","text":"<p>Vulcan, by leveraging SQLGlot, understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.</p>"},{"location":"concepts-old/glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.</p>"},{"location":"concepts-old/glossary/#table","title":"Table","text":"<p>A table is the visual representation of data stored in rows and columns.</p>"},{"location":"concepts-old/glossary/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.</p>"},{"location":"concepts-old/glossary/#view","title":"View","text":"<p>A view is the result of a SQL query on a database.</p>"},{"location":"concepts-old/glossary/#virtual-environments","title":"Virtual Environments","text":"<p>Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See plan application for more information.</p>"},{"location":"concepts-old/glossary/#virtual-layer","title":"Virtual Layer","text":"<p>The virtual layer is Vulcan's abstraction layer over the physical layer and physical data storage. While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.</p>"},{"location":"concepts-old/glossary/#virtual-update","title":"Virtual Update","text":"<p>Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See Virtual Update for more information.</p>"},{"location":"concepts-old/glossary/#virtual-preview","title":"Virtual Preview","text":"<p>Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.</p>"},{"location":"concepts-old/overview/","title":"Overview","text":""},{"location":"concepts-old/overview/#overview","title":"Overview","text":"<p>This page provides a conceptual overview of what Vulcan does and how its components fit together.</p>"},{"location":"concepts-old/overview/#what-vulcan-is","title":"What Vulcan is","text":"<p>Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of execution engines.</p> <p>It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.</p> <p>You can use Vulcan with the CLI.</p>"},{"location":"concepts-old/overview/#how-vulcan-works","title":"How Vulcan works","text":""},{"location":"concepts-old/overview/#create-models","title":"Create models","text":"<p>You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.</p>"},{"location":"concepts-old/overview/#make-a-plan","title":"Make a plan","text":"<p>Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.</p> <p>Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change before you expend the time and resources to actually perform the computations.</p> <p>Vulcan automatically identifies all affected models and the computations a change entails by creating a \"Vulcan plan.\" When you execute the <code>plan</code> command, Vulcan generates the plan for the environment specified in the command (e.g., dev, test, prod).</p> <p>The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.</p> <p>Learn more about plans.</p>"},{"location":"concepts-old/overview/#apply-the-plan","title":"Apply the plan","text":"<p>After using <code>plan</code> to understand the impacts of a change in an environment, Vulcan offers to execute the computations by <code>apply</code>ing the plan. However, you must provide additional information that determines the scope of what computations are executed.</p> <p>The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.</p> <p>\"Backfilling\" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.</p> <p>Most business data is temporal \u2014 each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.</p> <p>The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.</p>"},{"location":"concepts-old/overview/#build-a-virtual-environment","title":"Build a Virtual Environment","text":"<p>Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.</p> <p>One challenge with using multiple data environments is that backfill and other computations must happen twice \u2014 once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.</p> <p>Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.</p> <p>Vulcan uses its knowledge of equivalent outputs to create a Virtual Environment. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but without computation or data movement.</p> <p>Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.</p>"},{"location":"concepts-old/overview/#test-your-code-and-data","title":"Test your code and data","text":"<p>Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.</p>"},{"location":"concepts-old/overview/#tests","title":"Tests","text":"<p>Vulcan \"tests\" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model code \u2014 you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.</p> <p>Vulcan automatically runs tests when you apply a <code>plan</code>, or you can run them on demand with the <code>test</code> command.</p>"},{"location":"concepts-old/overview/#audits","title":"Audits","text":"<p>In contrast to tests, Vulcan \"audits\" validate the results of model code applied to your actual data.</p> <p>You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure <code>your_field</code> has no <code>NULL</code> values would include <code>WHERE your_field IS NULL</code>. If any NULLs are detected, the query will return at least one row and the audit will fail.</p> <p>Audits are flexible \u2014 they can be tied to a specific model's contents, or you can use macros to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.</p> <p>You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.</p> <p>Vulcan automatically runs audits when you apply a <code>plan</code> to an environment, or you can run them on demand with the <code>audit</code> command.</p>"},{"location":"concepts-old/overview/#infrastructure-and-orchestration","title":"Infrastructure and orchestration","text":"<p>Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use \u2014 its only requirement is access to the target SQL/analytics engine.</p> <p>Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a <code>vulcan</code> schema in your data warehouse for its internal metadata.</p>"},{"location":"concepts-old/plans/","title":"Plans","text":""},{"location":"concepts-old/plans/#plans","title":"Plans","text":"<p>A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target environment. In order for any model changes to take effect in a target environment, a plan needs to be created and applied.</p>"},{"location":"concepts-old/plans/#plan-architecture-overview","title":"Plan Architecture Overview","text":"<p>The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:</p> <pre><code>flowchart TD\n    subgraph \"1\ufe0f\u20e3 Local Development\"\n        A[\ud83d\udc68\u200d\ud83d\udcbb Developer modifies model files&lt;br/&gt;\ud83d\udcdd Edit SQL/Python models]\n        B[\ud83d\udcc1 Local project state&lt;br/&gt;\u2728 Your changes ready]\n    end\n\n    subgraph \"2\ufe0f\u20e3 Plan Creation\"\n        C[\u26a1 vulcan plan&lt;br/&gt;\ud83d\ude80 Command execution]\n        D[\ud83d\udd0e Compare local vs environment&lt;br/&gt;\ud83d\udcca State comparison]\n        E{\ud83d\udd0d Changes detected?}\n        F[\ud83d\udccb Generate plan summary&lt;br/&gt;\u2728 Plan ready for review]\n        G[\ud83c\udff7\ufe0f Change categorization&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking / \ud83d\udfe1 Forward-only]\n    end\n\n    subgraph \"3\ufe0f\u20e3 Plan Review\"\n        H[\ud83d\udc40 Review plan output&lt;br/&gt;\ud83d\udcca Check changes &amp; impacts]\n        I{\u2705 Apply plan?}\n        J[\u274c Cancel&lt;br/&gt;\ud83d\udeab No changes applied]\n    end\n\n    subgraph \"4\ufe0f\u20e3 Plan Application\"\n        K[\ud83d\udd37 Create model variants&lt;br/&gt;\ud83d\udd11 With unique fingerprints]\n        L[\ud83d\uddc4\ufe0f Create physical tables&lt;br/&gt;\ud83d\udcbe In data warehouse]\n        M[\ud83d\udd04 Backfill data&lt;br/&gt;\ud83d\udcc8 Process historical data]\n        N[\ud83d\udc41\ufe0f Update virtual layer&lt;br/&gt;\ud83d\udd0d Create/update views]\n        O[\ud83c\udf0d Update environment references&lt;br/&gt;\ud83d\udd17 Point to new variants]\n    end\n\n    subgraph \"5\ufe0f\u20e3 Result\"\n        P[\u2705 Environment updated&lt;br/&gt;\ud83c\udf89 Changes deployed]\n        Q[\ud83d\udd0d Models accessible via views&lt;br/&gt;\ud83d\udcca Ready for queries]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\u27a1\ufe0f\"| C\n    C --&gt;|\"\ud83d\udd0d\"| D\n    D --&gt;|\"\ud83d\udd0e\"| E\n    E --&gt;|\"\u2705 Yes\"| F\n    E --&gt;|\"\u274c No\"| P\n    F --&gt;|\"\ud83c\udff7\ufe0f\"| G\n    G --&gt;|\"\ud83d\udccb\"| H\n    H --&gt;|\"\ud83d\udc40\"| I\n    I --&gt;|\"\u2705 Yes\"| K\n    I --&gt;|\"\u274c No\"| J\n    K --&gt;|\"\ud83d\udd37\"| L\n    L --&gt;|\"\ud83d\udcbe\"| M\n    M --&gt;|\"\ud83d\udd04\"| N\n    N --&gt;|\"\ud83d\udc41\ufe0f\"| O\n    O --&gt;|\"\ud83d\udd17\"| P\n    P --&gt;|\"\u2728\"| Q\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre>"},{"location":"concepts-old/plans/#plan-components","title":"Plan Components","text":"<pre><code>graph LR\n    subgraph \"\ud83d\udccb Plan Contents\"\n        PC1[\u2795 Added Models&lt;br/&gt;\u2728 New models to create]\n        PC2[\u2796 Removed Models&lt;br/&gt;\ud83d\uddd1\ufe0f Models to delete]\n        PC3[\u270f\ufe0f Modified Models&lt;br/&gt;\ud83d\udcdd With diffs]\n        PC4[\ud83d\udd17 Indirectly Affected&lt;br/&gt;\ud83d\udcca Downstream models]\n        PC5[\ud83d\udcc5 Backfill Requirements&lt;br/&gt;\ud83d\udcc6 Date ranges]\n    end\n\n    subgraph \"\ud83c\udff7\ufe0f Change Types\"\n        CT1[\ud83d\udd34 Breaking Change&lt;br/&gt;\u26a0\ufe0f Requires downstream backfill&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        CT2[\ud83d\udfe2 Non-Breaking Change&lt;br/&gt;\u2705 Only direct model backfill&lt;br/&gt;\ud83c\udfaf Isolated impact]\n        CT3[\ud83d\udfe1 Forward-Only&lt;br/&gt;\u267b\ufe0f Reuses existing tables&lt;br/&gt;\ud83d\udcb0 Cost-effective]\n    end\n\n    PC3 --&gt;|\"\ud83d\udd34\"| CT1\n    PC3 --&gt;|\"\ud83d\udfe2\"| CT2\n    PC3 --&gt;|\"\ud83d\udfe1\"| CT3\n    PC4 --&gt;|\"\ud83d\udd34\"| CT1\n\n    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000</code></pre> <p>During plan creation:</p> <ul> <li>The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.</li> <li>Users may be prompted to categorize changes to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through configuration.</li> <li>Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.</li> </ul> <p>The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:</p> <ul> <li>A list of added models</li> <li>A list of removed models</li> <li>A list of directly modified models and a text diff of changes that have been made</li> <li>A list of indirectly modified models</li> <li>Missing data intervals for affected models</li> <li>A date range that will be affected by the plan application</li> </ul> <p>To create a new plan, run the following command: </p><pre><code>vulcan plan [environment name]\n</code></pre><p></p> <p>If no environment name is specified, the plan is generated for the <code>prod</code> environment.</p>"},{"location":"concepts-old/plans/#change-categories","title":"Change categories","text":"<p>Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.</p> <p>If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.</p>"},{"location":"concepts-old/plans/#change-propagation-flow","title":"Change Propagation Flow","text":"<p>The following diagram illustrates how changes propagate through the dependency graph:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Dependencies\"\n        A[\ud83d\udce5 raw.raw_orders&lt;br/&gt;\u2b06\ufe0f Upstream]\n        B[\ud83d\udcca sales.daily_sales&lt;br/&gt;\ud83d\udd04 Midstream]\n        C[\ud83d\udcc8 sales.weekly_sales&lt;br/&gt;\u2b07\ufe0f Downstream]\n        D[\ud83d\udcc9 analytics.revenue_report&lt;br/&gt;\u2b07\ufe0f Downstream]\n    end\n\n    subgraph \"\ud83d\udfe2 Scenario 1: Non-Breaking Change\"\n        NB1[\u2795 Add column to daily_sales&lt;br/&gt;\u2728 New column added]\n        NB2[\u2705 Only daily_sales backfilled&lt;br/&gt;\ud83d\udd04 Single model update]\n        NB3[\u23ed\ufe0f weekly_sales NOT affected&lt;br/&gt;\u2705 No cascade]\n        NB4[\u23ed\ufe0f revenue_report NOT affected&lt;br/&gt;\u2705 No cascade]\n    end\n\n    subgraph \"\ud83d\udd34 Scenario 2: Breaking Change\"\n        BC1[\ud83d\udd0d Add WHERE clause to daily_sales&lt;br/&gt;\u26a0\ufe0f Filter logic changed]\n        BC2[\ud83d\udd04 daily_sales backfilled&lt;br/&gt;\ud83d\udcca Data reprocessed]\n        BC3[\ud83d\udd04 weekly_sales backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        BC4[\ud83d\udd04 revenue_report backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\ud83d\udce4\"| C\n    B --&gt;|\"\ud83d\udce4\"| D\n\n    NB1 --&gt;|\"\u270f\ufe0f\"| B\n    B --&gt;|\"\u2705\"| NB2\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| C\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| D\n\n    BC1 --&gt;|\"\u26a0\ufe0f\"| B\n    B --&gt;|\"\ud83d\udd04\"| BC2\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC3\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC4\n    BC3 --&gt;|\"\ud83d\udd04\"| C\n    BC4 --&gt;|\"\ud83d\udd04\"| D\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000</code></pre>"},{"location":"concepts-old/plans/#breaking-change","title":"Breaking change","text":"<p>If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.</p> <p>In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to backfilling).</p> <p>Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's <code>WHERE</code> clause is a breaking change because downstream models contain rows that would now be filtered out.</p>"},{"location":"concepts-old/plans/#non-breaking-change","title":"Non-breaking change","text":"<p>A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.</p> <p>This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.</p> <p>If any downstream models contain a <code>select *</code> from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.</p>"},{"location":"concepts-old/plans/#summary","title":"Summary","text":"Change Category Change Type Behaviour Breaking Direct or Indirect Backfill Non-breaking Direct Backfill Non-breaking Indirect No Backfill"},{"location":"concepts-old/plans/#forward-only-change","title":"Forward-only change","text":"<p>In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.</p> <p>A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the <code>prod</code> environment). This means that no backfill will take place.</p> <p>While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.</p> <p>In either case the data produced this way in the development environment can only be used for preview and will not be reused once the change is deployed to production. See Forward-only Plans for more details.</p> <p>This category is assigned by Vulcan automatically either when a user opts into using a forward-only plan or when a model is explicitly configured to be forward-only.</p>"},{"location":"concepts-old/plans/#plan-application","title":"Plan application","text":"<p>Once a plan has been created and reviewed, it is then applied to the target environment in order for its changes to take effect.</p> <p>Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a snapshot with a unique fingerprint is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for forward-only plans.</p> <p>When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.</p>"},{"location":"concepts-old/plans/#model-versioning-architecture","title":"Model Versioning Architecture","text":"<p>The following diagram shows how model variants, physical tables, and environments relate:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcdd Model Definitions\"\n        M1[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 1&lt;br/&gt;\u2728 Original]\n        M2[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 2 - Modified&lt;br/&gt;\u270f\ufe0f Updated]\n    end\n\n    subgraph \"\ud83d\udd37 Model Variants &amp; Snapshots\"\n        V1[\ud83d\udd37 Variant 1&lt;br/&gt;\ud83d\udd11 Fingerprint: abc123&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        V2[\ud83d\udd37 Variant 2&lt;br/&gt;\ud83d\udd11 Fingerprint: def456&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        S1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd10 Immutable state]\n        S2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd10 Immutable state]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Tables\"\n        T1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__abc123&lt;br/&gt;\ud83d\udcbe Actual data storage]\n        T2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__def456&lt;br/&gt;\ud83d\udcbe Actual data storage]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer Views\"\n        VL1[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 1&lt;br/&gt;\ud83d\udd17 Reference mapping]\n        VL2[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 2&lt;br/&gt;\ud83d\udd17 Reference mapping]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;\u2705 References Variant 1&lt;br/&gt;\ud83c\udf10 Live production data]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;\ud83d\udd2c References Variant 2&lt;br/&gt;\ud83e\uddea Testing environment]\n    end\n\n    M1 --&gt;|\"\u2728\"| V1\n    M2 --&gt;|\"\u270f\ufe0f\"| V2\n    V1 --&gt;|\"\ud83d\udcf8\"| S1\n    V2 --&gt;|\"\ud83d\udcf8\"| S2\n    S1 --&gt;|\"\ud83d\udcbe\"| T1\n    S2 --&gt;|\"\ud83d\udcbe\"| T2\n    T1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    T2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n    PROD --&gt;|\"\ud83d\udd17\"| V1\n    DEV --&gt;|\"\ud83d\udd17\"| V2\n\n    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000</code></pre> <p></p> <p>Each model variant gets its own physical table while environments only contain references to these tables.</p> <p>This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.</p> <p>Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to Virtual Update) as long as its physical table hasn't been garbage collected by the janitor process.</p> <p>Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.</p>"},{"location":"concepts-old/plans/#backfilling","title":"Backfilling","text":"<p>Despite all the benefits, the approach described above is not without trade-offs.</p> <p>When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.</p> <p>We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:</p> <ul> <li>When a VIEW model is created</li> <li>When a FULL model is built</li> <li>When an INCREMENTAL model is built for the first time</li> <li>When an INCREMENTAL model has recent data appended to it</li> <li>When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)</li> </ul> <p>Note for incremental models: despite the fact that backfilling can happen incrementally (see <code>batch_size</code> parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a forward-only plan instead.</p>"},{"location":"concepts-old/plans/#virtual-update","title":"Virtual Update","text":"<p>A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.</p> <p>With this approach, the process of promoting a change to production is reduced to reference swapping.</p> <p>If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.</p>"},{"location":"concepts-old/plans/#start-and-end-dates","title":"Start and end dates","text":"<p>The <code>plan</code> command provides two temporal options: <code>--start</code> and <code>--end</code>. These options are only applicable to plans for non-prod environments.</p> <p>For context, every model has a start date. The start can be specified in the model definition, in the project configuration's <code>model_defaults</code>, or by Vulcan's default value of yesterday.</p> <p>Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the <code>plan</code> command's <code>--start</code> and <code>--end</code> options are not supported for regular plans against prod. The options are supported for restatement plans against prod to allow re-processing a subset of existing data.</p> <p>Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the <code>--start</code> and <code>--end</code> options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.</p>"},{"location":"concepts-old/plans/#model-kind-limitations","title":"Model kind limitations","text":"<p>Some model kinds do not support backfilling a limited date range.</p> <p>For context, Vulcan strives to make models idempotent, meaning that if we ran them multiple times we would get the same correct result every time.</p> <p>However, some model kinds are inherently non-idempotent:</p> <ul> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>SCD_TYPE_2_BY_TIME</li> <li>SCD_TYPE_2_BY_COLUMN</li> <li>Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)</li> </ul> <p>Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:</p> <ul> <li>If the <code>--start</code> option date is the same as or before the model's start date, the model is fully refreshed for all of time</li> <li>If the <code>--start</code> option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production</li> </ul>"},{"location":"concepts-old/plans/#example","title":"Example","text":"<p>Consider a Vulcan project with a default start date of 2024-09-20.</p> <p>It contains the following <code>INCREMENTAL_BY_UNIQUE_KEY</code> model that specifies an explicit start date of 2024-09-23:</p> <pre><code>MODEL (\n  name vulcan_example.start_end_model,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key item_id\n  ),\n  start '2024-09-23'\n);\n\nSELECT\n  item_id,\n  num_orders\nFROM\n  vulcan_example.full_model\n</code></pre> <p>When we run the project's first plan, we see that Vulcan correctly detected a different start date for our <code>start_end_model</code> than the other models (which have the project default start of 2024-09-20):</p> <pre><code>\u276f vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u251c\u2500\u2500 vulcan_example.seed_model\n    \u2514\u2500\u2500 vulcan_example.start_end_model\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example.full_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.incremental_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.seed_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example.start_end_model: 2024-09-23 - 2024-09-26\nApply - Backfill Tables [y/n]:\n</code></pre> <p>After executing that plan, we add columns to both the <code>incremental_model</code> and <code>start_end_model</code> queries.</p> <p>We then execute <code>vulcan plan dev</code> to create the new <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre> <p>Note two things about the output:</p> <ol> <li>As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for <code>incremental_model</code> and 2024-09-23 for <code>start_end_model</code></li> <li>Vulcan prompted us for a backfill start date because we didn't pass the <code>--start</code> option to the <code>vulcan plan dev</code> command</li> </ol> <p>Let's cancel that plan and start a new one, passing a start date of 2024-09-24.</p> <p>The <code>start_end_model</code> is of kind <code>INCREMENTAL_BY_UNIQUE_KEY</code>, which is non-idempotent and cannot be backfilled for a limited time range.</p> <p>Because the command's <code>--start</code> of 2024-09-24 is after <code>start_end_model</code>'s start date 2024-09-23, <code>start_end_model</code> is marked as preview:</p> <pre><code>\u276f vulcan plan dev --start 2024-09-24\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':\n</code></pre>"},{"location":"concepts-old/plans/#minimum-intervals","title":"Minimum intervals","text":"<p>When you run a plan with a fixed <code>--start</code> or <code>--end</code> date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.</p> <p>For example, if you have a model like so:</p> <pre><code>MODEL(\n    name vulcan_example.monthly_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column month\n    ),\n    cron '@monthly'\n);\n\nSELECT SUM(a) AS sum_a, MONTH(day) AS month\nFROM vulcan_example.upstream_model\nWHERE day BETWEEN @start_ds AND @end_ds\n</code></pre> <p>make a change to it and run the following:</p> <pre><code>$ vulcan plan dev --start '1 day ago' \n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\nSKIP: No model batches to execute\n</code></pre> <p>No data will be backfilled because <code>1 day ago</code> does not contain a complete month. However, you can use the <code>--min-intervals</code> option to override this behaviour like so:</p> <pre><code>$ vulcan plan dev --start '1 day ago' --min-intervals 1\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\n[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   \nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00                                                             \n\n\u2714 Model batches executed\n</code></pre> <p>This will ensure that regardless of the plan <code>--start</code> date, all added or modified models will have at least <code>--min-intervals</code> intervals considered for backfill.</p> <p>Info</p> <p>If you are running plans manually you can just adjust the <code>--start</code> date to be wide enough to cover the models in question.</p> <p>The <code>--min-intervals</code> option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) \"2 weeks worth of data\" in the target environment.</p>"},{"location":"concepts-old/plans/#data-preview-for-forward-only-changes","title":"Data preview for forward-only changes","text":"<p>As mentioned earlier, the data output produced by forward-only changes in a development environment can only be used for preview and will not be reused in production.</p> <p>The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.</p> <p>Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with <code>(preview)</code> in the plan summary: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28\n\u251c\u2500\u2500 sushi__dev.top_waiters: 2023-12-22 - 2023-12-28\n\u2514\u2500\u2500 sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)\n</code></pre><p></p>"},{"location":"concepts-old/plans/#forward-only-plans","title":"Forward-only plans","text":"<p>Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.</p> <p>The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and restatements.</p> <p>Note that once a forward-only change is applied to <code>prod</code>, all development environments that referred to the previous versions of the updated models will be impacted.</p> <p>A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, <code>vulcan plan [environment name]</code> evaluates code in non-<code>prod</code> environments while targeting shallow (a.k.a. \"zero-copy\") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.</p> <p>This means that only a limited preview of changes is available in the development environment before the change is promoted to <code>prod</code>. The date range of the preview is provided as part of plan creation command.</p> <p>Engines for which table cloning is supported include:</p> <ul> <li><code>BigQuery</code></li> <li><code>Databricks</code></li> <li><code>Snowflake</code></li> </ul> <p>Note that all changes made as part of a forward-only plan automatically get a forward-only category assigned to them. These types of changes can't be mixed together with breaking and non-breaking changes within the same plan.</p> <p>To create a forward-only plan, add the <code>--forward-only</code> option to the <code>plan</code> command: </p><pre><code>vulcan plan [environment name] --forward-only\n</code></pre><p></p> <p>Note</p> <p>The <code>--forward-only</code> flag is not required when applying changes to models that have been explicitly configured as forward-only.</p> <p>Use it only if you need to provide a time range for the preview window or the effective date.</p>"},{"location":"concepts-old/plans/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to forward-only models - learn more here.</p> <p>Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy: </p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models.  Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. </p> <p>For example, to allow destructive changes to all models in the <code>analytics</code> schema: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"analytics.*\"\n</code></pre><p></p> <p>Or to allow destructive changes to multiple specific models: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"sales.revenue_model\" --allow-destructive-model \"marketing.campaign_model\"\n</code></pre><p></p> <p>Learn more about model selectors here.</p>"},{"location":"concepts-old/plans/#effective-date","title":"Effective date","text":"<p>Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:</p> <pre><code>vulcan plan --forward-only --effective-from 2023-01-01\n</code></pre> <p>This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.</p>"},{"location":"concepts-old/plans/#restatement-plans","title":"Restatement plans","text":"<p>Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.</p> <p>For example, these scenarios all require re-evaluating model data that already exists:</p> <ul> <li>Correcting an upstream data issue by reprocessing some of a model's existing data</li> <li>Retroactively applying a forward-only plan change to some historical data</li> <li>Fully refreshing a model</li> </ul> <p>In Vulcan, reprocessing existing data is called a \"restatement.\"</p> <p>Restate one or more models' data with the <code>plan</code> command's <code>--restate-model</code> selector. The selector lets you specify which models to restate by name, wildcard, or tag (syntax below).</p> <p>No changes allowed</p> <p>Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.</p> <p>You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running <code>vulcan plan</code> without the <code>--restate-model</code> option.</p> <p>Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.</p> <p>You may restate external models. An external model is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.</p> <p>The plan's <code>--start</code> and <code>--end</code> date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more below.</p> <p>Just catching up</p> <p>Restatement plans \"catch models up\" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.</p> <p>If you pass an <code>--end</code> date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.</p> <p>To prevent models from ever being restated, set the disable_restatement attribute to <code>true</code>.</p> <p> These examples demonstrate how to select which models to restate based on model names or model tags.</p> Names OnlyUpstreamWildcardsUpstream + WildcardsSpecific Date Range <pre><code>vulcan plan --restate-model \"db.model_a\" --restate-model \"tag:expensive\"\n</code></pre> <pre><code># All selected models (including upstream models) will also include their downstream models\nvulcan plan --restate-model \"+db.model_a\" --restate-model \"+tag:expensive\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db*\" --restate-model \"tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"+db*\" --restate-model \"+tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre>"},{"location":"concepts-old/plans/#restating-production-vs-development","title":"Restating production vs development","text":"<p>Restatement plans behave differently depending on if you're targeting the <code>prod</code> environment or a development environment.</p> <p>If you target a development environment by including an environment name like <code>dev</code>:</p> <pre><code>vulcan plan dev --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the requested intervals for the specified model in the <code>dev</code> environment. In other environments, the model will be unaffected.</p> <p>However, if you target the <code>prod</code> environment by omitting an environment name:</p> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the intervals in the <code>prod</code> table and clear the model's time intervals from state in every other environment.</p> <p>The next time you do a run in <code>dev</code>, the intervals already reprocessed in <code>prod</code> are reprocessed in <code>dev</code> as well. This is to prevent old data from getting promoted to <code>prod</code> in the future.</p> <p>This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:</p> <ul> <li>Table <code>A</code> exists in <code>prod</code></li> <li>A virtual environment <code>dev</code> is created with new tables <code>B</code> and <code>C</code> downstream of <code>A</code><ul> <li>the DAG in <code>prod</code> looks like <code>A</code></li> <li>the DAG in <code>dev</code> looks like <code>A &lt;- B &lt;- C</code></li> </ul> </li> <li>A restatement plan is executed against table <code>A</code> in <code>prod</code></li> <li>Vulcan will clear the affected intervals for <code>B</code> and <code>C</code> in <code>dev</code> even though those tables do not exist in <code>prod</code></li> </ul> <p>Bringing development environments up to date</p> <p>A restatement plan against <code>prod</code> clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.</p> <p>Execute <code>vulcan run &lt;environment name&gt;</code> to trigger reprocessing in the development environment.</p> <p>This is necessary because a <code>prod</code> restatement plan only does work in the <code>prod</code> environment for speed and efficiency.</p>"},{"location":"concepts-old/state/","title":"State","text":""},{"location":"concepts-old/state/#state","title":"State","text":"<p>Vulcan stores information about your project in a state database that is usually separate from your main warehouse.</p> <p>The Vulcan state database contains:</p> <ul> <li>Information about every Model Version in your project (query, loaded intervals, dependencies)</li> <li>A list of every Virtual Data Environment in the project</li> <li>Which model versions are promoted into each Virtual Data Environment</li> <li>Information about any auto restatements present in your project</li> <li>Other metadata about your project such as current Vulcan / SQLGlot version</li> </ul> <p>The state database is how Vulcan \"remembers\" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for incremental models so you dont need to add branching logic into the model query to handle this.</p> <p>State database performance</p> <p>The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.</p> <p>For the best experience, we recommend databases designed for OLTP workloads such as PostgreSQL.</p> <p>Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and will lead to poor performance and consistency.</p> <p>For more information on engines suitable for the Vulcan state database, see the configuration guide.</p>"},{"location":"concepts-old/state/#exporting-importing-state","title":"Exporting / Importing State","text":"<p>Vulcan supports exporting the state database to a <code>.json</code> file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.</p>"},{"location":"concepts-old/state/#exporting-state","title":"Exporting state","text":"<p>Vulcan can export the state database to a file like so:</p> <pre><code>$ vulcan state export -o state.json\nExporting state to 'state.json' from the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\nContinue? [y/n]: y\n\n    Exporting versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Exporting snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nExporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState exported successfully to 'state.json'\n</code></pre> <p>This will produce a file <code>state.json</code> in the current directory containing the Vulcan state.</p> <p>The state file is a simple <code>json</code> file that looks like:</p> <pre><code>{\n    /* State export metadata */\n    \"metadata\": {\n        \"timestamp\": \"2025-03-16 23:09:00+00:00\", /* UTC timestamp of when the file was produced */\n        \"file_version\": 1, /* state export file format version */\n        \"importable\": true /* whether or not this file can be imported with `vulcan state import` */\n    },\n    /* Library versions used to produce this state export file */\n    \"versions\": {\n        \"schema_version\": 76 /* vulcan state database schema version */,\n        \"sqlglot_version\": \"26.10.1\" /* version of SQLGlot used to produce the state file */,\n        \"vulcan_version\": \"0.165.1\" /* version of Vulcan used to produce the state file */,\n    },\n    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */\n    \"snapshots\": [\n        { \"name\": \"...\" }\n    ],\n    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */\n    \"environments\": {\n        \"prod\": {\n            /* information about the environment itself */\n            \"environment\": {\n                \"...\"\n            },\n            /* information about any before_all / after_all statements for this environment */\n            \"statements\": [\n                \"...\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"concepts-old/state/#specific-environments","title":"Specific environments","text":"<p>You can export a specific environment like so:</p> <pre><code>$ vulcan state export --environment my_dev -o my_dev_state.json\n</code></pre> <p>Note that every snapshot that is part of the environment will be exported, not just the differences from <code>prod</code>. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.</p>"},{"location":"concepts-old/state/#local-state","title":"Local state","text":"<p>You can export local state like so:</p> <pre><code>$ vulcan state export --local -o local_state.json\n</code></pre> <p>This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.</p> <p>Therefore, a local state export will only have <code>snapshots</code> populated. <code>environments</code> will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as not importable so it cannot be used with a subsequent <code>vulcan state import</code> command.</p>"},{"location":"concepts-old/state/#importing-state","title":"Importing state","text":"<p>Back up your state database first!</p> <p>Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.</p> <p>Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means a import error has the potential to leave the state database in an inconsistent state.</p> <p>Vulcan can import a state file into the state database like so:</p> <pre><code>$ vulcan state import -i state.json --replace\nLoading state from 'state.json' into the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\n[WARNING] This destructive operation will delete all existing state against the 'dev' gateway\nand replace it with what\\'s in the 'state.json' file.\n\nAre you sure? [y/n]: y\n\nState File Information:\n\u251c\u2500\u2500 Creation Timestamp: 2025-03-31 02:15:00+00:00\n\u251c\u2500\u2500 File Version: 1\n\u251c\u2500\u2500 Vulcan version: 0.170.1.dev0\n\u251c\u2500\u2500 Vulcan migration version: 76\n\u2514\u2500\u2500 SQLGlot version: 26.12.0\n\n    Importing versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Importing snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nImporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState imported successfully from 'state.json'\n</code></pre> <p>Note that the state database structure needs to be present and up to date, so run <code>vulcan migrate</code> before running <code>vulcan state import</code> if you get a version mismatch error.</p> <p>If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the <code>--replace</code> parameter:</p> <pre><code>$ vulcan state import -i state.json\n...\n\n[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.\nMatching snapshots or environments will be replaced.\nNon-matching snapshots or environments will be ignored.\n\nAre you sure? [y/n]: y\n\n...\nState imported successfully from 'state.json'\n</code></pre>"},{"location":"concepts-old/state/#specific-gateways","title":"Specific gateways","text":"<p>If your project has multiple gateways with different state connections per gateway, you can target the state_connection of a specific gateway like so:</p> <pre><code># state export\n$ vulcan --gateway &lt;gateway&gt; state export -o state.json\n\n# state import\n$ vulcan --gateway &lt;gateway&gt; state import -i state.json\n</code></pre>"},{"location":"concepts-old/state/#version-compatibility","title":"Version Compatibility","text":"<p>When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.</p> <p>If you attempt to import state with an incompatible version, you will get the following error:</p> <pre><code>$ vulcan state import -i state.json\n...SNIP...\n\nState import failed!\nError: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.\nPlease upgrade/downgrade your Vulcan version to match the state file before performing the import.\n</code></pre>"},{"location":"concepts-old/state/#upgrading-a-state-file","title":"Upgrading a state file","text":"<p>You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:</p> <ul> <li>Loading it into a local database using the older Vulcan version</li> <li>Installing the newer Vulcan version</li> <li>Running <code>vulcan migrate</code> to upgrade the state within the local database</li> <li>Running <code>vulcan state export</code> to export it back out again. The new export is now compatible with the newer version of Vulcan.</li> </ul> <p>Below is an example of how to upgrade a state file created with Vulcan <code>0.164.1</code> to be compatible with Vulcan <code>0.165.1</code>.</p> <p>First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:</p> <pre><code>$ python -m venv migration-env\n\n$ . ./migration-env/bin/activate\n\n(migration-env)$\n</code></pre> <p>Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg <code>the state file was created with '0.164.1'</code> means you need to install Vulcan <code>0.164.1</code>:</p> <pre><code>(migration-env)$ pip install \"vulcan==0.164.1\"\n</code></pre> <p>Add a gateway to your <code>config.yaml</code> like so:</p> <pre><code>gateways:\n  migration:\n    connection:\n      type: duckdb\n      database: ./state-migration.duckdb\n</code></pre> <p>The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the <code>model_defaults</code> from your project in order to migrate state correctly which is why we have not used an isolated directory.</p> <p>Warning</p> <p>From here on, be sure to specify <code>--gateway migration</code> to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway</p> <p>You can now import your state export using the same version of Vulcan it was created with:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n\n(migration-env)$ vulcan --gateway migration state import -i state.json\n...\nState imported successfully from 'state.json'\n</code></pre> <p>Now we have the state imported, we can upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, eg <code>You are running '0.165.1'</code></p> <p>To upgrade Vulcan, simply install the new version:</p> <pre><code>(migration-env)$ pip install --upgrade \"vulcan==0.165.1\"\n</code></pre> <p>Migrate the state to the new version:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n</code></pre> <p>And finally, create a new state file which is now compatible with the new Vulcan version:</p> <pre><code> (migration-env)$ vulcan --gateway migration state export -o state-migrated.json\n</code></pre> <p>The <code>state-migrated.json</code> file is now compatible with the newer version of Vulcan. You can then transfer it to the place you originally needed it and import it in:</p> <pre><code>$ vulcan state import -i state-migrated.json\n...\nState imported successfully from 'state-migrated.json'\n</code></pre>"},{"location":"concepts-old/architecture/serialization/","title":"Serialization","text":""},{"location":"concepts-old/architecture/serialization/#serialization","title":"Serialization","text":"<p>Vulcan executes Python code through macros and Python models. Each Python model is stored as a standalone snapshot, which includes all of the Python code necessary to generate it.</p>"},{"location":"concepts-old/architecture/serialization/#serialization-format","title":"Serialization format","text":"<p>Rather than using Python's <code>pickle</code> format, Vulcan has its own serialization format. This is because <code>pickle</code> is not compatible across Python versions, and would, for example, prevent you from developing on Python 3.9 and then running Python 3.10 in production.</p> <p>Instead, Vulcan stores the string representation of your Python implementation and then re-evaluates it. Given a custom Python function or macro, Vulcan reads the Abstract Syntax Tree (AST) of the function and converts that into a string representation, along with all dependencies and global variables. For more information, refer to snapshot fingerprinting.</p>"},{"location":"concepts-old/architecture/serialization/#limitations","title":"Limitations","text":"<p>Vulcan only serializes the Python code you write and does not include libraries, which means the module of your code must match your Vulcan config path. In addition, any references to libraries will be converted to imports, so you must ensure that any libraries you are using are installed everywhere that Vulcan is running.</p>"},{"location":"concepts-old/architecture/snapshots/","title":"Snapshots","text":""},{"location":"concepts-old/architecture/snapshots/#snapshots","title":"Snapshots","text":"<p>A snapshot is a record of a model at a given time. Along with a copy of the model, a snapshot contains everything needed to evaluate the model and render its query. This allows Vulcan to have a consistent view of your project's history and its data as the project and its models evolve and change. Since model queries can have macros, each snapshot stores a copy of all macro definitions and global variables at the time the snapshot is taken. Additionally, snapshots store the intervals of time for which they have data.</p>"},{"location":"concepts-old/architecture/snapshots/#fingerprinting","title":"Fingerprinting","text":"<p>Snapshots have unique fingerprints that are derived from their models. Vulcan uses these fingerprints to determine when existing tables can be reused, or whether a backfill is needed as a model's query has changed.</p> <p>Because Vulcan can understand SQL with SQLGlot, it can generate fingerprints such that superficial changes to a model, such as applying formatting to its query, will not return a new fingerprint.</p>"},{"location":"concepts-old/architecture/snapshots/#change-categories","title":"Change categories","text":"<p>Refer to change categories.</p>"},{"location":"concepts-old/macros/jinja_macros/","title":"Jinja","text":""},{"location":"concepts-old/macros/jinja_macros/#jinja","title":"Jinja","text":"<p>Vulcan supports macros from the Jinja templating system.</p> <p>Jinja's macro approach is pure string substitution. Unlike Vulcan macros, they assemble SQL query text without building a semantic representation.</p> <p>NOTE: Vulcan projects support the standard Jinja function library only - they do not support dbt-specific jinja functions like <code>{{ ref() }}</code>. dbt-specific functions are allowed in dbt projects being run with the Vulcan adapter.</p>"},{"location":"concepts-old/macros/jinja_macros/#basics","title":"Basics","text":"<p>Jinja uses curly braces <code>{}</code> to differentiate macro from non-macro text. It uses the second character after the left brace to determine what the text inside the braces will do.</p> <p>The three curly brace symbols are:</p> <ul> <li><code>{{...}}</code> creates Jinja expressions. Expressions are replaced by text that is incorporated into the rendered SQL query; they can contain macro variables and functions.</li> <li><code>{%...%}</code> creates Jinja statements. Statements give instructions to Jinja, such as setting variable values, control flow with <code>if</code>, <code>for</code> loops, and defining macro functions.</li> <li><code>{#...#}</code> creates Jinja comments. These comments will not be included in the rendered SQL query.</li> </ul> <p>Since Jinja strings are not syntactically valid SQL expressions and cannot be parsed as such, the model query must be wrapped in a special <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code> block in order for Vulcan to detect it:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>Similarly, to use Jinja expressions as part of statements that should be evaluated before or after the model query, the <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code> block should be used:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#vulcan-predefined-variables","title":"Vulcan predefined variables","text":"<p>Vulcan provides multiple predefined macro variables you may reference in jinja code.</p> <p>Some predefined variables provide information about the Vulcan project itself, like the <code>runtime_stage</code> and <code>this_model</code> variables.</p> <p>Other predefined variables are temporal, like <code>start_ds</code> and <code>execution_date</code>. They are used to build incremental model queries and are only available in incremental model kinds.</p> <p>Access predefined macro variables by passing their unquoted name in curly braces. For example, this demonstrates how to access the <code>start_ds</code> and <code>end_ds</code> variables:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Because the two macro variables return string values, we must surround the curly braces with single quotes <code>'</code>. Other macro variables, such as <code>start_epoch</code>, return numeric values and do not require the single quotes.</p> <p>The <code>gateway</code> variable uses a slightly different syntax than other predefined variables because it is a function call. Instead of the bare name <code>{{ gateway }}</code>, it must include parentheses: <code>{{ gateway() }}</code>.</p>"},{"location":"concepts-old/macros/jinja_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports two kinds of user-defined macro variables: global and local.</p> <p>Global macro variables are defined in the project configuration file and can be accessed in any project model.</p> <p>Local macro variables are defined in a model definition and can only be accessed in that model.</p>"},{"location":"concepts-old/macros/jinja_macros/#global-variables","title":"Global variables","text":"<p>Learn more about defining global variables in the Vulcan macros documentation.</p> <p>Access global variable values in a model definition using the <code>{{ var() }}</code> jinja function. The function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, a model would access a global variable named <code>int_var</code> like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>A default value can be passed as a second argument to the <code>{{ var() }}</code> jinja function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> if no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key. Learn more about defining gateway variables in the Vulcan macros documentation.</p> <p>Access gateway variables in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the configuration file's root <code>variables</code> key.</p>"},{"location":"concepts-old/macros/jinja_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Blueprint variables can be accessed using the <code>{{ blueprint_var() }}</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>{{ var() }}</code>).</p>"},{"location":"concepts-old/macros/jinja_macros/#local-variables","title":"Local variables","text":"<p>Define your own variables with the Jinja statement <code>{% set ... %}</code>. For example, we could specify the name of the <code>num_orders</code> column in the <code>vulcan_example.full_model</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>Note that the Jinja set statement is written after the <code>MODEL</code> statement and before the SQL query.</p> <p>Jinja variables can be string, integer, or float data types. They can also be an iterable data structure, such as a list, tuple, or dictionary. Each of these data types and structures supports multiple Python methods, such as the <code>upper()</code> method for strings.</p>"},{"location":"concepts-old/macros/jinja_macros/#macro-operators","title":"Macro operators","text":""},{"location":"concepts-old/macros/jinja_macros/#control-flow-operators","title":"Control flow operators","text":""},{"location":"concepts-old/macros/jinja_macros/#for-loops","title":"for loops","text":"<p>For loops let you iterate over a collection of items to condense repetitive code and easily change the values used by the code.</p> <p>Jinja for loops begin with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>. This example demonstrates creating indicator variables with <code>CASE WHEN</code> using a Jinja for loop:</p> <pre><code>SELECT\n  {% for vehicle_type in ['car', 'truck', 'bus']}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>Note that the <code>vehicle_type</code> values are quoted in the list <code>['car', 'truck', 'bus']</code>. Jinja removes those quotes during processing, so the reference <code>'{{ vehicle_type }}</code> in the <code>CASE WHEN</code> statement must be in quotes. The reference <code>vehicle_{{ vehicle_type }}</code> does not require quotes.</p> <p>Also note that a comma is present at the end of the <code>CASE WHEN</code> line. Trailing commas are not valid SQL and would normally require special handling, but Vulcan's semantic understanding of the query allows it to identify and remove the offending comma.</p> <p>The example renders to this after Vulcan processing:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>In general, it is a best practice to define lists of values separately from their use. We could do that like this:</p> <pre><code>{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types }\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n</code></pre> <p>The rendered query would be the same as before.</p>"},{"location":"concepts-old/macros/jinja_macros/#if","title":"if","text":"<p>if statements allow you to take an action (or not) based on some condition.</p> <p>Jinja if statements begin with <code>{% if ... %}</code> and end with <code>{% endif %}</code>. The starting <code>if</code> statement must contain code that evaluates to <code>True</code> or <code>False</code>. For example, all of <code>True</code>, <code>1 + 1 == 2</code>, and <code>'a' in ['a', 'b']</code> evaluate to <code>True</code>.</p> <p>As an example, you might want a model to only include a column if the model was being run for testing purposes. We can do that by setting a variable indicating whether it's a testing run that determines whether the query includes <code>testing_column</code>:</p> <pre><code>{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n</code></pre> <p>Because <code>testing</code> is <code>True</code>, the rendered query would be:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/jinja_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Jinja macro functions should be placed in <code>.sql</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.sql</code> file, or they can be distributed across multiple files.</p> <p>Jinja macro functions are defined with the <code>{% macro %}</code> and <code>{% endmacro %}</code> statements. The macro function name and arguments are specified in the <code>{% macro %}</code> statement.</p> <p>For example, a macro function named <code>print_text</code> that takes no arguments could be defined with:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>This macro function would be called in a SQL model with <code>{{ print_text() }}</code>, which would be substituted with <code>text</code>\" in the rendered query.</p> <p>Macro function arguments are placed in the parentheses next to the macro name. For example, this macro generates a SQL column with an alias based on the arguments <code>expression</code> and <code>alias</code>:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>We might call this macro function in a SQL query like this:</p> <pre><code>SELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Note that both argument values are quoted in the call <code>alias('item_id', 'item_id2')</code> but are not quoted in the rendered query. During the rendering process, Vulcan uses its semantic understanding of the query to build the rendered text - it recognizes that the first argument is a column name and that column aliases are unquoted by default.</p> <p>In that example, the SQL query selects the column <code>item_id</code> with the alias <code>item_id2</code>. If instead we wanted to select the string <code>'item_id'</code> with the name <code>item_id2</code>, we would pass the <code>expression</code> argument with double quotes around it: <code>\"'item_id'\"</code>:</p> <pre><code>SELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n</code></pre> <p>After processing, it would render to this:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes around <code>\"'item_id'\"</code> signal to Vulcan that it is not a column name.</p> <p>Some SQL dialects interpret double and single quotes differently. We could replace the rendered single quoted <code>'item_id'</code> with double quoted <code>\"item_id\"</code> in the previous example by switching the placement of quotes in the macro function call. Instead of <code>alias(\"'item_id'\", 'item_id2')</code> we would use <code>alias('\"item_id\"', 'item_id2')</code>.</p>"},{"location":"concepts-old/macros/jinja_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both the Jinja and Vulcan macro systems. We strongly recommend using only one system in a single model - if both are present, they may fail or behave in unintuitive ways.</p> <p>Predefined Vulcan macro variables can be used in a query containing user-defined Jinja variables and functions. However, predefined variables passed as arguments to a user-defined Jinja macro function must use the Jinja curly brace syntax <code>{{ start_ds }}</code> instead of the Vulcan macro <code>@</code> prefix syntax <code>@start_ds</code>. Note that curly brace syntax may require quoting to generate the equivalent of the <code>@</code> syntax.</p>"},{"location":"concepts-old/macros/macro_variables/","title":"Variables","text":""},{"location":"concepts-old/macros/macro_variables/#variables","title":"Variables","text":"<p>Macro variables are placeholders whose values are substituted in when the macro is rendered.</p> <p>They enable dynamic macro behavior - for example, a date parameter's value might be based on when the macro was run.</p> <p>Note</p> <p>This page discusses Vulcan's built-in macro variables. Learn more about custom, user-defined macro variables on the Vulcan macros page.</p>"},{"location":"concepts-old/macros/macro_variables/#example","title":"Example","text":"<p>Consider a SQL query that filters by date in the <code>WHERE</code> clause.</p> <p>Instead of manually changing the date each time the model is run, you can use a macro variable to make the date dynamic. With the dynamic approach, the date changes automatically based on when the query is run.</p> <p>This query filters for rows where column <code>my_date</code> is after '2023-01-01':</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>To make this query's date dynamic you could use the predefined Vulcan macro variable <code>@execution_ds</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan that <code>@execution_ds</code> is a macro variable that requires substitution before the SQL is executed.</p> <p>The macro variable <code>@execution_ds</code> is predefined, so its value will be automatically set by Vulcan based on when the execution started. If the model was executed on February 1, 2023 the rendered query would be:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>This example used one of Vulcan's predefined variables, but you can also define your own macro variables.</p> <p>We describe Vulcan's predefined variables below; user-defined macro variables are discussed in the Vulcan macros and Jinja macros pages.</p>"},{"location":"concepts-old/macros/macro_variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan comes with predefined variables that can be used in your queries. They are automatically set by the Vulcan runtime.</p> <p>Most predefined variables are related to time and use a combination of prefixes (start, end, etc.) and postfixes (date, ds, ts, etc.). They are described in the next section; other predefined variables are discussed in the following section.</p>"},{"location":"concepts-old/macros/macro_variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses the python datetime module for handling dates and times. It uses the standard Unix epoch start of 1970-01-01.</p> <p>Important</p> <p>Predefined variables with a time component always use the UTC time zone.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes:</p> <ul> <li>start - The inclusive starting interval of a model run</li> <li>end - The inclusive end interval of a model run</li> <li>execution - The timestamp of when the execution started</li> </ul> <p>Postfixes:</p> <ul> <li>dt - A python datetime object that converts into a native SQL <code>TIMESTAMP</code> (or SQL engine equivalent)</li> <li>dtntz - A python datetime object that converts into a native SQL <code>TIMESTAMP WITHOUT TIME ZONE</code> (or SQL engine equivalent)</li> <li>date - A python date object that converts into a native SQL <code>DATE</code></li> <li>ds - A date string with the format: '%Y-%m-%d'</li> <li>ts - An ISO 8601 datetime formatted string: '%Y-%m-%d %H:%M:%S'</li> <li>tstz - An ISO 8601 datetime formatted string with timezone: '%Y-%m-%d %H:%M:%S%z'</li> <li>hour - An integer representing the hour of the day, with values 0-23</li> <li>epoch - An integer representing seconds since Unix epoch</li> <li>millis - An integer representing milliseconds since Unix epoch</li> </ul> <p>All predefined temporal macro variables:</p> <ul> <li> <p>dt</p> <ul> <li>@start_dt</li> <li>@end_dt</li> <li>@execution_dt</li> </ul> </li> <li> <p>dtntz</p> <ul> <li>@start_dtntz</li> <li>@end_dtntz</li> <li>@execution_dtntz</li> </ul> </li> <li> <p>date</p> <ul> <li>@start_date</li> <li>@end_date</li> <li>@execution_date</li> </ul> </li> <li> <p>ds</p> <ul> <li>@start_ds</li> <li>@end_ds</li> <li>@execution_ds</li> </ul> </li> <li> <p>ts</p> <ul> <li>@start_ts</li> <li>@end_ts</li> <li>@execution_ts</li> </ul> </li> <li> <p>tstz</p> <ul> <li>@start_tstz</li> <li>@end_tstz</li> <li>@execution_tstz</li> </ul> </li> <li> <p>hour</p> <ul> <li>@start_hour</li> <li>@end_hour</li> <li>@execution_hour</li> </ul> </li> <li> <p>epoch</p> <ul> <li>@start_epoch</li> <li>@end_epoch</li> <li>@execution_epoch</li> </ul> </li> <li> <p>millis</p> <ul> <li>@start_millis</li> <li>@end_millis</li> <li>@execution_millis</li> </ul> </li> </ul>"},{"location":"concepts-old/macros/macro_variables/#runtime-variables","title":"Runtime variables","text":"<p>Vulcan provides additional predefined variables used to modify model behavior based on information available at runtime.</p> <ul> <li>@runtime_stage - A string value denoting the current stage of the Vulcan runtime. Typically used in models to conditionally execute pre/post-statements (learn more here). It returns one of these values:<ul> <li>'loading' - The project is being loaded into Vulcan's runtime context.</li> <li>'creating' - The model tables are being created for the first time. The data may be inserted during table creation.</li> <li>'evaluating' - The model query logic is evaluated, and the data is inserted into the existing model table.</li> <li>'promoting' - The model is being promoted in the target environment (view created during virtual layer update).</li> <li>'demoting' - The model is being demoted in the target environment (view dropped during virtual layer update).</li> <li>'auditing' - The audit is being run.</li> <li>'testing' - The model query logic is being evaluated in the context of a unit test.</li> </ul> </li> <li>@gateway - A string value containing the name of the current gateway.</li> <li>@this_model - The physical table name that the model's view selects from. Typically used to create generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</li> <li>@model_kind_name - A string value containing the name of the current model kind. Intended to be used in scenarios where you need to control the physical properties in model defaults.</li> </ul> <p>Embedding variables in strings</p> <p>Macro variable references sometimes use the curly brace syntax <code>@{variable}</code>, which serves a different purpose than the regular <code>@variable</code> syntax.</p> <p>The curly brace syntax tells Vulcan that the rendered string should be treated as an identifier, instead of simply replacing the macro variable value.</p> <p>For example, if <code>variable</code> is defined as <code>@DEF(</code>variable<code>, foo.bar)</code>, then <code>@variable</code> produces <code>foo.bar</code>, while <code>@{variable}</code> produces <code>\"foo.bar\"</code>. This is because Vulcan converts <code>foo.bar</code> into an identifier, using double quotes to correctly include the <code>.</code> character in the identifier name.</p> <p>In practice, <code>@{variable}</code> is most commonly used to interpolate a value within an identifier, e.g., <code>@{variable}_suffix</code>, whereas <code>@variable</code> is used to do plain substitutions for string literals.</p> <p>Learn more in the Vulcan macros documentation.</p>"},{"location":"concepts-old/macros/macro_variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>The following variables are also available in <code>before_all</code> and <code>after_all</code> statements, as well as in macros invoked within them.</p> <ul> <li>@this_env - A string value containing the name of the current environment.</li> <li>@schemas - A list of the schema names of the virtual layer of the current environment.</li> <li>@views - A list of the view names of the virtual layer of the current environment.</li> </ul>"},{"location":"concepts-old/macros/overview/","title":"Overview","text":""},{"location":"concepts-old/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language. It does not natively have features like variables or control flow logic (if-then, for loops) that allow SQL commands to behave differently in different situations.</p> <p>However, data pipelines are dynamic and need different behavior depending on context. SQL is made dynamic with macros.</p> <p>Vulcan supports two macro systems: Vulcan macros and the Jinja templating system.</p> <p>Learn more about macros in Vulcan:</p> <ul> <li>Pre-defined macro variables available in both macro systems</li> <li>Vulcan macros</li> <li>Jinja macros</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/","title":"Built In","text":""},{"location":"concepts-old/macros/vulcan_macros/#built-in","title":"Built In","text":""},{"location":"concepts-old/macros/vulcan_macros/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros behave differently than those of templating systems like Jinja.</p> <p>Macro systems are based on string substitution. The macro system scans code files, identifies special characters that signify macro content, and replaces the macro elements with other text.</p> <p>In a general sense, that is the entire functionality of templating systems. They have tools that provide control flow logic (if-then) and other functionality, but that functionality is solely to support substituting in the correct strings.</p> <p>Templating systems are intentionally agnostic to the programming language being templated, and most of them work for everything from blog posts to HTML to SQL.</p> <p>In contrast, Vulcan macros are designed specifically for generating SQL code. They have semantic understanding of the SQL code being created by analyzing it with the Python sqlglot library, and they allow use of Python code so users can tidily implement sophisticated macro logic.</p>"},{"location":"concepts-old/macros/vulcan_macros/#vulcan-macro-approach","title":"Vulcan macro approach","text":"<p>This section describes how Vulcan macros work under the hood. Feel free to skip over this section and return if and when it is useful. This information is not required to use Vulcan macros, but it will be useful for debugging any macros exhibiting puzzling behavior.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</li> <li>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</li> <li>Macro functions, both Vulcan's and user-defined</li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"concepts-old/macros/vulcan_macros/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></li> <li>Identifiers, which reference database objects like column, table, alias, and function names<ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint and local.</p> <p>Global and gateway macro variables are defined in the project configuration file and can be accessed in any project model. Blueprint and macro variables are defined in a model definition and can only be accessed in that model.</p> <p>Macro variables with the same name may be specified at any or all of the global, gateway, blueprint and local levels. When variables are specified at multiple levels, the value of the most specific level takes precedence. For example, the value of a local variable takes precedence over the value of a blueprint or gateway variable with the same name, and the value of a gateway variable takes precedence over the value of a global variable.</p>"},{"location":"concepts-old/macros/vulcan_macros/#global-variables","title":"Global variables","text":"<p>Global variables are defined in the project configuration file <code>variables</code> key.</p> <p>Global variable values may be any of the following data types or lists or dictionaries containing these types: <code>int</code>, <code>float</code>, <code>bool</code>, <code>str</code>.</p> <p>Access global variable values in a model definition using the <code>@&lt;VAR_NAME&gt;</code> macro or the <code>@VAR()</code> macro function. The latter function requires the name of the variable in single quotes as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"concepts-old/macros/vulcan_macros/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"concepts-old/macros/vulcan_macros/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"concepts-old/macros/vulcan_macros/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system has multiple operators that allow different forms of dynamic behavior in models.</p>"},{"location":"concepts-old/macros/vulcan_macros/#each","title":"@EACH","text":"<p><code>@EACH</code> is used to transform a list of items by applying a function to each of them, analogous to a <code>for</code> loop.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>Vulcan macros support placing macro values at the end of a column name using <code>column_@x</code>.</p> <p>However, if you wish to substitute the variable anywhere else in the identifier, you need to use the more explicit curly brace syntax <code>@{}</code> to avoid ambiguity. For example, these are valid uses: <code>@{x}_column</code> or <code>my_@{x}_column</code>.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"concepts-old/macros/vulcan_macros/#if","title":"@IF","text":"<p>Vulcan's <code>@IF</code> macro allows components of a SQL query to change based on the result of a logical condition.</p> <p>It includes three elements:</p> <ol> <li>A logical condition that evaluates to <code>TRUE</code> or <code>FALSE</code></li> <li>A value to return if the condition is <code>TRUE</code></li> <li>A value to return if the condition is <code>FALSE</code> [optional]</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</li> <li>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</li> <li>Between: <code>[number] BETWEEN [low number] AND [high number]</code></li> <li>Membership: <code>[item] IN ([comma-separated list of items])</code></li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li><code>'a' = 'a'</code></li> <li><code>'a' != 'b'</code></li> <li><code>0 &lt; 1</code></li> <li><code>1 &gt;= 1</code></li> <li><code>2 BETWEEN 1 AND 3</code></li> <li><code>'a' IN ('a', 'b')</code></li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"concepts-old/macros/vulcan_macros/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"concepts-old/macros/vulcan_macros/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The relation/table whose columns are being selected</li> <li><code>alias</code> (optional): The alias of the relation (if it has one)</li> <li><code>exclude</code> (optional): A list of columns to exclude</li> <li><code>prefix</code> (optional): A string to use as a prefix for all selected column names</li> <li><code>suffix</code> (optional): A string to use as a suffix for all selected column names</li> <li><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</li> <li>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</li> <li>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</li> <li>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></li> <li>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</li> <li><code>my_column</code> is present in the query</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by: 1. <code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type) 2. Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column 3. Concatenating the column values after steps (1) and (2) 4. Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</li> <li><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>lat1</code>: Latitude of the first point</li> <li><code>lon1</code>: Longitude of the first point</li> <li><code>lat2</code>: Latitude of the second point</li> <li><code>lon2</code>: Longitude of the second point</li> <li><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>column</code>: The column to pivot</li> <li><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</li> <li><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</li> <li><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></li> <li><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></li> <li><code>prefix</code> (optional): A prefix to use for all aliases</li> <li><code>suffix</code> (optional): A suffix to use for all aliases</li> <li><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></li> <li><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></li> <li><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</li> <li><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>relation</code>: The table or CTE name to deduplicate</li> <li><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</li> <li><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</li> <li><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</li> <li><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>. - For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</p>"},{"location":"concepts-old/macros/vulcan_macros/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</li> <li>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li><code>template</code> - The string template to render into an AST node</li> <li><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></li> <li><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></li> <li><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li><code>@WITH</code>: common table expression <code>WITH</code> clause</li> <li><code>@JOIN</code>: table <code>JOIN</code> clause(s)</li> <li><code>@WHERE</code>: filtering <code>WHERE</code> clause</li> <li><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</li> <li><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</li> <li><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</li> <li><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"concepts-old/macros/vulcan_macros/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"concepts-old/macros/vulcan_macros/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"concepts-old/macros/vulcan_macros/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>User-defined macro functions allow the same macro code to be used in multiple models.</p> <p>Vulcan supports user-defined macro functions written in two languages - SQL and Python:</p> <ul> <li>SQL macro functions must use the Jinja templating system.</li> <li>Python macro functions use the SQLGlot library to allow more complex operations than macro variables and operators provide alone.</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#python-macro-functions","title":"Python macro functions","text":""},{"location":"concepts-old/macros/vulcan_macros/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"concepts-old/macros/vulcan_macros/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"concepts-old/macros/vulcan_macros/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li><code>@add_args(1)</code> returns 1</li> <li><code>@add_args(1, 2)</code> returns 3</li> <li><code>@add_args(1, 2, 3)</code> returns 6</li> </ul>"},{"location":"concepts-old/macros/vulcan_macros/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li>Numbers are parsed into <code>Literal</code> expressions</li> <li>Quoted strings are parsed into <code>Literal</code> expressions</li> <li>Unquoted strings are parsed into <code>Column</code> expressions</li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></li> <li>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in the <code>dev</code> environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li>Example<ul> <li>Model name is <code>db.test_model</code></li> <li><code>plan</code> is running in any environment</li> <li><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</li> <li>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</li> <li>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"concepts-old/macros/vulcan_macros/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"concepts-old/macros/vulcan_macros/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"concepts-old/macros/vulcan_macros/#typed-macros","title":"Typed Macros","text":"<p>Typed macros in Vulcan bring the power of type hints from Python, enhancing readability, maintainability, and usability of your SQL macros. These macros enable developers to specify expected types for arguments, making the macros more intuitive and less error-prone.</p>"},{"location":"concepts-old/macros/vulcan_macros/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"concepts-old/macros/vulcan_macros/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"concepts-old/macros/vulcan_macros/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</li> <li><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</li> <li><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li><code>exp.Table</code></li> <li><code>exp.Column</code></li> <li><code>exp.Literal</code></li> <li><code>exp.Identifier</code></li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"concepts-old/macros/vulcan_macros/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p> <p>See examples of the coercion function in action in the test suite here.</p>"},{"location":"concepts-old/macros/vulcan_macros/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"concepts-old/macros/vulcan_macros/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan and Jinja macro systems. We strongly recommend using only one system in a model - if both are present, they may fail or behave in unintuitive ways.</p>"},{"location":"configurations/overview/","title":"Overview","text":""},{"location":"configurations/overview/#overview","title":"Overview","text":"<p>Vulcan projects are configured through a central configuration file that defines how your data pipeline connects to databases, manages environments, and handles model execution.</p>"},{"location":"configurations/overview/#configuration-file","title":"Configuration File","text":"<p>Every Vulcan project requires a configuration file in the project root directory:</p> <ul> <li><code>config.yaml</code> - YAML format (recommended for most users)</li> <li><code>config.py</code> - Python format (for advanced use cases requiring dynamic configuration)</li> </ul>"},{"location":"configurations/overview/#quick-start-example","title":"Quick Start Example","text":"<p>Here's a practical example of a Vulcan configuration file:</p> <pre><code># Project metadata\nname: orders360\ntenant: sales\ndescription: Daily sales analytics pipeline\n\n# Gateway Connection\ngateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: \"{{ env_var('DB_PASSWORD') }}\"\n    state_connection:\n      type: postgres\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: \"{{ env_var('STATE_DB_PASSWORD') }}\"\n\ndefault_gateway: default\n\n# Model Defaults (required)\nmodel_defaults:\n  dialect: postgres\n  start: 2024-01-01\n  cron: '@daily'\n\n# Linting Rules\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre>"},{"location":"configurations/overview/#configuration-structure","title":"Configuration Structure","text":"<pre><code>graph TB\n    Config[config.yaml]\n    Config --&gt; Project[Project Settings]\n    Config --&gt; Gateways[Gateways]\n    Config --&gt; ModelDefaults[Model Defaults]\n    Config --&gt; Options[Optional Features]\n    Gateways --&gt; Connection[connection]\n    Gateways --&gt; StateConn[state_connection]\n    Gateways --&gt; TestConn[test_connection]\n    Options --&gt; Linter[linter]\n    Options --&gt; Notifications[notifications]\n    Options --&gt; Variables[variables]\n    Options --&gt; Format[format]</code></pre>"},{"location":"configurations/overview/#configuration-sections","title":"Configuration Sections","text":""},{"location":"configurations/overview/#project-settings","title":"Project Settings","text":"<p>Basic project metadata for identification.</p> Option Description Type <code>name</code> Project name string <code>tenant</code> Tenant or organization name string <code>description</code> Human-readable project description string"},{"location":"configurations/overview/#gateways","title":"Gateways","text":"<p>Gateways define connections to your data warehouse, state backend, and other services.</p> Component Description Default <code>connection</code> Primary data warehouse connection Required <code>state_connection</code> Where Vulcan stores internal state Uses <code>connection</code> <code>test_connection</code> Connection for running tests Uses <code>connection</code> <code>scheduler</code> Scheduler configuration <code>builtin</code> <code>state_schema</code> Schema name for state tables <code>vulcan</code> <p>\u2192 See Configuration Reference for detailed gateway options.</p>"},{"location":"configurations/overview/#model-defaults-required","title":"Model Defaults (Required)","text":"<p>The <code>model_defaults</code> section is required and must include at least the <code>dialect</code> key.</p> <pre><code>model_defaults:\n  dialect: postgres     # Required\n  owner: data-team\n  start: 2024-01-01\n  cron: '@daily'\n</code></pre> <p>\u2192 See Model Defaults for all available options.</p>"},{"location":"configurations/overview/#variables","title":"Variables","text":"<p>Configure environment variables, <code>.env</code> files, and configuration overrides.</p> <p>\u2192 See Variables for details.</p>"},{"location":"configurations/overview/#execution-hooks","title":"Execution Hooks","text":"<p>Execute SQL statements at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands using <code>before_all</code> and <code>after_all</code>.</p> <p>\u2192 See Execution Hooks for detailed examples and use cases.</p>"},{"location":"configurations/overview/#linter","title":"Linter","text":"<p>Enable automatic code quality checks for your models.</p> <p>\u2192 See Linter for rules and custom linter configuration.</p>"},{"location":"configurations/overview/#notifications","title":"Notifications","text":"<p>Configure alerts via Slack or email for pipeline events.</p> <p>\u2192 See Notifications for Slack webhooks, API, and email setup.</p>"},{"location":"configurations/overview/#supported-engines","title":"Supported Engines","text":"<p>Vulcan supports connecting to various data warehouses:</p> <ul> <li>PostgreSQL - Open-source relational database</li> <li>Snowflake - Cloud data warehouse</li> </ul>"},{"location":"configurations/overview/#configuration-reference","title":"Configuration Reference","text":"Topic Description Configuration Reference Complete list of all configuration parameters Variables Environment variables and <code>.env</code> files Model Defaults Default settings for all models Execution Hooks <code>before_all</code> and <code>after_all</code> statements Linter Code quality rules and custom linters Notifications Slack and email notification setup"},{"location":"configurations/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for sensitive data like passwords and API keys</li> <li>Set meaningful defaults in <code>model_defaults</code> to reduce boilerplate</li> <li>Enable linting to catch common errors early in development</li> <li>Separate state connection from data warehouse for better isolation</li> <li>Use multiple gateways for different environments (dev, staging, prod)</li> </ol>"},{"location":"configurations/ci_cd/","title":"CI/CD","text":""},{"location":"configurations/ci_cd/#cicd","title":"CI/CD","text":"<p>Coming soon...</p>"},{"location":"configurations/engines/postgres/postgres/","title":"Postgres","text":""},{"location":"configurations/engines/postgres/postgres/#postgres","title":"Postgres","text":""},{"location":"configurations/engines/postgres/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"configurations/engines/postgres/postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"configurations/engines/snowflake/snowflake/","title":"Snowflake","text":""},{"location":"configurations/engines/snowflake/snowflake/#snowflake","title":"Snowflake","text":"<p>This page provides information about how to use Vulcan with the Snowflake SQL engine.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the built-in.</p>"},{"location":"configurations/engines/snowflake/snowflake/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.</p> <p>It demonstrates connecting to Snowflake with the <code>snowflake-connector-python</code> library bundled with Vulcan.</p> <p>Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are described below.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"configurations/engines/snowflake/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Snowflake account and know your username and password</li> <li>Your Snowflake account has at least one warehouse available for running computations</li> <li>Your computer has Vulcan installed with the Snowflake extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[snowflake]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init snowflake</code></li> </ul> </li> </ol>"},{"location":"configurations/engines/snowflake/snowflake/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access different types of database objects.</p> <p>Vulcan's core functionality requires relatively broad permissions, including:</p> <ol> <li>Ability to create and delete schemas in a database</li> <li>Ability to create, modify, delete, and query tables and views in the schemas it creates</li> </ol> <p>If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.</p> <p>We now describe how to grant Vulcan appropriate permissions.</p>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-roles","title":"Snowflake roles","text":"<p>Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a \"role\" that you then grant to the user.</p> <p>Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.</p> <p>The role must be granted <code>USAGE</code> on a warehouse so it can execute computations. We describe other permissions below.</p>"},{"location":"configurations/engines/snowflake/snowflake/#database-permissions","title":"Database permissions","text":"<p>The top-level object container in Snowflake is a \"database\" (often called a \"catalog\" in other engines). Vulcan does not need permission to create databases; it may use an existing one.</p> <p>The simplest way to grant Vulcan sufficient permissions for a database is to give it <code>OWNERSHIP</code> of the database, which includes all the necessary permissions.</p> <p>Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.</p>"},{"location":"configurations/engines/snowflake/snowflake/#granting-the-permissions","title":"Granting the permissions","text":"<p>This section provides example code for creating a <code>vulcan</code> role, granting it sufficient permissions, and granting it to a user.</p> <p>The code must be executed by a user with <code>USERADMIN</code> level permissions or higher. We provide two versions of the code, one that grants database <code>OWNERSHIP</code> to the role and another that does not.</p> <p>Both examples create a role named <code>vulcan</code>, grant it usage of the warehouse <code>compute_wh</code>, create a database named <code>demo_db</code>, and assign the role to the user <code>demo_user</code>. The step that creates the database can be omitted if the database already exists.</p> With database ownershipWithout database ownership <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\n\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\nGRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre> <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\n\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\nGRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database\n\nGRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates\nGRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas\nGRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas\nGRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas\nGRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#get-connection-info","title":"Get connection info","text":"<p>Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.</p>"},{"location":"configurations/engines/snowflake/snowflake/#account-name","title":"Account name","text":"<p>Snowflake connection configurations require the <code>account</code> parameter that identifies the Snowflake account Vulcan should connect to.</p> <p>Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a <code>/</code>.</p> <p>This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:</p> <p></p> <p>In this example, our organization name is <code>idapznw</code>, and our account name is <code>wq29399</code>.</p> <p>We concatenate the two components, separated by a <code>-</code>, for the Vulcan <code>account</code> parameter: <code>idapznw-wq29399</code>.</p>"},{"location":"configurations/engines/snowflake/snowflake/#warehouse-name","title":"Warehouse name","text":"<p>Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.</p> <p>Some Snowflake user accounts may have a default warehouse they automatically use when connecting.</p> <p>The connection configuration's <code>warehouse</code> parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.</p>"},{"location":"configurations/engines/snowflake/snowflake/#database-name","title":"Database name","text":"<p>Snowflake user accounts may have a \"Default Namespace\" that includes a default database they automatically use when connecting.</p> <p>The connection configuration's <code>database</code> parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.</p>"},{"location":"configurations/engines/snowflake/snowflake/#configure-the-connection","title":"Configure the connection","text":"<p>We now have the information we need to configure Vulcan's connection to Snowflake.</p> <p>We start the configuration by adding a gateway named <code>snowflake</code> to our example project's config.yaml file and making it our <code>default_gateway</code>:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>And we specify the <code>account</code>, <code>user</code>, <code>password</code>, <code>database</code>, and <code>warehouse</code> connection parameters using the information from above:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Warning</p> <p>Best practice for storing secrets like passwords is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>SNOWFLAKE_PASSWORD</code> for the configuration's <code>password</code> parameter:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      password: {{ env_var('SNOWFLAKE_PASSWORD') }}\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>snowflake</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Snowflake SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"configurations/engines/snowflake/snowflake/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>snowflake</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>snowflake_state.db</code> file:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; your password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n    state_connection:\n      type: duckdb\n      database: snowflake_state.db\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Now we no longer see the warning when running <code>vulcan info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"configurations/engines/snowflake/snowflake/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in Snowflake:</p> <p></p> <p>And confirm that our schemas and objects exist in the Snowflake catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Snowflake!</p>"},{"location":"configurations/engines/snowflake/snowflake/#where-are-the-row-counts","title":"Where are the row counts?","text":"<p>Vulcan reports the number of rows processed by each model in its <code>plan</code> and <code>run</code> terminal output.</p> <p>However, due to limitations in the Snowflake Python connector, row counts cannot be determined for <code>CREATE TABLE AS</code> statements. Therefore, Vulcan does not report row counts for certain model kinds, such as <code>FULL</code> models.</p> <p>Learn more about the connector limitation on Github.</p>"},{"location":"configurations/engines/snowflake/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"configurations/engines/snowflake/snowflake/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[snowflake]\"\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account name string Y <code>user</code> The Snowflake username string N <code>password</code> The Snowflake password string N <code>authenticator</code> The Snowflake authenticator method string N <code>warehouse</code> The Snowflake warehouse name string N <code>database</code> The Snowflake database name string N <code>role</code> The Snowflake role name string N <code>token</code> The Snowflake OAuth 2.0 access token string N <code>private_key</code> The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). string N <code>private_key_path</code> The optional path to the private key to use for authentication. This would be used instead of <code>private_key</code>. string N <code>private_key_passphrase</code> The optional passphrase to use to decrypt <code>private_key</code> (if in PEM format) or <code>private_key_path</code>. Keys can be created without encryption so only provide this if needed. string N <code>session_parameters</code> The optional session parameters to set for the connection. dict N"},{"location":"configurations/engines/snowflake/snowflake/#lowercase-object-names","title":"Lowercase object names","text":"<p>Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command <code>CREATE SCHEMA vulcan</code> will generate a schema named <code>VULCAN</code> in Snowflake.</p> <p>If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.</p> <p>For example, a connection to the database <code>\"my_db\"</code> would include:</p> <pre><code>connection:\n  type: snowflake\n  &lt;other connection options&gt;\n  database: '\"my_db\"' # outer single and inner double quotes\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-authorization-methods","title":"Snowflake authorization methods","text":"<p>The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.</p> <p>This section describes how to configure other authorization methods.</p>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-sso-authorization","title":"Snowflake SSO Authorization","text":"<p>Vulcan supports Snowflake SSO authorization connections using the <code>externalbrowser</code> authenticator method. For example:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: ************\n      user: ************\n      authenticator: externalbrowser\n      warehouse: ************\n      database: ************\n      role: ************\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-oauth-authorization","title":"Snowflake OAuth Authorization","text":"<p>Vulcan supports Snowflake OAuth authorization connections using the <code>oauth</code> authenticator method. For example:</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      authenticator: oauth\n      token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                authenticator=\"oauth\",\n                token=\"eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#snowflake-private-key-authorization","title":"Snowflake Private Key Authorization","text":"<p>Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).</p> <p>The <code>account</code> and <code>user</code> parameters are required for each of these methods.</p> <p>Private Key Path</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key_path: '/path/to/key.key'\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key_path=\"/path/to/key.key\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key PEM</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: |\n        -----BEGIN PRIVATE KEY-----\n        ...\n        -----END PRIVATE KEY-----\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"\"\"\n                -----BEGIN PRIVATE KEY-----\n                ...\n                -----END PRIVATE KEY-----\"\"\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#private-key-base64","title":"Private Key Base64","text":"<p>Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Bytes</p> YAMLPython <p>Base64 encode the bytes and follow Private Key Base64 instructions.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    GatewayConfig,\n    ModelDefaultsConfig,\n    SnowflakeConnectionConfig,\n)\n\nfrom cryptography.hazmat.primitives import serialization\n\nkey = \"\"\"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\"\"\".encode()\n\np_key= serialization.load_pem_private_key(key, password=None)\n\npkb = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=pkb,\n            ),\n        ),\n    }\n)\n</code></pre> <p>The authenticator method is assumed to be <code>snowflake_jwt</code> when <code>private_key</code> is provided, but it can also be explicitly provided in the connection configuration.</p>"},{"location":"configurations/engines/snowflake/snowflake/#configuring-virtual-warehouses","title":"Configuring Virtual Warehouses","text":"<p>The Snowflake Virtual Warehouse a model should use can be specified in the <code>session_properties</code> attribute of the model definition:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  session_properties (\n    'warehouse' = TEST_WAREHOUSE,\n  ),\n);\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#custom-view-and-table-types","title":"Custom View and Table types","text":"<p>Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the <code>physical_properties</code> and <code>virtual_properties</code> attributes respectively. For example:</p>"},{"location":"configurations/engines/snowflake/snowflake/#secure-views","title":"Secure Views","text":"<p>A table can be exposed through a <code>SECURE</code> view in the virtual layer by specifying the <code>creatable_type</code> property and setting it to <code>SECURE</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  virtual_properties (\n      creatable_type = SECURE\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#transient-tables","title":"Transient Tables","text":"<p>A model can use a <code>TRANSIENT</code> table in the physical layer by specifying the <code>creatable_type</code> property and setting it to <code>TRANSIENT</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  physical_properties (\n      creatable_type = TRANSIENT\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#iceberg-tables","title":"Iceberg Tables","text":"<p>In order for Snowflake to be able to create an Iceberg table, there must be an External Volume configured to store the Iceberg table data on.</p> <p>Once that is configured, you can create a model backed by an Iceberg table by using <code>table_format iceberg</code> like so:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = '&lt;external volume name&gt;'\n  )\n);\n</code></pre> <p>To prevent having to specify <code>catalog = 'snowflake'</code> and <code>external_volume = '&lt;external volume name&gt;'</code> on every model, see the Snowflake documentation for:</p> <ul> <li>Configuring a default Catalog</li> <li>Configuring a default External Volume</li> </ul> <p>Alternatively you can also use model defaults to set defaults at the Vulcan level instead.</p> <p>To utilize the wide variety of optional properties that Snowflake makes available for Iceberg tables, simply specify them as <code>physical_properties</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = 'my_external_volume',\n    base_location = 'my/product_reviews/'\n  )\n);\n</code></pre> <p>External catalogs</p> <p>Setting <code>catalog = 'snowflake'</code> to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake does not support writing to Iceberg tables configured under external catalogs.</p> <p>You can however still reference a table from an external catalog in your model as a normal external table.</p>"},{"location":"configurations/engines/snowflake/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configurations/engines/snowflake/snowflake/#frequent-authentication-prompts","title":"Frequent Authentication Prompts","text":"<p>When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.</p> <p>To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:</p> <ul> <li>For general authentication, see Connection Caching Documentation</li> <li>For MFA specifically, see MFA Token Caching Documentation.</li> </ul>"},{"location":"configurations/options/execution_hooks/","title":"Execution Hooks","text":""},{"location":"configurations/options/execution_hooks/#execution-hooks","title":"Execution Hooks","text":"<p>Vulcan provides <code>before_all</code> and <code>after_all</code> hooks that execute SQL statements or macros at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands. These hooks are powerful tools for automating setup, cleanup, and privilege management across your data pipeline.</p>"},{"location":"configurations/options/execution_hooks/#overview","title":"Overview","text":"Hook When it Runs Common Use Cases <code>before_all</code> Before any model is processed Setup tables, initialize logging, validate prerequisites <code>after_all</code> After all models are processed Grant privileges, cleanup, send notifications, update metadata"},{"location":"configurations/options/execution_hooks/#basic-configuration","title":"Basic Configuration","text":"YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\n  - INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\n\nafter_all:\n  - \"@grant_select_privileges()\"\n  - UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all=[\n        \"CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\",\n        \"INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\"\n    ],\n    after_all=[\n        \"@grant_select_privileges()\",\n        \"UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\"\n    ],\n)\n</code></pre>"},{"location":"configurations/options/execution_hooks/#using-macros-in-hooks","title":"Using Macros in Hooks","text":"<p>Hooks can execute Vulcan macros using the <code>@macro_name()</code> syntax. Macros provide access to runtime context like view names, schemas, and the current environment.</p>"},{"location":"configurations/options/execution_hooks/#available-context-variables","title":"Available Context Variables","text":"<p>Macros invoked in hooks have access to:</p> Property Type Description <code>evaluator.views</code> <code>list[str]</code> All view names created in the virtual layer <code>evaluator.schemas</code> <code>list[str]</code> All schema names used by models <code>evaluator.this_env</code> <code>str</code> Current environment name (e.g., <code>prod</code>, <code>dev</code>) <code>evaluator.gateway</code> <code>str</code> Current gateway name"},{"location":"configurations/options/execution_hooks/#use-cases","title":"Use Cases","text":""},{"location":"configurations/options/execution_hooks/#1-granting-privileges-on-views","title":"1. Granting Privileges on Views","text":"<p>Instead of adding privilege grants to each model individually, use <code>after_all</code> to grant access to all views at once:</p> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    \"\"\"Grant SELECT on all views to the analytics role.\"\"\"\n    if not evaluator.views:\n        return []\n\n    return [\n        f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE analytics_role;\"\n        for view_name in evaluator.views\n    ]\n</code></pre> config.yaml<pre><code>after_all:\n  - \"@grant_select_privileges()\"\n</code></pre> <p>Preventing Name Replacement</p> <p>The comment <code>/* sqlglot.meta replace=false */</code> ensures Vulcan doesn't replace the view name with the physical table name during SQL rendering.</p>"},{"location":"configurations/options/execution_hooks/#2-environment-specific-execution","title":"2. Environment-Specific Execution","text":"<p>Use the <code>@IF</code> macro to conditionally execute statements based on the environment:</p> config.yaml<pre><code>after_all:\n  # Only grant schema usage in production\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n\n  # Only run cleanup in development\n  - \"@IF(@this_env != 'prod', @cleanup_dev_tables())\"\n</code></pre> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    \"\"\"Grant USAGE on all schemas to admin role (production only).\"\"\"\n    if evaluator.this_env != \"prod\" or not evaluator.schemas:\n        return []\n\n    return [\n        f\"GRANT USAGE ON SCHEMA {schema} TO ROLE admin_role;\"\n        for schema in evaluator.schemas\n    ]\n\n@macro()\ndef cleanup_dev_tables(evaluator):\n    \"\"\"Clean up temporary tables in development environments.\"\"\"\n    return [\n        \"DROP TABLE IF EXISTS temp_debug_output;\",\n        \"DROP TABLE IF EXISTS temp_test_data;\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#3-audit-logging","title":"3. Audit Logging","text":"<p>Track pipeline execution with audit tables:</p> config.yaml<pre><code>before_all:\n  - |\n    CREATE TABLE IF NOT EXISTS pipeline_audit (\n      run_id VARCHAR,\n      environment VARCHAR,\n      started_at TIMESTAMP,\n      completed_at TIMESTAMP,\n      status VARCHAR\n    )\n  - \"@log_pipeline_start()\"\n\nafter_all:\n  - \"@log_pipeline_end()\"\n</code></pre> macros/audit.py<pre><code>from vulcan.core.macros import macro\nimport uuid\n\n@macro()\ndef log_pipeline_start(evaluator):\n    run_id = str(uuid.uuid4())[:8]\n    return [\n        f\"\"\"\n        INSERT INTO pipeline_audit (run_id, environment, started_at, status)\n        VALUES ('{run_id}', '{evaluator.this_env}', CURRENT_TIMESTAMP, 'running')\n        \"\"\"\n    ]\n\n@macro()\ndef log_pipeline_end(evaluator):\n    return [\n        f\"\"\"\n        UPDATE pipeline_audit \n        SET completed_at = CURRENT_TIMESTAMP, status = 'completed'\n        WHERE environment = '{evaluator.this_env}' \n          AND status = 'running'\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#4-schema-and-database-setup","title":"4. Schema and Database Setup","text":"<p>Ensure required schemas exist before models run:</p> config.yaml<pre><code>before_all:\n  - CREATE SCHEMA IF NOT EXISTS staging\n  - CREATE SCHEMA IF NOT EXISTS analytics\n  - CREATE SCHEMA IF NOT EXISTS reporting\n  - \"@setup_external_tables()\"\n</code></pre> macros/setup.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef setup_external_tables(evaluator):\n    \"\"\"Create external tables for data ingestion.\"\"\"\n    return [\n        \"\"\"\n        CREATE EXTERNAL TABLE IF NOT EXISTS staging.raw_events (\n            event_id VARCHAR,\n            event_type VARCHAR,\n            event_data VARCHAR,\n            created_at TIMESTAMP\n        )\n        LOCATION 's3://data-lake/events/'\n        FILE_FORMAT = (TYPE = 'PARQUET')\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#5-data-quality-gates","title":"5. Data Quality Gates","text":"<p>Run data quality checks before processing:</p> config.yaml<pre><code>before_all:\n  - \"@validate_source_data()\"\n</code></pre> macros/validation.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef validate_source_data(evaluator):\n    \"\"\"Validate that source data meets quality requirements.\"\"\"\n    return [\n        \"\"\"\n        DO $$\n        DECLARE\n            row_count INTEGER;\n        BEGIN\n            SELECT COUNT(*) INTO row_count FROM raw_data.events WHERE created_at &gt;= CURRENT_DATE;\n            IF row_count = 0 THEN\n                RAISE EXCEPTION 'No data found for today in raw_data.events';\n            END IF;\n        END $$;\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#6-refresh-materialized-views","title":"6. Refresh Materialized Views","text":"<p>Refresh dependent materialized views after models are updated:</p> config.yaml<pre><code>after_all:\n  - \"@refresh_materialized_views()\"\n</code></pre> macros/refresh.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef refresh_materialized_views(evaluator):\n    \"\"\"Refresh all materialized views that depend on our models.\"\"\"\n    materialized_views = [\n        \"reporting.daily_summary_mv\",\n        \"reporting.weekly_trends_mv\",\n        \"analytics.user_metrics_mv\"\n    ]\n\n    return [\n        f\"REFRESH MATERIALIZED VIEW {mv};\"\n        for mv in materialized_views\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#7-notification-integration","title":"7. Notification Integration","text":"<p>Send notifications after pipeline completion:</p> config.yaml<pre><code>after_all:\n  - \"@notify_completion()\"\n</code></pre> macros/notify.py<pre><code>from vulcan.core.macros import macro\nimport os\n\n@macro()\ndef notify_completion(evaluator):\n    \"\"\"Log completion status (integrate with your notification system).\"\"\"\n    # This example logs to a table; you could also call an external API\n    view_count = len(evaluator.views) if evaluator.views else 0\n    schema_count = len(evaluator.schemas) if evaluator.schemas else 0\n\n    return [\n        f\"\"\"\n        INSERT INTO notifications_log (\n            environment, \n            message, \n            view_count, \n            schema_count, \n            created_at\n        )\n        VALUES (\n            '{evaluator.this_env}',\n            'Pipeline completed successfully',\n            {view_count},\n            {schema_count},\n            CURRENT_TIMESTAMP\n        )\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#execution-order","title":"Execution Order","text":"<pre><code>graph TD\n    A[Start] --&gt; B[before_all]\n    B --&gt; C[Process Models]\n    C --&gt; D[after_all]\n    D --&gt; E[Complete]</code></pre>"},{"location":"configurations/options/execution_hooks/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use macros for complex logic - Keep YAML configuration clean by moving complex SQL generation to Python macros</p> </li> <li> <p>Make hooks idempotent - Hooks may run multiple times; use <code>IF NOT EXISTS</code>, <code>ON CONFLICT</code>, or similar patterns</p> </li> <li> <p>Use environment checks - Gate production-only operations with <code>@IF(@this_env = 'prod', ...)</code></p> </li> <li> <p>Handle failures gracefully - Consider what happens if a hook fails; use transactions where appropriate</p> </li> <li> <p>Document your hooks - Add comments explaining why each hook exists and what it does</p> </li> <li> <p>Test in development first - Always test hooks in a development environment before running in production</p> </li> </ol>"},{"location":"configurations/options/execution_hooks/#comparison-with-model-level-hooks","title":"Comparison with Model-Level Hooks","text":"Feature <code>before_all</code> / <code>after_all</code> Model <code>pre_statements</code> / <code>post_statements</code> Scope Entire pipeline Single model Runs Once per plan/run Once per model execution Access to All views, schemas, environment Model-specific context Use for Global setup, cleanup, privileges Model-specific operations <p>Use <code>before_all</code>/<code>after_all</code> for operations that apply to the entire pipeline. Use model-level hooks for operations specific to individual models.</p>"},{"location":"configurations/options/linter/","title":"Linter","text":""},{"location":"configurations/options/linter/#linter","title":"Linter","text":"<p>Linting is a powerful tool for improving code quality and consistency. It enables you to automatically validate model definition, ensuring they adhere to your team's best practices.</p> <p>When a Vulcan plan is created, each model's code is checked for compliance with a set of rules you choose.</p> <p>Vulcan provides built-in rules, and you can define custom rules. This improves code quality and helps detect issues early in the development cycle when they are simpler to debug.</p>"},{"location":"configurations/options/linter/#rules","title":"Rules","text":"<p>Each linting rule is responsible for identifying a pattern in a model's code.</p> <p>Some rules validate that a pattern is not present, such as not allowing <code>SELECT *</code> in a model's outermost query. Other rules validate that a pattern is present, like ensuring that every model's <code>owner</code> field is specified. We refer to both of these below as \"validating a pattern\".</p> <p>Rules are defined in Python. Each rule is an individual Python class that inherits from Vulcan's <code>Rule</code> base class and defines the logic for validating a pattern.</p> <p>We display a portion of the <code>Rule</code> base class's code below (full source code). Its methods and properties illustrate the most important components of the subclassed rules you define.</p> <p>Each rule class you create has four vital components:</p> <ol> <li>Name: the class's name is used as the rule's name.</li> <li>Description: the class should define a docstring that provides a short explanation of the rule's purpose.</li> <li>Pattern validation logic: the class should define a <code>check_model()</code> method containing the core logic that validates the rule's pattern. The method can access any <code>Model</code> attribute.</li> <li>Rule violation logic: if a rule's pattern is not validated, the rule is \"violated\" and the class should return a <code>RuleViolation</code> object. The <code>RuleViolation</code> object should include the contextual information a user needs to understand and fix the problem.</li> </ol> <pre><code># Class name used as rule's name\nclass Rule:\n    # Docstring provides rule's description\n    \"\"\"The base class for a rule.\"\"\"\n\n    # Pattern validation logic goes in `check_model()` method\n    @abc.abstractmethod\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        \"\"\"The evaluation function that checks for a violation of this rule.\"\"\"\n\n    # Rule violation object returned by `violation()` method\n    def violation(self, violation_msg: t.Optional[str] = None) -&gt; RuleViolation:\n        \"\"\"Return a RuleViolation instance if this rule is violated\"\"\"\n        return RuleViolation(rule=self, violation_msg=violation_msg or self.summary)\n</code></pre>"},{"location":"configurations/options/linter/#built-in-rules","title":"Built-in rules","text":"<p>Vulcan includes a set of predefined rules that check for potential SQL errors or enforce code style.</p> <p>An example of the latter is the <code>NoSelectStar</code> rule, which prohibits a model from using <code>SELECT *</code> in its query's outer-most select statement.</p> <p>Here is code for the built-in <code>NoSelectStar</code> rule class, with the different components annotated:</p> <pre><code># Rule's name is the class name `NoSelectStar`\nclass NoSelectStar(Rule):\n    # Docstring explaining rule\n    \"\"\"Query should not contain SELECT * on its outer most projections, even if it can be expanded.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # If this model does not contain a SQL query, there is nothing to validate\n        if not isinstance(model, SqlModel):\n            return None\n\n        # Use the query's `is_star` property to detect the `SELECT *` pattern.\n        # If present, call the `violation()` method to return a `RuleViolation` object.\n        return self.violation() if model.query.is_star else None\n</code></pre> <p>Here are all of Vulcan's built-in linting rules:</p> Name Check type Explanation <code>ambiguousorinvalidcolumn</code> Correctness Vulcan found duplicate columns or was unable to determine whether a column is duplicated or not <code>invalidselectstarexpansion</code> Correctness The query's top-level selection may be <code>SELECT *</code>, but only if Vulcan can expand the <code>SELECT *</code> into individual columns <code>noselectstar</code> Stylistic The query's top-level selection may not be <code>SELECT *</code>, even if Vulcan can expand the <code>SELECT *</code> into individual columns <code>nomissingaudits</code> Governance Vulcan did not find any <code>audits</code> in the model's configuration to test data quality."},{"location":"configurations/options/linter/#user-defined-rules","title":"User-defined rules","text":"<p>You may define custom rules to implement your team's best practices.</p> <p>For instance, you could ensure all models have an <code>owner</code> by defining the following linting rule:</p> linter/user.py<pre><code>import typing as t\n\nfrom vulcan.core.linter.rule import Rule, RuleViolation\nfrom vulcan.core.model import Model\n\nclass NoMissingOwner(Rule):\n    \"\"\"Model owner should always be specified.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # Rule violated if the model's owner field (`model.owner`) is not specified\n        return self.violation() if not model.owner else None\n</code></pre> <p>Place a rule's code in the project's <code>linter/</code> directory. Vulcan will load all subclasses of <code>Rule</code> from that directory.</p> <p>If the rule is specified in the project's configuration file, Vulcan will run it when: - A plan is created during <code>vulcan plan</code> - The command <code>vulcan lint</code> is ran</p> <p>Vulcan will error if a model violates the rule, informing you which model(s) violated the rule. In this example, <code>full_model.sql</code> violated the <code>NoMissingOwner</code> rule, essentially halting execution:</p> <pre><code>$ vulcan plan\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Or through the standalone command, for faster iterations:</p> <pre><code>$ vulcan lint\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Use <code>vulcan lint --help</code> for more information.</p>"},{"location":"configurations/options/linter/#applying-linting-rules","title":"Applying linting rules","text":"<p>Specify which linting rules a project should apply in the project's configuration file.</p> <p>Rules are specified as lists of rule names under the <code>linter</code> key. Globally enable or disable linting with the <code>enabled</code> key, which is <code>false</code> by default.</p> <p>NOTE: you must set the <code>enabled</code> key to <code>true</code> key to apply the project's linting rules.</p>"},{"location":"configurations/options/linter/#specific-linting-rules","title":"Specific linting rules","text":"<p>This example specifies that the <code>\"ambiguousorinvalidcolumn\"</code> and <code>\"invalidselectstarexpansion\"</code> linting rules should be enforced:</p> YAMLPython <pre><code>linter:\n  enabled: true\n  rules: [\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=[\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#all-linting-rules","title":"All linting rules","text":"<p>Apply every built-in and user-defined rule by specifying <code>\"ALL\"</code> instead of a list of rules:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\"\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=\"all\",\n    )\n)\n</code></pre> <p>If you want to apply all rules except for a few, you can specify <code>\"ALL\"</code> and list the rules to ignore in the <code>ignored_rules</code> key:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\" # apply all built-in and user-defined rules and error if violated\n  ignored_rules: [\"noselectstar\"] # but don't run the `noselectstar` rule\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # apply all built-in and user-defined linting rules and error if violated\n        rules=\"all\",\n         # but don't run the `noselectstar` rule\n        ignored_rules=[\"noselectstar\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#exclude-a-model-from-linting","title":"Exclude a model from linting","text":"<p>You can specify that a specific model ignore a linting rule by specifying <code>ignored_rules</code> in its <code>MODEL</code> block.</p> <p>This example specifies that the model <code>docs_example.full_model</code> should not run the <code>invalidselectstarexpansion</code> rule:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  ignored_rules [\"invalidselectstarexpansion\"] # or \"ALL\" to turn off linting completely\n);\n</code></pre>"},{"location":"configurations/options/linter/#rule-violation-behavior","title":"Rule violation behavior","text":"<p>Linting rule violations raise an error by default, preventing the project from running until the violation is addressed.</p> <p>You may specify that a rule's violation should not error and only log a warning by specifying it in the <code>warn_rules</code> key instead of the <code>rules</code> key.</p> YAMLPython <pre><code>linter:\n  enabled: True\n  # error if `ambiguousorinvalidcolumn` rule violated\n  rules: [\"ambiguousorinvalidcolumn\"]\n  # but only warn if \"invalidselectstarexpansion\" is violated\n  warn_rules: [\"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # error if `ambiguousorinvalidcolumn` rule violated\n        rules=[\"ambiguousorinvalidcolumn\"],\n        # but only warn if \"invalidselectstarexpansion\" is violated\n        warn_rules=[\"invalidselectstarexpansion\"],\n    )\n)\n</code></pre> <p>Vulcan will raise an error if the same rule is included in more than one of the <code>rules</code>, <code>warn_rules</code>, and <code>ignored_rules</code> keys since they should be mutually exclusive.</p>"},{"location":"configurations/options/model_defaults/","title":"Model defaults","text":""},{"location":"configurations/options/model_defaults/#model-defaults","title":"Model defaults","text":"<p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed. Other values are set automatically unless explicitly overridden in the model definition.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference page.</p>"},{"location":"configurations/options/model_defaults/#basic-configuration","title":"Basic configuration","text":"<p>Example configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless overridden with the <code>kind</code> key. For more information on model kinds, refer to model concepts page.</p>"},{"location":"configurations/options/model_defaults/#identifier-resolution","title":"Identifier resolution","text":"<p>When a SQL engine receives a query such as <code>SELECT id FROM \"some_table\"</code>, it eventually needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This process is usually referred to as identifier (or name) resolution.</p> <p>Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.</p> <p>Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it normalizes and quotes all identifiers in those queries, respecting each dialect's resolution rules.</p> <p>The \"normalization strategy\", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.</p> <p>See here to learn more about the supported normalization strategies.</p>"},{"location":"configurations/options/model_defaults/#gateway-specific-model-defaults","title":"Gateway-specific model defaults","text":"<p>You can also define gateway specific <code>model_defaults</code> in the <code>gateways</code> section, which override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This allows you to tailor the behavior of models for each gateway without affecting the global <code>model_defaults</code>.</p> <p>For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.</p> <p>Gateway-specific <code>model_defaults</code> allow you to change how Vulcan performs identifier normalization by engine to align the different engines' behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway configuration overrides that global default dialect with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>That value tells Vulcan that the <code>redshift</code> gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"configurations/options/notifications/","title":"Notifications","text":""},{"location":"configurations/options/notifications/#notifications","title":"Notifications","text":"<p>Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.</p>"},{"location":"configurations/options/notifications/#notification-targets","title":"Notification targets","text":"<p>Notifications are configured with <code>notification targets</code>. Targets are specified in a project's configuration file (<code>config.yml</code> or <code>config.py</code>), and multiple targets can be specified for a project.</p> <p>A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each event type (e.g., notifications for <code>run</code> will be sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification target, corresponding to the two Slack notification methods and email notification. They are specified in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"configurations/options/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter may be useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"configurations/options/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"configurations/options/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"configurations/options/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n      - apply_failure\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#api-configuration","title":"API Configuration","text":"<p>If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n      - apply_end\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"configurations/options/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, new notification targets can be configured to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>). See the definitions of these classes on Github here.</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/variables/","title":"Variables","text":""},{"location":"configurations/options/variables/#variables","title":"Variables","text":"<p>This page covers environment variables and configuration overrides for your Vulcan project.</p>"},{"location":"configurations/options/variables/#environment-variables","title":"Environment Variables","text":"<p>Vulcan can access environment variables during configuration, enabling you to store secrets outside configuration files and dynamically change settings based on the user running Vulcan.</p>"},{"location":"configurations/options/variables/#using-env-files","title":"Using .env Files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>Security</p> <p>Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information.</p>"},{"location":"configurations/options/variables/#custom-env-file-location","title":"Custom .env File Location","text":"<p>Specify a custom path using the <code>--dotenv</code> CLI flag:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Or set the <code>VULCAN_DOTENV_PATH</code> environment variable:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag must be placed before the subcommand (e.g., <code>plan</code>, <code>run</code>).</p>"},{"location":"configurations/options/variables/#accessing-variables-in-configuration","title":"Accessing Variables in Configuration","text":"YAMLPython <p>Use <code>{{ env_var('VARIABLE_NAME') }}</code> syntax:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: admin\n      password: \"{{ env_var('SNOWFLAKE_PW') }}\"\n      account: my_account\n</code></pre> <p>Use <code>os.environ</code>:</p> <pre><code>import os\nfrom vulcan.core.config import Config, GatewayConfig, SnowflakeConnectionConfig\n\nconfig = Config(\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"admin\",\n                password=os.environ['SNOWFLAKE_PW'],\n                account=\"my_account\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/options/variables/#configuration-overrides","title":"Configuration Overrides","text":"<p>Environment variables have the highest precedence and will override configuration file values if they follow the <code>VULCAN__</code> naming convention.</p>"},{"location":"configurations/options/variables/#override-naming-structure","title":"Override Naming Structure","text":"<p>Use double underscores <code>__</code> to navigate the configuration hierarchy:</p> <pre><code>VULCAN__&lt;ROOT_KEY&gt;__&lt;NESTED_KEY&gt;__&lt;FIELD&gt;=value\n</code></pre> <p>Example: Override a gateway connection password:</p> <pre><code># config.yaml\ngateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      password: dummy_pw  # This will be overridden\n</code></pre> <pre><code># Override with environment variable\nexport VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre>"},{"location":"configurations/options/variables/#dynamic-configuration","title":"Dynamic Configuration","text":""},{"location":"configurations/options/variables/#user-based-target-environment","title":"User-based Target Environment","text":"<p>Use the <code>{{ user() }}</code> function to dynamically set configuration based on the current user:</p> YAMLPython <pre><code># Each user gets their own dev environment\ndefault_target_environment: dev_{{ user() }}\n</code></pre> <pre><code>import getpass\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre> <p>This allows running <code>vulcan plan</code> instead of <code>vulcan plan dev_username</code>.</p>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#cookbook","title":"Cookbook","text":"<p>Coming soon...</p>"},{"location":"examples/overview/","title":"Orders360 - Postgres Mini Example","text":""},{"location":"examples/overview/#orders360-postgres-mini-example","title":"Orders360 - Postgres Mini Example","text":"<p>A minimal e-commerce data product demonstrating Vulcan's core capabilities with a simple orders, customers, and products domain.</p>"},{"location":"examples/overview/#objective","title":"Objective","text":"<p>Build a lightweight sales analytics pipeline that: - Ingests raw transactional data (customers, orders, products) - Transforms into daily sales aggregations - Exposes semantic layer for BI tools and analytics</p>"},{"location":"examples/overview/#user-story","title":"User Story","text":"<p>As a Sales Operations Manager, I want a daily view of order volumes and revenue, So that I can track sales performance and identify trends.</p>"},{"location":"examples/overview/#business-context","title":"Business Context","text":"<p>The marketing team runs campaigns across multiple channels and needs to measure their impact on daily sales. Currently, they rely on manual spreadsheet exports that are error-prone and outdated by the time they're reviewed.</p>"},{"location":"examples/overview/#key-questions-this-data-product-answers","title":"Key Questions This Data Product Answers","text":"<ol> <li> <p>How many orders did we process yesterday?     Query <code>total_orders</code> from <code>daily_sales</code></p> </li> <li> <p>What was our revenue for last week?     Use weekly granularity on <code>order_date</code> with <code>total_daily_revenue</code> measure</p> </li> <li> <p>Which days exceeded our $100 daily target?     Apply the <code>high_revenue_days</code> segment</p> </li> <li> <p>Are there any data quality issues in incoming orders?     Automated checks validate completeness, uniqueness, and value ranges</p> </li> </ol>"},{"location":"examples/overview/#stakeholders","title":"Stakeholders","text":"Role Usage Sales Manager Daily dashboard for revenue tracking Marketing Analyst Campaign performance correlation Finance Team Monthly revenue reconciliation Data Engineer Pipeline health monitoring"},{"location":"examples/overview/#project-structure","title":"Project Structure","text":"<pre><code>postgres-mini/\n seeds/                  # Raw CSV data files\n models/\n seeds/              # Seed model definitions (raw data ingestion)\n daily_sales.sql     # Aggregated daily sales model\n semantics/              # Semantic layer definitions\n checks/                 # Data quality checks (Soda-style)\n tests/                  # Unit tests for models\n audits/                 # Custom audit definitions\n config.yaml             # Project configuration\n</code></pre>"},{"location":"examples/overview/#data-models","title":"Data Models","text":""},{"location":"examples/overview/#seed-models-raw-layer","title":"Seed Models (Raw Layer)","text":"Model Description Grain <code>raw.raw_customers</code> Customer master data <code>customer_id</code> <code>raw.raw_orders</code> Order transactions <code>order_id</code> <code>raw.raw_products</code> Product catalog <code>product_id</code>"},{"location":"examples/overview/#transformation-models","title":"Transformation Models","text":"Model Description Grain Schedule <code>sales.daily_sales</code> Daily aggregated sales metrics <code>order_date</code> <code>@daily</code> <p>Daily Sales aggregates orders by date with: - <code>total_orders</code> - Count of orders per day - <code>total_revenue</code> - Sum of order amounts per day - <code>last_order_id</code> - Latest order ID processed</p>"},{"location":"examples/overview/#semantic-layer","title":"Semantic Layer","text":"<p>The semantic layer (<code>semantics/daily_sales.yml</code>) provides:</p> <p>Dimensions with time granularities: - <code>order_date</code> (weekly, monthly, quarterly rollups)</p> <p>Measures: - <code>total_daily_orders</code> - Sum of orders across date range - <code>total_daily_revenue</code> - Sum of revenue across date range</p> <p>Segments: - <code>high_revenue_days</code> - Days with $100+ revenue</p>"},{"location":"examples/overview/#data-quality","title":"Data Quality","text":""},{"location":"examples/overview/#assertions-model-level","title":"Assertions (Model-level)","text":"<ul> <li>Unique grain validation</li> <li>Not-null constraints on required columns</li> <li>Positive value checks on amounts</li> </ul>"},{"location":"examples/overview/#checks-soda-style","title":"Checks (Soda-style)","text":"<ul> <li>Completeness checks (missing counts, row counts)</li> <li>Validity checks (negative values, data consistency)</li> <li>Anomaly detection on row counts</li> </ul>"},{"location":"examples/overview/#quick-start","title":"Quick Start","text":"<pre><code># Start infrastructure\nmake setup\n\n# plan to see changes\nvulcan plan\n</code></pre>"},{"location":"examples/overview/#configuration","title":"Configuration","text":"<ul> <li>Dialect: PostgreSQL</li> <li>State Store: PostgreSQL</li> </ul>"},{"location":"examples/exhaustive_model/","title":"B2B SaaS Example - Split Docker Compose Setup","text":""},{"location":"examples/exhaustive_model/#b2b-saas-example-split-docker-compose-setup","title":"B2B SaaS Example - Split Docker Compose Setup","text":"<p>This project uses split Docker Compose files for better service management and scalability. Services are organized into separate compose files that share a common external Docker network.</p>"},{"location":"examples/exhaustive_model/#setup","title":"Setup","text":"<p>You can use <code>make</code> commands for convenience, or run the docker compose commands directly.</p> <p>make setup </p> <p>make vulcan-up</p> <p>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev vulcan\"</p> <p>vulcan plan </p> <p>make vulcan-down </p> <p>make all-down</p> <p>vulcan transpile --format sql \"select measure(total_order_lines) from orders\"</p> <p>vulcan create_test ANALYTICS.ORDERS --query DEMO.RAW_DATA.ORDERS \"select ORDER_ID ,ORDER_DATE ,CUSTOMER_ID ,PRODUCT_ID ,QUANTITY ,UNIT_PRICE ,DISCOUNT ,TAX ,SHIPPING_COST ,TOTAL_AMOUNT FROM DEMO.RAW_DATA.ORDERS WHERE ORDER_DATE  BETWEEN '2025-01-01' and '2025-01-15'\"</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Coming soon...</p>"},{"location":"getting_started/","title":"Quickstart","text":""},{"location":"getting_started/#quickstart","title":"Quickstart","text":"<p>Welcome to the Vulcan quickstart! This guide will help you get up and running with Vulcan quickly.</p>"},{"location":"getting_started/#choose-your-path","title":"Choose Your Path","text":"<p>Vulcan can be run in different ways. Choose the option that works best for you:</p>"},{"location":"getting_started/#docker-recommended","title":"Docker (Recommended)","text":"<p>Best for: Getting started quickly, consistent environments, production-like setup</p> <p>The Docker approach uses Docker Compose to set up all necessary infrastructure and services. This is the fastest way to start working with Vulcan and provides a production-like environment.</p> <p>\ud83d\udc49 Start with Docker Quickstart</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure your system meets the prerequisites for using Vulcan.</p>"},{"location":"getting_started/cli/","title":"CLI","text":""},{"location":"getting_started/cli/#cli","title":"CLI","text":"<p>In this quickstart, you'll use the Vulcan command line interface (CLI) to get up and running with Vulcan's scaffold generator.</p> <p>It will create an example project that runs locally on your computer using DuckDB as an embedded SQL engine.</p> <p>Before beginning, ensure that you meet all the prerequisites for using Vulcan.</p> <p>Using Docker?</p> <p>If you're using the Docker installation method (recommended), you'll need to run the <code>vulcan</code> commands inside the Docker shell. </p> <p>Start the Docker shell with: </p><pre><code>make vulcan-shell\n</code></pre> or <pre><code>docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell\n</code></pre><p></p> <p>Alternatively, you can create a temporary alias: </p><pre><code>alias vulcan=\"docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell vulcan\"\n</code></pre><p></p> <p>For a complete Docker setup guide, see the Docker Quickstart.</p> Learn more about the quickstart project structure <p>This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.</p> <p>The project contains three models with a CSV file as the only data source:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502seed_data.csv\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n             \u2502\n            \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502seed_model.sql\u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                          \u2502\n                         \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502incremental_model.sql\u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518\n                                              \u2502\n                                             \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                             \u2502full_model.sql\u2502\n                                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.</p>"},{"location":"getting_started/cli/#1-create-the-vulcan-project","title":"1. Create the Vulcan project","text":"<p>First, create a project directory and navigate to it:</p> <p></p><pre><code>mkdir vulcan-example\n</code></pre> <pre><code>cd vulcan-example\n</code></pre><p></p> <p>Docker Users</p> <p>If you're using Docker, make sure your Docker infrastructure is set up and running. See the Docker Quickstart for setup instructions. Then access the Vulcan shell before running the commands below.</p> <p>Python Library Users (Coming Soon)</p> <p>If using a Python virtual environment (when available), ensure it's activated first by running the <code>source .venv/bin/activate</code> command.</p>"},{"location":"getting_started/cli/#11-initialize-the-project","title":"1.1 Initialize the project","text":"<p>Vulcan includes a scaffold generator to initialize a new Vulcan project.</p> <p>The scaffold generator will ask you some questions and create a Vulcan configuration file based on your responses.</p> <p>Depending on your answers, it will also create multiple files for the Vulcan example project used in this quickstart.</p> <p>Start the scaffold generator by executing the <code>vulcan init</code> command:</p> <pre><code>vulcan init\n</code></pre> Skip the questions <p>If you don't want to use the interactive scaffold generator, you can initialize your project with arguments to the <code>vulcan init</code> command.</p> <p>The only required argument is <code>engine</code>, which specifies the SQL engine your project will use. Specify one of the engine <code>type</code>s from the supported execution engines.</p> <p>In this example, we specify the <code>duckdb</code> engine:</p> <pre><code>vulcan init duckdb\n</code></pre> <p>The scaffold will include a Vulcan configuration file and example project directories and files. You're now ready to continue the quickstart below.</p>"},{"location":"getting_started/cli/#project-type","title":"Project type","text":"<p>The first question asks about the type of project you want to create. Enter the number corresponding to the type of project you want to create and press <code>Enter</code>.</p> <pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nWelcome to Vulcan!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nWhat type of project do you want to set up?\n\n    [1] DEFAULT - Create Vulcan example project models and files\n    [2] dbt     - You have an existing dbt project and want to run it with Vulcan\n    [3] EMPTY   - Create a Vulcan configuration file and project directories only\n\nEnter a number: 1\n</code></pre> <p>For this quickstart, choose the <code>DEFAULT</code> option <code>1</code> so the example project files are included in the project directories.</p>"},{"location":"getting_started/cli/#sql-engine","title":"SQL engine","text":"<p>The second question asks which SQL engine your project will use. Vulcan will include that engine's connection settings in the configuration file, which you will fill in later to connect your project to the engine.</p> <p>For this quickstart, choose the <code>DuckDB</code> option <code>1</code> so we can run the example project with the built-in DuckDB engine that doesn't need additional configuration.</p> <pre><code>Choose your SQL engine:\n\n    [1]  DuckDB\n    [2]  Snowflake\n    [3]  Databricks\n    [4]  BigQuery\n    [5]  MotherDuck\n    [6]  ClickHouse\n    [7]  Redshift\n    [8]  Spark\n    [9]  Trino\n    [10] Azure SQL\n    [11] MSSQL\n    [12] Postgres\n    [13] GCP Postgres\n    [14] MySQL\n    [15] Athena\n    [16] RisingWave\n\nEnter a number: 1\n</code></pre>"},{"location":"getting_started/cli/#cli-mode","title":"CLI mode","text":"<p>Vulcan's core commands have multiple options that alter their behavior. Some of those options streamline the Vulcan <code>plan</code> workflow and CLI output.</p> <p>If you prefer a streamlined workflow (no prompts, no file diff previews, auto-apply changes), choose the <code>FLOW</code> CLI mode to automatically include those options in your project configuration file.</p> <p>If you prefer to see all the output Vulcan provides, choose <code>DEFAULT</code> mode, which we will use in this quickstart:</p> <pre><code>Choose your Vulcan CLI experience:\n\n    [1] DEFAULT - See and control every detail\n    [2] FLOW    - Automatically run changes and show summary output\n\nEnter a number: 1\n</code></pre>"},{"location":"getting_started/cli/#ready-to-go","title":"Ready to go","text":"<p>Your project is now ready to go, and Vulcan displays a message with some good next steps.</p> <p>If you chose the DuckDB engine, you're ready to move forward and run the example project with DuckDB.</p> <p>If you chose a different engine, add your engine's connection information to the <code>config.yaml</code> file before you run any additional Vulcan commands.</p> <pre><code>Your Vulcan project is ready!\n\nNext steps:\n- Update your gateway connection settings (e.g., username/password) in the project configuration file:\n    /vulcan-example/config.yaml\n- Run command in CLI: vulcan plan\n- (Optional) Explain a plan: vulcan plan --explain\n\nQuickstart guide:\nhttps://vulcan.readthedocs.io/en/stable/quickstart/cli/\n\nNeed help?\n- Docs:   https://vulcan.readthedocs.io\n- Slack:  https://www.tobikodata.com/slack\n- GitHub: https://github.com/TobikoData/vulcan/issues\n</code></pre> Learn more about the project's configuration: <code>config.yaml</code> <p>Vulcan project-level configuration parameters are specified in the <code>config.yaml</code> file in the project directory.</p> <p>This example project uses the embedded DuckDB SQL engine, so its configuration specifies <code>duckdb</code> as the gateway's connection type. All available configuration settings are included in the file, with optional settings set to their default value and commented out.</p> <p>Vulcan requires a default model SQL dialect. Vulcan automatically specifies the SQL dialect for your project's SQL engine, which it places in the config <code>model_defaults</code> <code>dialect</code> key. In this example, we specified the DuckDB engine, so <code>duckdb</code> is the default SQL dialect:</p> <pre><code># --- Gateway Connection ---\ngateways:\n  duckdb:\n    connection:\n      # For more information on configuring the connection to your execution engine, visit:\n      # https://vulcan.readthedocs.io/en/stable/reference/configuration/#connection\n      # https://vulcan.readthedocs.io/en/stable/integrations/engines/duckdb/#connection-options\n      #\n      type: duckdb               # &lt;-- DuckDB engine\n      database: db.db\n      # concurrent_tasks: 1\n      # register_comments: True  # &lt;-- Optional setting `register_comments` has a default value of True\n      # pre_ping: False\n      # pretty_sql: False\n      # catalogs:                # &lt;-- Optional setting `catalogs` has no default value\n      # extensions:\n      # connector_config:\n      # secrets:\n      # token:\n\ndefault_gateway: duckdb\n\n# --- Model Defaults ---\n# https://vulcan.readthedocs.io/en/stable/reference/model_configuration/#model-defaults\n\nmodel_defaults:\n  dialect: duckdb                # &lt;-- Models written in DuckDB SQL dialect by default\n  start: 2025-06-12 # Start date for backfill history\n  cron: '@daily'    # Run models daily at 12am UTC (can override per model)\n\n# --- Linting Rules ---\n# Enforce standards for your team\n# https://vulcan.readthedocs.io/en/stable/guides/linter/\n\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre> <p>Learn more about Vulcan project configuration here.</p> <p>The scaffold generator creates multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).</p> Learn more about the project directories and files <p>Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.</p> <p>The scaffold generator will create the following configuration file and directories:</p> <ul> <li>config.yaml<ul> <li>The file for project configuration. More info about configuration here.</li> </ul> </li> <li>./models<ul> <li>SQL and Python models. More info about models here.</li> </ul> </li> <li>./seeds<ul> <li>Seed files. More info about seeds here.</li> </ul> </li> <li>./audits<ul> <li>Shared audit files. More info about audits here.</li> </ul> </li> <li>./tests<ul> <li>Unit test files. More info about tests here.</li> </ul> </li> <li>./macros<ul> <li>Macro files. More info about macros here.</li> </ul> </li> </ul> <p>It will also create the files needed for this quickstart example:</p> <ul> <li>./models<ul> <li>full_model.sql</li> <li>incremental_model.sql</li> <li>seed_model.sql</li> </ul> </li> <li>./seeds<ul> <li>seed_data.csv</li> </ul> </li> <li>./audits<ul> <li>assert_positive_order_ids.sql</li> </ul> </li> <li>./tests<ul> <li>test_full_model.yaml</li> </ul> </li> </ul> <p>Finally, the scaffold generator creates data for the example project to use.</p> Learn more about the project's data <p>The data used in this example project is contained in the <code>seed_data.csv</code> file in the <code>/seeds</code> project directory. The data reflects sales of 3 items over 7 days in January 2020.</p> <p>The file contains three columns, <code>id</code>, <code>item_id</code>, and <code>event_date</code>, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.</p> <p>This is the complete dataset:</p> id item_id event_date 1 2 2020-01-01 2 1 2020-01-01 3 3 2020-01-03 4 1 2020-01-04 5 1 2020-01-05 6 1 2020-01-06 7 1 2020-01-07"},{"location":"getting_started/cli/#2-create-a-prod-environment","title":"2. Create a prod environment","text":"<p>Vulcan's key actions are creating and applying plans to environments. At this point, the only environment is the empty <code>prod</code> environment.</p> Learn more about Vulcan plans and environments <p>Vulcan's key actions are creating and applying plans to environments.</p> <p>A Vulcan environment is an isolated namespace containing models and the data they generated.</p> <p>The most important environment is <code>prod</code> (\"production\"), which consists of the databases behind the applications your business uses to operate each day. Environments other than <code>prod</code> provide a place where you can test and preview changes to model code before they go live and affect business operations.</p> <p>A Vulcan plan contains a comparison of one environment to another and the set of changes needed to bring them into alignment.</p> <p>For example, if a new SQL model was added, tested, and run in the <code>dev</code> environment, it would need to be added and run in the <code>prod</code> environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.</p> <p>Breaking changes are those that invalidate data already existing in an environment. For example, if a <code>WHERE</code> clause was added to a model in the <code>dev</code> environment, existing data created by that model in the <code>prod</code> environment are now invalid because they may contain rows that would be filtered out by the new <code>WHERE</code> clause.</p> <p>Other changes, like adding a new column to a model in <code>dev</code>, are non-breaking because all the existing data in <code>prod</code> are still valid to use - only new data must be added to align the environments.</p> <p>After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to \"backfill\" data to apply the plan. (In this context, backfill is a generic term for updating or adding to a table's data, including an initial load or full refresh.)</p> Learn more about a plan's actions: <code>vulcan plan --explain</code> <p>Before applying a plan, you can view a detailed description of the actions it will take by passing the explain flag in your <code>vulcan plan</code> command:</p> <pre><code>vulcan plan --explain\n</code></pre> <p>Passing the explain flag for the quickstart example project above adds the following information to the output:</p> <pre><code>Explained plan\n\u251c\u2500\u2500 Validate SQL and create physical layer tables and views if they do not exist\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502       \u2514\u2500\u2500 Create table if it doesn't exist\n\u251c\u2500\u2500 Backfill models by running their queries and run standalone audits\n\u2502   \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u2502   \u2514\u2500\u2500 Fully refresh table\n\u2502   \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n\u2502       \u2514\u2500\u2500 Fully refresh table\n\u2514\u2500\u2500 Update the virtual layer for environment 'prod'\n    \u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n        \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n        \u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n        \u2514\u2500\u2500 vulcan_example.incremental_model -&gt; db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781\n</code></pre> <p>The explanation has three top-level sections, corresponding to the three types of actions a plan takes:</p> <ul> <li>Validate SQL and create physical layer tables and views if they do not exist</li> <li>Backfill models by running their queries and run standalone audits</li> <li>Update the virtual layer for environment 'prod'</li> </ul> <p>Each section lists the affected models and provides more information about what will occur. For example, the first model in the first section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.seed_model -&gt; db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172\n\u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n</code></pre> <p>The first line shows the model name <code>vulcan_example.seed_model</code> and the physical layer table Vulcan will create to store its data: <code>db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172</code>. The second and third lines tell us that in this step Vulcan will dry-run the model query and create the physical layer table if it doesn't exist.</p> <p>The second section describes what will occur during the backfill step. The second model in this section is:</p> <pre><code>\u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n\u2502   \u251c\u2500\u2500 Fully refresh table\n\u2502   \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n</code></pre> <p>The first line shows the model name <code>vulcan_example.full_model</code> and the physical layer table Vulcan will insert the model's data into: <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>. The second and third lines tell us that the backfill action will fully refresh the model's physical table and run the <code>assert_positive_order_ids</code> audit.</p> <p>The final section describes Vulcan's action during the virtual layer update step. The first model in this section is:</p> <pre><code>\u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables and views\n    \u251c\u2500\u2500 vulcan_example.full_model -&gt; db.vulcan__vulcan_example.vulcan_example__full_model__2278521865\n</code></pre> <p>The virtual layer step will update the <code>vulcan_example.full_model</code> virtual layer view to <code>SELECT * FROM</code> the physical table <code>db.vulcan__vulcan_example.vulcan_example__full_model__2278521865</code>.</p> <p>The first Vulcan plan must execute every model to populate the production environment. Running <code>vulcan plan</code> will generate the plan and the following output:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb in 0.1 seconds.\n----------------------------------------------------------------------\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u2514\u2500\u2500 vulcan_example.seed_model\nModels needing backfill:\n\u251c\u2500\u2500 vulcan_example.full_model: [full refresh]\n\u251c\u2500\u2500 vulcan_example.incremental_model: [2020-01-01 - 2025-06-22]\n\u2514\u2500\u2500 vulcan_example.seed_model: [full refresh]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 3 of the output notes that <code>vulcan plan</code> successfully executed the project's test <code>tests/test_full_model.yaml</code> with duckdb.</p> <p>Line 6 describes what environments the plan will affect when applied - a new <code>prod</code> environment in this case.</p> <p>Lines 8-12 of the output show that Vulcan detected three new models relative to the current empty environment.</p> <p>Lines 13-16 list each model that will be executed by the plan, along with the date intervals or refresh types. For both <code>full_model</code> and <code>seed_model</code>, it shows <code>[full refresh]</code>, while for <code>incremental_model</code> it shows a specific date range <code>[2020-01-01 - 2025-06-22]</code>. The incremental model date range begins from 2020-01-01 because its definition specifies a model start date of <code>2020-01-01</code>.</p> Learn more about the project's models <p>A plan's actions are determined by the kinds of models the project uses. This example project uses three model kinds:</p> <ol> <li><code>SEED</code> models read data from CSV files stored in the Vulcan project directory.</li> <li><code>FULL</code> models fully refresh (rewrite) the data associated with the model every time the model is run.</li> <li><code>INCREMENTAL_BY_TIME_RANGE</code> models use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.</li> </ol> <p>We now briefly review each model in the project.</p> <p>The first model is a <code>SEED</code> model that imports <code>seed_data.csv</code>. This model consists of only a <code>MODEL</code> statement because <code>SEED</code> models do not query a database.</p> <p>In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the <code>grain</code> of the model to the columns that collectively form the model's unique identifier, <code>id</code> and <code>event_date</code>.</p> <pre><code>MODEL (\n  name vulcan_example.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INTEGER,\n    item_id INTEGER,\n    event_date DATE\n  ),\n  grain (id, event_date)\n);\n</code></pre> <p>The second model is an <code>INCREMENTAL_BY_TIME_RANGE</code> model that includes both a <code>MODEL</code> statement and a SQL query selecting from the first seed model.</p> <p>The <code>MODEL</code> statement's <code>kind</code> property includes the required specification of the data column containing each record's timestamp. It also includes the optional <code>start</code> property specifying the earliest date/time for which the model should process data and the <code>cron</code> property specifying that the model should run daily. It sets the model's grain to columns <code>id</code> and <code>event_date</code>.</p> <p>The SQL query includes a <code>WHERE</code> clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre> <p>The final model in the project is a <code>FULL</code> model. In addition to properties used in the other models, its <code>MODEL</code> statement includes the <code>audits</code> property. The project includes a custom <code>assert_positive_order_ids</code> audit in the project <code>audits</code> directory; it verifies that all <code>item_id</code> values are positive numbers. It will be run every time the model is executed.</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grain item_id,\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>Line 18 asks you whether to proceed with executing the model backfills described in lines 13-16. Enter <code>y</code> and press <code>Enter</code>, and Vulcan will execute the models and return this output:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example.seed_model          [insert seed file]                 0.01s\n[1/1] vulcan_example.incremental_model   [insert 2020-01-01 - 2025-06-22]   0.01s\n[1/1] vulcan_example.full_model          [full refresh, audits \u27141]          0.01s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Vulcan performs three actions when applying the plan:</p> <ul> <li>Creating and storing new versions of the models</li> <li>Evaluating/running the models</li> <li>Virtually updating the plan's target environment</li> </ul> <p>Lines 2-4 show the progress and completion of the first step - updating the physical layer (creating new model versions).</p> <p>Lines 6-11 show the execution of each model with their specific operations and timing. Line 6 shows the seed model being inserted, line 8 shows the incremental model being inserted for the specified date range, and line 10 shows the full model being processed with its audit check passing.</p> <p>Lines 12-14 show the progress and completion of the second step - executing model batches.</p> <p>Lines 16-18 show the progress and completion of the final step - virtually updating the plan's target environment, which makes the data available for querying.</p> <p>Let's take a quick look at the project's DuckDB database file to see the objects Vulcan created. First, we open the built-in DuckDB CLI tool with the <code>duckdb db.db</code> command, then run our two queries.</p> <p>Our first query shows the three physical tables Vulcan created in the <code>vulcan__vulcan_example</code> schema (one table for each model):</p> <p></p> <p>Our second query shows that in the <code>vulcan</code> schema Vulcan created three virtual layer views that read from the three physical tables:</p> <p></p> <p>You've now created a new production environment with all of history backfilled!</p>"},{"location":"getting_started/cli/#3-update-a-model","title":"3. Update a model","text":"<p>Now that we have populated the <code>prod</code> environment, let's modify one of the SQL models.</p> <p>We modify the incremental SQL model by adding a new column to the query. Open the <code>models/incremental_model.sql</code> file and add <code>'z' AS new_column</code> below <code>item_id</code> as follows:</p> <pre><code>MODEL (\n  name vulcan_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grain (id, event_date)\n);\n\nSELECT\n  id,\n  item_id,\n  'z' AS new_column, -- Added column\n  event_date,\nFROM\n  vulcan_example.seed_model\nWHERE\n  event_date between @start_date and @end_date\n</code></pre>"},{"location":"getting_started/cli/#4-work-with-a-development-environment","title":"4. Work with a development environment","text":""},{"location":"getting_started/cli/#41-create-a-dev-environment","title":"4.1 Create a dev environment","text":"<p>Now that you've modified a model, it's time to create a development environment so that you can validate the model change without affecting production.</p> <p>Run <code>vulcan plan dev</code> to create a development environment called <code>dev</code>:</p> <pre><code>$ vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nNew environment `dev` will be created from `prod`\n\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example__dev.incremental_model\n(Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.incremental_model: [2020-01-01 - 2025-04-17]\nApply - Backfill Tables [y/n]:\n</code></pre> <p>Line 6 of the output states that a new environment <code>dev</code> will be created from the existing <code>prod</code> environment.</p> <p>Lines 10-15 summarize the differences between the modified model and the <code>prod</code> environment, detecting that we directly modified <code>incremental_model</code> and that <code>full_model</code> was indirectly modified because it selects from the incremental model. Note that the model schemas are <code>vulcan_example__dev</code>, indicating that they are being created in the <code>dev</code> environment.</p> <p>On line 31, we see that Vulcan automatically classified the change as <code>Non-breaking</code> because it understood that the change was additive (added a column not used by <code>full_model</code>) and did not invalidate any data already in <code>prod</code>.</p> <p>Enter <code>y</code> at the prompt and press <code>Enter</code> to apply the plan and execute the backfill:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n\nUpdating physical layer \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Physical layer updated\n\n[1/1] vulcan_example__dev.incremental_model  [insert 2020-01-01 - 2025-04-17] 0.03s\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00\n\n\u2714 Model batches executed\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Lines 3-5 show the progress and completion of updating the physical layer.</p> <p>Line 7 shows that Vulcan applied the change and evaluated <code>vulcan_example__dev.incremental_model</code> for the date range from 2020-01-01 to 2025-04-17.</p> <p>Lines 9-11 show the progress and completion of executing model batches.</p> <p>Lines 13-15 show the progress and completion of updating the virtual layer.</p> <p>Vulcan did not need to backfill anything for the <code>full_model</code> since the change was <code>Non-breaking</code>.</p>"},{"location":"getting_started/cli/#42-validate-updates-in-dev","title":"4.2 Validate updates in dev","text":"<p>You can now view this change by querying data from <code>incremental_model</code> with <code>vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"</code>.</p> <p>Note that the environment name <code>__dev</code> is appended to the schema namespace <code>vulcan_example</code> in the query:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example__dev.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre> <p>You can see that <code>new_column</code> was added to the dataset. The production table was not modified; you can validate this by querying the production table using <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>.</p> <p>Note that nothing has been appended to the schema namespace <code>vulcan_example</code> in this query because <code>prod</code> is the default environment.</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id   event_date\n0   1        2   2020-01-01\n1   2        1   2020-01-01\n2   3        3   2020-01-03\n3   4        1   2020-01-04\n4   5        1   2020-01-05\n5   6        1   2020-01-06\n6   7        1   2020-01-07\n</code></pre> <p>The production table does not have <code>new_column</code> because the changes to <code>dev</code> have not yet been applied to <code>prod</code>.</p>"},{"location":"getting_started/cli/#5-update-the-prod-environment","title":"5. Update the prod environment","text":""},{"location":"getting_started/cli/#51-apply-updates-to-prod","title":"5.1 Apply updates to prod","text":"<p>Now that we've tested the changes in dev, it's time to move them to production. Run <code>vulcan plan</code> to plan and apply your changes to the <code>prod</code> environment.</p> <p>Enter <code>y</code> and press <code>Enter</code> at the <code>Apply - Virtual Update [y/n]:</code> prompt to apply the plan and execute the backfill:</p> <pre><code>$ vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 vulcan_example.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example.full_model\n\n---\n\n+++\n\n@@ -14,6 +14,7 @@\n\n SELECT\n   id,\n   item_id,\n+  'z' AS new_column,\n   event_date\n FROM vulcan_example.seed_model\n WHERE\n\nDirectly Modified: vulcan_example.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example.full_model (Indirect Non-breaking)\nApply - Virtual Update [y/n]: y\n\nSKIP: No physical layer updates to perform\n\nSKIP: No model batches to execute\n\nUpdating virtual layer  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:00\n\n\u2714 Virtual layer updated\n</code></pre> <p>Note that a backfill was not necessary and only a Virtual Update occurred, as indicated by the \"SKIP: No physical layer updates to perform\" and \"SKIP: No model batches to execute\" messages. This is because the changes were already calculated and executed in the <code>dev</code> environment, and Vulcan is smart enough to recognize that it only needs to update the virtual references to the existing tables rather than recomputing everything.</p>"},{"location":"getting_started/cli/#52-validate-updates-in-prod","title":"5.2 Validate updates in prod","text":"<p>Double-check that the data updated in <code>prod</code> by running <code>vulcan fetchdf \"select * from vulcan_example.incremental_model\"</code>:</p> <pre><code>$ vulcan fetchdf \"select * from vulcan_example.incremental_model\"\n\n   id  item_id new_column  event_date\n0   1        2          z  2020-01-01\n1   2        1          z  2020-01-01\n2   3        3          z  2020-01-03\n3   4        1          z  2020-01-04\n4   5        1          z  2020-01-05\n5   6        1          z  2020-01-06\n6   7        1          z  2020-01-07\n</code></pre>"},{"location":"getting_started/cli/#6-next-steps","title":"6. Next steps","text":"<p>Congratulations, you've now conquered the basics of using Vulcan!</p> <p>From here, you can:</p> <ul> <li>Learn more about Vulcan CLI commands</li> <li>Set up a connection to a database or SQL engine</li> <li>Learn more about Vulcan concepts</li> <li>Join our Slack community</li> </ul>"},{"location":"getting_started/prerequisites/","title":"Prerequisites","text":""},{"location":"getting_started/prerequisites/#prerequisites","title":"Prerequisites","text":"<p>This page describes the system prerequisites needed to run Vulcan and provides instructions for meeting them.</p>"},{"location":"getting_started/prerequisites/#docker-prerequisites","title":"Docker Prerequisites","text":"<p>The recommended way to run Vulcan is using Docker. This provides a consistent, production-like environment.</p>"},{"location":"getting_started/prerequisites/#docker-desktop","title":"Docker Desktop","text":"<p>You'll need Docker Desktop installed and running on your machine:</p> <ul> <li>macOS: Download from Docker Desktop for Mac</li> <li>Windows: Download from Docker Desktop for Windows</li> <li>Linux: Install Docker Engine and Docker Compose following the official Docker installation guide</li> </ul>"},{"location":"getting_started/prerequisites/#verify-docker-installation","title":"Verify Docker Installation","text":"<p>Verify that Docker is installed and running:</p> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>You should see version numbers for both commands. If Docker Desktop is running, you should also be able to run:</p> <pre><code>docker ps\n</code></pre>"},{"location":"getting_started/prerequisites/#system-requirements","title":"System Requirements","text":"<ul> <li>RAM: At least 4GB of available RAM (8GB recommended)</li> <li>Disk Space: At least 5GB of free disk space</li> <li>CPU: Modern multi-core processor</li> </ul>"},{"location":"getting_started/prerequisites/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is included with Docker Desktop. If you're on Linux and need to install it separately, follow the Docker Compose installation guide.</p>"},{"location":"getting_started/prerequisites/#python-prerequisites","title":"Python Prerequisites","text":"<p>Python Library Installation</p> <p>Vulcan will be available as a Python library in the future. When available, you'll need Python 3.8 or higher.</p> <p>For now, use the Docker installation method above.</p>"},{"location":"guides/data_quality/","title":"Data Quality","text":""},{"location":"guides/data_quality/#data-quality","title":"Data Quality","text":"<p>This guide explains how to use Audits, Checks, and Tests together to ensure data quality in your Orders360 project. Learn when to use each tool and see complex examples where they work together.</p>"},{"location":"guides/data_quality/#the-three-layer-quality-strategy","title":"The Three-Layer Quality Strategy","text":"<pre><code>flowchart TB\n    subgraph \"Layer 1: Audits - Critical Blocking\"\n        AUDIT[Audits&lt;br/&gt;Block invalid data&lt;br/&gt;Run with model]\n        EXAMPLES1[\"\u2022 Primary keys unique&lt;br/&gt;\u2022 Revenue non-negative&lt;br/&gt;\u2022 Foreign keys valid\"]\n    end\n\n    subgraph \"Layer 2: Checks - Monitoring\"\n        CHECK[Checks&lt;br/&gt;Track quality trends&lt;br/&gt;Non-blocking]\n        EXAMPLES2[\"\u2022 Row count anomalies&lt;br/&gt;\u2022 Completeness trends&lt;br/&gt;\u2022 Cross-model validation\"]\n    end\n\n    subgraph \"Layer 3: Tests - Logic Validation\"\n        TEST[Tests&lt;br/&gt;Validate transformations&lt;br/&gt;Unit testing]\n        EXAMPLES3[\"\u2022 SQL logic correct&lt;br/&gt;\u2022 Expected outputs&lt;br/&gt;\u2022 Edge cases\"]\n    end\n\n    AUDIT --&gt; EXAMPLES1\n    CHECK --&gt; EXAMPLES2\n    TEST --&gt; EXAMPLES3\n\n    style AUDIT fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style TEST fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>When to Use Each:</p> Tool Purpose Blocks Pipeline? Best For Audits Critical validation \u2705 Yes (always) Business rules, data integrity Checks Quality monitoring \u274c No Trends, anomalies, monitoring Tests Logic validation \u274c No SQL correctness, edge cases <p>[Screenshot: Visual comparison of the three quality tools]</p>"},{"location":"guides/data_quality/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/data_quality/#audits-critical-blocking-validation","title":"Audits: Critical Blocking Validation","text":"<p>Use audits when: Data must be correct or the pipeline should stop.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Primary key validation\n    not_null(columns := (order_date)),\n    unique_values(columns := (order_date)),\n\n    -- Business rules\n    positive_values(column := total_revenue),\n    accepted_range(column := total_orders, min_v := 0, max_v := 10000)\n  )\n);\n</code></pre> <p>Characteristics: - \u2705 Always blocking - fails stop execution - \u2705 Run automatically with model execution - \u2705 Fast feedback during development - \u2705 Perfect for critical business rules</p> <p>[Screenshot: Audit failure blocking plan execution]</p>"},{"location":"guides/data_quality/#checks-quality-monitoring","title":"Checks: Quality Monitoring","text":"<p>Use checks when: You want to monitor trends and detect anomalies over time.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n</code></pre> <p>Characteristics: - \u2705 Non-blocking - warnings, not failures - \u2705 Historical tracking - see trends over time - \u2705 Anomaly detection - statistical analysis - \u2705 Perfect for monitoring and alerting</p> <p>[Screenshot: Check results showing trends over time]</p>"},{"location":"guides/data_quality/#tests-logic-validation","title":"Tests: Logic Validation","text":"<p>Use tests when: You need to validate SQL transformations and edge cases.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_aggregation\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n</code></pre> <p>Characteristics: - \u2705 Unit testing for SQL logic - \u2705 Validates expected outputs - \u2705 Tests edge cases - \u2705 Perfect for development</p> <p>[Screenshot: Test execution showing pass/fail results]</p>"},{"location":"guides/data_quality/#complex-example-orders360-daily-sales","title":"Complex Example: Orders360 Daily Sales","text":"<p>Let's see how all three tools work together for the <code>sales.daily_sales</code> model:</p>"},{"location":"guides/data_quality/#the-model","title":"The Model","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  assertions (\n    -- Audit: Critical validations\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    unique_values(columns := (order_date)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue),\n    accepted_range(column := total_revenue, min_v := 0, max_v := 1000000)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"guides/data_quality/#layer-1-audits-critical-blocking","title":"Layer 1: Audits (Critical Blocking)","text":"<p>Why: These rules must never fail. Invalid data should not flow downstream.</p> <pre><code>-- audits/revenue_consistency.sql\nAUDIT (name assert_revenue_consistency);\n-- Ensure revenue matches sum of individual orders\nSELECT \n  ds.order_date,\n  ds.total_revenue,\n  SUM(o.total_amount) as calculated_revenue\nFROM @this_model ds\nJOIN raw.raw_orders o ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 0.01;\n</code></pre> <p>Attach to model: </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- ... other audits ...\n    assert_revenue_consistency  -- Custom audit\n  )\n);\n</code></pre><p></p> <p>[Screenshot: Audit failure showing revenue mismatch]</p>"},{"location":"guides/data_quality/#layer-2-checks-monitoring","title":"Layer 2: Checks (Monitoring)","text":"<p>Why: Monitor trends and detect anomalies without blocking the pipeline.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    # Completeness: Ensure data exists\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n            severity: error\n\n      - missing_count(order_date) = 0:\n          name: no_missing_dates\n          attributes:\n            description: \"All dates must be present\"\n\n    # Validity: Check data ranges\n    validity:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            SELECT order_date, total_revenue\n            FROM sales.daily_sales\n            WHERE total_revenue &gt; 500000 OR total_revenue &lt; 0\n          samples limit: 10\n          attributes:\n            description: \"Revenue outside expected range\"\n            severity: warning\n\n    # Accuracy: Anomaly detection\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n            severity: warning\n\n      - anomaly detection for total_orders:\n          name: order_count_anomaly\n          attributes:\n            description: \"Detect unusual order volume\"\n\n    # Consistency: Cross-model validation\n    consistency:\n      - failed rows:\n          name: revenue_mismatch_with_raw\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue as daily_revenue,\n              SUM(o.total_amount) as raw_revenue\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 1.0\n          samples limit: 5\n          attributes:\n            description: \"Daily revenue should match sum of raw orders\"\n            severity: error\n\n    # Timeliness: Check data freshness\n    timeliness:\n      - change for row_count &gt;= -20%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if daily records drop more than 20%\"\n            severity: warning\n</code></pre> <p>[Screenshot: Check dashboard showing trends and anomalies]</p>"},{"location":"guides/data_quality/#layer-3-tests-logic-validation","title":"Layer 3: Tests (Logic Validation)","text":"<p>Why: Validate SQL logic and edge cases during development.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_single_order\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          customer_id: CUST-001\n          product_id: PROD-001\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n        last_order_id: ORD-001\n\n  - name: test_daily_sales_multiple_orders\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-15\n          total_amount: 50.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 3\n        total_revenue: 350.00\n        last_order_id: ORD-003\n\n  - name: test_daily_sales_empty_day\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders: []\n    outputs: []\n\n  - name: test_daily_sales_date_grouping\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15 10:00:00\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15 15:30:00\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-16 09:00:00\n          total_amount: 150.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 2\n        total_revenue: 300.00\n      - order_date: 2025-01-16\n        total_orders: 1\n        total_revenue: 150.00\n</code></pre> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/data_quality/#how-they-work-together","title":"How They Work Together","text":"<pre><code>flowchart TB\n    subgraph \"Development Workflow\"\n        DEV[Developer writes model]\n        TEST[Run Tests&lt;br/&gt;Validate logic]\n        PLAN[Run Plan&lt;br/&gt;Apply changes]\n    end\n\n    subgraph \"Execution Flow\"\n        EXEC[Model Executes]\n        AUDIT_RUN[Audits Run&lt;br/&gt;Block if fail]\n        CHECK_RUN[Checks Run&lt;br/&gt;Track trends]\n    end\n\n    subgraph \"Results\"\n        PASS[\u2705 Pass&lt;br/&gt;Data flows]\n        FAIL[\u274c Fail&lt;br/&gt;Pipeline stops]\n        TREND[\ud83d\udcca Trends&lt;br/&gt;Monitor quality]\n    end\n\n    DEV --&gt; TEST\n    TEST --&gt; PLAN\n    PLAN --&gt; EXEC\n    EXEC --&gt; AUDIT_RUN\n    EXEC --&gt; CHECK_RUN\n\n    AUDIT_RUN --&gt;|Pass| PASS\n    AUDIT_RUN --&gt;|Fail| FAIL\n    CHECK_RUN --&gt; TREND\n\n    style DEV fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style TEST fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style AUDIT_RUN fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CHECK_RUN fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PASS fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FAIL fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <p>Execution Order: 1. Tests run during development (validate logic) 2. Plan applies changes to environment 3. Model executes transformation 4. Audits run immediately (block if fail) 5. Checks run (track trends, don't block)</p> <p>[Screenshot: Complete workflow showing all three layers]</p>"},{"location":"guides/data_quality/#complex-scenario-revenue-validation","title":"Complex Scenario: Revenue Validation","text":"<p>Here's a complex example where audits and checks work together to validate revenue data:</p>"},{"location":"guides/data_quality/#the-problem","title":"The Problem","text":"<p>We need to ensure: 1. Critical: Revenue is always positive (audit - blocks) 2. Critical: Daily totals match raw order sums (audit - blocks) 3. Monitoring: Revenue trends are normal (check - warns) 4. Monitoring: Detect unusual spikes/drops (check - warns)</p>"},{"location":"guides/data_quality/#solution-combined-approach","title":"Solution: Combined Approach","text":"<p>Audits (Critical - Blocking): </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Basic validation\n    positive_values(column := total_revenue),\n    not_null(columns := (order_date, total_revenue)),\n\n    -- Complex validation: Revenue consistency\n    assert_revenue_matches_raw_orders\n  )\n);\n\n-- audits/revenue_matches_raw.sql\nAUDIT (name assert_revenue_matches_raw_orders);\nSELECT \n  ds.order_date,\n  ds.total_revenue as daily_total,\n  COALESCE(SUM(o.total_amount), 0) as raw_total,\n  ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) as difference\nFROM @this_model ds\nLEFT JOIN raw.raw_orders o \n  ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) &gt; 0.01;\n</code></pre><p></p> <p>Checks (Monitoring - Non-Blocking): </p><pre><code># checks/revenue_monitoring.yml\nchecks:\n  sales.daily_sales:\n    accuracy:\n      # Anomaly detection for revenue\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly_detection\n          attributes:\n            description: \"Detect statistically unusual revenue\"\n            severity: warning\n\n      # Trend monitoring\n      - change for total_revenue &gt;= 50%:\n          name: revenue_spike_alert\n          attributes:\n            description: \"Alert if revenue increases &gt;50% day-over-day\"\n            severity: warning\n\n      - change for total_revenue &lt;= -30%:\n          name: revenue_drop_alert\n          attributes:\n            description: \"Alert if revenue drops &gt;30% day-over-day\"\n            severity: error\n\n    consistency:\n      # Cross-model validation (non-blocking)\n      - failed rows:\n          name: revenue_vs_raw_check\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue,\n              SUM(o.total_amount) as raw_sum,\n              ABS(ds.total_revenue - SUM(o.total_amount)) as diff\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 10.0\n          samples limit: 5\n          attributes:\n            description: \"Monitor revenue consistency (wider tolerance than audit)\"\n            severity: warning\n</code></pre><p></p> <p>Why Both? - Audit: Stops pipeline if revenue is wrong (critical) - Check: Warns about trends and anomalies (monitoring) - Together: Critical issues blocked, trends monitored</p> <p>[Screenshot: Dashboard showing audit blocks vs check warnings]</p>"},{"location":"guides/data_quality/#running-quality-tools","title":"Running Quality Tools","text":""},{"location":"guides/data_quality/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\nvulcan test\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_single_order\n\n# Run tests matching pattern\nvulcan test tests/test_daily*\n</code></pre> <p>[Screenshot: Test execution output]</p>"},{"location":"guides/data_quality/#run-audits","title":"Run Audits","text":"<pre><code># Run all audits\nvulcan audit\n\n# Audits also run automatically with plan\nvulcan plan dev\n</code></pre> <p>[Screenshot: Audit execution output]</p>"},{"location":"guides/data_quality/#run-checks","title":"Run Checks","text":"<pre><code># Run all checks\nvulcan check\n\n# Run checks for specific model\nvulcan check --select sales.daily_sales\n\n# Checks also run automatically with plan/run\nvulcan plan dev\n</code></pre> <p>[Screenshot: Check execution output with trends]</p>"},{"location":"guides/data_quality/#best-practices","title":"Best Practices","text":""},{"location":"guides/data_quality/#do","title":"\u2705 DO:","text":"<ol> <li>Start with Audits - Add critical blocking validations first</li> <li>Add Checks Gradually - Monitor trends, then add anomaly detection</li> <li>Test During Development - Write tests before deploying</li> <li>Use Descriptive Names - Makes debugging easier</li> <li>Order Audits Efficiently - Fast checks first, slow checks last</li> </ol>"},{"location":"guides/data_quality/#dont","title":"\u274c DON'T:","text":"<ol> <li>Don't use Checks for Critical Rules - Use audits instead</li> <li>Don't Skip Audit Failures - Fix the root cause</li> <li>Don't Over-Audit - Focus on critical business rules</li> <li>Don't Ignore Check Trends - They indicate data quality issues</li> </ol>"},{"location":"guides/data_quality/#summary","title":"Summary","text":"<p>Three-Layer Strategy: - Audits = Critical blocking validation (must pass) - Checks = Quality monitoring (trends, anomalies) - Tests = Logic validation (development)</p> <p>Use Together: - Audits block invalid data - Checks monitor quality trends - Tests validate SQL logic</p> <p>Orders360 Example: - Audits ensure revenue is positive and matches raw data - Checks detect anomalies and trends - Tests validate aggregation logic</p>"},{"location":"guides/data_quality/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Built-in Audits</li> <li>Explore Check Dimensions</li> <li>Read about Testing</li> <li>See Orders360 Example for complete project</li> </ul>"},{"location":"guides/incremental_by_time/","title":"Incremental by Time","text":""},{"location":"guides/incremental_by_time/#incremental-by-time","title":"Incremental by Time","text":"<p>This guide explains how incremental by time models work in Vulcan using the Orders360 example project. You'll learn why they're efficient, how they process data, and how to create them.</p> <p>See the models guide for general model information or the model kinds page for all model types.</p>"},{"location":"guides/incremental_by_time/#why-use-incremental-models","title":"Why Use Incremental Models?","text":""},{"location":"guides/incremental_by_time/#the-problem-full-refreshes-are-expensive","title":"The Problem: Full Refreshes Are Expensive","text":"<p>Imagine you have a table with sales data from the last year (365 days). Every time you run a <code>FULL</code> model, it processes all 365 days:</p> <pre><code>flowchart LR\n    subgraph \"\ud83d\udd04 FULL Model - Every Run\"\n        FULL[FULL Model Run]\n        PROCESS[Process ALL 365 Days]\n        DAY1[Day 1 \u2705]\n        DAY2[Day 2 \u2705]\n        DAY3[Day 3 \u2705]\n        DOTS[...]\n        DAY365[Day 365 \u2705]\n    end\n\n    subgraph \"\ud83d\udcca Results\"\n        TIME1[\u23f1\ufe0f Time: 10 minutes]\n        COST1[\ud83d\udcb0 Cost: $10]\n        DATA1[\ud83d\udce6 All 365 days]\n    end\n\n    FULL --&gt; PROCESS\n    PROCESS --&gt; DAY1\n    PROCESS --&gt; DAY2\n    PROCESS --&gt; DAY3\n    PROCESS --&gt; DOTS\n    PROCESS --&gt; DAY365\n\n    DAY365 --&gt; TIME1\n    DAY365 --&gt; COST1\n    DAY365 --&gt; DATA1\n\n    style FULL fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style PROCESS fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style TIME1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing FULL model processing all 365 days]</p>"},{"location":"guides/incremental_by_time/#the-solution-only-process-whats-new","title":"The Solution: Only Process What's New","text":"<p>With incremental models, Vulcan only processes new or missing days:</p> <pre><code>flowchart LR\n    subgraph \"\u26a1 INCREMENTAL Model - Every Run\"\n        INCR[INCREMENTAL Model Run]\n        CHECK[Check State Database]\n        SKIP[Skip Days 1-364 \u2705]\n        PROCESS_NEW[Process Day 365 Only \ud83d\udd04]\n    end\n\n    subgraph \"\ud83d\udcca Results\"\n        TIME2[\u23f1\ufe0f Time: 30 seconds]\n        COST2[\ud83d\udcb0 Cost: $0.20]\n        DATA2[\ud83d\udce6 Only Day 365]\n    end\n\n    INCR --&gt; CHECK\n    CHECK --&gt; SKIP\n    CHECK --&gt; PROCESS_NEW\n\n    PROCESS_NEW --&gt; TIME2\n    PROCESS_NEW --&gt; COST2\n    PROCESS_NEW --&gt; DATA2\n\n    style INCR fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PROCESS_NEW fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TIME2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing incremental model processing only Day 365]</p> <p>Result: 50x faster and 50x cheaper! \ud83c\udf89</p>"},{"location":"guides/incremental_by_time/#how-incremental-models-work","title":"How Incremental Models Work","text":"<p>Incremental models use time intervals to track what's been processed. Think of it like a calendar where Vulcan checks off each day.</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udd04 Incremental Processing Flow\"\n        START[vulcan run]\n        CHECK[\ud83d\udd0d Check State Database&lt;br/&gt;What's already processed?]\n\n        subgraph \"\ud83d\udcca State Database\"\n            PROCESSED1[\u2705 Jan 1-7: Processed]\n            PROCESSED2[\u2705 Jan 8-14: Processed]\n            MISSING[\u274c Jan 15-21: Missing]\n        end\n\n        CALC[\ud83d\udcc5 Calculate Missing Intervals&lt;br/&gt;Jan 15-21 needs processing]\n        SET_MACROS[\ud83d\udd27 Set Macros&lt;br/&gt;@start_ds = '2025-01-15'&lt;br/&gt;@end_ds = '2025-01-21']\n        QUERY[\ud83d\udcbe Run Query&lt;br/&gt;WHERE order_date BETWEEN @start_ds AND @end_ds]\n        INSERT[\ud83d\udce5 Insert Results&lt;br/&gt;Into weekly_sales table]\n        UPDATE[\ud83d\udcbe Update State&lt;br/&gt;Mark Jan 15-21 as processed]\n    end\n\n    START --&gt; CHECK\n    CHECK --&gt; PROCESSED1\n    CHECK --&gt; PROCESSED2\n    CHECK --&gt; MISSING\n\n    MISSING --&gt; CALC\n    CALC --&gt; SET_MACROS\n    SET_MACROS --&gt; QUERY\n    QUERY --&gt; INSERT\n    INSERT --&gt; UPDATE\n\n    UPDATE --&gt; PROCESSED1\n    UPDATE --&gt; PROCESSED2\n\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESSED1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style PROCESSED2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style MISSING fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CALC fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style SET_MACROS fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style QUERY fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style INSERT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style UPDATE fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/incremental_by_time/#step-1-vulcan-checks-whats-already-done","title":"Step 1: Vulcan Checks What's Already Done","text":"<p>When you run <code>vulcan run</code>, Vulcan looks at your state database and asks:</p> <ul> <li>\"What dates have I already processed?\"</li> <li>\"What dates are missing?\"</li> </ul> <pre><code>State Database Check:\n\u2705 Jan 1-7:   Already processed\n\u2705 Jan 8-14:  Already processed  \n\u274c Jan 15-21: Missing - needs processing\n</code></pre> <p>[Screenshot: Visual diagram showing state database check with processed vs missing intervals]</p>"},{"location":"guides/incremental_by_time/#step-2-vulcan-processes-only-missing-intervals","title":"Step 2: Vulcan Processes Only Missing Intervals","text":"<p>Vulcan then processes only the missing dates:</p> <pre><code>Processing Jan 15-21:\n@start_ds = '2025-01-15'\n@end_ds   = '2025-01-21'\n\nQuery runs:\nSELECT ... FROM daily_sales\nWHERE order_date BETWEEN '2025-01-15' AND '2025-01-21'\n</code></pre> <p>[Screenshot: Visual showing how @start_ds and @end_ds are used in the query]</p>"},{"location":"guides/incremental_by_time/#step-3-results-are-inserted","title":"Step 3: Results Are Inserted","text":"<p>The processed data is inserted into your table, and Vulcan records that these dates are now complete:</p> <pre><code>\u2705 Jan 15-21: Now processed and recorded\n</code></pre> <p>[Screenshot: Visual showing data insertion and state update]</p>"},{"location":"guides/incremental_by_time/#understanding-time-intervals","title":"Understanding Time Intervals","text":"<p>Vulcan divides time into intervals based on your model's schedule.</p>"},{"location":"guides/incremental_by_time/#daily-intervals-example","title":"Daily Intervals Example","text":"<p>For a daily model (<code>cron '@daily'</code>), each day is one interval:</p> <pre><code>gantt\n    title Daily Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Jan 1 Complete     :done, interval1, 2025-01-01, 1d\n    Jan 2 Complete     :done, interval2, 2025-01-02, 1d\n    Jan 3 In Progress :active, interval3, 2025-01-03, 1d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 3, 2025 at 2pm\n\nIntervals:\n- Jan 1: \u2705 Complete (full day passed)\n- Jan 2: \u2705 Complete (full day passed)\n- Jan 3: \u23f3 In progress (day not finished yet)\n</code></pre> <p>[Screenshot: Calendar view showing daily intervals with Jan 1-2 complete, Jan 3 in progress]</p>"},{"location":"guides/incremental_by_time/#weekly-intervals-example","title":"Weekly Intervals Example","text":"<p>For a weekly model (<code>cron '@weekly'</code>), each week is one interval:</p> <pre><code>gantt\n    title Weekly Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Week 1 Jan 1-7     :done, week1, 2025-01-01, 7d\n    Week 2 Jan 8-14    :done, week2, 2025-01-08, 7d\n    Week 3 Jan 15-21   :active, week3, 2025-01-15, 7d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 15, 2025\n\nIntervals:\n- Week 1 (Jan 1-7):   \u2705 Complete\n- Week 2 (Jan 8-14):  \u2705 Complete\n- Week 3 (Jan 15-21): \u23f3 In progress\n</code></pre> <p>[Screenshot: Calendar view showing weekly intervals]</p>"},{"location":"guides/incremental_by_time/#how-vulcan-tracks-intervals","title":"How Vulcan Tracks Intervals","text":"<p>When you first run <code>vulcan plan</code> on an incremental model, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udcc5 First Plan - Jan 15, 2025\"\n        PLAN1[vulcan plan dev]\n        CALC1[\ud83d\udcca Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;3 weeks total]\n        PROCESS1[\ud83d\udd04 Process All Intervals&lt;br/&gt;Backfill everything]\n        RECORD1[\ud83d\udcbe Record in State DB&lt;br/&gt;Weeks 1-3 processed]\n\n        subgraph \"\ud83d\udcbe State Database After Plan\"\n            W1[\u2705 Week 1: Jan 1-7]\n            W2[\u2705 Week 2: Jan 8-14]\n            W3[\u2705 Week 3: Jan 15-21]\n        end\n    end\n\n    PLAN1 --&gt; CALC1\n    CALC1 --&gt; PROCESS1\n    PROCESS1 --&gt; RECORD1\n    RECORD1 --&gt; W1\n    RECORD1 --&gt; W2\n    RECORD1 --&gt; W3\n\n    style PLAN1 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CALC1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESS1 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RECORD1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates all intervals from the start date to now</li> <li>Processes all missing intervals (backfill)</li> <li>Records what was processed in the state database</li> </ol> <pre><code>First Plan (Jan 15, 2025):\n- Calculates: 3 weeks of intervals\n- Processes: All 3 weeks\n- Records: \"Weeks 1-3 processed\"\n\nState Database:\n\u2705 Week 1 (Jan 1-7)\n\u2705 Week 2 (Jan 8-14)\n\u2705 Week 3 (Jan 15-21)\n</code></pre> <p>[Screenshot: Visual showing first plan calculating and processing all intervals]</p> <p>When you run <code>vulcan run</code> later, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"\u26a1 Second Run - Jan 22, 2025\"\n        RUN2[vulcan run]\n        CALC2[\ud83d\udcca Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;4 weeks total]\n        CHECK[\ud83d\udd0d Check State DB&lt;br/&gt;What's already processed?]\n\n        subgraph \"\ud83d\udcbe Current State\"\n            W1_EXIST[\u2705 Week 1: Jan 1-7]\n            W2_EXIST[\u2705 Week 2: Jan 8-14]\n            W3_EXIST[\u2705 Week 3: Jan 15-21]\n            W4_MISS[\u274c Week 4: Jan 22-28]\n        end\n\n        PROCESS2[\ud83d\udd04 Process Only Week 4&lt;br/&gt;Skip Weeks 1-3]\n        RECORD2[\ud83d\udcbe Update State DB&lt;br/&gt;Week 4 now processed]\n\n        subgraph \"\ud83d\udcbe Updated State\"\n            W1_NEW[\u2705 Week 1: Jan 1-7]\n            W2_NEW[\u2705 Week 2: Jan 8-14]\n            W3_NEW[\u2705 Week 3: Jan 15-21]\n            W4_NEW[\u2705 Week 4: Jan 22-28]\n        end\n    end\n\n    RUN2 --&gt; CALC2\n    CALC2 --&gt; CHECK\n    CHECK --&gt; W1_EXIST\n    CHECK --&gt; W2_EXIST\n    CHECK --&gt; W3_EXIST\n    CHECK --&gt; W4_MISS\n\n    W4_MISS --&gt; PROCESS2\n    PROCESS2 --&gt; RECORD2\n\n    RECORD2 --&gt; W1_NEW\n    RECORD2 --&gt; W2_NEW\n    RECORD2 --&gt; W3_NEW\n    RECORD2 --&gt; W4_NEW\n\n    style RUN2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CALC2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style W4_MISS fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PROCESS2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RECORD2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates intervals from start to now</li> <li>Compares with what's already processed</li> <li>Processes only new intervals</li> </ol> <pre><code>Second Run (Jan 22, 2025):\n- Calculates: 4 weeks total\n- Already processed: Weeks 1-3\n- Missing: Week 4 (Jan 22-28)\n- Processes: Only Week 4\n\nState Database:\n\u2705 Week 1 (Jan 1-7)\n\u2705 Week 2 (Jan 8-14)\n\u2705 Week 3 (Jan 15-21)\n\u2705 Week 4 (Jan 22-28) \u2190 NEW\n</code></pre> <p>[Screenshot: Visual showing second run processing only new Week 4]</p>"},{"location":"guides/incremental_by_time/#creating-an-incremental-model","title":"Creating an Incremental Model","text":"<p>Let's create a weekly sales aggregation model for Orders360.</p>"},{"location":"guides/incremental_by_time/#step-1-create-the-model-file","title":"Step 1: Create the Model File","text":"<pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing new weekly_sales.sql file]</p>"},{"location":"guides/incremental_by_time/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit <code>models/sales/weekly_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,  -- \u23f0 This column contains the date\n    batch_size 1             -- Process 1 week at a time\n  ),\n  start '2025-01-01',       -- Start processing from this date\n  cron '@weekly',            -- Run weekly\n  grain [order_date],        -- One row per week\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- \ud83d\udd0d Filter by time range\nGROUP BY DATE_TRUNC('week', order_date)\nORDER BY order_date\n</code></pre> <p>[Screenshot: Code editor showing complete weekly_sales.sql model]</p>"},{"location":"guides/incremental_by_time/#key-components-explained","title":"Key Components Explained","text":""},{"location":"guides/incremental_by_time/#1-time-column-declaration","title":"1. Time Column Declaration","text":"<pre><code>kind INCREMENTAL_BY_TIME_RANGE (\n  time_column order_date  -- Tell Vulcan which column has dates\n)\n</code></pre> <p>What it does: Tells Vulcan which column contains the timestamp/date for each row.</p> <p>[Screenshot: Code highlighting time_column declaration]</p>"},{"location":"guides/incremental_by_time/#2-where-clause-with-macros","title":"2. WHERE Clause with Macros","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>What it does: Filters data to only the time range being processed.</p> <ul> <li><code>@start_ds</code> = Start date of the interval (e.g., '2025-01-15')</li> <li><code>@end_ds</code> = End date of the interval (e.g., '2025-01-21')</li> </ul> <p>Vulcan automatically replaces these with the correct dates!</p> <p>[Screenshot: Code highlighting WHERE clause with macros, showing how they're replaced]</p>"},{"location":"guides/incremental_by_time/#3-start-date","title":"3. Start Date","text":"<pre><code>start '2025-01-01'\n</code></pre> <p>What it does: Tells Vulcan when your data begins. Vulcan will backfill from this date.</p> <p>[Screenshot: Code highlighting start date]</p>"},{"location":"guides/incremental_by_time/#step-3-apply-the-model","title":"Step 3: Apply the Model","text":"<p>Run <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales model to be added]</p> <p>Vulcan will process each week incrementally:</p> <pre><code>[1/3] sales.weekly_sales  [insert 2025-01-01 - 2025-01-07]  1.2s\n[2/3] sales.weekly_sales  [insert 2025-01-08 - 2025-01-14]  1.1s\n[3/3] sales.weekly_sales  [insert 2025-01-15 - 2025-01-21]  1.3s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:03\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre> <p>[Screenshot: Backfill progress showing each week being processed]</p>"},{"location":"guides/incremental_by_time/#real-example-daily-sales-from-orders360","title":"Real Example: Daily Sales from Orders360","text":"<p>Here's the actual <code>daily_sales</code> model from Orders360 (currently FULL, but could be incremental):</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,  -- Could be INCREMENTAL_BY_TIME_RANGE\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing complete model]</p> <p>To make this incremental, you would:</p> <ol> <li>Change <code>kind FULL</code> to <code>kind INCREMENTAL_BY_TIME_RANGE</code></li> <li>Add <code>time_column order_date</code></li> <li>Add <code>WHERE order_date BETWEEN @start_ds AND @end_ds</code></li> </ol> <pre><code>MODEL (\n  name sales.daily_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  -- ... rest stays the same\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- ADD THIS\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: Comparison showing FULL vs INCREMENTAL changes]</p>"},{"location":"guides/incremental_by_time/#understanding-the-where-clause","title":"Understanding the WHERE Clause","text":"<p>You might wonder: \"Why do I need a WHERE clause if Vulcan adds one automatically?\"</p>"},{"location":"guides/incremental_by_time/#two-where-clauses-two-purposes","title":"Two WHERE Clauses, Two Purposes","text":"<p>Vulcan actually uses two WHERE clauses:</p>"},{"location":"guides/incremental_by_time/#1-your-models-where-clause","title":"1. Your Model's WHERE Clause","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data read into the model - Only reads necessary data from upstream tables - Saves processing time and resources - You control this in your SQL</p> <p>[Screenshot: Visual showing model WHERE clause filtering input data]</p>"},{"location":"guides/incremental_by_time/#2-vulcans-automatic-where-clause","title":"2. Vulcan's Automatic WHERE Clause","text":"<p>Vulcan automatically adds another filter on the output:</p> <pre><code>-- Vulcan adds this automatically:\nWHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data output by the model - Prevents data leakage (ensures no rows outside the time range) - Safety mechanism - Vulcan controls this automatically</p> <p>[Screenshot: Visual showing Vulcan's automatic WHERE clause filtering output]</p>"},{"location":"guides/incremental_by_time/#why-both-are-needed","title":"Why Both Are Needed","text":"<ul> <li>Your WHERE clause: Optimizes performance by reading less data</li> <li>Vulcan's WHERE clause: Ensures correctness by preventing data leakage</li> </ul> <p>Always include the WHERE clause in your model SQL!</p> <p>[Screenshot: Side-by-side comparison showing both WHERE clauses and their purposes]</p>"},{"location":"guides/incremental_by_time/#running-incremental-models","title":"Running Incremental Models","text":"<p>Vulcan has two commands for processing models:</p>"},{"location":"guides/incremental_by_time/#vulcan-plan-for-model-changes","title":"<code>vulcan plan</code> - For Model Changes","text":"<p>Use when you've changed a model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>What it does: - Detects model changes - Shows what will be affected - Backfills missing intervals - Applies changes to the environment</p> <p>[Screenshot: Plan command output showing model changes]</p>"},{"location":"guides/incremental_by_time/#vulcan-run-for-scheduled-execution","title":"<code>vulcan run</code> - For Scheduled Execution","text":"<p>Use when no models have changed:</p> <pre><code>vulcan run\n</code></pre> <p>What it does: - Checks each model's <code>cron</code> schedule - Processes only models that are due - Processes only missing intervals - Fast and efficient</p> <p>[Screenshot: Run command output showing scheduled execution]</p>"},{"location":"guides/incremental_by_time/#how-cron-schedules-work","title":"How Cron Schedules Work","text":"<p>Each model has a <code>cron</code> parameter that determines how often it should run:</p> <pre><code>flowchart LR\n    subgraph \"\u23f0 Cron Schedules\"\n        DAILY[@daily&lt;br/&gt;Every 24 hours]\n        WEEKLY[@weekly&lt;br/&gt;Every 7 days]\n        HOURLY[@hourly&lt;br/&gt;Every 1 hour]\n    end\n\n    subgraph \"\ud83d\udcca Example Models\"\n        M1[sales.daily_sales&lt;br/&gt;cron: @daily]\n        M2[sales.weekly_sales&lt;br/&gt;cron: @weekly]\n    end\n\n    DAILY --&gt; M1\n    WEEKLY --&gt; M2\n\n    style DAILY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style WEEKLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style HOURLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style M1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style M2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>cron '@daily'   -- Run once per day\ncron '@weekly'  -- Run once per week\ncron '@hourly'  -- Run once per hour\n</code></pre> <p>Example from Orders360:</p> <pre><code>-- Daily model\nMODEL (\n  name sales.daily_sales,\n  cron '@daily'  -- Runs every day\n);\n\n-- Weekly model\nMODEL (\n  name sales.weekly_sales,\n  cron '@weekly'  -- Runs once per week\n);\n</code></pre> <p>[Screenshot: Visual showing cron schedules for daily vs weekly models]</p> <p>When you run <code>vulcan run</code>:</p> <pre><code>flowchart TB\n    subgraph \"\ud83d\udd04 vulcan run Execution\"\n        RUN[vulcan run&lt;br/&gt;at 2pm on Jan 15]\n\n        subgraph \"\ud83d\udccb Model Evaluation\"\n            CHECK1[Check daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Last run: 24h ago]\n            CHECK2[Check weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Last run: 2 days ago]\n        end\n\n        subgraph \"\u2705 Decision\"\n            DUE1[\u2705 Due!&lt;br/&gt;Process daily_sales]\n            SKIP[\u23ed\ufe0f Not due&lt;br/&gt;Skip weekly_sales]\n        end\n\n        EXEC1[\ud83d\udd04 Execute daily_sales&lt;br/&gt;Process missing intervals]\n    end\n\n    RUN --&gt; CHECK1\n    RUN --&gt; CHECK2\n\n    CHECK1 --&gt; DUE1\n    CHECK2 --&gt; SKIP\n\n    DUE1 --&gt; EXEC1\n\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DUE1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Vulcan checks each model's <code>cron</code></li> <li>Determines if enough time has passed since last run</li> <li>Processes only models that are due</li> </ol> <pre><code>vulcan run at 2pm on Jan 15:\n\n\u2705 daily_sales (@daily):   Last run 24h ago \u2192 Due, process!\n\u23ed\ufe0f weekly_sales (@weekly): Last run 2 days ago \u2192 Not due, skip\n</code></pre> <p>[Screenshot: Visual showing cron evaluation logic]</p>"},{"location":"guides/incremental_by_time/#batch-processing","title":"Batch Processing","text":"<p>For large datasets, you can process intervals in batches using <code>batch_size</code>:</p> <pre><code>flowchart TB\n    subgraph \"Without batch_size Default\"\n        ALL[12 Weeks Missing]\n        SINGLE[\"Single Job&lt;br/&gt;Process all 12 weeks\"]\n        RESULT1[\"All done in 1 job&lt;br/&gt;30 minutes\"]\n    end\n\n    subgraph \"With batch_size = 4\"\n        ALL2[12 Weeks Missing]\n        BATCH1[\"Batch 1&lt;br/&gt;Weeks 1-4\"]\n        BATCH2[\"Batch 2&lt;br/&gt;Weeks 5-8\"]\n        BATCH3[\"Batch 3&lt;br/&gt;Weeks 9-12\"]\n        RESULT2[\"All done in 3 jobs&lt;br/&gt;10 min each\"]\n    end\n\n    ALL --&gt; SINGLE\n    SINGLE --&gt; RESULT1\n\n    ALL2 --&gt; BATCH1\n    ALL2 --&gt; BATCH2\n    ALL2 --&gt; BATCH3\n    BATCH1 --&gt; RESULT2\n    BATCH2 --&gt; RESULT2\n    BATCH3 --&gt; RESULT2\n\n    style ALL fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style SINGLE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style ALL2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style BATCH1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RESULT2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 4  -- Process 4 weeks at a time\n  )\n);\n</code></pre> <p>Without batch_size (default): - Processes all missing intervals in one job - Example: 12 weeks = 1 job</p> <p>With batch_size: - Divides intervals into batches - Example: 12 weeks \u00f7 4 = 3 jobs</p> <p>[Screenshot: Visual comparison showing batch processing vs single job]</p> <p>When to use batches: - \u2705 Large datasets that might timeout - \u2705 Need better progress tracking - \u2705 Want to parallelize processing</p> <p>When not to use batches: - \u2705 Small datasets (&lt; 1GB) - \u2705 Fast queries (&lt; 1 minute) - \u2705 Simple transformations</p>"},{"location":"guides/incremental_by_time/#forward-only-models","title":"Forward-Only Models","text":"<p>Sometimes you have models so large that rebuilding them is impossible. Forward-only models solve this.</p>"},{"location":"guides/incremental_by_time/#what-are-forward-only-models","title":"What Are Forward-Only Models?","text":"<p>Forward-only models never rebuild historical data. Changes are only applied going forward in time.</p> <pre><code>flowchart TB\n    subgraph \"Regular Model Change\"\n        REG_CHANGE[\"Breaking Change Detected\"]\n        REG_REBUILD[\"Rebuild Entire Table&lt;br/&gt;All dates: Jan 1 - Dec 31\"]\n        REG_RESULT[\"All data updated\"]\n    end\n\n    subgraph \"Forward-Only Model Change\"\n        FWD_CHANGE[\"Breaking Change Detected&lt;br/&gt;forward_only: true\"]\n        FWD_CHECK[\"Check Existing Data&lt;br/&gt;Jan 1 - Dec 15: Keep as-is\"]\n        FWD_APPLY[\"Apply Change Forward&lt;br/&gt;Dec 16 - Dec 31: New data\"]\n        FWD_RESULT[\"Historical preserved&lt;br/&gt;Future updated\"]\n    end\n\n    REG_CHANGE --&gt; REG_REBUILD\n    REG_REBUILD --&gt; REG_RESULT\n\n    FWD_CHANGE --&gt; FWD_CHECK\n    FWD_CHECK --&gt; FWD_APPLY\n    FWD_APPLY --&gt; FWD_RESULT\n\n    style REG_CHANGE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style REG_REBUILD fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style REG_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_CHANGE fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style FWD_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style FWD_APPLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Regular Model Change: </p><pre><code>Breaking change \u2192 Rebuild entire table (all dates)\n</code></pre><p></p> <p>Forward-Only Model Change: </p><pre><code>Breaking change \u2192 Only apply to new dates going forward\n</code></pre><p></p> <p>[Screenshot: Visual comparison showing regular rebuild vs forward-only]</p>"},{"location":"guides/incremental_by_time/#when-to-use-forward-only","title":"When to Use Forward-Only","text":"<p>\u2705 Use forward-only when: - Tables are too large to rebuild - Historical data can't be reprocessed - You only care about future data</p> <p>\u274c Don't use forward-only when: - You need to fix historical data - Schema changes affect past data - You want full data consistency</p>"},{"location":"guides/incremental_by_time/#making-a-model-forward-only","title":"Making a Model Forward-Only","text":"<p>Add <code>forward_only true</code> to your model:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    forward_only true  -- All changes are forward-only\n  )\n);\n</code></pre> <p>[Screenshot: Code showing forward_only configuration]</p>"},{"location":"guides/incremental_by_time/#forward-only-plans","title":"Forward-Only Plans","text":"<p>You can also make a specific plan forward-only:</p> <pre><code>vulcan plan dev --forward-only\n</code></pre> <p>This treats all changes in the plan as forward-only, even if models aren't configured that way.</p> <p>[Screenshot: Plan command with --forward-only flag]</p>"},{"location":"guides/incremental_by_time/#schema-changes-in-forward-only-models","title":"Schema Changes in Forward-Only Models","text":"<p>When you change a forward-only model, Vulcan checks for schema changes that could cause problems.</p>"},{"location":"guides/incremental_by_time/#types-of-schema-changes","title":"Types of Schema Changes","text":""},{"location":"guides/incremental_by_time/#destructive-changes","title":"Destructive Changes","text":"<p>Changes that remove or modify existing data:</p> <pre><code>flowchart LR\n    subgraph \"Destructive Changes\"\n        DROP[\"Dropping Column&lt;br/&gt;total_amount removed\"]\n        RENAME[\"Renaming Column&lt;br/&gt;order_id to id\"]\n        TYPE[\"Changing Type&lt;br/&gt;INT to STRING&lt;br/&gt;may cause loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE1[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER1[\"After:&lt;br/&gt;order_id only\"]\n    end\n\n    DROP --&gt; BEFORE1\n    BEFORE1 --&gt; AFTER1\n\n    style DROP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RENAME fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style TYPE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style BEFORE1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <ul> <li>\u274c Dropping a column</li> <li>\u274c Renaming a column  </li> <li>\u274c Changing data type (could cause data loss)</li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (destructive - drops total_amount)\nSELECT order_id FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing destructive change example]</p>"},{"location":"guides/incremental_by_time/#additive-changes","title":"Additive Changes","text":"<p>Changes that add new data without removing existing:</p> <pre><code>flowchart LR\n    subgraph \"Additive Changes\"\n        ADD[\"Adding Column&lt;br/&gt;customer_name added\"]\n        TYPE2[\"Compatible Type Change&lt;br/&gt;INT to STRING&lt;br/&gt;no data loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE2[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER2[\"After:&lt;br/&gt;order_id, total_amount,&lt;br/&gt;customer_name\"]\n    end\n\n    ADD --&gt; BEFORE2\n    BEFORE2 --&gt; AFTER2\n\n    style ADD fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TYPE2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style BEFORE2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ul> <li>\u2705 Adding a new column</li> <li>\u2705 Changing data type (compatible, e.g., INT \u2192 STRING)</li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (additive - adds customer_name)\nSELECT order_id, total_amount, customer_name FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing additive change example]</p>"},{"location":"guides/incremental_by_time/#controlling-schema-change-behavior","title":"Controlling Schema Change Behavior","text":"<p>You can control how Vulcan handles schema changes:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change error,  -- Block destructive changes\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>Options: - <code>error</code> - Stop and raise an error (default for destructive) - <code>warn</code> - Log a warning but continue - <code>allow</code> - Silently proceed (default for additive) - <code>ignore</code> - Skip the check entirely (dangerous!)</p> <p>[Screenshot: Code showing schema change configuration options]</p>"},{"location":"guides/incremental_by_time/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/incremental_by_time/#strict-schema-control","title":"Strict Schema Control","text":"<p>Prevent any schema changes:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change error,  -- Block destructive\n    on_additive_change error       -- Block even new columns\n  )\n);\n</code></pre> <p>[Screenshot: Strict schema control example]</p>"},{"location":"guides/incremental_by_time/#development-model","title":"Development Model","text":"<p>Allow all changes for rapid iteration:</p> <pre><code>MODEL (\n  name sales.dev_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change allow,  -- Allow dropping columns\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Development model example]</p>"},{"location":"guides/incremental_by_time/#production-safety","title":"Production Safety","text":"<p>Allow safe changes, warn about risky ones:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change warn,   -- Warn but allow\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Production safety example]</p>"},{"location":"guides/incremental_by_time/#important-notes","title":"Important Notes","text":""},{"location":"guides/incremental_by_time/#time-column-must-be-utc","title":"\u26a0\ufe0f Time Column Must Be UTC","text":"<p>Always use UTC timezone for your <code>time_column</code>:</p> <pre><code>-- \u2705 Good: UTC timezone\ntime_column order_date_utc\n\n-- \u274c Bad: Local timezone\ntime_column order_date_local\n</code></pre> <p>Why? Ensures correct interval calculations and proper interaction with Vulcan's scheduler.</p> <p>[Screenshot: Visual warning about UTC requirement]</p>"},{"location":"guides/incremental_by_time/#always-include-where-clause","title":"\u2705 Always Include WHERE Clause","text":"<p>Your model SQL must include a WHERE clause with <code>@start_ds</code> and <code>@end_ds</code>:</p> <pre><code>-- \u2705 Required\nWHERE order_date BETWEEN @start_ds AND @end_ds\n\n-- \u274c Missing WHERE clause\n-- WHERE clause is required!\n</code></pre> <p>[Screenshot: Code showing required WHERE clause]</p>"},{"location":"guides/incremental_by_time/#set-a-start-date","title":"\u2705 Set a Start Date","text":"<p>Always specify when your data begins:</p> <pre><code>start '2025-01-01'  -- Start processing from this date\n</code></pre> <p>[Screenshot: Code showing start date configuration]</p>"},{"location":"guides/incremental_by_time/#choose-appropriate-batch_size","title":"\u2705 Choose Appropriate batch_size","text":"<ul> <li>Start with <code>batch_size 1</code> for small datasets</li> <li>Increase for larger datasets that might timeout</li> <li>Monitor performance to find the sweet spot</li> </ul> <p>[Screenshot: Visual guide for choosing batch_size]</p>"},{"location":"guides/incremental_by_time/#summary","title":"Summary","text":"<p>Incremental by time models: - \u2705 Only process new or missing time intervals - \u2705 Much faster and cheaper than full refreshes - \u2705 Perfect for time-based data (orders, events, transactions) - \u2705 Require a time column and WHERE clause - \u2705 Use cron schedules to control execution frequency</p> <p>Key concepts: - Intervals: Time periods (days, weeks, hours) that Vulcan tracks - Backfill: Processing historical intervals when first creating a model - Cron: Schedule that determines how often a model runs - Forward-only: Models that never rebuild historical data</p>"},{"location":"guides/incremental_by_time/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Kinds for all model types</li> <li>Read the Models Guide for working with models</li> <li>Check the Plan Guide for applying changes</li> <li>See Run Guide for scheduled execution</li> <li>Explore Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/model_selection/","title":"Model Selection","text":""},{"location":"guides/model_selection/#model-selection","title":"Model Selection","text":"<p>This guide explains how to select specific models to include in a Vulcan plan using the Orders360 example project. This is useful when you only want to test or apply changes to a subset of your models.</p> <p>Note: The selector syntax described below is also used for the Vulcan <code>plan</code> <code>--allow-destructive-model</code> and <code>--allow-additive-model</code> selectors.</p>"},{"location":"guides/model_selection/#background","title":"Background","text":"<p>A Vulcan plan automatically detects changes between your local project and the deployed environment. When applied, it backfills directly modified models and their downstream dependencies.</p> <p>In large projects, a single model change can impact many downstream models, making plans take a long time. Model selection lets you filter which changes to include, so you can test specific models without processing everything.</p> <p>Key Concept: - Directly Modified: Models you changed in your code - Indirectly Modified: Downstream models affected by your changes</p> <p>[Screenshot: Visual showing directly vs indirectly modified models]</p>"},{"location":"guides/model_selection/#understanding-model-dependencies","title":"Understanding Model Dependencies","text":"<p>Before we dive into selection, let's understand how models relate to each other in Orders360:</p> <pre><code>flowchart TD\n    subgraph \"Orders360 Model DAG\"\n        RAW_CUSTOMERS[raw.raw_customers&lt;br/&gt;Seed Model]\n        RAW_ORDERS[raw.raw_orders&lt;br/&gt;Seed Model]\n        RAW_PRODUCTS[raw.raw_products&lt;br/&gt;Seed Model]\n\n        DAILY_SALES[sales.daily_sales&lt;br/&gt;Daily Aggregation]\n        WEEKLY_SALES[sales.weekly_sales&lt;br/&gt;Weekly Aggregation]\n    end\n\n    RAW_CUSTOMERS --&gt; DAILY_SALES\n    RAW_ORDERS --&gt; DAILY_SALES\n    RAW_PRODUCTS --&gt; DAILY_SALES\n\n    DAILY_SALES --&gt; WEEKLY_SALES\n\n    style RAW_CUSTOMERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_ORDERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_PRODUCTS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY_SALES fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style WEEKLY_SALES fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Dependency Flow: - <code>raw.raw_orders</code> \u2192 <code>sales.daily_sales</code> \u2192 <code>sales.weekly_sales</code> - Changing <code>raw.raw_orders</code> affects <code>daily_sales</code> (indirectly modified) - Changing <code>daily_sales</code> affects <code>weekly_sales</code> (indirectly modified)</p> <p>[Screenshot: Orders360 project structure showing model files]</p>"},{"location":"guides/model_selection/#syntax","title":"Syntax","text":"<p>Model selections use the <code>--select-model</code> argument in <code>vulcan plan</code>. You can select models in several ways:</p>"},{"location":"guides/model_selection/#basic-selection","title":"Basic Selection","text":"<p>Select a single model by name:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>Select multiple models:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\" --select-model \"raw.raw_orders\"\n</code></pre> <p>[Screenshot: Plan output showing multiple models selected]</p>"},{"location":"guides/model_selection/#wildcard-selection","title":"Wildcard Selection","text":"<p>Use <code>*</code> to match multiple models:</p> <pre><code># Select all models starting with \"raw.\"\nvulcan plan dev --select-model \"raw.*\"\n\n# Select all models ending with \"_sales\"\nvulcan plan dev --select-model \"sales.*_sales\"\n\n# Select all models containing \"daily\"\nvulcan plan dev --select-model \"*daily*\"\n</code></pre> <p>Examples: - <code>\"raw.*\"</code> matches <code>raw.raw_customers</code>, <code>raw.raw_orders</code>, <code>raw.raw_products</code> - <code>\"sales.*_sales\"</code> matches <code>sales.daily_sales</code>, <code>sales.weekly_sales</code> - <code>\"*.daily_sales\"</code> matches <code>sales.daily_sales</code></p> <p>[Screenshot: Plan output showing wildcard selection results]</p>"},{"location":"guides/model_selection/#tag-selection","title":"Tag Selection","text":"<p>Select models by tags using <code>tag:tag_name</code>:</p> <pre><code># Select all models with \"seed\" tag\nvulcan plan dev --select-model \"tag:seed\"\n\n# Select all models with tags starting with \"reporting\"\nvulcan plan dev --select-model \"tag:reporting*\"\n</code></pre> <p>Example: If <code>raw.raw_orders</code> and <code>raw.raw_customers</code> have the <code>seed</code> tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n# Selects: raw.raw_orders, raw.raw_customers\n</code></pre> <p>[Screenshot: Plan output showing tag-based selection]</p>"},{"location":"guides/model_selection/#upstreamdownstream-selection","title":"Upstream/Downstream Selection","text":"<p>Use <code>+</code> to include upstream or downstream models:</p> <ul> <li><code>+model_name</code> = Include upstream models (dependencies)</li> <li><code>model_name+</code> = Include downstream models (dependents)</li> </ul> <pre><code>flowchart LR\n    subgraph \"Model Dependencies\"\n        RAW[raw.raw_orders]\n        DAILY[sales.daily_sales]\n        WEEKLY[sales.weekly_sales]\n    end\n\n    RAW --&gt;|upstream| DAILY\n    DAILY --&gt;|downstream| WEEKLY\n\n    style RAW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000\n    style WEEKLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Examples:</p> <pre><code># Select daily_sales only\nvulcan plan dev --select-model \"sales.daily_sales\"\n# Result: daily_sales (directly modified)\n\n# Select daily_sales + upstream (raw.raw_orders)\nvulcan plan dev --select-model \"+sales.daily_sales\"\n# Result: raw.raw_orders, daily_sales\n\n# Select daily_sales + downstream (weekly_sales)\nvulcan plan dev --select-model \"sales.daily_sales+\"\n# Result: daily_sales, weekly_sales\n\n# Select daily_sales + both upstream and downstream\nvulcan plan dev --select-model \"+sales.daily_sales+\"\n# Result: raw.raw_orders, daily_sales, weekly_sales\n</code></pre> <p>[Screenshot: Plan outputs showing different selection results]</p>"},{"location":"guides/model_selection/#git-based-selection","title":"Git-Based Selection","text":"<p>Select models changed in a git branch:</p> <pre><code># Select models changed in feature branch\nvulcan plan dev --select-model \"git:feature\"\n\n# Select changed models + downstream\nvulcan plan dev --select-model \"git:feature+\"\n\n# Select changed models + upstream\nvulcan plan dev --select-model \"+git:feature\"\n</code></pre> <p>What it includes: - Untracked files (new models) - Uncommitted changes - Committed changes different from target branch</p> <p>[Screenshot: Plan output showing git-based selection]</p>"},{"location":"guides/model_selection/#complex-selections","title":"Complex Selections","text":"<p>Combine conditions with logical operators:</p> <ul> <li><code>&amp;</code> (AND): Both conditions must be true</li> <li><code>|</code> (OR): Either condition must be true</li> <li><code>^</code> (NOT): Negates a condition</li> </ul> <pre><code># Models with finance tag that don't have deprecated tag\nvulcan plan dev --select-model \"(tag:finance &amp; ^tag:deprecated)\"\n\n# daily_sales + upstream OR weekly_sales + downstream\nvulcan plan dev --select-model \"(+sales.daily_sales | sales.weekly_sales+)\"\n\n# Changed models that also have finance tag\nvulcan plan dev --select-model \"(tag:finance &amp; git:main)\"\n\n# Models in sales schema without test tag\nvulcan plan dev --select-model \"^(tag:test) &amp; sales.*\"\n</code></pre> <p>[Screenshot: Plan output showing complex selection results]</p>"},{"location":"guides/model_selection/#examples-with-orders360","title":"Examples with Orders360","text":"<p>Let's see how model selection works with the Orders360 project. We'll modify <code>raw.raw_orders</code> and <code>sales.daily_sales</code> to demonstrate different selection scenarios.</p>"},{"location":"guides/model_selection/#example-setup","title":"Example Setup","text":"<p>We've modified two models: - <code>raw.raw_orders</code> (directly modified) - <code>sales.daily_sales</code> (directly modified)</p> <p>The dependency chain: </p><pre><code>raw.raw_orders \u2192 sales.daily_sales \u2192 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Orders360 project showing modified files]</p>"},{"location":"guides/model_selection/#no-selection-default","title":"No Selection (Default)","text":"<p>Without selection, Vulcan includes all directly modified models and their downstream dependencies:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing all modified models]</p> <p>What Happened: - Both directly modified models are included - <code>weekly_sales</code> is indirectly modified (depends on <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-single-model","title":"Select Single Model","text":"<p>Select only <code>sales.daily_sales</code>:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>What Happened: - <code>raw.raw_orders</code> is excluded (not selected) - <code>daily_sales</code> is included (directly modified) - <code>weekly_sales</code> is included (indirectly modified, downstream of <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-with-upstream-indicator","title":"Select with Upstream Indicator","text":"<p>Select <code>daily_sales</code> and include its upstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"+sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 raw.raw_orders\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream selection]</p> <p>What Happened: - <code>raw.raw_orders</code> is included (upstream of <code>daily_sales</code>) - <code>daily_sales</code> is included (selected) - <code>weekly_sales</code> is included (downstream of <code>daily_sales</code>)</p>"},{"location":"guides/model_selection/#select-with-downstream-indicator","title":"Select with Downstream Indicator","text":"<p>Select <code>daily_sales</code> and include its downstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales+\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 sales.weekly_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    (none)\n</code></pre><p></p> <p>[Screenshot: Plan output showing downstream selection]</p> <p>What Happened: - <code>daily_sales</code> is included (selected) - <code>weekly_sales</code> is included (downstream, now directly modified) - <code>raw.raw_orders</code> is excluded (not selected)</p>"},{"location":"guides/model_selection/#select-with-wildcard","title":"Select with Wildcard","text":"<p>Select all models matching a pattern:</p> <pre><code>vulcan plan dev --select-model \"sales.*_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing wildcard selection]</p> <p>What Happened: - <code>sales.daily_sales</code> matches the pattern (selected) - <code>sales.weekly_sales</code> matches the pattern but is indirectly modified - <code>raw.raw_orders</code> doesn't match (excluded)</p>"},{"location":"guides/model_selection/#select-with-tags","title":"Select with Tags","text":"<p>If models have tags, select by tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sales.daily_sales\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing tag-based selection]</p> <p>What Happened: - <code>raw.raw_orders</code> has <code>seed</code> tag (selected) - Downstream models are indirectly modified</p>"},{"location":"guides/model_selection/#select-with-git-changes","title":"Select with Git Changes","text":"<p>Select models changed in a git branch:</p> <pre><code>vulcan plan dev --select-model \"git:feature\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales  # Changed in feature branch\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing git-based selection]</p> <p>What Happened: - Only models changed in <code>feature</code> branch are selected - Downstream models are included automatically</p>"},{"location":"guides/model_selection/#backfill-selection","title":"Backfill Selection","text":"<p>By default, Vulcan backfills all models in a plan. You can limit which models are backfilled using <code>--backfill-model</code>.</p> <p>Important: <code>--backfill-model</code> only works in development environments (not <code>prod</code>).</p>"},{"location":"guides/model_selection/#how-backfill-selection-works","title":"How Backfill Selection Works","text":"<pre><code>flowchart TB\n    subgraph \"Backfill Selection Flow\"\n        PLAN[vulcan plan dev]\n        SELECT[--select-model&lt;br/&gt;Which models in plan?]\n        BACKFILL[--backfill-model&lt;br/&gt;Which models to backfill?]\n\n        subgraph \"Plan Includes\"\n            IN_PLAN[Models in Plan&lt;br/&gt;daily_sales, weekly_sales]\n        end\n\n        subgraph \"Backfill Includes\"\n            BACKFILL_LIST[Models to Backfill&lt;br/&gt;Only daily_sales]\n        end\n\n        RESULT[Result:&lt;br/&gt;Plan shows all models&lt;br/&gt;Only selected models backfilled]\n    end\n\n    PLAN --&gt; SELECT\n    PLAN --&gt; BACKFILL\n    SELECT --&gt; IN_PLAN\n    BACKFILL --&gt; BACKFILL_LIST\n    IN_PLAN --&gt; RESULT\n    BACKFILL_LIST --&gt; RESULT\n\n    style PLAN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style SELECT fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BACKFILL fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Key Points: - <code>--select-model</code> determines which models appear in the plan - <code>--backfill-model</code> determines which models are actually backfilled - Upstream models are always backfilled (required for downstream models)</p> <p>[Screenshot: Visual diagram explaining backfill selection]</p>"},{"location":"guides/model_selection/#backfill-examples","title":"Backfill Examples","text":""},{"location":"guides/model_selection/#no-backfill-selection-default","title":"No Backfill Selection (Default)","text":"<p>All models in the plan are backfilled:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing all models needing backfill]</p>"},{"location":"guides/model_selection/#backfill-specific-model","title":"Backfill Specific Model","text":"<p>Only backfill <code>daily_sales</code>:</p> <pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u2514\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales needs backfill]</p> <p>What Happened: - <code>weekly_sales</code> is excluded from backfill - Only <code>daily_sales</code> will be processed</p>"},{"location":"guides/model_selection/#backfill-with-upstream","title":"Backfill with Upstream","text":"<p>When you backfill a model, its upstream dependencies are automatically included:</p> <pre><code>vulcan plan dev --backfill-model \"sales.weekly_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 raw__dev.raw_orders: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream models included in backfill]</p> <p>What Happened: - <code>weekly_sales</code> is selected for backfill - <code>raw.raw_orders</code> is automatically included (upstream dependency) - <code>daily_sales</code> is excluded (not upstream of <code>weekly_sales</code>)</p>"},{"location":"guides/model_selection/#visual-selection-guide","title":"Visual Selection Guide","text":"<p>Here's a quick reference for common selection patterns:</p> <pre><code>flowchart LR\n    subgraph \"Selection Patterns\"\n        PAT1[\"sales.daily_sales&lt;br/&gt;Select only this model\"]\n        PAT2[\"+sales.daily_sales&lt;br/&gt;Select + upstream\"]\n        PAT3[\"sales.daily_sales+&lt;br/&gt;Select + downstream\"]\n        PAT4[\"+sales.daily_sales+&lt;br/&gt;Select + both\"]\n        PAT5[\"sales.*_sales&lt;br/&gt;Wildcard match\"]\n        PAT6[\"tag:seed&lt;br/&gt;Tag selection\"]\n    end\n\n    subgraph \"Results\"\n        RES1[daily_sales only]\n        RES2[raw_orders + daily_sales]\n        RES3[daily_sales + weekly_sales]\n        RES4[All connected]\n        RES5[All matching]\n        RES6[All tagged]\n    end\n\n    PAT1 --&gt; RES1\n    PAT2 --&gt; RES2\n    PAT3 --&gt; RES3\n    PAT4 --&gt; RES4\n    PAT5 --&gt; RES5\n    PAT6 --&gt; RES6\n\n    style PAT1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT4 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT5 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT6 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual cheat sheet for selection patterns]</p>"},{"location":"guides/model_selection/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Start Small: Select only the models you're testing    </p><pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre><p></p> </li> <li> <p>Use Wildcards: When selecting multiple related models    </p><pre><code>vulcan plan dev --select-model \"sales.*\"\n</code></pre><p></p> </li> <li> <p>Include Dependencies: Use <code>+</code> when you need upstream/downstream models    </p><pre><code>vulcan plan dev --select-model \"+sales.daily_sales+\"\n</code></pre><p></p> </li> <li> <p>Limit Backfill: Use <code>--backfill-model</code> to save time in development    </p><pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre><p></p> </li> <li> <p>Use Tags: Organize models with tags for easier selection    </p><pre><code>vulcan plan dev --select-model \"tag:reporting\"\n</code></pre><p></p> </li> </ol>"},{"location":"guides/model_selection/#summary","title":"Summary","text":"<p>Model Selection: - \u2705 Filter which models appear in a plan - \u2705 Use wildcards, tags, and git changes - \u2705 Include upstream/downstream with <code>+</code> - \u2705 Combine with logical operators</p> <p>Backfill Selection: - \u2705 Limit which models are actually backfilled - \u2705 Upstream models are always included - \u2705 Only works in development environments - \u2705 Saves time when testing specific models</p>"},{"location":"guides/model_selection/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plans for understanding plan behavior</li> <li>Read the Plan Guide for applying changes</li> <li>Check Model Configuration for model properties</li> <li>Explore Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/models/","title":"Models","text":""},{"location":"guides/models/#models","title":"Models","text":"<p>This guide walks you through working with models in Vulcan using the Orders360 example project. You'll learn how to add, edit, evaluate, and manage models with practical examples.</p>"},{"location":"guides/models/#prerequisites","title":"Prerequisites","text":"<p>Before adding a model, ensure that you have:</p> <ul> <li>Created your project </li> <li>Applied your first plan</li> <li>Working in a dev environment for testing changes</li> </ul>"},{"location":"guides/models/#understanding-models","title":"Understanding Models","text":"<p>Models in Vulcan consist of two core components:</p> <ol> <li>DDL (Data Definition Language): The <code>MODEL</code> block that defines structure, metadata, and behavior</li> <li>DML (Data Manipulation Language): The <code>SELECT</code> query that contains transformation logic</li> </ol>"},{"location":"guides/models/#example-daily-sales-model","title":"Example: Daily Sales Model","text":"<p>Here's a real example from Orders360:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file in editor showing the complete model definition]</p>"},{"location":"guides/models/#adding-a-model","title":"Adding a Model","text":"<p>To add a new model to your Orders360 project:</p>"},{"location":"guides/models/#step-1-create-model-file","title":"Step 1: Create Model File","text":"<p>Create a new file in your <code>models</code> directory. For example, let's add a weekly sales aggregation:</p> <pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing models/sales directory structure]</p>"},{"location":"guides/models/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit the file and add your model definition:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@weekly',\n  grain [order_date],\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(total_amount) AS total_revenue,\n  AVG(total_amount) AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY DATE_TRUNC('week', order_date)\n</code></pre> <p>[Screenshot: weekly_sales.sql file in editor with model definition]</p>"},{"location":"guides/models/#step-3-check-model-status","title":"Step 3: Check Model Status","text":"<p>Verify your model is detected:</p> <pre><code>vulcan info\n</code></pre> <p>Expected Output: </p><pre><code>Connection: \u2705 Connected\nModels: 5\n  - raw.raw_customers\n  - raw.raw_orders\n  - raw.raw_products\n  - sales.daily_sales\n  - sales.weekly_sales  \u2190 NEW MODEL\n...\n</code></pre><p></p> <p>[Screenshot: <code>vulcan info</code> output showing the new weekly_sales model]</p>"},{"location":"guides/models/#step-4-apply-the-model","title":"Step 4: Apply the Model","text":"<p>Use <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing new weekly_sales model to be added]</p> <p>Type <code>y</code> to apply and backfill the model.</p>"},{"location":"guides/models/#editing-an-existing-model","title":"Editing an Existing Model","text":"<p>To edit an existing model, modify the model file and use Vulcan's tools to preview and apply changes.</p>"},{"location":"guides/models/#step-1-edit-the-model-file","title":"Step 1: Edit the Model File","text":"<p>Let's modify <code>sales.daily_sales</code> to add a new column. Open <code>models/sales/daily_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day',\n    avg_order_value = 'Average order value for the day'  -- NEW COLUMN DESCRIPTION\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,  -- NEW COLUMN\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing the added avg_order_value column]</p>"},{"location":"guides/models/#step-2-evaluate-the-model-optional","title":"Step 2: Evaluate the Model (Optional)","text":"<p>Preview the model output without materializing it:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate command output showing the new avg_order_value column]</p> <p>What Happened? - The <code>evaluate</code> command runs the model query without creating tables - Shows you the output with the new column - Useful for testing changes before applying them</p>"},{"location":"guides/models/#step-3-preview-changes-with-plan","title":"Step 3: Preview Changes with Plan","text":"<p>See what will change and how it affects downstream models:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Non-breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -22,6 +22,7 @@\n      SELECT\n        CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n        COUNT(order_id)::INTEGER AS total_orders,\n        SUM(total_amount)::FLOAT AS total_revenue,\n    +   AVG(total_amount)::FLOAT AS avg_order_value,\n        MAX(order_id)::VARCHAR AS last_order_id\n      FROM raw.raw_orders\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing non-breaking change with diff highlighting the new column]</p> <p>Understanding the Output: - Non-breaking: Vulcan detected this as non-breaking (adding a column) - Diff: Shows exactly what changed (green <code>+</code> indicates added line) - No downstream impact: <code>sales.weekly_sales</code> is not listed because it doesn't use this column yet</p>"},{"location":"guides/models/#step-4-apply-the-changes","title":"Step 4: Apply the Changes","text":"<p>Type <code>y</code> to apply the plan:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:05\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Plan application showing daily_sales being backfilled]</p>"},{"location":"guides/models/#making-a-breaking-change","title":"Making a Breaking Change","text":"<p>Breaking changes affect downstream models. Let's see how Vulcan handles this.</p>"},{"location":"guides/models/#step-1-add-a-filter-to-daily-sales","title":"Step 1: Add a Filter to Daily Sales","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a WHERE clause:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE total_amount &gt; 10  -- NEW FILTER: Only orders &gt; $10\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql showing the WHERE clause filter]</p>"},{"location":"guides/models/#step-2-create-plan","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,6 +26,7 @@\n      FROM raw.raw_orders\n    + WHERE total_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales.weekly_sales (Indirect Breaking)\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing breaking change with downstream impact on weekly_sales]</p> <p>Understanding Breaking Changes: - Breaking: Adding a WHERE clause filters data, making existing data invalid - Indirectly Modified: <code>sales.weekly_sales</code> depends on <code>daily_sales</code>, so it's affected - Cascading backfill: Both models need to be reprocessed</p>"},{"location":"guides/models/#evaluating-a-model","title":"Evaluating a Model","text":"<p>The <code>evaluate</code> command lets you test models without materializing data. Perfect for iteration and debugging.</p>"},{"location":"guides/models/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing single day results]</p>"},{"location":"guides/models/#evaluate-multiple-days","title":"Evaluate Multiple Days","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-10 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-10 00:00:00           38         1120.25           29.48        ORD-00110\n2025-01-11 00:00:00           45         1350.75           30.02        ORD-00111\n2025-01-12 00:00:00           41         1225.50           29.89        ORD-00112\n2025-01-13 00:00:00           39         1180.00           30.26        ORD-00113\n2025-01-14 00:00:00           44         1320.50           30.01        ORD-00114\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing multiple days of data]</p>"},{"location":"guides/models/#evaluate-with-filters","title":"Evaluate with Filters","text":"<p>Test your model logic with different conditions:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15 --where \"total_amount &gt; 50\"\n</code></pre> <p>[Screenshot: Evaluate command with WHERE clause filter]</p> <p>Use Cases for Evaluate: - \u2705 Test model logic before applying changes - \u2705 Debug query issues - \u2705 Verify data transformations - \u2705 Check data quality - \u2705 Iterate quickly without materialization costs</p>"},{"location":"guides/models/#reverting-a-change","title":"Reverting a Change","text":"<p>Vulcan makes it easy to revert model changes using Virtual Updates.</p>"},{"location":"guides/models/#step-1-revert-the-change","title":"Step 1: Revert the Change","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to remove the WHERE clause we added:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\n-- WHERE total_amount &gt; 10  -- REMOVED FILTER\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql with WHERE clause removed/commented out]</p>"},{"location":"guides/models/#step-2-apply-reverted-plan","title":"Step 2: Apply Reverted Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,7 +26,6 @@\n      FROM raw.raw_orders\n    - WHERE total_amount &gt; 10\n      GROUP BY order_date\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan showing reverted change with diff]</p> <p>Virtual Update: - No backfill required - just updates references - Fast operation - completes in seconds - Previous data remains available</p>"},{"location":"guides/models/#validating-models","title":"Validating Models","text":"<p>Vulcan provides multiple ways to validate your models.</p>"},{"location":"guides/models/#automatic-validation","title":"Automatic Validation","text":"<p>Vulcan automatically validates models when you run <code>plan</code>:</p> <ol> <li>Unit Tests: Run automatically to validate logic</li> <li>Audits: Execute when data is loaded to tables</li> <li>Assertions: Check data quality constraints</li> </ol> <p>Example Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n</code></pre><p></p> <p>[Screenshot: Plan output showing tests passed]</p>"},{"location":"guides/models/#manual-validation-options","title":"Manual Validation Options","text":"<ol> <li> <p>Evaluate: Test model output without materialization    </p><pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre><p></p> </li> <li> <p>Unit Tests: Write tests in <code>tests/</code> directory    </p><pre><code>vulcan test\n</code></pre><p></p> </li> <li> <p>Plan Preview: See changes before applying    </p><pre><code>vulcan plan dev\n</code></pre><p></p> </li> </ol> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/models/#deleting-a-model","title":"Deleting a Model","text":"<p>To remove a model from your project:</p>"},{"location":"guides/models/#step-1-delete-model-file","title":"Step 1: Delete Model File","text":"<pre><code>rm models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing weekly_sales.sql deleted]</p>"},{"location":"guides/models/#step-2-delete-associated-tests-if-any","title":"Step 2: Delete Associated Tests (if any)","text":"<pre><code>rm tests/test_weekly_sales.yaml\n</code></pre>"},{"location":"guides/models/#step-3-apply-deletion-plan","title":"Step 3: Apply Deletion Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 1 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales as removed]</p> <p>Type <code>y</code> to apply the deletion.</p> <p>Expected Output: </p><pre><code>Virtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\nVirtual Update executed successfully\n</code></pre><p></p> <p>[Screenshot: Virtual update completing successfully]</p>"},{"location":"guides/models/#step-4-apply-to-production","title":"Step 4: Apply to Production","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Production plan showing model removal]</p>"},{"location":"guides/models/#model-examples-from-orders360","title":"Model Examples from Orders360","text":""},{"location":"guides/models/#seed-model-raw-orders","title":"Seed Model: Raw Orders","text":"<pre><code>MODEL (\n  name raw.raw_orders,\n  kind SEED (\n    path '../../seeds/raw_orders.csv'\n  ),\n  description 'Seed model loading raw order data from CSV file',\n  columns (\n    order_id VARCHAR,\n    order_date DATE,\n    customer_id VARCHAR,\n    product_id VARCHAR,\n    total_amount FLOAT\n  ),\n  column_descriptions (\n    order_id = 'Unique identifier for each order',\n    order_date = 'Date when the order was placed',\n    customer_id = 'Reference to customer who placed the order',\n    product_id = 'Reference to product that was ordered',\n    total_amount = 'Total order amount in dollars'\n  ),\n  assertions (\n    unique_values(columns := (order_id)),\n    not_null(columns := (order_id, order_date, customer_id, product_id)),\n    positive_values(column := total_amount)\n  ),\n  grain order_id\n);\n</code></pre> <p>[Screenshot: raw_orders.sql seed model file]</p>"},{"location":"guides/models/#transformation-model-daily-sales","title":"Transformation Model: Daily Sales","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grain order_date,\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql transformation model file]</p>"},{"location":"guides/models/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive names: <code>sales.daily_sales</code> is clearer than <code>sales.ds</code></li> <li>Add column descriptions: Document what each column represents</li> <li>Use assertions: Validate data quality at the model level</li> <li>Test before applying: Use <code>evaluate</code> to preview changes</li> <li>Review plans carefully: Check diffs and downstream impacts</li> <li>Use dev environments: Test changes before production</li> </ol>"},{"location":"guides/models/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Kinds for different model types</li> <li>Explore Model Properties for advanced configuration</li> <li>Read about Plan Guide for applying model changes</li> <li>Check Testing Guide for model validation strategies</li> <li>See Orders360 Example for complete project reference</li> </ul>"},{"location":"guides/plan/","title":"Plan","text":""},{"location":"guides/plan/#plan","title":"Plan","text":"<p>This guide walks you through Vulcan's plan functionality with practical scenarios using the Orders360 example project. You'll learn how plans work, how to interpret them, and how to apply changes to your data warehouse.</p>"},{"location":"guides/plan/#plan-architecture","title":"Plan Architecture","text":"<p>The following diagram illustrates how Vulcan's plan system works, showing the relationship between local project state, plans, model variants, physical tables, and environments:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcc1 Local Project\"\n        LP[\ud83d\udcdd Local Project Files&lt;br/&gt;Your SQL/Python Models]\n        M1[\ud83d\udcca Model: daily_sales v1]\n        M2[\ud83d\udcca Model: weekly_sales v1]\n    end\n\n    subgraph \"\ud83d\udd0d Plan Creation\"\n        PC[\u26a1 vulcan plan]\n        COMP[\ud83d\udd0e Compare Local vs Environment]\n        CAT[\ud83c\udff7\ufe0f Categorize Changes&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking]\n        PLAN[\ud83d\udccb Plan Generated&lt;br/&gt;Ready for Review]\n    end\n\n    subgraph \"\ud83d\udd00 Model Variants &amp; Snapshots\"\n        MV1[\ud83d\udd37 Model Variant 1&lt;br/&gt;daily_sales__hash1]\n        MV2[\ud83d\udd37 Model Variant 2&lt;br/&gt;daily_sales__hash2]\n        MV3[\ud83d\udd37 Model Variant 3&lt;br/&gt;weekly_sales__hash1]\n        SNAP1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd11 Fingerprint: hash1]\n        SNAP2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd11 Fingerprint: hash2]\n        SNAP3[\ud83d\udcf8 Snapshot 3&lt;br/&gt;\ud83d\udd11 Fingerprint: hash3]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Layer\"\n        PT1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;db.vulcan__sales.daily_sales__hash1]\n        PT2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;db.vulcan__sales.daily_sales__hash2]\n        PT3[\ud83d\uddc4\ufe0f Physical Table 3&lt;br/&gt;db.vulcan__sales.weekly_sales__hash1]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer\"\n        VL1[\ud83d\udd0d View: sales.daily_sales]\n        VL2[\ud83d\udd0d View: sales.weekly_sales]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;References Variant 1 &amp; 3]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;References Variant 2 &amp; 3]\n    end\n\n    subgraph \"\u2699\ufe0f Backfill Process\"\n        BF[\ud83d\udd04 Backfill Execution]\n        INC[\ud83d\udcc8 Incremental Backfill]\n        FULL[\ud83d\udd04 Full Refresh]\n    end\n\n    LP --&gt;|\"\ud83d\udce4\"| M1\n    LP --&gt;|\"\ud83d\udce4\"| M2\n    M1 --&gt;|\"\u27a1\ufe0f\"| PC\n    M2 --&gt;|\"\u27a1\ufe0f\"| PC\n    PC --&gt;|\"\ud83d\udd0d\"| COMP\n    COMP --&gt;|\"\ud83c\udff7\ufe0f\"| CAT\n    CAT --&gt;|\"\u2705\"| PLAN\n    PLAN --&gt;|\"\u2728\"| MV1\n    PLAN --&gt;|\"\u2728\"| MV2\n    PLAN --&gt;|\"\u2728\"| MV3\n\n    MV1 --&gt;|\"\ud83d\udd17\"| SNAP1\n    MV2 --&gt;|\"\ud83d\udd17\"| SNAP2\n    MV3 --&gt;|\"\ud83d\udd17\"| SNAP3\n\n    SNAP1 --&gt;|\"\ud83d\udcbe\"| PT1\n    SNAP2 --&gt;|\"\ud83d\udcbe\"| PT2\n    SNAP3 --&gt;|\"\ud83d\udcbe\"| PT3\n\n    PT1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    PT2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    PT3 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n\n    PROD --&gt;|\"\ud83d\udd17\"| MV1\n    PROD --&gt;|\"\ud83d\udd17\"| MV3\n    DEV --&gt;|\"\ud83d\udd17\"| MV2\n    DEV --&gt;|\"\ud83d\udd17\"| MV3\n\n    PLAN --&gt;|\"\u2699\ufe0f\"| BF\n    BF --&gt;|\"\ud83d\udcc8\"| INC\n    BF --&gt;|\"\ud83d\udd04\"| FULL\n    INC --&gt;|\"\ud83d\udcbe\"| PT1\n    INC --&gt;|\"\ud83d\udcbe\"| PT2\n    FULL --&gt;|\"\ud83d\udcbe\"| PT3\n\n    style LP fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style PC fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style PLAN fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style PROD fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style DEV fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000\n    style BF fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style MV1 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style MV2 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style MV3 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style PT1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px\n    style PT2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px\n    style PT3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px</code></pre>"},{"location":"guides/plan/#key-concepts-illustrated","title":"Key Concepts Illustrated","text":"<ol> <li>Local Project: Your model files define the desired state</li> <li>Plan Creation: Vulcan compares local state to environment state and generates a plan</li> <li>Model Variants: Each model change creates a new variant with a unique fingerprint</li> <li>Physical Tables: Each variant gets its own physical table in the warehouse</li> <li>Virtual Layer: Views point to the appropriate physical tables based on environment</li> <li>Environments: Collections of references to model variants (not the data itself)</li> <li>Backfill: Process of populating physical tables with data</li> </ol>"},{"location":"guides/plan/#what-is-a-plan","title":"What is a Plan?","text":"<p>A plan is Vulcan's way of comparing your local project state with a target environment and determining what changes need to be applied. Before any model changes take effect, Vulcan creates a plan that shows:</p> <ul> <li>Added models - New models to be created</li> <li>Removed models - Models to be deleted</li> <li>Modified models - Changes to existing models (with diffs)</li> <li>Indirectly affected models - Downstream models that depend on changed models</li> <li>Backfill requirements - Date ranges that need data reprocessing</li> </ul> <p>Plans allow you to review and verify all changes before they're applied to your data warehouse.</p>"},{"location":"guides/plan/#prerequisites","title":"Prerequisites","text":"<p>Before following this guide, ensure you have:</p> <ol> <li>Orders360 example project set up (see Examples Overview)</li> <li>Docker environment running (see Docker Quickstart)</li> <li>Vulcan CLI accessible via <code>vulcan</code> command or <code>vulcan.bat</code> (Windows)</li> </ol>"},{"location":"guides/plan/#scenario-1-first-plan-initializing-production","title":"Scenario 1: First Plan - Initializing Production","text":"<p>When you first create a project, you need to initialize the production environment. This scenario shows how Vulcan detects all models and creates the initial plan.</p>"},{"location":"guides/plan/#step-1-check-project-status","title":"Step 1: Check Project Status","text":"<p>First, verify your project setup:</p> <pre><code>vulcan info\n</code></pre> <p>Expected Output: </p><pre><code>Connection: \u2705 Connected\nModels: 4\nMacros: 0\nTests: 2\n...\n</code></pre><p></p> <p>[Screenshot: <code>vulcan info</code> output showing project status]</p>"},{"location":"guides/plan/#step-2-create-your-first-plan","title":"Step 2: Create Your First Plan","text":"<p>Run the plan command to see what Vulcan will create:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 raw.raw_customers\n    \u251c\u2500\u2500 raw.raw_orders\n    \u251c\u2500\u2500 raw.raw_products\n    \u2514\u2500\u2500 sales.daily_sales\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 raw.raw_customers: 2025-01-01 - 2025-01-15\n\u251c\u2500\u2500 raw.raw_orders: 2025-01-01 - 2025-01-15\n\u251c\u2500\u2500 raw.raw_products: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: First plan output showing all models to be added]</p>"},{"location":"guides/plan/#understanding-the-output","title":"Understanding the Output","text":"<ul> <li><code>prod</code> environment will be initialized: This is your first plan, so Vulcan is creating the production environment</li> <li>Added models: All 4 models are new and will be created</li> <li>Backfill dates: Each model needs data from its start date (2025-01-01) to the current date</li> </ul>"},{"location":"guides/plan/#step-3-apply-the-plan","title":"Step 3: Apply the Plan","text":"<p>Type <code>y</code> and press Enter to apply the plan:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/4] raw.raw_customers          [full refresh]                   2.3s\n[2/4] raw.raw_orders             [full refresh]                   1.8s\n[3/4] raw.raw_products           [full refresh]                   0.5s\n[4/4] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 4.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 4/4 \u2022 0:00:09\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Plan application progress showing all models being backfilled]</p>"},{"location":"guides/plan/#what-happened","title":"What Happened?","text":"<ol> <li>Seed models (<code>raw.*</code>) were fully refreshed - all data was reloaded</li> <li>Daily sales model was incrementally backfilled - data was inserted for each day from start date to today</li> <li>Physical tables were created in your warehouse</li> <li>Virtual layer was updated to point to the new tables</li> </ol>"},{"location":"guides/plan/#scenario-2-adding-a-new-model","title":"Scenario 2: Adding a New Model","text":"<p>After your initial setup, you'll add new models. This scenario shows how Vulcan detects new models and determines their dependencies.</p>"},{"location":"guides/plan/#step-1-add-a-new-model","title":"Step 1: Add a New Model","text":"<p>Create a new model file <code>models/sales/weekly_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@weekly',\n  grain [order_date]\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY DATE_TRUNC('week', order_date)\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan-for-new-model","title":"Step 2: Create Plan for New Model","text":"<p>Run the plan command:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing new weekly_sales model]</p>"},{"location":"guides/plan/#step-3-review-dependencies","title":"Step 3: Review Dependencies","text":"<p>Notice that: - Only the new model appears in the plan - The upstream model (<code>sales.daily_sales</code>) is not affected because adding a downstream model doesn't change upstream data - Backfill is needed from the model's start date</p>"},{"location":"guides/plan/#step-4-apply-the-plan","title":"Step 4: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.1s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:03\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Weekly sales model being backfilled]</p>"},{"location":"guides/plan/#scenario-3-modifying-a-model-non-breaking-change","title":"Scenario 3: Modifying a Model - Non-Breaking Change","text":"<p>Non-breaking changes don't affect existing data validity. Adding a new column is a common non-breaking change.</p>"},{"location":"guides/plan/#step-1-modify-the-model","title":"Step 1: Modify the Model","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a new column:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,  -- NEW COLUMN\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Non-breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -5,6 +5,7 @@\n      SELECT\n        order_date,\n        COUNT(DISTINCT order_id) AS total_orders,\n        SUM(order_amount) AS total_revenue,\n    +   AVG(order_amount) AS avg_order_value,\n        MAX(order_id) AS last_order_id\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing non-breaking change with diff]</p>"},{"location":"guides/plan/#understanding-non-breaking-changes","title":"Understanding Non-Breaking Changes","text":"<ul> <li>Directly Modified: The model you changed</li> <li>Non-breaking: Vulcan automatically detected this as non-breaking because:</li> <li>You added a new column</li> <li>Existing columns weren't modified</li> <li>Downstream models aren't affected (they don't use this column yet)</li> <li>Backfill required: The modified model needs to be backfilled to populate the new column</li> </ul>"},{"location":"guides/plan/#step-3-check-downstream-models","title":"Step 3: Check Downstream Models","text":"<p>Notice that <code>sales.weekly_sales</code> (which depends on <code>daily_sales</code>) is not listed. This is because: - The change is non-breaking - Downstream models don't need to be reprocessed - They'll automatically see the new column once <code>daily_sales</code> is backfilled</p>"},{"location":"guides/plan/#step-4-apply-the-plan_1","title":"Step 4: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:05\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Daily sales model being backfilled with new column]</p>"},{"location":"guides/plan/#scenario-4-modifying-a-model-breaking-change","title":"Scenario 4: Modifying a Model - Breaking Change","text":"<p>Breaking changes invalidate existing data and require downstream models to be reprocessed. Adding a WHERE clause is a common breaking change.</p>"},{"location":"guides/plan/#step-1-modify-the-model-with-a-filter","title":"Step 1: Modify the Model with a Filter","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a WHERE clause:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\n  AND order_amount &gt; 10  -- NEW FILTER: Only orders &gt; $10\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-plan_1","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -8,6 +8,7 @@\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n    +   AND order_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales.weekly_sales (Indirect Breaking)\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan showing breaking change with downstream impact]</p>"},{"location":"guides/plan/#understanding-breaking-changes","title":"Understanding Breaking Changes","text":"<ul> <li>Directly Modified: <code>sales.daily_sales</code> - the model you changed</li> <li>Breaking: Vulcan detected this as breaking because:</li> <li>You added a WHERE clause that filters data</li> <li>Existing data may now be invalid (rows that should be filtered out)</li> <li>Indirectly Modified: <code>sales.weekly_sales</code> - downstream model affected</li> <li>Cascading backfill: Both models need to be reprocessed</li> </ul>"},{"location":"guides/plan/#step-3-apply-the-plan_1","title":"Step 3: Apply the Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.1s\n[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.8s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:09\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Both models being backfilled due to breaking change]</p>"},{"location":"guides/plan/#what-happened_1","title":"What Happened?","text":"<ol> <li>Daily sales was backfilled first (upstream)</li> <li>Weekly sales was backfilled second (downstream), using the updated daily sales data</li> <li>Both models now reflect the filtered data (only orders &gt; $10)</li> </ol>"},{"location":"guides/plan/#scenario-5-creating-a-development-environment","title":"Scenario 5: Creating a Development Environment","text":"<p>Development environments let you test changes without affecting production. This scenario shows how to create and use a dev environment.</p>"},{"location":"guides/plan/#step-1-revert-your-changes","title":"Step 1: Revert Your Changes","text":"<p>First, revert the breaking change from Scenario 4 to restore production:</p> <pre><code># Revert daily_sales.sql to remove the WHERE clause filter\n# (Edit the file to remove: AND order_amount &gt; 10)\n</code></pre>"},{"location":"guides/plan/#step-2-apply-reverted-plan-to-production","title":"Step 2: Apply Reverted Plan to Production","text":"<pre><code>vulcan plan\n# Type 'y' to apply\n</code></pre>"},{"location":"guides/plan/#step-3-create-development-environment","title":"Step 3: Create Development Environment","text":"<p>Now create a dev environment with your breaking change:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales__dev.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales__dev.weekly_sales\n\nDirectly Modified: sales__dev.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -8,6 +8,7 @@\n      FROM raw.raw_orders\n      WHERE order_date BETWEEN @start_ds AND @end_ds\n    +   AND order_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales__dev.weekly_sales (Indirect Breaking)\n\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre><p></p> <p>[Screenshot: Creating dev environment with date prompt]</p>"},{"location":"guides/plan/#step-4-specify-date-range","title":"Step 4: Specify Date Range","text":"<p>For faster development, backfill only recent data:</p> <pre><code>Enter the backfill start date: 2025-01-10\n</code></pre> <p>Expected Output: </p><pre><code>Enter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2025-01-15 00:00:00':\n</code></pre><p></p> <p>Press Enter to use the default end date (today).</p> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sales__dev.daily_sales: 2025-01-10 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-10 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Dev environment plan with limited date range]</p>"},{"location":"guides/plan/#step-5-apply-dev-plan","title":"Step 5: Apply Dev Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales__dev.daily_sales     [insert 2025-01-10 - 2025-01-15] 2.1s\n[2/2] sales__dev.weekly_sales    [insert 2025-01-13 - 2025-01-13] 1.5s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:04\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Dev environment models being backfilled]</p>"},{"location":"guides/plan/#understanding-dev-environments","title":"Understanding Dev Environments","text":"<ul> <li>Isolated namespace: Models are prefixed with <code>__dev</code> (e.g., <code>sales__dev.daily_sales</code>)</li> <li>Separate tables: Dev environment has its own physical tables</li> <li>Limited backfill: Only recent data was processed (2025-01-10 to 2025-01-15)</li> <li>Production unaffected: Production data remains unchanged</li> </ul>"},{"location":"guides/plan/#step-6-query-dev-environment","title":"Step 6: Query Dev Environment","text":"<p>You can query the dev environment to verify changes:</p> <pre><code>vulcan fetchdf \"SELECT * FROM sales__dev.daily_sales LIMIT 5\"\n</code></pre> <p>Compare with production:</p> <pre><code>vulcan fetchdf \"SELECT * FROM sales.daily_sales LIMIT 5\"\n</code></pre> <p>[Screenshot: Comparing dev vs prod query results]</p>"},{"location":"guides/plan/#scenario-6-forward-only-plans","title":"Scenario 6: Forward-Only Plans","text":"<p>Forward-only plans reuse existing tables instead of creating new ones, avoiding backfill costs. This is useful for expensive models.</p>"},{"location":"guides/plan/#step-1-modify-model-for-forward-only","title":"Step 1: Modify Model for Forward-Only","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a comment (minimal change):</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grain [order_date]\n);\n\n-- Updated: Added comment for documentation\nSELECT\n  order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value,\n  MAX(order_id) AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan/#step-2-create-forward-only-plan","title":"Step 2: Create Forward-Only Plan","text":"<pre><code>vulcan plan --forward-only\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Forward-only)\n\u2514\u2500\u2500 Diff:\n    @@ -1,6 +1,7 @@\n    MODEL (\n      name sales.daily_sales,\n    ...\n    +-- Updated: Added comment for documentation\n      SELECT\n        order_date,\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-15 - 2025-01-15 (preview)\n\nApply - Virtual Update [y/n]:\n</code></pre><p></p> <p>[Screenshot: Forward-only plan showing preview backfill]</p>"},{"location":"guides/plan/#understanding-forward-only-plans","title":"Understanding Forward-Only Plans","text":"<ul> <li>Forward-only category: Automatically assigned</li> <li>Preview backfill: Only processes the latest interval for preview</li> <li>Virtual Update: No new physical table created</li> <li>Reuses existing table: Production will use the same physical table</li> </ul>"},{"location":"guides/plan/#step-3-apply-forward-only-plan","title":"Step 3: Apply Forward-Only Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Virtual Update [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-15 - 2025-01-15] 0.8s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:01\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Forward-only plan applied with minimal backfill]</p>"},{"location":"guides/plan/#benefits-of-forward-only","title":"Benefits of Forward-Only","text":"<ul> <li>Fast: Only processes latest interval</li> <li>Cost-effective: No full backfill required</li> <li>Safe for production: Reuses existing tables</li> </ul>"},{"location":"guides/plan/#scenario-7-restatement-plans","title":"Scenario 7: Restatement Plans","text":"<p>Restatement plans reprocess existing data without changing model definitions. Useful for fixing data issues or reprocessing after upstream corrections.</p>"},{"location":"guides/plan/#step-1-check-current-state","title":"Step 1: Check Current State","text":"<p>First, verify what data exists:</p> <pre><code>vulcan fetchdf \"SELECT MIN(order_date), MAX(order_date), COUNT(*) FROM sales.daily_sales\"\n</code></pre> <p>[Screenshot: Current data range in daily_sales]</p>"},{"location":"guides/plan/#step-2-create-restatement-plan","title":"Step 2: Create Restatement Plan","text":"<p>Restate the <code>daily_sales</code> model for a specific date range:</p> <pre><code>vulcan plan --restate-model \"sales.daily_sales\" --start \"2025-01-10\" --end \"2025-01-12\"\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nRestatement plan for `prod` environment\n\nModels to restate:\n\u2514\u2500\u2500 sales.daily_sales\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-10 - 2025-01-12\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-06 - 2025-01-13\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Restatement plan showing date range]</p>"},{"location":"guides/plan/#understanding-restatement","title":"Understanding Restatement","text":"<ul> <li>No model changes: Model definition unchanged</li> <li>Cascading restatement: Downstream models (<code>weekly_sales</code>) also need restatement</li> <li>Date range: Only specified dates will be reprocessed</li> </ul>"},{"location":"guides/plan/#step-3-apply-restatement-plan","title":"Step 3: Apply Restatement Plan","text":"<p>Type <code>y</code> to apply:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/2] sales.daily_sales          [insert 2025-01-10 - 2025-01-12] 1.2s\n[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 2.3s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:04\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Restatement plan execution]</p>"},{"location":"guides/plan/#use-cases-for-restatement","title":"Use Cases for Restatement","text":"<ul> <li>Upstream data correction: Raw data was fixed, need to reprocess</li> <li>Bug fixes: Found an issue in data processing logic (after fixing the model)</li> <li>Data refresh: Need to refresh specific date ranges</li> </ul>"},{"location":"guides/plan/#scenario-8-plan-with-explain-flag","title":"Scenario 8: Plan with Explain Flag","text":"<p>The <code>--explain</code> flag provides detailed information about what a plan will do.</p>"},{"location":"guides/plan/#step-1-create-plan-with-explain","title":"Step 1: Create Plan with Explain","text":"<pre><code>vulcan plan --explain\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nExplained plan\n\u251c\u2500\u2500 Validate SQL and create physical layer tables and views if they do not exist\n\u2502   \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n\u2502   \u2502   \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502   \u2502   \u2514\u2500\u2500 Create table if it doesn't exist\n\u2502   \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n\u2502       \u251c\u2500\u2500 Dry run model query without inserting results\n\u2502       \u2514\u2500\u2500 Create table if it doesn't exist\n\u251c\u2500\u2500 Backfill models by running their queries and run standalone audits\n\u2502   \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n\u2502   \u2502   \u2514\u2500\u2500 Fully refresh table\n\u2502   \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n\u2502       \u251c\u2500\u2500 Insert 2025-01-01 - 2025-01-15\n\u2502       \u2514\u2500\u2500 Run 'assert_positive_order_ids' audit\n\u2514\u2500\u2500 Update the virtual layer for environment 'prod'\n    \u2514\u2500\u2500 Create or update views in the virtual layer to point at new physical tables\n        \u251c\u2500\u2500 raw.raw_customers -&gt; db.vulcan__raw.vulcan__raw_customers__1234567890\n        \u2514\u2500\u2500 sales.daily_sales -&gt; db.vulcan__sales.vulcan__sales__daily_sales__9876543210\n</code></pre><p></p> <p>[Screenshot: Explained plan output showing detailed actions]</p>"},{"location":"guides/plan/#understanding-explained-plans","title":"Understanding Explained Plans","text":"<p>The explain output shows three main phases:</p> <ol> <li> <p>Validation &amp; Table Creation:     - Dry runs each model query    - Creates physical tables if needed    - Shows mapping: <code>model_name -&gt; physical_table_name</code></p> </li> <li> <p>Backfill:    - Shows which models will be backfilled    - Indicates backfill type (full refresh vs incremental)    - Lists audits that will run</p> </li> <li> <p>Virtual Layer Update:    - Shows how views will be created/updated    - Maps virtual layer names to physical tables</p> </li> </ol>"},{"location":"guides/plan/#common-plan-scenarios-summary","title":"Common Plan Scenarios Summary","text":"Scenario Command When to Use First Plan <code>vulcan plan</code> Initializing production environment Add Model <code>vulcan plan</code> Adding new models to existing project Non-Breaking Change <code>vulcan plan</code> Adding columns, comments, or non-functional changes Breaking Change <code>vulcan plan</code> Modifying logic that affects downstream models Dev Environment <code>vulcan plan dev</code> Testing changes without affecting production Forward-Only <code>vulcan plan --forward-only</code> Expensive models, avoiding backfill costs Restatement <code>vulcan plan --restate-model \"model\"</code> Reprocessing existing data Explain <code>vulcan plan --explain</code> Understanding detailed plan actions"},{"location":"guides/plan/#best-practices","title":"Best Practices","text":"<ol> <li>Always review plans before applying them</li> <li>Use dev environments for testing breaking changes</li> <li>Use forward-only for expensive models when appropriate</li> <li>Check downstream impact - breaking changes cascade</li> <li>Use <code>--explain</code> when unsure about plan actions</li> <li>Restate carefully - it reprocesses existing data</li> </ol>"},{"location":"guides/plan/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plans Concepts for deeper understanding</li> <li>Explore Environments for managing multiple environments</li> <li>Read about Model Kinds to understand different model types</li> <li>Check Run Guide for scheduled execution after applying plans</li> <li>Set up Notifications to monitor plan execution</li> </ul>"},{"location":"guides/run_and_scheduling/","title":"Run and Scheduling","text":""},{"location":"guides/run_and_scheduling/#run-and-scheduling","title":"Run and Scheduling","text":"<p>This guide covers Vulcan's run functionality and scheduling strategies. Learn how <code>vulcan run</code> processes new data intervals and how to automate it for production.</p>"},{"location":"guides/run_and_scheduling/#run-and-scheduler-architecture","title":"Run and Scheduler Architecture","text":"<p>The following diagram illustrates how Vulcan's run system works with cron-based scheduling:</p> <pre><code>graph TB\n    subgraph \"\u23f0 Scheduler Triggers\"\n        CRON[\ud83d\udd04 Cron Job / CI/CD&lt;br/&gt;Runs periodically]\n        MANUAL[\ud83d\udc64 Manual Execution&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\ud83d\udd0d Run Process\"\n        START[\u26a1 vulcan run&lt;br/&gt;Command starts]\n        CHECK[\ud83d\udd0e Check for missing intervals&lt;br/&gt;Compare with state]\n        CRON_CHECK[\ud83d\udcc5 Check cron schedules&lt;br/&gt;Which models are due?]\n        FILTER[\ud83d\udd3d Filter models&lt;br/&gt;Only process due intervals]\n    end\n\n    subgraph \"\ud83d\udcca Model Execution\"\n        M1[\ud83d\udcc8 sales.daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Due: \u2705]\n        M2[\ud83d\udcca sales.weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Due: \u274c]\n        M3[\ud83d\udcc9 sales.monthly_sales&lt;br/&gt;cron: @monthly&lt;br/&gt;Due: \u274c]\n    end\n\n    subgraph \"\ud83d\udcbe State Management\"\n        STATE[\ud83d\uddc4\ufe0f State Database&lt;br/&gt;Tracks processed intervals]\n        UPDATE[\ud83d\udcdd Update State&lt;br/&gt;Mark intervals as processed]\n    end\n\n    subgraph \"\u2699\ufe0f Execution Flow\"\n        EXEC1[\ud83d\udd04 Execute daily_sales&lt;br/&gt;Process missing intervals]\n        EXEC2[\u23ed\ufe0f Skip weekly_sales&lt;br/&gt;Not due yet]\n        EXEC3[\u23ed\ufe0f Skip monthly_sales&lt;br/&gt;Not due yet]\n    end\n\n    subgraph \"\u2705 Results\"\n        SUCCESS[\u2705 Run Complete&lt;br/&gt;Intervals processed]\n        LOG[\ud83d\udccb Log Results&lt;br/&gt;Execution summary]\n    end\n\n    CRON --&gt;|\"\u23f0 Scheduled\"| START\n    MANUAL --&gt;|\"\ud83d\udc64 Triggered\"| START\n    START --&gt;|\"\ud83d\udd0d\"| CHECK\n    CHECK --&gt;|\"\ud83d\udcca\"| CRON_CHECK\n    CRON_CHECK --&gt;|\"\ud83d\udcc5\"| FILTER\n    FILTER --&gt;|\"\u2705 Due\"| M1\n    FILTER --&gt;|\"\u274c Not due\"| M2\n    FILTER --&gt;|\"\u274c Not due\"| M3\n\n    M1 --&gt;|\"\ud83d\udd04\"| EXEC1\n    M2 --&gt;|\"\u23ed\ufe0f\"| EXEC2\n    M3 --&gt;|\"\u23ed\ufe0f\"| EXEC3\n\n    EXEC1 --&gt;|\"\ud83d\udcbe\"| STATE\n    EXEC2 -.-&gt;|\"\u23ed\ufe0f\"| STATE\n    EXEC3 -.-&gt;|\"\u23ed\ufe0f\"| STATE\n\n    STATE --&gt;|\"\ud83d\udcdd\"| UPDATE\n    UPDATE --&gt;|\"\u2705\"| SUCCESS\n    SUCCESS --&gt;|\"\ud83d\udccb\"| LOG\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style M1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style M2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style M3 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#key-concepts-illustrated","title":"Key Concepts Illustrated","text":"<ol> <li>Scheduler Triggers: Run can be triggered by cron jobs, CI/CD pipelines, or manually</li> <li>Interval Detection: Vulcan checks for missing intervals by comparing current state with model schedules</li> <li>Cron-Based Filtering: Only models whose cron schedules indicate they're due are executed</li> <li>State Tracking: Processed intervals are tracked in the state database</li> <li>Efficient Execution: Models not due are skipped, saving computational resources</li> </ol>"},{"location":"guides/run_and_scheduling/#cron-schedule-flow","title":"Cron Schedule Flow","text":"<p>The following diagram shows how different cron schedules determine model execution:</p> <pre><code>gantt\n    title Model Execution Timeline (Example: Hourly, Daily, Weekly)\n    dateFormat YYYY-MM-DD HH:mm\n    axisFormat %H:%M\n\n    section Hourly Model\n    Run every hour    :active, hourly1, 2025-01-20 00:00, 1h\n    Run every hour    :active, hourly2, 2025-01-20 01:00, 1h\n    Run every hour    :active, hourly3, 2025-01-20 02:00, 1h\n    Run every hour    :active, hourly4, 2025-01-20 03:00, 1h\n\n    section Daily Model\n    Run once daily    :active, daily1, 2025-01-20 00:00, 24h\n\n    section Weekly Model\n    Run once weekly   :active, weekly1, 2025-01-20 00:00, 168h</code></pre> <p>Visual Explanation:  - Hourly models run every hour when <code>vulcan run</code> executes - Daily models run once per day (at the scheduled time) - Weekly models run once per week (at the scheduled time)</p>"},{"location":"guides/run_and_scheduling/#understanding-run-vs-plan","title":"Understanding Run vs Plan","text":"Aspect <code>vulcan plan</code> <code>vulcan run</code> Purpose Apply model changes to environment Execute existing models on schedule When to Use When models are modified/added/removed When no changes, just process new data Change Detection Compares local files vs environment No file comparison needed Backfill Backfills based on changes Processes missing intervals only Cron Schedule Not used (processes all affected dates) Uses model's cron to determine what runs User Interaction Prompts for change categorization Runs automatically Output Shows diffs and change summary Shows execution progress <p>Key Insight: Use <code>plan</code> when you've changed code. Use <code>run</code> for regular scheduled execution.</p>"},{"location":"guides/run_and_scheduling/#how-run-works","title":"How Run Works","text":"<p>The <code>vulcan run</code> command processes missing data intervals for models that haven't changed:</p> <pre><code>flowchart TD\n    START[\u26a1 vulcan run&lt;br/&gt;Command starts] --&gt; CHECK{\ud83d\udd0d Check model&lt;br/&gt;definitions}\n\n    CHECK --&gt;|\"\u274c Changed\"| ERROR[\ud83d\udeab Error: Use 'vulcan plan'&lt;br/&gt;to apply changes first]\n    CHECK --&gt;|\"\u2705 No changes\"| STATE[\ud83d\udcca Query state database&lt;br/&gt;Get processed intervals]\n\n    STATE --&gt; CRON[\ud83d\udcc5 Check cron schedules&lt;br/&gt;Which models are due?]\n\n    CRON --&gt; FILTER{\ud83d\udd3d Filter models&lt;br/&gt;by cron schedule}\n\n    FILTER --&gt;|\"\u2705 Due\"| EXEC1[\ud83d\udd04 Execute Model 1&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"\u2705 Due\"| EXEC2[\ud83d\udd04 Execute Model 2&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"\u274c Not due\"| SKIP1[\u23ed\ufe0f Skip Model 3&lt;br/&gt;Not due yet]\n    FILTER --&gt;|\"\u274c Not due\"| SKIP2[\u23ed\ufe0f Skip Model 4&lt;br/&gt;Not due yet]\n\n    EXEC1 --&gt; UPDATE[\ud83d\udcbe Update state database&lt;br/&gt;Mark intervals as processed]\n    EXEC2 --&gt; UPDATE\n    SKIP1 -.-&gt;|\"\u23ed\ufe0f\"| UPDATE\n    SKIP2 -.-&gt;|\"\u23ed\ufe0f\"| UPDATE\n\n    UPDATE --&gt; SUCCESS[\u2705 Run complete&lt;br/&gt;Summary output]\n\n    ERROR --&gt; END[\u274c Exit with error]\n    SUCCESS --&gt; END\n\n    style START fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style ERROR fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CRON fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style SKIP1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style SKIP2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre> <p>Process Steps:</p> <ol> <li>No Model Changes: Assumes no model definitions have changed</li> <li>Cron-Based Execution: Each model's <code>cron</code> parameter determines if it should run</li> <li>Missing Intervals: Only processes intervals that haven't been processed yet</li> <li>Automatic: No prompts or user interaction required</li> </ol> <p>Interactive Diagrams</p> <p>All diagrams in this guide are interactive! Double-click any diagram to zoom in and explore details. Use drag to pan, arrow keys to navigate, or the zoom controls.</p>"},{"location":"guides/run_and_scheduling/#scenario-1-first-run-processing-new-data","title":"Scenario 1: First Run - Processing New Data","text":"<p>After applying your first plan, use <code>run</code> to process new data as it arrives.</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nModels to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-16 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:02\n\n[1/1] sales.daily_sales          [insert 2025-01-16 - 2025-01-16]   2.1s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: First run output showing new interval processing]</p> <p>What Happened? - <code>sales.daily_sales</code> has <code>cron: '@daily'</code>, so it runs daily - Yesterday's plan processed up to 2025-01-15 - Today (2025-01-16) is a new interval that needs processing - <code>run</code> automatically processes this missing interval</p>"},{"location":"guides/run_and_scheduling/#scenario-2-cron-based-execution","title":"Scenario 2: Cron-Based Execution","text":"<p>Different models can have different <code>cron</code> schedules. <code>run</code> respects each model's schedule.</p>"},{"location":"guides/run_and_scheduling/#daily-model-execution","title":"Daily Model Execution","text":"<pre><code>vulcan run\n</code></pre> <p>Expected Output (Day 2): </p><pre><code>Models to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-17 (1 interval)\n</code></pre><p></p> <p>[Screenshot: Daily run showing only daily model executed]</p>"},{"location":"guides/run_and_scheduling/#weekly-model-execution","title":"Weekly Model Execution","text":"<p>After 7 days, both daily and weekly models run:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>Models to execute:\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-18 - 2025-01-24 (7 intervals)\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-20 - 2025-01-20 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:08\n\n[1/2] sales.daily_sales          [insert 2025-01-18 - 2025-01-24]   5.2s\n[2/2] sales.weekly_sales         [insert 2025-01-20 - 2025-01-20]   2.8s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: Weekly run showing both daily and weekly models]</p> <p>Understanding Cron Schedules: - Daily model (<code>@daily</code>): Processes missing daily intervals - Weekly model (<code>@weekly</code>): Only processes when 7 days have elapsed - Efficient: Each model only processes what's due based on its schedule</p>"},{"location":"guides/run_and_scheduling/#scenario-3-run-with-no-missing-intervals","title":"Scenario 3: Run with No Missing Intervals","text":"<p>When all intervals are up to date, <code>run</code> skips execution:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nNo models to execute. All intervals are up to date.\n\n\u2714 Run completed successfully\n</code></pre><p></p> <p>[Screenshot: Run output showing no models to execute]</p> <p>This is normal when running frequently - nothing to process means everything is up to date.</p>"},{"location":"guides/run_and_scheduling/#scenario-4-run-after-model-changes-error-case","title":"Scenario 4: Run After Model Changes (Error Case)","text":"<p>If models have changed, Vulcan detects this and requires a plan first:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nError: Model definitions have changed. Use 'vulcan plan' to apply changes first.\n\nChanged models:\n\u2514\u2500\u2500 sales.daily_sales\n\nPlease run 'vulcan plan' to apply these changes before using 'vulcan run'.\n</code></pre><p></p> <p>[Screenshot: Error message when trying to run with model changes]</p> <p>Workflow: Always <code>plan</code> first to apply changes, then <code>run</code> for scheduled execution.</p>"},{"location":"guides/run_and_scheduling/#scheduling-for-production","title":"Scheduling for Production","text":"<p>The <code>vulcan run</code> command doesn't run continuously - it executes once and exits. For production, you need to schedule it to run periodically.</p>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-architecture","title":"Built-in Scheduler Architecture","text":"<pre><code>graph TB\n    subgraph \"\ud83d\udd04 Automation Layer - Triggers\"\n        CRON[\u23f0 Cron Job&lt;br/&gt;Schedule: Every hour&lt;br/&gt;Example: 0 * * * *]\n        CI[\ud83d\ude80 CI/CD Pipeline&lt;br/&gt;GitHub Actions / GitLab CI&lt;br/&gt;Scheduled workflows]\n        K8S[\u2638\ufe0f Kubernetes CronJob&lt;br/&gt;Container orchestration&lt;br/&gt;K8s native scheduling]\n        MANUAL[\ud83d\udc64 Manual Trigger&lt;br/&gt;Developer runs manually&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\u26a1 Vulcan Run Command\"\n        RUN[vulcan run&lt;br/&gt;Command starts]\n        VALIDATE[\u2705 Validate Models&lt;br/&gt;Check for changes&lt;br/&gt;Error if modified]\n        QUERY[\ud83d\udd0d Query State Database&lt;br/&gt;Get execution history&lt;br/&gt;Read processed intervals]\n    end\n\n    subgraph \"\ud83d\udcbe State Database\"\n        STATE[\ud83d\uddc4\ufe0f State Storage&lt;br/&gt;PostgreSQL / SQL Engine&lt;br/&gt;Transaction-safe storage]\n\n        subgraph \"\ud83d\udcca State Tables\"\n            INTERVALS[\ud83d\udccb Processed Intervals&lt;br/&gt;model_name, start_ds, end_ds&lt;br/&gt;status: completed]\n            CRON_STATE[\u23f0 Cron Execution State&lt;br/&gt;model_name, last_run_time&lt;br/&gt;next_run_time]\n            MODEL_STATE[\ud83d\udd37 Model State&lt;br/&gt;model_name, fingerprint&lt;br/&gt;environment, version]\n        end\n    end\n\n    subgraph \"\ud83d\udcc5 Cron Evaluation Engine\"\n        CRON_CHECK[\ud83d\udcc5 Evaluate Cron Schedules&lt;br/&gt;Compare current time&lt;br/&gt;with last execution]\n        CALC[\ud83e\uddee Calculate Missing Intervals&lt;br/&gt;Determine what's due&lt;br/&gt;Based on cron + state]\n        FILTER[\ud83d\udd3d Filter Models&lt;br/&gt;Only select due models&lt;br/&gt;Skip not-due models]\n    end\n\n    subgraph \"\ud83d\udcca Model Execution Queue\"\n        QUEUE[\ud83d\udccb Execution Queue&lt;br/&gt;Ordered by dependencies&lt;br/&gt;Upstream first]\n        EXEC1[\ud83d\udd04 Execute Hourly Model&lt;br/&gt;@hourly - Due \u2705&lt;br/&gt;Process missing intervals]\n        EXEC2[\ud83d\udd04 Execute Daily Model&lt;br/&gt;@daily - Due \u2705&lt;br/&gt;Process missing intervals]\n        SKIP[\u23ed\ufe0f Skip Weekly Model&lt;br/&gt;@weekly - Not due \u274c&lt;br/&gt;Wait for next week]\n    end\n\n    subgraph \"\ud83d\udcbe Update State\"\n        UPDATE[\ud83d\udcdd Update State Database&lt;br/&gt;Mark intervals processed&lt;br/&gt;Update cron state]\n        COMMIT[\u2705 Commit Transaction&lt;br/&gt;Ensure consistency&lt;br/&gt;Rollback on error]\n    end\n\n    subgraph \"\ud83d\udcca Results &amp; Logging\"\n        LOG[\ud83d\udccb Log Execution&lt;br/&gt;Summary output&lt;br/&gt;Success/failure status]\n        NOTIFY[\ud83d\udd14 Notifications&lt;br/&gt;Optional: Slack/Email&lt;br/&gt;On success/failure]\n    end\n\n    CRON --&gt;|\"\u23f0 Scheduled trigger\"| RUN\n    CI --&gt;|\"\ud83d\ude80 Pipeline trigger\"| RUN\n    K8S --&gt;|\"\u2638\ufe0f K8s trigger\"| RUN\n    MANUAL --&gt;|\"\ud83d\udc64 Manual trigger\"| RUN\n\n    RUN --&gt;|\"1\ufe0f\u20e3 Validate\"| VALIDATE\n    VALIDATE --&gt;|\"2\ufe0f\u20e3 Query state\"| QUERY\n    QUERY --&gt;|\"\ud83d\udcca Read\"| STATE\n\n    STATE --&gt;|\"\ud83d\udccb Intervals\"| INTERVALS\n    STATE --&gt;|\"\u23f0 Cron state\"| CRON_STATE\n    STATE --&gt;|\"\ud83d\udd37 Model state\"| MODEL_STATE\n\n    INTERVALS --&gt;|\"\ud83d\udd0e Compare\"| CRON_CHECK\n    CRON_STATE --&gt;|\"\ud83d\udcc5 Check schedule\"| CRON_CHECK\n    MODEL_STATE --&gt;|\"\ud83d\udd37 Get models\"| CRON_CHECK\n\n    CRON_CHECK --&gt;|\"\ud83d\udcc5 Evaluate\"| CALC\n    CALC --&gt;|\"\ud83e\uddee Calculate\"| FILTER\n\n    FILTER --&gt;|\"\u2705 Due models\"| QUEUE\n    FILTER -.-&gt;|\"\u274c Skip\"| SKIP\n\n    QUEUE --&gt;|\"\ud83d\udd04 Execute\"| EXEC1\n    QUEUE --&gt;|\"\ud83d\udd04 Execute\"| EXEC2\n\n    EXEC1 --&gt;|\"\ud83d\udcbe Update\"| UPDATE\n    EXEC2 --&gt;|\"\ud83d\udcbe Update\"| UPDATE\n    SKIP -.-&gt;|\"\u23ed\ufe0f No update\"| UPDATE\n\n    UPDATE --&gt;|\"\ud83d\udcbe Commit\"| COMMIT\n    COMMIT --&gt;|\"\u2705 Success\"| LOG\n    LOG --&gt;|\"\ud83d\udd14 Optional\"| NOTIFY\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CI fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style K8S fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style MANUAL fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style VALIDATE fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style STATE fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style INTERVALS fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style MODEL_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CALC fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style QUEUE fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style EXEC2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style COMMIT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style LOG fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NOTIFY fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-components","title":"Built-in Scheduler Components","text":"<p>The built-in scheduler consists of several key components working together:</p> <ol> <li>Automation Layer: External triggers (cron, CI/CD, Kubernetes) that periodically execute <code>vulcan run</code></li> <li>State Database: Stores execution history, processed intervals, and cron state</li> <li>Cron Evaluation Engine: Determines which models are due based on their schedules</li> <li>Execution Queue: Orders models by dependencies and executes them</li> <li>State Updates: Records what was processed for future runs</li> </ol> <p>Key Features: - \u2705 Stores state in your SQL engine (or separate state database) - \u2705 Automatically detects missing intervals - \u2705 Respects each model's <code>cron</code> schedule - \u2705 Processes only what's due - \u2705 Transaction-safe state updates - \u2705 Dependency-aware execution order</p>"},{"location":"guides/run_and_scheduling/#setting-up-automation","title":"Setting Up Automation","text":"<p>Run <code>vulcan run</code> periodically using one of these methods:</p>"},{"location":"guides/run_and_scheduling/#option-1-linuxmac-cron-job","title":"Option 1: Linux/Mac Cron Job","text":"<pre><code># Edit crontab\ncrontab -e\n\n# Run every hour\n0 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n\n# Run every 15 minutes\n*/15 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n</code></pre>"},{"location":"guides/run_and_scheduling/#option-2-cicd-pipeline","title":"Option 2: CI/CD Pipeline","text":"<p>GitHub Actions Example: </p><pre><code>name: Vulcan Run\non:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  workflow_dispatch:\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Vulcan\n        run: |\n          docker run --network=vulcan --rm \\\n            -v $PWD:/workspace \\\n            tmdcio/vulcan:latest vulcan run\n</code></pre><p></p> <p>GitLab CI Example: </p><pre><code>vulcan_run:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  script:\n    - docker run --network=vulcan --rm \\\n        -v $PWD:/workspace \\\n        tmdcio/vulcan:latest vulcan run\n</code></pre><p></p>"},{"location":"guides/run_and_scheduling/#option-3-kubernetes-cronjob","title":"Option 3: Kubernetes CronJob","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: vulcan-run\nspec:\n  schedule: \"0 * * * *\"  # Every hour\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: vulcan\n            image: tmdcio/vulcan:latest\n            command: [\"vulcan\", \"run\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"guides/run_and_scheduling/#determining-run-frequency","title":"Determining Run Frequency","text":"<p>Set your automation frequency based on your most frequent model's <code>cron</code>:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Cron Schedules\"\n        H[\u23f0 Hourly Model&lt;br/&gt;cron: @hourly]\n        D[\ud83d\udcc5 Daily Model&lt;br/&gt;cron: @daily]\n        W[\ud83d\udcc6 Weekly Model&lt;br/&gt;cron: @weekly]\n    end\n\n    subgraph \"\ud83d\udd04 Automation Frequency\"\n        AUTO_H[\u23f0 Run every hour&lt;br/&gt;vulcan run]\n        AUTO_D[\ud83d\udcc5 Run daily&lt;br/&gt;vulcan run]\n        AUTO_W[\ud83d\udcc6 Run weekly&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"\u2705 Execution Result\"\n        RESULT1[\u2705 Hourly: Runs every time&lt;br/&gt;\u2705 Daily: Runs when due&lt;br/&gt;\u2705 Weekly: Runs when due]\n        RESULT2[\u23ed\ufe0f Hourly: Skipped&lt;br/&gt;\u2705 Daily: Runs when due&lt;br/&gt;\u2705 Weekly: Runs when due]\n        RESULT3[\u23ed\ufe0f Hourly: Skipped&lt;br/&gt;\u23ed\ufe0f Daily: Skipped&lt;br/&gt;\u2705 Weekly: Runs when due]\n    end\n\n    H --&gt;|\"Requires\"| AUTO_H\n    D --&gt;|\"Can use\"| AUTO_H\n    W --&gt;|\"Can use\"| AUTO_H\n\n    AUTO_H --&gt;|\"Hour 1\"| RESULT1\n    AUTO_H --&gt;|\"Hour 2-23\"| RESULT2\n    AUTO_H --&gt;|\"Week 1\"| RESULT3\n\n    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style W fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style AUTO_H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RESULT2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT3 fill:#ffe082,stroke:#f9a825,stroke-width:2px,color:#000</code></pre> <p>Rule: Schedule <code>vulcan run</code> based on your fastest model's cron.</p> <ul> <li>Hourly models \u2192 Run automation every hour</li> <li>Daily models \u2192 Run automation daily  </li> <li>Weekly models \u2192 Run automation weekly</li> </ul> <p>Example: If your fastest model runs <code>@hourly</code>, schedule <code>vulcan run</code> to execute hourly. Models with slower schedules (daily, weekly) will only process when their intervals are due.</p>"},{"location":"guides/run_and_scheduling/#advanced-run-options","title":"Advanced Run Options","text":""},{"location":"guides/run_and_scheduling/#run-specific-models","title":"Run Specific Models","text":"<pre><code>vulcan run --select-model \"sales.daily_sales\"\n</code></pre> <p>Processes only the specified model and its upstream dependencies.</p>"},{"location":"guides/run_and_scheduling/#ignore-cron-schedules","title":"Ignore Cron Schedules","text":"<pre><code>vulcan run --ignore-cron\n</code></pre> <p>Processes all missing intervals regardless of cron schedules. Use sparingly - typically for catching up after downtime.</p>"},{"location":"guides/run_and_scheduling/#custom-execution-time","title":"Custom Execution Time","text":"<pre><code>vulcan run --execution-time \"2025-01-20 10:00:00\"\n</code></pre> <p>Simulates running at a specific time. Useful for testing cron schedules.</p>"},{"location":"guides/run_and_scheduling/#run-in-different-environments","title":"Run in Different Environments","text":"<pre><code>vulcan run dev\n</code></pre> <p>Runs models in the <code>dev</code> environment, maintaining separate execution state from production.</p>"},{"location":"guides/run_and_scheduling/#state-database-considerations","title":"State Database Considerations","text":"<p>By default, Vulcan stores scheduler state in your SQL engine. For production:</p> <p>Recommended: Use a separate PostgreSQL database for state storage when: - Your SQL engine is BigQuery (not optimized for frequent transactions) - You observe performance degradation - You need better isolation</p> <p>See Connections Guide for configuring a separate state database.</p>"},{"location":"guides/run_and_scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>run</code> for scheduled execution - Don't use <code>plan</code> for regular data processing</li> <li>Set up automation - Schedule <code>vulcan run</code> based on your most frequent model's cron</li> <li>Monitor execution - Check logs to ensure intervals are processing correctly</li> <li>Use <code>--ignore-cron</code> sparingly - Only when catching up on missed intervals</li> <li>Separate state database - Consider PostgreSQL for state storage in production</li> <li>Handle errors gracefully - Set up notifications for run failures</li> </ol>"},{"location":"guides/run_and_scheduling/#quick-reference","title":"Quick Reference","text":"Scenario Command When to Use Regular Run <code>vulcan run</code> Scheduled execution (cron jobs, CI/CD) Dev Environment <code>vulcan run dev</code> Running models in dev environment Select Models <code>vulcan run --select-model \"model\"</code> Running specific models only Ignore Cron <code>vulcan run --ignore-cron</code> Catch up on all missing intervals Custom Time <code>vulcan run --execution-time \"...\"</code> Testing/simulating runs"},{"location":"guides/run_and_scheduling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plan Guide for applying model changes</li> <li>Check Run Command for complete CLI reference</li> <li>Set up Notifications to monitor run execution</li> <li>Explore Environments for managing multiple environments</li> <li>Configure Connections for state database setup</li> </ul>"},{"location":"guides/transpiling_semantics/","title":"Transpiling Semantics","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantics","title":"Transpiling Semantics","text":"<p>The <code>vulcan transpile</code> command converts semantic queries into executable SQL, allowing you to preview, debug, and validate semantic logic before execution.</p>"},{"location":"guides/transpiling_semantics/#what-is-transpilation","title":"What is Transpilation?","text":"<p>Transpilation transforms semantic layer queries into database-specific SQL:</p> <ul> <li>Semantic SQL \u2192 Native SQL: Converts semantic SQL queries with <code>MEASURE()</code> functions into standard SQL</li> <li>REST API Payload \u2192 Native SQL: Converts JSON query payloads into executable SQL statements</li> <li>Validation: Catches errors before query execution</li> <li>Debugging: Inspect the generated SQL to understand query behavior</li> </ul>"},{"location":"guides/transpiling_semantics/#basic-structure","title":"Basic Structure","text":""},{"location":"guides/transpiling_semantics/#semantic-sql-query-structure","title":"Semantic SQL Query Structure","text":"<p>Semantic SQL queries follow standard SQL syntax with semantic layer extensions:</p> <pre><code>SELECT \n  alias.dimension_name,           # Dimensions: attributes for grouping and filtering\n  MEASURE(alias.measure_name)  # Measures: aggregated calculations (required wrapper)\nFROM alias                        # Semantic model alias (business-friendly name)\nCROSS JOIN other_alias            # Optional: join multiple models\nWHERE \n  alias.dimension_name = 'value'  # Optional: filter on dimensions\n  AND segment_name = true         # Optional: use segments (only = true supported)\nGROUP BY alias.dimension_name     # Required: all non-aggregated columns\nORDER BY MEASURE(alias.measure_name)    # Optional: sort results\nLIMIT 100                         # Optional: limit result set\nOFFSET 0                          # Optional: pagination offset\n</code></pre> <p>Key Components: - <code>alias.dimension_name</code> \u2014 Reference dimensions using semantic model alias - <code>MEASURE(measure_name)</code> \u2014 Required wrapper for measures to apply aggregation - <code>FROM alias</code> \u2014 Use semantic model alias, not physical model name - <code>CROSS JOIN</code> \u2014 Join syntax (join conditions automatically inferred) - <code>segment_name = true</code> \u2014 Segments only support <code>= true</code>, not <code>= false</code></p>"},{"location":"guides/transpiling_semantics/#rest-api-payload-structure","title":"REST API Payload Structure","text":"<p>REST API queries use JSON payloads with semantic query definitions:</p> <pre><code>{\n  \"query\": {\n    \"measures\": [\"alias.measure_name\"],              # Required: array of measure names\n    \"dimensions\": [\"alias.dimension_name\"],         # Optional: array of dimension names\n    \"segments\": [\"segment_name\"],                    # Optional: array of segment names\n    \"timeDimensions\": [{                             # Optional: array of time dimension objects\n      \"dimension\": \"alias.time_dimension\",           # Required: time dimension member\n      \"dateRange\": [\"2024-01-01\", \"2024-12-31\"],    # Optional: date range array or string\n      \"granularity\": \"month\"                         # Optional: hour, day, week, month, quarter, year\n    }],\n    \"filters\": [{                                    # Optional: array of filter objects\n      \"member\": \"alias.dimension_name\",              # Required: fully qualified member name\n      \"operator\": \"equals\",                          # Required: filter operator\n      \"values\": [\"value1\", \"value2\"]                 # Optional: array of filter values\n    }],\n    \"order\": {                                       # Optional: sort order object\n      \"alias.measure_name\": \"desc\",                  # Member name: \"asc\" or \"desc\"\n      \"alias.dimension_name\": \"asc\"\n    },\n    \"limit\": 100,                                    # Optional: maximum rows to return\n    \"offset\": 0,                                     # Optional: rows to skip\n    \"timezone\": \"UTC\",                               # Optional: timezone for date parsing\n    \"renewQuery\": false                              # Optional: bypass cache if true\n  },\n  \"ttl_minutes\": 60                                  # Optional: cache duration in minutes\n}\n</code></pre> <p>Key Components: - <code>measures</code> \u2014 Array of fully qualified measure names: <code>\"alias.measure_name\"</code> - <code>dimensions</code> \u2014 Array of fully qualified dimension names: <code>\"alias.dimension_name\"</code> - <code>segments</code> \u2014 Array of segment names (no alias prefix needed) - <code>timeDimensions</code> \u2014 Array of objects with <code>dimension</code>, <code>dateRange</code>, and <code>granularity</code> - <code>filters</code> \u2014 Array of filter objects with <code>member</code>, <code>operator</code>, and <code>values</code> - <code>order</code> \u2014 Object mapping member names to sort direction (<code>\"asc\"</code> or <code>\"desc\"</code>)</p>"},{"location":"guides/transpiling_semantics/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql-queries","title":"Transpiling Semantic SQL Queries","text":"<p>Convert semantic SQL queries to native SQL:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Output: Generated SQL that can be executed directly against your database.</p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads","title":"Transpiling REST API Payloads","text":"<p>Convert JSON query payloads to native SQL:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Output: Generated SQL from the REST-style query definition.</p>"},{"location":"guides/transpiling_semantics/#command-syntax","title":"Command Syntax","text":""},{"location":"guides/transpiling_semantics/#basic-format","title":"Basic Format","text":"<pre><code>vulcan transpile --format &lt;format&gt; \"&lt;query&gt;\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>--format</code> (required) \u2014 Output format: <code>sql</code> or <code>json</code></li> <li><code>\"&lt;query&gt;\"</code> (required) \u2014 The semantic query to transpile</li> <li>For SQL format: Semantic SQL query string</li> <li>For JSON format: JSON query payload string</li> </ul>"},{"location":"guides/transpiling_semantics/#advanced-options","title":"Advanced Options","text":"<pre><code>vulcan transpile --format sql \"&lt;query&gt;\" [--disable-post-processing]\n</code></pre> <p>Options:</p> <ul> <li><code>--disable-post-processing</code> \u2014 Enable pushdown mode for CTE support and advanced SQL features</li> <li>Default: Post-processing enabled (CTEs not supported)</li> <li>With flag: Pushdown enabled (CTEs supported, no pre-aggregations)</li> </ul>"},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql","title":"Transpiling Semantic SQL","text":""},{"location":"guides/transpiling_semantics/#basic-query","title":"Basic Query","text":"<p>Transpile a simple semantic SQL query:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions and grouping:</p> <pre><code>vulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".plan_type, sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\nGROUP BY \"users\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters","title":"Query with Filters","text":"<p>Transpile queries with WHERE conditions:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-grouping","title":"Query with Time Grouping","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format sql \"SELECT DATE_TRUNC('month', subscriptions.start_date) as month, MEASURE(total_arr) FROM subscriptions GROUP BY month\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS month,\n       sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-joins","title":"Query with Joins","text":"<p>Transpile queries joining multiple models:</p> <pre><code>vulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users GROUP BY users.industry\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".industry, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".user_id = \"users\".user_id\nGROUP BY \"users\".industry\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads_1","title":"Transpiling REST API Payloads","text":""},{"location":"guides/transpiling_semantics/#minimal-query","title":"Minimal Query","text":"<p>Transpile a basic REST API query:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions_1","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"dimensions\": [\"subscriptions.plan_type\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"subscriptions\".plan_type, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY \"subscriptions\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-dimensions","title":"Query with Time Dimensions","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"orders.total_revenue\"], \"timeDimensions\": [{\"dimension\": \"orders.order_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"orders\".order_date) AS orders_order_date_month,\n       sum(\"orders\".amount) AS total_revenue\nFROM analytics.orders AS \"orders\"\nWHERE \"orders\".order_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"orders\".order_date &lt;= '2024-12-31T23:59:59.999'\nGROUP BY DATE_TRUNC('month', \"orders\".order_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters_1","title":"Query with Filters","text":"<p>Transpile queries with filters:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-segments","title":"Query with Segments","text":"<p>Transpile queries using segments:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"segments\": [\"subscriptions.active_subscriptions\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".end_date IS NULL\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#complex-query","title":"Complex Query","text":"<p>Transpile complex queries with multiple components:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\", \"subscriptions.total_seats\"], \"dimensions\": [\"subscriptions.plan_type\", \"users.industry\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}], \"timeDimensions\": [{\"dimension\": \"subscriptions.start_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}], \"order\": {\"subscriptions.total_arr\": \"desc\"}, \"limit\": 100}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS subscriptions_start_date_month,\n       \"subscriptions\".plan_type,\n       \"users\".industry,\n       sum(\"subscriptions\".arr) AS total_arr,\n       sum(\"subscriptions\".seats) AS total_seats\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".start_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"subscriptions\".start_date &lt;= '2024-12-31T23:59:59.999'\n  AND \"subscriptions\".user_id = \"users\".user_id\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date),\n         \"subscriptions\".plan_type,\n         \"users\".industry\nORDER BY sum(\"subscriptions\".arr) DESC\nLIMIT 100\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#use-cases","title":"Use Cases","text":""},{"location":"guides/transpiling_semantics/#query-validation","title":"Query Validation","text":"<p>Validate semantic queries before execution:</p> <pre><code># Check if query syntax is correct\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>If the query is invalid, you'll get an error message indicating the issue.</p>"},{"location":"guides/transpiling_semantics/#debugging-query-behavior","title":"Debugging Query Behavior","text":"<p>Inspect generated SQL to understand how semantic queries are translated:</p> <pre><code># See how measures are aggregated\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#performance-analysis","title":"Performance Analysis","text":"<p>Review generated SQL to identify optimization opportunities:</p> <pre><code># Check join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#documentation","title":"Documentation","text":"<p>Generate SQL examples for documentation or training:</p> <pre><code># Create SQL reference from semantic queries\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"guides/transpiling_semantics/#error-unknown-member-x","title":"Error: \"Unknown member: X\"","text":"<p>Cause: Member doesn't exist in semantic model or is misspelled.</p> <p>Solution: - Verify member exists in your semantic model - Check spelling and casing (case-sensitive) - Use fully qualified format: <code>alias.member_name</code></p>"},{"location":"guides/transpiling_semantics/#error-measure-not-found-x","title":"Error: \"Measure not found: X\"","text":"<p>Cause: Measure referenced without proper qualification or doesn't exist.</p> <p>Solution: - Use <code>MEASURE(measure_name)</code> wrapper for SQL format - Use fully qualified format: <code>alias.measure_name</code> for JSON format - Verify measure is defined in semantic model</p>"},{"location":"guides/transpiling_semantics/#error-model-not-found-x","title":"Error: \"Model not found: X\"","text":"<p>Cause: Alias doesn't match any semantic model.</p> <p>Solution: - Check semantic model aliases in your <code>semantics/</code> directory - Verify alias spelling and casing - Ensure semantic models are properly defined</p>"},{"location":"guides/transpiling_semantics/#error-invalid-json-format","title":"Error: \"Invalid JSON format\"","text":"<p>Cause: JSON payload is malformed.</p> <p>Solution: - Validate JSON syntax - Ensure proper quoting of strings - Check array and object structure</p>"},{"location":"guides/transpiling_semantics/#error-projection-references-non-aggregate-values","title":"Error: \"Projection references non-aggregate values\"","text":"<p>Cause: Non-aggregated columns not in GROUP BY, or measures missing MEASURE() wrapper.</p> <p>Solution: - Add all non-aggregated columns to GROUP BY - Use MEASURE() wrapper for all measures in SQL format</p>"},{"location":"guides/transpiling_semantics/#best-practices","title":"Best Practices","text":""},{"location":"guides/transpiling_semantics/#validate-before-execution","title":"Validate Before Execution","text":"<p>Always transpile queries before running them in production:</p> <pre><code># \u2705 Good: Validate first\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n# Review output, then execute\n\n# \u274c Bad: Execute without validation\n# Direct execution without checking generated SQL\n</code></pre>"},{"location":"guides/transpiling_semantics/#use-transpilation-for-debugging","title":"Use Transpilation for Debugging","text":"<p>When queries return unexpected results, transpile to inspect generated SQL:</p> <pre><code># Debug query behavior\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Compare generated SQL with expected behavior\n</code></pre>"},{"location":"guides/transpiling_semantics/#document-query-patterns","title":"Document Query Patterns","text":"<p>Use transpilation output to document common query patterns:</p> <pre><code># Generate SQL examples for documentation\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#test-both-formats","title":"Test Both Formats","text":"<p>When building applications, test both SQL and JSON formats:</p> <pre><code># Test SQL format\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n\n# Test equivalent JSON format\nvulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre>"},{"location":"guides/transpiling_semantics/#choose-appropriate-mode","title":"Choose Appropriate Mode","text":"<p>Select post-processing or pushdown mode based on needs:</p> <ul> <li>Post-processing (default): Use for queries that benefit from pre-aggregations and caching</li> <li>Pushdown (<code>--disable-post-processing</code>): Use when you need CTEs or complex SQL structures</li> </ul>"},{"location":"guides/transpiling_semantics/#integration-with-development-workflow","title":"Integration with Development Workflow","text":""},{"location":"guides/transpiling_semantics/#pre-commit-validation","title":"Pre-commit Validation","text":"<p>Add transpilation checks to your development workflow:</p> <pre><code># Validate semantic queries in CI/CD\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#query-testing","title":"Query Testing","text":"<p>Use transpilation to generate test SQL:</p> <pre><code># Generate SQL for testing\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Use output in test assertions\n</code></pre>"},{"location":"guides/transpiling_semantics/#performance-tuning","title":"Performance Tuning","text":"<p>Analyze generated SQL for optimization:</p> <pre><code># Review join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Semantic Models that define the queryable members</li> <li>Explore Business Metrics for time-series analysis</li> <li>See the Semantics Overview for the complete picture</li> </ul>"},{"location":"guides/get-started/docker/","title":"Get Started","text":""},{"location":"guides/get-started/docker/#get-started","title":"Get Started","text":"<p>Welcome to the Vulcan quickstart, which will get you up and running with an example project.</p> <p>The example project runs locally on your machine with a Postgres SQL engine, and Vulcan will generate all the necessary project files - no configuration necessary!</p> <p>All you need to do is download Vulcan on your machine - get started by ensuring your system meets the basic prerequisites for using Vulcan.</p>"},{"location":"guides/get-started/docker/#prerequisites","title":"Prerequisites","text":"Mac/LinuxWindows <ol> <li>Docker Desktop is installed and running, at least with 4GB RAM, Verify that Docker is installed and running:  <pre><code>docker --version\ndocker compose version\n</code></pre></li> <li> <p>If not Download from Docker Desktop for Mac</p> </li> <li> <p>Install Docker Engine and Docker Compose from Docker for Linux</p> </li> </ol> <ol> <li> <p>Docker Desktop for Windows installed and running, at least 4GB RAM, Verify that Docker is installed and running:  </p><pre><code>docker --version\ndocker compose version\n</code></pre><p></p> </li> <li> <p>If not Download from Docker Desktop for Windows</p> </li> </ol>"},{"location":"guides/get-started/docker/#vulcan-setup-locally","title":"Vulcan Setup Locally","text":"Mac/LinuxWindows <p> Download for Mac/Linux</p> <p>Contains: Docker Compose files, Makefile, and README</p> <ol> <li> <p>Extract the zip file and navigate to the directory: </p><pre><code>cd vulcan-project\n</code></pre><p></p> </li> <li> <p>Run setup: </p><pre><code>make setup\n</code></pre>    This creates:<p></p> <ul> <li>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, &amp; execution history, and Vulcan uses it to persist the semantic model, plans, and track materialization state</li> <li>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates, and  Vulcan uses it to store query results and artifacts, enabling efficient data retrieval and caching</li> <li>minio-init: Initializes MinIO buckets and policies, and these services are essential for Vulcan's operation and must be running before you can use Vulcan</li> </ul> </li> <li> <p>Access Vulcan: </p><pre><code>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan\"\n</code></pre> Note: This alias is temporary and will be lost when you close your shell session. To make it permanent, add it to your shell configuration file (~/.bashrc or ~/.zshrc).<p></p> </li> <li> <p>Start API Services: </p><pre><code>make vulcan-up\n</code></pre>    This starts vulcan-api for querying your semantic model by REST API(available at <code>http://localhost:8000</code>) &amp;     vulcan-transpiler for transpiling semantic queries to SQL<p></p> </li> </ol> <p> Download for Windows</p> <p>Contains: Docker Compose files, Windows batch scripts, and README</p> <ol> <li> <p>Extract the zip file and navigate to the directory: </p><pre><code>cd vulcan-project\n</code></pre><p></p> </li> <li> <p>Run setup: </p><pre><code>setup.bat\n</code></pre>    This creates:<p></p> <ul> <li>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, &amp; execution history, and Vulcan uses it to persist the semantic model, plans, and track materialization state</li> <li>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates, and  Vulcan uses it to store query results and artifacts, enabling efficient data retrieval and caching</li> <li>minio-init: Initializes MinIO buckets and policies, and these services are essential for Vulcan's operation and must be running before you can use Vulcan</li> </ul> </li> <li> <p>Access Vulcan: </p><pre><code>vulcan.bat\n</code></pre><p></p> </li> <li> <p>Start API Services: </p><pre><code>start-vulcan-api.bat\n</code></pre>    This starts:<p></p> <p>vulcan-api for querying your semantic model by REST API(available at <code>http://localhost:8000</code>) &amp;</p> <p>vulcan-transpiler for transpiling semantic queries to SQL</p> </li> </ol>"},{"location":"guides/get-started/docker/#create-your-first-project","title":"Create Your First Project","text":"Mac/Linux <ol> <li> <p>Initialize project: Click here </p><pre><code>vulcan init\n</code></pre>        Choose <code>DEFAULT</code> project type and <code>Postgres</code> as SQL engine. <p></p> <pre><code>It creates 7 `directories` containing SQL/PYTHON models,seed data files, audit files, test files, macro files, checks files, and semantics files\n</code></pre> </li> <li> <p>Check connection and number of models, macros, and other project components: Click here </p><pre><code>vulcan info\n</code></pre>       It verifies that the setup is correct before running plans<p></p> </li> <li> <p>Run the plan: Click here </p><pre><code>vulcan plan\n</code></pre>       This will:<p></p> <pre><code>1. Validate and creates the necessary database objects (tables, views, etc.) based on your models\n2. Backfills historical data according to your model's `start` date and `cron` schedule\n3. Prompt you to apply the plan\n\nEnter `y` when prompted to apply the plan and backfill your models.\n</code></pre> </li> <li> <p>Query the models: Click here </p><pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre>        Executes a SQL query and returns results as a pandas DataFrame<p></p> </li> <li> <p>Query the Semantic: Click here </p><pre><code> vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre>       Returns the generated SQL that can be executed against your warehouse<p></p> </li> </ol> Windows <ol> <li> <p>Initialize project: Click here </p><pre><code>vulcan init\n</code></pre>        Choose <code>DEFAULT</code> project type and <code>Postgres</code> as SQL engine. <p></p> <pre><code>It creates 7 `directories` containing SQL/PYTHON models,seed data files, audit files, test files, macro files, checks files, and semantics files\n</code></pre> </li> <li> <p>Check connection and number of models, macros, and other project components: Click here </p><pre><code>vulcan info\n</code></pre>        It verifies that the setup is correct before running plans<p></p> </li> <li> <p>Run the plan: Click here </p><pre><code>vulcan plan\n</code></pre>       This will:<p></p> <pre><code>1. Validate and creates the necessary database objects (tables, views, etc.) based on your models\n2. Backfills historical data according to your model's `start` date and `cron` schedule\n3. Prompt you to apply the plan\n\nEnter `y` when prompted to apply the plan and backfill your models.\n</code></pre> </li> <li> <p>Query the models: Click here </p><pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre>        Executes a SQL query and returns results as a pandas DataFrame<p></p> </li> <li> <p>Query the Semantic: Click here </p><pre><code> vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre>       Returns the generated SQL that can be executed against your warehouse<p></p> </li> </ol>"},{"location":"guides/get-started/docker/#stopping-services","title":"Stopping Services","text":"Mac/LinuxWindows <p>To stop all services:   </p><pre><code>make all-down       # Stop all services\nmake all-clean      # Stop and remove volumes (this will delete all data)\nmake vulcan-down     # Stop only Vulcan services\n</code></pre> To stop individual services: <pre><code>make vulcan-down     # Stop Vulcan services\nmake infra-down      # Stop infrastructure services\nmake warehouse-down  # Stop warehouse services\n</code></pre><p></p> <p>To stop services, you can use batch scripts:</p> <pre><code>stop-all.bat           # Stop all services\nclean.bat              # Stop and remove volumes (this will delete all data)\nvulcan-down.bat        # Stop only Vulcan API services\n</code></pre>"},{"location":"guides/get-started/docker/#troubleshooting","title":"Troubleshooting","text":"Troubleshooting <p>Services won't start: Ensure Docker Desktop is running with at least 4GB RAM allocated.</p> <p>Network errors: Ensure the <code>vulcan</code> network exists:</p> Mac/LinuxWindows <p></p><pre><code>docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p></p><pre><code> docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p>Port conflicts: If ports 5431, 5433, 9000, 9001, or 8000 are already in use, you can modify the port mappings in the Docker Compose files.</p> <p>Can't connect to services: Make sure all services are running: </p><pre><code>docker compose -f docker/docker-compose.infra.yml ps\ndocker compose -f docker/docker-compose.warehouse.yml ps\n</code></pre><p></p> <p>MinIO console: You can access the MinIO console at <code>http://localhost:9001</code> with: - Username: <code>admin</code> - Password: <code>password</code></p>"},{"location":"guides/get-started/docker/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Vulcan CLI commands</li> <li>Explore Vulcan concepts</li> <li>Set up connections to different warehouses</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/","title":"Vulcan Docker Quickstart","text":""},{"location":"guides/get-started/zip-mac/vulcan-project/#vulcan-docker-quickstart","title":"Vulcan Docker Quickstart","text":"<p>This package contains all the Docker Compose files and configuration needed to get started with Vulcan using Docker.</p>"},{"location":"guides/get-started/zip-mac/vulcan-project/#contents","title":"Contents","text":"<ul> <li><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</li> <li><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</li> <li><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</li> <li><code>Makefile</code> - Convenient commands for managing services</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop installed and running</li> <li>Docker Compose (included with Docker Desktop)</li> <li>At least 4GB of available RAM</li> <li>A terminal/command line interface</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Start all infrastructure: </p><pre><code>make setup\n</code></pre>    This will:    - Create the Docker network    - Start statestore (PostgreSQL) on port 5431    - Start MinIO object storage on ports 9000 and 9001    - Start warehouse database (PostgreSQL) on port 5433<p></p> </li> <li> <p>Access Vulcan: </p><pre><code>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan\"\n</code></pre><p></p> </li> <li> <p>Initialize your project: </p><pre><code>vulcan init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code>gateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\nmodel_defaults:\n  dialect: postgres\n  start: 2025-01-01\n  cron: '@daily'\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-mac/vulcan-project/#available-make-commands","title":"Available Make Commands","text":"<ul> <li><code>make setup</code> - Run all setup steps</li> <li> <p><code>make vulcan-up</code> - Start Vulcan API services</p> </li> <li> <p><code>make network</code> - Create the Docker network</p> </li> <li><code>make infra</code> - Start infrastructure services</li> <li> <p><code>make warehouse</code> - Start warehouse database</p> </li> <li> <p><code>make all-down</code> - Stop all services</p> </li> <li><code>make all-clean</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li>Statestore: 5431</li> <li>Warehouse: 5433</li> <li>MinIO API: 9000</li> <li>MinIO Console: 9001 (admin/password)</li> <li>Vulcan API: 8000</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/","title":"Vulcan Docker Quickstart - Windows","text":""},{"location":"guides/get-started/zip-window/vulcan-project/#vulcan-docker-quickstart-windows","title":"Vulcan Docker Quickstart - Windows","text":"<p>This package contains all the Docker Compose files and Windows batch scripts needed to get started with Vulcan using Docker on Windows.</p>"},{"location":"guides/get-started/zip-window/vulcan-project/#contents","title":"Contents","text":"<ul> <li><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</li> <li><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</li> <li><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</li> <li><code>setup.bat</code> - Setup script to start all infrastructure</li> <li><code>vulcan.bat</code> - Wrapper script to run Vulcan CLI commands</li> <li><code>start-vulcan-api.bat</code> - Start Vulcan API services</li> <li><code>stop-all.bat</code> - Stop all services</li> <li><code>clean.bat</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop for Windows installed and running</li> <li>At least 4GB of available RAM</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Run the setup script: </p><pre><code>setup.bat\n</code></pre>    This will:    - Create the Docker network    - Start statestore (PostgreSQL) on port 5431    - Start MinIO object storage on ports 9000 and 9001    - Start warehouse database (PostgreSQL) on port 5433<p></p> </li> <li> <p>Access Vulcan: </p><pre><code>vulcan.bat\n</code></pre><p></p> </li> <li> <p>Initialize your project: </p><pre><code>vulcan.bat init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code>gateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\nmodel_defaults:\n  dialect: postgres\n  start: 2025-01-01\n  cron: '@daily'\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan.bat plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-window/vulcan-project/#available-scripts","title":"Available Scripts","text":"<ul> <li><code>setup.bat</code> - Create network and start all infrastructure</li> <li><code>start-vulcan-api.bat</code> - Start Vulcan API services</li> <li><code>stop-all.bat</code> - Stop all services</li> <li><code>clean.bat</code> - Stop all services and remove volumes</li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li>Statestore: 5431</li> <li>Warehouse: 5433</li> <li>MinIO API: 9000</li> <li>MinIO Console: 9001 (admin/password)</li> <li>Vulcan API: 8000 ```</li> </ul>"},{"location":"guides-old/configuration/","title":"Configuration","text":""},{"location":"guides-old/configuration/#configuration","title":"Configuration","text":"<p>Vulcan's behavior is determined by three things: a project's files (e.g., models), user actions (e.g., creating a <code>plan</code>), and how Vulcan is configured.</p> <p>This page describes how Vulcan configuration works and discusses the aspects of Vulcan behavior that can be modified via configuration.</p> <p>The configuration reference page contains concise lists of all configuration parameters and their default values.</p>"},{"location":"guides-old/configuration/#configuration-files","title":"Configuration files","text":"<p>NOTE: Vulcan project configurations have the following two requirements:</p> <ol> <li>A <code>config.yaml</code> or <code>config.py</code> file must be present in the project's folder.</li> <li>That configuration file must contain a default SQL dialect for the project's models in the <code>model_defaults</code> <code>dialect</code> key.</li> </ol> <p>Vulcan configuration parameters can be set as environment variables, in a configuration file in the <code>~/.vulcan</code> folder, and in the configuration file within a project folder.</p> <p>The sources have the following order of precedence:</p> <ol> <li>Environment variable (e.g., <code>VULCAN__MODEL_DEFAULTS__DIALECT</code>). [HIGHEST PRECEDENCE]</li> <li><code>config.yaml</code> or <code>config.py</code> in the <code>~/.vulcan</code> folder.</li> <li><code>config.yaml</code> or <code>config.py</code> in a project folder. [LOWEST PRECEDENCE]</li> </ol> <p>Note</p> <p>To relocate the <code>.vulcan</code> folder, set the <code>VULCAN_HOME</code> environment variable to your preferred directory path.</p>"},{"location":"guides-old/configuration/#file-type","title":"File type","text":"<p>You can specify a Vulcan configuration in either YAML or Python.</p> <p>YAML configuration is simpler, and we recommend it for most projects. Python configuration is more complex, but it enables functionality that YAML does not support.</p> <p>Because Python configuration files are evaluated by Python when Vulcan reads them, they support dynamic parameters based on the computational environment in which Vulcan is running.</p> <p>For example, Python configuration files enable use of third-party secrets managers for storing passwords and other sensitive information. They also support user-specific parameters such as automatically setting project defaults based on which user account is running Vulcan.</p>"},{"location":"guides-old/configuration/#yaml","title":"YAML","text":"<p>YAML configuration files consist of configuration keys and values. Strings are not quoted, and some keys are \"dictionaries\" that contain one or more sub-keys.</p> <p>For example, the <code>default_gateway</code> key specifies the default gateway Vulcan should use when executing commands. It takes a single, unquoted gateway name as its value:</p> <pre><code>default_gateway: local\n</code></pre> <p>In contrast, the <code>gateways</code> key takes dictionaries as values, and each gateway dictionary contains one or more connection dictionaries. This example specifies the <code>my_gateway</code> gateway with a Snowflake <code>connection</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>Gateway dictionaries can contain multiple connection dictionaries if different Vulcan components should use different connections (e.g., Vulcan <code>test</code>s should run in a different database than Vulcan <code>plan</code>s). See the gateways section for more information on gateway configuration.</p>"},{"location":"guides-old/configuration/#python","title":"Python","text":"<p>Python configuration files consist of statements that import Vulcan configuration classes and a configuration specification using those classes.</p> <p>At minimum, a Python configuration file must:</p> <ol> <li>Create an object of the Vulcan <code>Config</code> class named <code>config</code></li> <li>Specify that object's <code>model_defaults</code> argument with a <code>ModelDefaultsConfig()</code> object specifying the default SQL dialect for the project's models</li> </ol> <p>For example, this minimal configuration specifies a default SQL dialect of <code>duckdb</code> and uses the default values for all other configuration parameters:</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n)\n</code></pre> <p>Python configuration files may optionally define additional configuration objects and switch between the configurations when issuing <code>vulcan</code> commands. For example, if a configuration file contained a second configuration object <code>my_second_config</code>, you could create a plan using that config with <code>vulcan --config my_second_config plan</code>.</p> <p>Different <code>Config</code> arguments accept different object types. Some, such as <code>model_defaults</code>, take Vulcan configuration objects. Others, such as <code>default_gateway</code>, take strings or other Python object types like dictionaries.</p> <p>Vulcan's Python configuration components are documented in the <code>vulcan.core.config</code> module's API documentation.</p> <p>The <code>config</code> sub-module API documentation describes the individual classes used for the relevant <code>Config</code> arguments:</p> <ul> <li>Model defaults configuration: <code>ModelDefaultsConfig()</code></li> <li>Gateway configuration: <code>GatewayConfig()</code><ul> <li>Connection configuration (separate classes for each supported database/engine)</li> <li>Scheduler configuration (separate classes for each supported scheduler)</li> </ul> </li> <li>Plan change categorization configuration: <code>CategorizerConfig()</code></li> <li>User configuration: <code>User()</code></li> <li>Notification configuration (separate classes for each notification target)</li> </ul> <p>See the notifications guide for more information about user and notification specification.</p>"},{"location":"guides-old/configuration/#environment-variables","title":"Environment variables","text":"<p>All software runs within a system environment that stores information as \"environment variables.\"</p> <p>Vulcan can access environment variables during configuration, which enables approaches like storing passwords/secrets outside the configuration file and changing configuration parameters dynamically based on which user is running Vulcan.</p> <p>You can specify environment variables in the configuration file or by storing them in a <code>.env</code> file.</p>"},{"location":"guides-old/configuration/#env-files","title":".env files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory. This provides a convenient way to manage environment variables without having to set them in your shell.</p> <p>Create a <code>.env</code> file in your project root with key-value pairs:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override specific Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>See the overrides section for a detailed explanation of how these are defined.</p> <p>The rest of the <code>.env</code> file variables can be used in your configuration files with <code>{{ env_var('VARIABLE_NAME') }}</code> syntax in YAML or accessed via <code>os.environ['VARIABLE_NAME']</code> in Python.</p>"},{"location":"guides-old/configuration/#custom-dot-env-file-location-and-name","title":"Custom dot env file location and name","text":"<p>By default, Vulcan loads <code>.env</code> files from each project directory. However, you can specify a custom path using the <code>--dotenv</code> CLI flag directly when running a command:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag is a global option and must be placed before the subcommand (e.g. <code>plan</code>, <code>run</code>), not after.</p> <p>Alternatively, you can export the <code>VULCAN_DOTENV_PATH</code> environment variable once, to persist a custom path across all subsequent commands in your shell session:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\nvulcan run\n</code></pre> <p>Important considerations: - Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information - Vulcan will only load the <code>.env</code> file if it exists in the project directory (unless a custom path is specified) - When using a custom path, that specific file takes precedence over any <code>.env</code> file in the project directory.</p>"},{"location":"guides-old/configuration/#configuration-file","title":"Configuration file","text":"<p>This section demonstrates using environment variables in YAML and Python configuration files.</p> <p>The examples specify a Snowflake connection whose password is stored in an environment variable <code>SNOWFLAKE_PW</code>.</p> YAMLPython <p>Specify environment variables in a YAML configuration with the syntax <code>{{ env_var('&lt;ENVIRONMENT VARIABLE NAME&gt;') }}</code>. Note that the environment variable name is contained in single quotes.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: {{ env_var('SNOWFLAKE_PW') }}\n      account: &lt;account&gt;\n</code></pre> <p>Python accesses environment variables via the <code>os</code> library's <code>environ</code> dictionary.</p> <p>Access the <code>SNOWFLAKE_PW</code> environment variable in a Snowflake connection configuration like this:</p> <pre><code>import os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=os.environ['SNOWFLAKE_PW'],\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#default-target-environment","title":"Default target environment","text":"<p>The Vulcan <code>plan</code> command acts on the <code>prod</code> environment by default (i.e., <code>vulcan plan</code> is equivalent to <code>vulcan plan prod</code>).</p> <p>In some organizations, users never run plans directly against <code>prod</code> - they do all Vulcan work in a development environment unique to them. In a standard Vulcan configuration, this means they need to include their development environment name every time they issue the <code>plan</code> command (e.g., <code>vulcan plan dev_tony</code>).</p> <p>If your organization works like this, it may be convenient to change the <code>plan</code> command's default environment from <code>prod</code> to each user's development environment. That way people can issue <code>vulcan plan</code> without typing the environment name every time.</p> <p>The Vulcan configuration <code>user()</code> function returns the name of the user currently logged in and running Vulcan. It retrieves the username from system environment variables like <code>USER</code> on MacOS/Linux or <code>USERNAME</code> on Windows.</p> <p>Call <code>user()</code> inside Jinja curly braces with the syntax <code>{{ user() }}</code>, which allows you to combine the user name with a prefix or suffix.</p> <p>The example configuration below constructs the environment name by appending the username to the end of the string <code>dev_</code>. If the user running Vulcan is <code>tony</code>, the default target environment when they run Vulcan will be <code>dev_tony</code>. In other words, <code>vulcan plan</code> will be equivalent to <code>vulcan plan dev_tony</code>.</p> YAMLPython <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <pre><code>default_target_environment: dev_{{ user() }}\n</code></pre> <p>Default target environment is <code>dev_</code> combined with the username running Vulcan.</p> <p>Retrieve the username with the <code>getpass.getuser()</code> function, and combine it with <code>dev_</code> in a Python f-string.</p> <pre><code>import getpass\nimport os\nfrom vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(),\n        ),\n    },\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre>"},{"location":"guides-old/configuration/#overrides","title":"Overrides","text":"<p>Environment variables have the highest precedence among configuration methods, as noted above. They will automatically override configuration file specifications if they follow a specific naming structure.</p> <p>The structure is based on the names of the configuration fields, with double underscores <code>__</code> between the field names. The environment variable name must begin with <code>VULCAN__</code>, followed by the YAML field names starting at the root and moving downward in the hierarchy.</p> <p>For example, we can override the password specified in a Snowflake connection. This is the YAML specification contained in our configuration file, which specifies a password <code>dummy_pw</code>:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: dummy_pw\n      account: &lt;account&gt;\n</code></pre> <p>We can override the <code>dummy_pw</code> value with the true password <code>real_pw</code> by creating the environment variable. This example demonstrates creating the variable with the bash <code>export</code> function:</p> <pre><code>$ export VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre> <p>After the initial string <code>VULCAN__</code>, the environment variable name components move down the key hierarchy in the YAML specification: <code>GATEWAYS</code> \u2192 <code>MY_GATEWAY</code> \u2192 <code>CONNECTION</code> \u2192 <code>PASSWORD</code>.</p>"},{"location":"guides-old/configuration/#configuration-types","title":"Configuration types","text":"<p>A Vulcan project configuration is hierarchical and consists of root level parameters within which other parameters are defined.</p> <p>Conceptually, we can group the root level parameters into the following types. Each type links to its table of parameters in the Vulcan configuration reference page:</p> <ol> <li>Project - configuration options for Vulcan project directories.</li> <li>Environment - configuration options for Vulcan environment creation/promotion, physical table schemas, and view schemas.</li> <li>Gateways - configuration options for how Vulcan should connect to the data warehouse, state backend, and scheduler.</li> <li>Gateway/connection defaults - configuration options for what should happen when gateways or connections are not all explicitly specified.</li> <li>Model defaults - configuration options for what should happen when model-specific configurations are not explicitly specified in a model's file.</li> <li>Debug mode - configuration option for Vulcan to print and log actions and full backtraces.</li> </ol>"},{"location":"guides-old/configuration/#configuration-details","title":"Configuration details","text":"<p>The rest of this page provides additional detail for some of the configuration options and provides brief examples. Comprehensive lists of configuration options are at the configuration reference page.</p>"},{"location":"guides-old/configuration/#cache-directory","title":"Cache directory","text":"<p>By default, the Vulcan cache is stored in a <code>.cache</code> directory within your project folder. You can customize the cache location using the <code>cache_dir</code> configuration option:</p> YAMLPython <pre><code># Relative path to project directory\ncache_dir: my_custom_cache\n\n# Absolute path\ncache_dir: /tmp/vulcan_cache\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    cache_dir=\"/tmp/vulcan_cache\",\n)\n</code></pre> <p>The cache directory is automatically created if it doesn't exist. You can clear the cache using the <code>vulcan clean</code> command.</p>"},{"location":"guides-old/configuration/#tableview-storage-locations","title":"Table/view storage locations","text":"<p>Vulcan creates schemas, physical tables, and views in the data warehouse/engine. You can override where Vulcan creates physical tables and views with the <code>physical_schema_mapping</code>, <code>environment_suffix_target</code>, and <code>environment_catalog_mapping</code> configuration options.</p> <p>You can also override what the physical tables are called by using the <code>physical_table_naming_convention</code> option.</p> <p>These options are in the environments section of the configuration reference page.</p>"},{"location":"guides-old/configuration/#physical-table-schemas","title":"Physical table schemas","text":"<p>By default, Vulcan creates physical schemas for a model with a naming convention of <code>vulcan__[model schema]</code>.</p> <p>This can be overridden on a per-schema basis using the <code>physical_schema_mapping</code> option, which removes the <code>vulcan__</code> prefix and uses the regex pattern you provide to map the schemas defined in your model to their corresponding physical schemas.</p> <p>This example configuration overrides the default physical schemas for the <code>my_schema</code> model schema and any model schemas starting with <code>dev</code>:</p> YAMLPython <pre><code>physical_schema_mapping:\n  '^my_schema$': my_new_schema,\n  '^dev.*': development\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_schema_mapping={\n        \"^my_schema$\": \"my_new_schema\",\n        '^dev.*': \"development\"\n    },\n)\n</code></pre> <p>This config causes the following mapping behaviour:</p> Model name Default physical location Resolved physical location <code>my_schema.my_table</code> <code>vulcan__my_schema.table_&lt;fingerprint&gt;</code> <code>my_new_schema.table_&lt;fingerprint&gt;</code> <code>dev_schema.my_table</code> <code>vulcan__dev_schema.table_&lt;fingerprint&gt;</code> <code>development.table_&lt;fingerprint&gt;</code> <code>other.my_table</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <code>vulcan__other.table_&lt;fingerprint&gt;</code> <p>This only applies to the physical tables that Vulcan creates - the views are still created in <code>my_schema</code> (prod) or <code>my_schema__&lt;env&gt;</code>.</p>"},{"location":"guides-old/configuration/#disable-environment-specific-schemas","title":"Disable environment-specific schemas","text":"<p>Vulcan stores <code>prod</code> environment views in the schema in a model's name - for example, the <code>prod</code> views for a model <code>my_schema.users</code> will be located in <code>my_schema</code>.</p> <p>By default, for non-prod environments Vulcan creates a new schema that appends the environment name to the model name's schema. For example, by default the view for a model <code>my_schema.users</code> in a Vulcan environment named <code>dev</code> will be located in the schema <code>my_schema__dev</code> as <code>my_schema__dev.users</code>.</p>"},{"location":"guides-old/configuration/#show-at-the-table-level-instead","title":"Show at the table level instead","text":"<p>This behavior can be changed to append a suffix at the end of a table/view name instead. Appending the suffix to a table/view name means that non-prod environment views will be created in the same schema as the <code>prod</code> environment. The prod and non-prod views are differentiated by non-prod view names ending with <code>__&lt;env&gt;</code>.</p> <p>For example, if you created a <code>dev</code> environment for a project containing a model named <code>my_schema.users</code>, the model view would be created as <code>my_schema.users__dev</code> instead of the default behavior of <code>my_schema__dev.users</code>.</p> <p>Config example:</p> YAMLPython <pre><code>environment_suffix_target: table\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.TABLE,\n)\n</code></pre> <p>Default behavior</p> <p>The default behavior of appending the suffix to schemas is recommended because it leaves production with a single clean interface for accessing the views. However, if you are deploying Vulcan in an environment with tight restrictions on schema creation then this can be a useful way of reducing the number of schemas Vulcan uses.</p>"},{"location":"guides-old/configuration/#show-at-the-catalog-level-instead","title":"Show at the catalog level instead","text":"<p>If neither the schema (default) nor the table level are sufficient for your use case, you can indicate the environment at the catalog level instead.</p> <p>This can be useful if you have downstream BI reporting tools and you would like to point them at a development environment to test something out without renaming all the table / schema references within the report query.</p> <p>In order to achieve this, you can configure environment_suffix_target like so:</p> YAMLPython <pre><code>environment_suffix_target: catalog\n</code></pre> <p>The Python <code>environment_suffix_target</code> argument takes an <code>EnvironmentSuffixTarget</code> enumeration with a value of <code>EnvironmentSuffixTarget.TABLE</code>, <code>EnvironmentSuffixTarget.CATALOG</code> or <code>EnvironmentSuffixTarget.SCHEMA</code> (default).</p> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_suffix_target=EnvironmentSuffixTarget.CATALOG,\n)\n</code></pre> <p>Given the example of a model called <code>my_schema.users</code> with a default catalog of <code>warehouse</code> this will cause the following behavior:</p> <ul> <li>For the <code>prod</code> environment, the default catalog as configured in the gateway will be used. So the view will be created at <code>warehouse.my_schema.users</code></li> <li>For any other environment, eg <code>dev</code>, the environment name will be appended to the default catalog. So the view will be created at <code>warehouse__dev.my_schema.users</code></li> <li>If a model is fully qualified with a catalog already, eg <code>finance_mart.my_schema.users</code>, then the environment catalog will be based off the model catalog and not the default catalog. In this example, the view will be created at <code>finance_mart__dev.my_schema.users</code></li> </ul> <p>Caveats</p> <ul> <li>Using <code>environment_suffix_target: catalog</code> only works on engines that support querying across different catalogs. If your engine does not support cross-catalog queries then you will need to use <code>environment_suffix_target: schema</code> or <code>environment_suffix_target: table</code> instead.</li> <li>Automatic catalog creation is not supported on all engines even if they support cross-catalog queries. For engines where it is not supported, the catalogs must be managed externally from Vulcan and exist prior to invoking Vulcan.</li> </ul>"},{"location":"guides-old/configuration/#physical-table-naming-convention","title":"Physical table naming convention","text":"<p>Out of the box, Vulcan has the following defaults set:</p> <ul> <li><code>environment_suffix_target: schema</code></li> <li><code>physical_table_naming_convention: schema_and_table</code></li> <li>no <code>physical_schema_mapping</code> overrides, so a <code>vulcan__&lt;model schema&gt;</code> physical schema will be created for each model schema</li> </ul> <p>This means that given a catalog of <code>warehouse</code> and a model named <code>finance_mart.transaction_events_over_threshold</code>, Vulcan will create physical tables using the following convention:</p> <pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;schema&gt;__&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.finance_mart__transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre> <p>This deliberately contains some redundancy with the model schema as it's repeated at the physical layer in both the physical schema name as well as the physical table name.</p> <p>This default exists to make the physical table names portable between different configurations. If you were to define a <code>physical_schema_mapping</code> that maps all models to the same physical schema, since the model schema is included in the table name as well, there are no naming conflicts.</p>"},{"location":"guides-old/configuration/#table-only","title":"Table only","text":"<p>Some engines have object name length limitations which cause them to silently truncate table and view names that exceed this limit. This behaviour breaks Vulcan, so we raise a runtime error if we detect the engine would silently truncate the name of the table we are trying to create.</p> <p>Having redundancy in the physical table names does reduce the number of characters that can be utilised in model names. To increase the number of characters available to model names, you can use <code>physical_table_naming_convention</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: table_only\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.TABLE_ONLY,\n)\n</code></pre> <p>This will cause Vulcan to omit the model schema from the table name and generate physical names that look like (using the above example): </p><pre><code># &lt;catalog&gt;.vulcan__&lt;schema&gt;.&lt;table&gt;__&lt;fingerprint&gt;\n\nwarehouse.vulcan__finance_mart.transaction_events_over_threshold__&lt;fingerprint&gt;\n</code></pre><p></p> <p>Notice that the model schema name is no longer part of the physical table name. This allows for slightly longer model names on engines with low identifier length limits, which may be useful for your project.</p> <p>In this configuration, it is your responsibility to ensure that any schema overrides in <code>physical_schema_mapping</code> result in each model schema getting mapped to a unique physical schema.</p> <p>For example, the following configuration will cause data corruption:</p> <pre><code>physical_table_naming_convention: table_only\nphysical_schema_mapping:\n  '.*': vulcan\n</code></pre> <p>This is because every model schema is mapped to the same physical schema but the model schema name is omitted from the physical table name.</p>"},{"location":"guides-old/configuration/#md5-hash","title":"MD5 hash","text":"<p>If you still need more characters, you can set <code>physical_table_naming_convention: hash_md5</code> like so:</p> YAMLPython <pre><code>physical_table_naming_convention: hash_md5\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    physical_table_naming_convention=TableNamingConvention.HASH_MD5,\n)\n</code></pre> <p>This will cause Vulcan generate physical names that are always 45-50 characters in length and look something like:</p> <pre><code># vulcan_md5__&lt;hash of what we would have generated using 'schema_and_table'&gt;\n\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00\n\n# or, for a dev preview\nvulcan_md5__d3b07384d113edec49eaa6238ad5ff00__dev\n</code></pre> <p>This has a downside that now it's much more difficult to determine which table corresponds to which model by just looking at the database with a SQL client. However, the table names have a predictable length so there are no longer any surprises with identfiers exceeding the max length at the physical layer.</p>"},{"location":"guides-old/configuration/#virtual-data-environment-modes","title":"Virtual Data Environment Modes","text":"<p>By default, Virtual Data Environments (VDE) are applied across both development and production environments. This allows Vulcan to reuse physical tables when appropriate, even when promoting from development to production.</p> <p>However, users may prefer their production environment to be non-virtual. The non-exhaustive list of reasons may include:</p> <ul> <li>Integration with third-party tools and platforms, such as data catalogs, may not work well with the virtual view layer that Vulcan imposes by default</li> <li>A desire to rely on time travel features provided by cloud data warehouses such as BigQuery, Snowflake, and Databricks</li> </ul> <p>To mitigate this, Vulcan offers an alternative 'dev-only' mode for using VDE. It can be enabled in the project configuration like so:</p> YAMLPython <pre><code>virtual_environment_mode: dev_only\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    virtual_environment_mode=\"dev_only\",\n)\n</code></pre> <p>'dev-only' mode means that VDE is applied only in development environments. While in production, model tables and views are updated directly and bypass the virtual layer. This also means that physical tables in production will be created using the original, unversioned model names. Users will still benefit from VDE and data reuse across development environments.</p> <p>Please note the following tradeoffs when enabling this mode:</p> <ul> <li>All data inserted in development environments is used only for preview and will not be reused in production</li> <li>Reverting a model to a previous version will be applied going forward and may require an explicit data restatement</li> </ul> <p>Warning</p> <p>Switching the mode for an existing project will result in a complete rebuild of all models in the project. Refer to the Table Migration Guide to migrate existing tables without rebuilding them from scratch.</p>"},{"location":"guides-old/configuration/#environment-view-catalogs","title":"Environment view catalogs","text":"<p>By default, Vulcan creates an environment view in the same catalog as the physical table the view points to. The physical table's catalog is determined by either the catalog specified in the model name or the default catalog defined in the connection.</p> <p>It can be desirable to create <code>prod</code> and non-prod virtual layer objects in separate catalogs instead. For example, there might be a \"prod\" catalog that contains all <code>prod</code> environment views and a separate \"dev\" catalog that contains all <code>dev</code> environment views.</p> <p>Separate prod and non-prod catalogs can also be useful if you have a CI/CD pipeline that creates environments, like the Vulcan Github Actions CI/CD Bot. You might want to store the CI/CD environment objects in a dedicated catalog since there can be many of them.</p> <p>Virtual layer only</p> <p>Note that the following setting only affects the virtual layer. If you need full segregation by catalog between environments in the physical layer as well, see the Isolated Systems Guide.</p> <p>To configure separate catalogs, provide a mapping from regex patterns to catalog names. Vulcan will compare the name of an environment to the regex patterns; when it finds a match it will store the environment's objects in the corresponding catalog.</p> <p>Vulcan evaluates the regex patterns in the order defined in the configuration; it uses the catalog for the first matching pattern. If no match is found, the catalog defined in the model or the default catalog defined on the connection will be used.</p> <p>Config example:</p> YAMLPython <pre><code>environment_catalog_mapping:\n  '^prod$': prod\n  '^dev.*': dev\n  '^analytics_repo.*': cicd\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    environment_catalog_mapping={\n        '^prod$': 'prod',\n        '^dev.*': 'dev',\n        '^analytics_repo.*': 'cicd',\n    },\n)\n</code></pre> <p>With the example configuration above, Vulcan would evaluate environment names as follows:</p> <ul> <li>If the environment name is <code>prod</code>, the catalog will be <code>prod</code>.</li> <li>If the environment name starts with <code>dev</code>, the catalog will be <code>dev</code>.</li> <li>If the environment name starts with <code>analytics_repo</code>, the catalog will be <code>cicd</code>.</li> </ul> <p>Warning</p> <p>This feature is mutually exclusive with <code>environment_suffix_target: catalog</code> in order to prevent ambiguous mappings from being defined. Attempting to specify both <code>environment_catalog_mapping</code> and <code>environment_suffix_target: catalog</code> will raise an error on project load</p> <p>Note: This feature is only available for engines that support querying across catalogs. At the time of writing, the following engines are NOT supported:</p> <ul> <li>MySQL</li> <li>Postgres</li> <li>GCP Postgres</li> </ul>"},{"location":"guides-old/configuration/#regex-tips","title":"Regex Tips","text":"<ul> <li>If you are less familiar with regex, you can use a tool like regex101 to help you build your regex patterns.<ul> <li>LLMs, like ChatGPT, can help with generating regex patterns. Make sure to validate the suggestion in regex101.</li> </ul> </li> <li>If you are wanting to do an exact word match then surround it with <code>^</code> and <code>$</code> like in the example above.</li> <li>If you want a catch-all at the end of your mapping, to avoid ever using the model catalog or default catalog, then use <code>.*</code> as the pattern. This will match any environment name that hasn't already been matched.</li> </ul>"},{"location":"guides-old/configuration/#auto-categorize-model-changes","title":"Auto-categorize model changes","text":"<p>Vulcan compares the current state of project files to an environment when <code>vulcan plan</code> is run. It detects changes to models, which can be classified as breaking or non-breaking.</p> <p>Vulcan can  attempt to automatically categorize the changes it detects. The <code>plan.auto_categorize_changes</code> option determines whether Vulcan should attempt automatic change categorization. This option is in the plan section of the configuration reference page.</p> <p>Supported values:</p> <ul> <li><code>full</code>: Never prompt the user for input, instead fall back to the most conservative category (breaking) if the category can't be determined automatically.</li> <li><code>semi</code>: Prompt the user for input only if the change category can't be determined automatically.</li> <li><code>off</code>: Always prompt the user for input; automatic categorization will not be attempted.</li> </ul> <p>Example showing default values:</p> YAMLPython <pre><code>plan:\n  auto_categorize_changes:\n    external: full\n    python: off\n    sql: full\n    seed: full\n</code></pre> <p>The Python <code>auto_categorize_changes</code> argument takes <code>CategorizerConfig</code> object. That object's arguments take an <code>AutoCategorizationMode</code> enumeration with values of <code>AutoCategorizationMode.FULL</code>, <code>AutoCategorizationMode.SEMI</code>, or <code>AutoCategorizationMode.OFF</code>.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    AutoCategorizationMode,\n    CategorizerConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        auto_categorize_changes=CategorizerConfig(\n            external=AutoCategorizationMode.FULL,\n            python=AutoCategorizationMode.OFF,\n            sql=AutoCategorizationMode.FULL,\n            seed=AutoCategorizationMode.FULL,\n        )\n    ),\n)\n</code></pre>"},{"location":"guides-old/configuration/#always-comparing-against-production","title":"Always comparing against production","text":"<p>By default, Vulcan compares the current state of project files to the target <code>&lt;env&gt;</code> environment when <code>vulcan plan &lt;env&gt;</code> is run. However, a common expectation is that local changes should always be compared to the production environment.</p> <p>The <code>always_recreate_environment</code> boolean plan option can alter this behavior. When enabled, Vulcan will always attempt to compare against the production environment by recreating the target environment; If <code>prod</code> does not exist, Vulcan will fall back to comparing against the target environment.</p> <p>NOTE:: Upon succesfull plan application, changes are still promoted to the target <code>&lt;env&gt;</code> environment.</p> YAMLPython <pre><code>plan:\n    always_recreate_environment: True\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PlanConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    plan=PlanConfig(\n        always_recreate_environment=True,\n    ),\n)\n</code></pre>"},{"location":"guides-old/configuration/#change-categorization-example","title":"Change Categorization Example","text":"<p>Consider this scenario with <code>always_recreate_environment</code> enabled:</p> <ol> <li> <p>Initial state in <code>prod</code>: </p><pre><code>MODEL (name vulcan_example.test_model, kind FULL);\nSELECT 1 AS col\n</code></pre><p></p> </li> <li> <p>First (breaking) change in <code>dev</code>: </p><pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL);\nSELECT 2 AS col\n</code></pre><p></p> </li> </ol> Output plan example #1 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n+++\n\n\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n</code></pre> <ol> <li>Second (metadata) change in <code>dev</code>: <pre><code>MODEL (name vulcan_example__dev.test_model, kind FULL, owner 'John Doe');\nSELECT 5 AS col\n</code></pre></li> </ol> Output plan example #2 <pre><code>New environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.test_model\n\n---\n\n+++\n\n@@ -1,8 +1,9 @@\n\nMODEL (\nname vulcan_example.test_model,\n+  owner \"John Doe\",\nkind FULL\n)\nSELECT\n-  1 AS col\n+  2 AS col\n\nDirectly Modified: vulcan_example__dev.test_model (Breaking)\nModels needing backfill:\n\u2514\u2500\u2500 vulcan_example__dev.test_model: [full refresh]\n</code></pre> <p>Even though the second change should have been a metadata change (thus not requiring a backfill), it will still be classified as a breaking change because the comparison is against production instead of the previous development state. This is intentional and may cause additional backfills as more changes are accumulated.</p>"},{"location":"guides-old/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> configuration defines how Vulcan should connect to the data warehouse, state backend, and scheduler. These options are in the gateway section of the configuration reference page.</p> <p>Each gateway key represents a unique gateway name and configures its connections. Gateway names are case-insensitive - Vulcan automatically normalizes gateway names to lowercase during configuration validation. This means you can use any case in your configuration files (e.g., <code>MyGateway</code>, <code>mygateway</code>, <code>MYGATEWAY</code>) and they will all work correctly.</p> <p>For example, this configures the <code>my_gateway</code> gateway:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      ...\n    state_connection:\n      ...\n    test_connection:\n      ...\n    scheduler:\n      ...\n</code></pre> <p>The Python <code>gateways</code> argument takes a dictionary of gateway names and <code>GatewayConfig</code> objects. A <code>GatewayConfig</code>'s connection-related arguments take an engine-specific connection config object, and the <code>scheduler</code> argument takes a scheduler config object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    ...\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=...,\n            state_connection=...,\n            test_connection=...,\n            scheduler=...,\n        ),\n    }\n)\n</code></pre> <p>Gateways do not need to specify all four components in the example above. The gateway defaults options control what happens if they are not all specified - find more information on gateway defaults below.</p>"},{"location":"guides-old/configuration/#connections","title":"Connections","text":"<p>The <code>connection</code> configuration controls the data warehouse connection. These options are in the connection section of the configuration reference page.</p> <p>The allowed keys include:</p> <ul> <li>The optional <code>concurrent_tasks</code> key specifies the maximum number of concurrent tasks Vulcan will run. Default value is 4 for engines that support concurrent tasks.</li> <li>Most keys are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</li> </ul> <p>Example snowflake connection configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n</code></pre> <p>A Snowflake connection is specified with a <code>SnowflakeConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#engine-connection-configuration","title":"Engine connection configuration","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Athena</li> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>Fabric</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides-old/configuration/#state-connection","title":"State connection","text":"<p>Configuration for the state backend connection if different from the data warehouse connection.</p> <p>The data warehouse connection is used to store Vulcan state if the <code>state_connection</code> key is not specified.</p> <p>Unlike data transformations, storing state information requires database transactions. Data warehouses aren\u2019t optimized for executing transactions, and storing state information in them can slow down your project or produce corrupted data due to simultaneous writes to the same table. Therefore, production Vulcan deployments should use a dedicated state connection.</p> <p>Note</p> <p>Using the same connection for data warehouse and state is not recommended for production deployments of Vulcan.</p> <p>The easiest and most reliable way to manage your state connection is for Tobiko Cloud to do it for you. If you'd rather handle it yourself, we list recommended and unsupported state engines below.</p> <p>Recommended state engines for production deployments:</p> <ul> <li>Postgres</li> <li>GCP Postgres</li> </ul> <p>Other state engines with fast and reliable database transactions (less tested than the recommended engines):</p> <ul> <li>DuckDB<ul> <li>With the caveat that it's a single user database so will not scale to production usage</li> </ul> </li> <li>MySQL</li> <li>MSSQL</li> </ul> <p>Unsupported state engines, even for development:</p> <ul> <li>ClickHouse</li> <li>Spark</li> <li>Trino</li> </ul> <p>This example gateway configuration uses Snowflake for the data warehouse connection and Postgres for the state backend connection:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      # snowflake credentials here\n      type: snowflake\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      account: &lt;account&gt;\n    state_connection:\n      # postgres credentials here\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n</code></pre> <p>A Postgres connection is specified with a <code>PostgresConnectionConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig,\n    SnowflakeConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            # snowflake credentials here\n            connection=SnowflakeConnectionConfig(\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                account=&lt;account&gt;,\n            ),\n            # postgres credentials here\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#state-schema-name","title":"State schema name","text":"<p>By default, the schema name used to store state tables is <code>vulcan</code>. This can be changed by providing the <code>state_schema</code> config key in the gateway configuration.</p> <p>Example configuration to store state information in a postgres database's <code>custom_name</code> schema:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    state_connection:\n      type: postgres\n      host: &lt;host&gt;\n      port: &lt;port&gt;\n      user: &lt;username&gt;\n      password: &lt;password&gt;\n      database: &lt;database&gt;\n    state_schema: custom_name\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    PostgresConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            state_connection=PostgresConnectionConfig(\n                host=&lt;host&gt;,\n                port=&lt;port&gt;,\n                user=&lt;username&gt;,\n                password=&lt;password&gt;,\n                database=&lt;database&gt;,\n            ),\n            state_schema=\"custom_name\",\n        ),\n    }\n)\n</code></pre> <p>This would create all state tables in the schema <code>custom_name</code>.</p>"},{"location":"guides-old/configuration/#test-connection","title":"Test connection","text":"<p>Configuration for a connection used to run unit tests. An in-memory DuckDB database is used if the <code>test_connection</code> key is not specified.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    test_connection:\n      type: duckdb\n</code></pre> <p>A DuckDB connection is specified with a <code>DuckDBConnectionConfig</code> object. A <code>DuckDBConnectionConfig</code> with no arguments specified uses an in-memory DuckDB database.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            test_connection=DuckDBConnectionConfig(),\n        ),\n    }\n)\n</code></pre>"},{"location":"guides-old/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans. By default, the scheduler type is set to <code>builtin</code>, which uses the existing SQL engine to store metadata.</p> <p>These options are in the scheduler section of the configuration reference page.</p>"},{"location":"guides-old/configuration/#builtin","title":"Builtin","text":"<p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    scheduler:\n      type: builtin\n</code></pre> <p>A built-in scheduler is specified with a <code>BuiltInSchedulerConfig</code> object.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    BuiltInSchedulerConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            scheduler=BuiltInSchedulerConfig(),\n        ),\n    }\n)\n</code></pre> <p>No additional configuration options are supported by this scheduler type.</p>"},{"location":"guides-old/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. These options are in the gateway/connection defaults section of the configuration reference page.</p> <p>The gateway specified in <code>default_gateway</code> is used when a <code>vulcan</code> command does not explicitly specify a gateway. All Vulcan CLI commands accept a gateway option after <code>vulcan</code> and before the command name; for example, <code>vulcan --gateway my_gateway plan</code>. If the option is not specified in a command call, the <code>default_gateway</code> is used.</p> <p>The three default connection types are used when some gateways in the <code>gateways</code> configuration dictionaries do not specify every connection type.</p>"},{"location":"guides-old/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> <p>Example configuration:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    &lt;gateway specification&gt;\ndefault_gateway: my_gateway\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            &lt;gateway specification&gt;\n        ),\n    },\n    default_gateway=\"my_gateway\",\n)\n</code></pre>"},{"location":"guides-old/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> <p>Example configuration specifying a Postgres default connection, in-memory DuckDB default test connection, and builtin default scheduler:</p> YAMLPython <pre><code>default_connection:\n  type: postgres\n  host: &lt;host&gt;\n  port: &lt;port&gt;\n  user: &lt;username&gt;\n  password: &lt;password&gt;\n  database: &lt;database&gt;\ndefault_test_connection:\n  type: duckdb\ndefault_scheduler:\n  type: builtin\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    PostgresConnectionConfig,\n    DuckDBConnectionConfig,\n    BuiltInSchedulerConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    default_connection=PostgresConnectionConfig(\n        host=&lt;host&gt;,\n        port=&lt;port&gt;,\n        user=&lt;username&gt;,\n        password=&lt;password&gt;,\n        database=&lt;database&gt;,\n    ),\n    default_test_connection=DuckDBConnectionConfig(),\n    default_scheduler=BuiltInSchedulerConfig(),\n)\n</code></pre>"},{"location":"guides-old/configuration/#models","title":"Models","text":""},{"location":"guides-old/configuration/#model-defaults","title":"Model defaults","text":"<p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key. All SQL dialects supported by the SQLGlot library are allowed. Other values are set automatically unless explicitly overridden in the model definition.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference page.</p> <p>Example configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless overridden with the <code>kind</code> key. For more information on model kinds, refer to model concepts page.</p>"},{"location":"guides-old/configuration/#identifier-resolution","title":"Identifier resolution","text":"<p>When a SQL engine receives a query such as <code>SELECT id FROM \"some_table\"</code>, it eventually needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This process is usually referred to as identifier (or name) resolution.</p> <p>Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.</p> <p>Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it normalizes and quotes all identifiers in those queries, respecting each dialect's resolution rules.</p> <p>The \"normalization strategy\", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.</p> <p>See here to learn more about the supported normalization strategies.</p>"},{"location":"guides-old/configuration/#gateway-specific-model-defaults","title":"Gateway-specific model defaults","text":"<p>You can also define gateway specific <code>model_defaults</code> in the <code>gateways</code> section, which override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This allows you to tailor the behavior of models for each gateway without affecting the global <code>model_defaults</code>.</p> <p>For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.</p> <p>Gateway-specific <code>model_defaults</code> allow you to change how Vulcan performs identifier normalization by engine to align the different engines' behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway configuration overrides that global default dialect with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>That value tells Vulcan that the <code>redshift</code> gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"guides-old/configuration/#model-kinds","title":"Model Kinds","text":"<p>Model kinds are required in each model file's <code>MODEL</code> DDL statement. They may optionally be used to specify a default kind in the model defaults configuration key.</p> <p>All model kind specification keys are listed in the models configuration reference page.</p> <p>The <code>VIEW</code>, <code>FULL</code>, and <code>EMBEDDED</code> model kinds are specified by name only, while other models kinds require additional parameters and are provided with an array of parameters:</p> YAML <p><code>FULL</code> model only requires a name:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  kind FULL\n);\n</code></pre> <p><code>INCREMENTAL_BY_TIME_RANGE</code> requires an array specifying the model's <code>time_column</code> (which should be in the UTC time zone):</p> <pre><code>MODEL(\n  name docs_example.incremental_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column model_time_column\n  )\n);\n</code></pre> <p>Python model kinds are specified with model kind objects. Python model kind objects have the same arguments as their SQL counterparts, listed in the models configuration reference page.</p> <p>This example demonstrates how to specify an incremental by time range model kind in Python:</p> Python <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan.core.model.kind import ModelKindName\n\n@model(\n    \"docs_example.incremental_model\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"ds\"\n    )\n)\n</code></pre> <p>Learn more about specifying Python models at the Python models concepts page.</p>"},{"location":"guides-old/configuration/#model-naming","title":"Model Naming","text":"<p>The <code>model_naming</code> configuration controls if model names are inferred based on the project's directory structure. If <code>model_naming</code> is not defined or <code>infer_names</code> is set to false, the model names must be provided explicitly.</p> <p>With <code>infer_names</code> set to true, model names are inferred based on their path. For example, a model located at <code>models/catalog/schema/model.sql</code> would be named <code>catalog.schema.model</code>. However, if a name is provided in the model definition, it will take precedence over the inferred name.</p> <p>Example enabling name inference:</p> YAMLPython <pre><code>model_naming:\n  infer_names: true\n</code></pre> <pre><code>from vulcan.core.config import Config, NameInferenceConfig\n\nconfig = Config(\n    model_naming=NameInferenceConfig(\n        infer_names=True\n    )\n)\n</code></pre>"},{"location":"guides-old/configuration/#before_all-and-after_all-statements","title":"Before_all and after_all Statements","text":"<p>The <code>before_all</code> and <code>after_all</code> statements are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands.</p> <p>These statements can be defined in the configuration file under the <code>before_all</code> and <code>after_all</code> keys, either as a list of SQL statements or by using Vulcan macros:</p> YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\nafter_all:\n  - \"@grant_select_privileges()\"\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all = [\n        \"CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)\"\n    ],\n    after_all = [\n        \"@grant_select_privileges()\",\n        \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n    ],\n)\n</code></pre>"},{"location":"guides-old/configuration/#examples","title":"Examples","text":"<p>These statements allow for actions to be executed before all individual model statements or after all have run, respectively. They can also simplify tasks such as granting privileges.</p>"},{"location":"guides-old/configuration/#example-granting-select-privileges","title":"Example: Granting Select Privileges","text":"<p>For example, rather than using an <code>on_virtual_update</code> statement in each model to grant privileges on the views of the virtual layer, a single macro can be defined and used at the end of the plan:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    if evaluator.views:\n        return [\n            f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE admin_role;\"\n            for view_name in evaluator.views\n        ]\n</code></pre> <p>By including the comment <code>/* sqlglot.meta replace=false */</code>, you further ensure that the evaluator does not replace the view name with the physical table name during rendering.</p>"},{"location":"guides-old/configuration/#example-granting-schema-privileges","title":"Example: Granting Schema Privileges","text":"<p>Similarly, you can define a macro to grant schema usage privileges and, as demonstrated in the configuration above, using <code>this_env</code> macro conditionally execute it only in the production environment.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    if evaluator.this_env == \"prod\" and evaluator.schemas:\n        return [\n            f\"GRANT USAGE ON SCHEMA {schema} TO admin_role;\"\n            for schema in evaluator.schemas\n        ]\n</code></pre> <p>As demonstrated in these examples, the <code>schemas</code>  and <code>views</code> are available within the macro evaluator for macros invoked within the <code>before_all</code> and <code>after_all</code> statements. Additionally, the macro <code>this_env</code> provides access to the current environment name, which can be helpful for more advanced use cases that require fine-grained control over their behaviour.</p>"},{"location":"guides-old/configuration/#linting","title":"Linting","text":"<p>Vulcan provides a linter that checks for potential issues in your models' code. Enable it and specify which linting rules to apply in the configuration file's <code>linter</code> key.</p> <p>Learn more about linting configuration in the linting guide.</p>"},{"location":"guides-old/configuration/#debug-mode","title":"Debug mode","text":"<p>To enable debug mode set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</p> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"guides-old/configuration/#python-library-dependencies","title":"Python library dependencies","text":"<p>Vulcan enables you to write Python models and macros which depend on third-party libraries. To ensure each run / evaluation uses the same version, you can specify versions in a <code>vulcan-requirements.lock</code> file in the root of your project.</p> <p>The vulcan.lock must be of the format <code>dep==version</code>. Only <code>==</code> is supported.</p> <p>For example:</p> <pre><code>numpy==2.1.2\npandas==2.2.3\n</code></pre> <p>This feature is only available in Tobiko Cloud.</p>"},{"location":"guides-old/configuration/#excluding-dependencies","title":"Excluding dependencies","text":"<p>You can exclude dependencies by prefixing the dependency with a <code>^</code>. For example:</p> <pre><code>^numpy\npandas==2.2.3\n</code></pre>"},{"location":"guides-old/connections/","title":"Connections","text":""},{"location":"guides-old/connections/#connections","title":"Connections","text":""},{"location":"guides-old/connections/#overview","title":"Overview","text":"<p>In order to deploy models and to apply changes to them, you must configure a connection to your Data Warehouse and, optionally, connection to the database where the Vulcan state is stored. This can be done in either the <code>config.yaml</code> file in your project folder, or the one in <code>~/.vulcan</code>.</p> <p>Each connection is configured as part of a gateway which has a unique name associated with it. The gateway name can be used to select a specific combination of connection settings  when using the CLI. For example:</p> <pre><code>gateways:\n  local_db:\n    connection:\n      type: duckdb\n</code></pre> <p>Now the defined connection can be selected in the <code>vulcan plan</code> CLI command as follows:</p> <pre><code>vulcan --gateway local_db plan\n</code></pre>"},{"location":"guides-old/connections/#state-connection","title":"State connection","text":"<p>By default, the data warehouse connection is also used to store the Vulcan state.</p> <p>The state connection can be changed by providing different connection settings in the <code>state_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    state_connection:\n      type: duckdb\n      database: state.db\n</code></pre> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p>"},{"location":"guides-old/connections/#default-connection","title":"Default connection","text":"<p>Additionally, you can set a default connection by defining its configuration in the <code>default_connection</code> key:</p> <pre><code>default_connection:\n  type: duckdb\n  database: local.db\n</code></pre> <p>This connection configuration will be used if one is not provided in the target gateway.</p>"},{"location":"guides-old/connections/#test-connection","title":"Test connection","text":"<p>By default, when running tests, Vulcan uses an in-memory DuckDB database connection. You can override this behavior by providing connection settings in the <code>test_connection</code> key of the gateway configuration:</p> <pre><code>gateways:\n  local_db:\n    test_connection:\n      type: duckdb\n      database: test.db\n</code></pre>"},{"location":"guides-old/connections/#default-test-connection","title":"Default test connection","text":"<p>To configure a default test connection for all gateways use the <code>default_test_connection</code> key:</p> <pre><code>default_test_connection:\n  type: duckdb\n  database: test.db\n</code></pre>"},{"location":"guides-old/connections/#default-gateway","title":"Default gateway","text":"<p>To change the default gateway used by the CLI when no gateway name is provided, set the desired name in the <code>default_gateway</code> key:</p> <pre><code>default_gateway: local_db\n</code></pre>"},{"location":"guides-old/connections/#supported-engines","title":"Supported engines","text":"<ul> <li>BigQuery</li> <li>Databricks</li> <li>DuckDB</li> <li>MotherDuck</li> <li>MySQL</li> <li>MSSQL</li> <li>Postgres</li> <li>GCP Postgres</li> <li>Redshift</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"guides-old/customizing_vulcan/","title":"Customizing Vulcan","text":""},{"location":"guides-old/customizing_vulcan/#customizing-vulcan","title":"Customizing Vulcan","text":"<p>Vulcan supports the workflows used by the vast majority of data engineering teams. However, your company may have bespoke processes or tools that require special integration with Vulcan.</p> <p>Fortunately, Vulcan is an open-source Python library, so you can view its underlying code and customize it for your needs.</p> <p>Customization generally involves subclassing Vulcan classes to extend or modify their functionality.</p> <p>Caution</p> <p>Customize Vulcan with extreme caution. Errors may cause Vulcan to produce unexpected results.</p>"},{"location":"guides-old/customizing_vulcan/#custom-loader","title":"Custom loader","text":"<p>Loading is the process of reading project files and converting their contents into Vulcan's internal Python objects.</p> <p>The loading stage is a convenient place to customize Vulcan behavior because you can access a project's objects after they've been ingested from file but before Vulcan uses them.</p> <p>Vulcan's <code>VulcanLoader</code> class handles the loading process - customize it by subclassing it and overriding its methods.</p> <p>Python configuration only</p> <p>Custom loaders require using the Python configuration format (YAML is not supported).</p>"},{"location":"guides-old/customizing_vulcan/#modify-every-model","title":"Modify every model","text":"<p>One reason to customize the loading process is to do something to every model. For example, you might want to add a post-statement to every model.</p> <p>The loading process parses all model SQL statements, so new or modified SQL must be parsed by SQLGlot before being passed to a model object.</p> <p>This custom loader example adds a post-statement to every model:</p> config.py<pre><code>from vulcan.core.loader import VulcanLoader\nfrom vulcan.utils import UniqueKeyDict\nfrom vulcan.core.dialect import parse_one\nfrom vulcan.core.config import Config\n\n# New `CustomLoader` class subclasses `VulcanLoader`\nclass CustomLoader(VulcanLoader):\n    # Override VulcanLoader's `_load_models` method to access every model\n    def _load_models(\n        self,\n        macros: \"MacroRegistry\",\n        jinja_macros: \"JinjaMacroRegistry\",\n        gateway: str | None,\n        audits: UniqueKeyDict[str, \"ModelAudit\"],\n        signals: UniqueKeyDict[str, \"signal\"],\n    ) -&gt; UniqueKeyDict[str, \"Model\"]:\n        # Call VulcanLoader's normal `_load_models` method to ingest models from file and parse model SQL\n        models = super()._load_models(macros, jinja_macros, gateway, audits, signals)\n\n        new_models = {}\n        # Loop through the existing model names/objects\n        for model_name, model in models.items():\n            # Create list of existing and new post-statements\n            new_post_statements = [\n                # Existing post-statements from model object\n                *model.post_statements,\n                # New post-statement is raw SQL, so we parse it with SQLGlot's `parse_one` function.\n                # Make sure to specify the SQL dialect if different from the project default.\n                parse_one(f\"VACUUM @this_model\"),\n            ]\n            # Create a copy of the model with the `post_statements_` field updated\n            new_models[model_name] = model.copy(update={\"post_statements_\": new_post_statements})\n\n        return new_models\n\n# Pass the CustomLoader class to the Vulcan configuration object\nconfig = Config(\n    # &lt; your configuration parameters here &gt;,\n    loader=CustomLoader,\n)\n</code></pre>"},{"location":"guides-old/isolated_systems/","title":"Isolated systems","text":""},{"location":"guides-old/isolated_systems/#isolated-systems","title":"Isolated systems","text":"<p>Vulcan is optimized for use in systems where developers have access to production data.</p> <p>Writing code against partial or unrepresentative data can cause problems because you don't become aware of changes in production data until errors have already occurred.</p> <p>Other data products, such as machine learning models, may depend on the distribution of values in the training data - building them on unrepresentative data may lead to different behavior in production than in development.</p> <p>However, some companies store production and non-production data in different data warehouses that can't talk to one another (\"isolated systems\"). This is usually due to information security concerns, as the non-production warehouse may be accessible to more users and/or have looser security restrictions.</p> <p>This guide explains how to use Vulcan with isolated systems and how isolating systems affects Vulcan's behavior.</p>"},{"location":"guides-old/isolated_systems/#terminology","title":"Terminology","text":"<p>Isolated systems are sometimes referred to as \"isolated environments,\" but we avoid that term because \"environments\" has a specific meaning in Vulcan.</p> <p>Instead, we will refer to them as isolated systems - the \"production system\" and \"non-production system.\"</p> <p>When we refer to \"environments,\" we are always talking about Vulcan environments - the isolated namespaces created and managed by Vulcan.</p>"},{"location":"guides-old/isolated_systems/#configuring-vulcan","title":"Configuring Vulcan","text":""},{"location":"guides-old/isolated_systems/#separate-state-data","title":"Separate state data","text":"<p>Vulcan maintains a record of every model version so it can identify changes when models are updated. Those records are called \"state\" data, as in \"the state of the model at that point in time.\"</p> <p>State data can be stored alongside other data in the primary data warehouse or in a separate database. We recommend using a separate transactional database for projects running on cloud SQL engines.</p> <p>Isolated systems must use a separate state database for each system. The state of models and other objects in the non-production system is not accurate for the production system, and sharing state data will prevent the project from running correctly.</p>"},{"location":"guides-old/isolated_systems/#multiple-gateways","title":"Multiple gateways","text":"<p>Vulcan database connections are configured with gateways that contain connections and other configuration parameters.</p> <p>A gateway must contain a connection to a SQL engine and may optionally contain a different connection to the database where Vulcan should store its state data.</p> <p>Isolated systems should configure two separate gateways: one for the production system and one for the non-production system.</p> <p>For example, this configuration creates gateways named <code>nonproduction</code> and <code>production</code>. You may omit the <code>state_connection</code> keys if state data will be stored in the gateway's primary connection.</p> <pre><code>gateways:\n  nonproduction:\n    connection:\n      ...[your non-production connection parameters]...\n    state_connection:\n      ...[your non-production state connection parameters]...\n  production:\n    connection:\n      ...[your production connection parameters]...\n    state_connection:\n      ...[your production state connection parameters]...\n</code></pre> <p>Vulcan will use the first gateway in the configuration as the default when executing a command. For example, with the configuration above Vulcan would use the <code>nonproduction</code> gateway when executing the command <code>vulcan plan</code>.</p> <p>Commands can override the default gateway with the <code>--gateway</code> option, such as <code>vulcan --gateway production plan</code>.</p>"},{"location":"guides-old/isolated_systems/#gateway-specific-schemas","title":"Gateway-specific schemas","text":"<p>We recommend using identical schema and model names in both systems, but in some scenarios that is not possible.</p> <p>Schema and model names may be parameterized by gateway using the predefined <code>@gateway</code> macro variable.</p> <p>This example demonstrates conditioning the model schema name on the current gateway with the Vulcan <code>@IF</code> macro operator. If the gateway is named <code>production</code>, <code>my_model</code>'s schema is <code>prod_schema</code>; otherwise, it is <code>dev_schema</code>.</p> <pre><code>MODEL (\n  name @IF(@gateway = 'production', prod_schema, dev_schema).my_model\n)\n</code></pre> <p>To embed the gateway name directly in the schema name, use the curly brace <code>@{gateway}</code> syntax:</p> <pre><code>MODEL (\n  name @{gateway}_schema.my_model\n)\n</code></pre> <p>Learn more about the curly brace <code>@{}</code> syntax here.</p>"},{"location":"guides-old/isolated_systems/#workflow","title":"Workflow","text":""},{"location":"guides-old/isolated_systems/#linking-systems","title":"Linking systems","text":"<p>The point of isolating systems is to prevent sharing of data by limiting network communications between the systems. Given this, how can a Vulcan project be shared between them at all?</p> <p>The Vulcan project files provide the link between the systems. The files should be stored in a mutually accessible location, such as a git repository.</p> <p></p>"},{"location":"guides-old/isolated_systems/#workflow-with-one-system","title":"Workflow with one system","text":"<p>This section describes workflows for updating Vulcan projects with one system.</p> <p>We assume that a version of the Vulcan project is currently running in production and serves as the starting point for code modifications.</p>"},{"location":"guides-old/isolated_systems/#basic-workflow","title":"Basic workflow","text":"<p>Use this workflow if your data system does not use CI/CD to implement changes:</p> <ul> <li>Make a change to a model</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in a local environment</li> <li>Run <code>vulcan plan</code> to apply the changes to the <code>prod</code> environment</li> </ul>"},{"location":"guides-old/isolated_systems/#cicd-workflow","title":"CI/CD workflow","text":"<p>Use this workflow with the Vulcan Github CI/CD bot:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Push the branch to the project repo and make a pull request. The bot will create a development environment for you to preview the changes if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>Learn more about synchronized and desynchronized deployments in the CI/CD bot documentation.</p>"},{"location":"guides-old/isolated_systems/#reusing-computations","title":"Reusing computations","text":"<p>Local environment previews are computed on the same data used by the <code>prod</code> environment in these workflows, so applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p>"},{"location":"guides-old/isolated_systems/#workflow-with-isolated-systems","title":"Workflow with isolated systems","text":"<p>This section describes the workflow with isolated systems.</p> <p>This workflow combines the basic and CI/CD workflows above, where the basic workflow is used in the non-production system and the CI/CD workflow is used in the production system:</p> <ul> <li><code>git clone</code> the project repo</li> <li>Make a change to a model in a git branch</li> <li>Run <code>vulcan plan dev</code> (or another environment name) to preview the changes in the nonproduction system. You may need to include the nonproduction <code>--gateway</code> option, depending on your project configuration.</li> <li>Push the branch to the project repo and make a pull request. The bot will create an environment to preview the changes in the production system if it is configured for synchronized deployments.</li> <li>Merge the branch into <code>main</code> to apply the changes to the <code>prod</code> environment</li> </ul> <p>The breaking/non-breaking change classifications in the non-production system will not be available to the production system because the systems do not share Vulcan state data. Therefore, the classifications must occur again in the production system.</p>"},{"location":"guides-old/isolated_systems/#reusing-computations_1","title":"Reusing computations","text":"<p>In isolated systems, Vulcan's virtual data environments operate normally within each system, but not across systems.</p> <p>In the non-production system, computations will be reused across preview environments. However, the system's data are not representative of the production data and will not be reused by the production system.</p> <p>In the production system, the CI/CD bot will execute the necessary computations when a pull request is submitted if it is configured for synchronized deployment. Merging to main and applying the changes to <code>prod</code> reuses the preview computations and only requires a virtual update.</p> <p>This approach enables true blue-green deployment. Deploying to production occurs with no system downtime because virtual updates only require swapping views. If issues are identified after changes have been pushed to production, reverting is quick and painless because it just swaps the views back.</p>"},{"location":"guides-old/migrations/","title":"Migrations","text":""},{"location":"guides-old/migrations/#migrations","title":"Migrations","text":"<p>New versions of Vulcan may be incompatible with the project's stored metadata format. Migrations provide a way to upgrade the project metadata format to operate with the new Vulcan version.</p>"},{"location":"guides-old/migrations/#detecting-incompatibility","title":"Detecting incompatibility","text":"<p>When issuing a Vulcan command, Vulcan will automatically check for incompatibilities between the installed version of Vulcan and the project's metadata format, prompting what action is required. Vulcan commands will not execute until the action is complete.</p>"},{"location":"guides-old/migrations/#installed-version-is-newer-than-metadata-format","title":"Installed version is newer than metadata format","text":"<p>In this scenario, the project's metadata format needs to be migrated.</p> <pre><code>&gt; vulcan plan my_dev\nError: Vulcan (local) is using version '2' which is ahead of '1' (remote). Please run a migration ('vulcan migrate' command).\n</code></pre>"},{"location":"guides-old/migrations/#installed-version-is-older-than-metadata-format","title":"Installed version is older than metadata format","text":"<p>Here, the installed version of Vulcan needs to be upgraded.</p> <pre><code>&gt; vulcan plan my_dev\nVulcanError: Vulcan (local) is using version '1' which is behind '2' (remote). Please upgrade Vulcan.\n</code></pre>"},{"location":"guides-old/migrations/#how-to-migrate","title":"How to migrate","text":""},{"location":"guides-old/migrations/#built-in-scheduler-migrations","title":"Built-in Scheduler Migrations","text":"<p>The project metadata can be migrated to the latest metadata format using Vulcan's migrate command.</p> <pre><code>&gt; vulcan migrate\n</code></pre> <p>Migration should be issued manually by a single user and the migration will affect all users of the project.  Migrations should ideally run when no one will be running plan/apply.  Migrations should not be run in parallel.  Due to these constraints, it is better for a person responsible for managing Vulcan to manually issue migrations.  Therefore, it is not recommended to issue migrations from CI/CD pipelines.</p>"},{"location":"guides-old/notifications/","title":"Notifications","text":""},{"location":"guides-old/notifications/#notifications","title":"Notifications","text":"<p>Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.</p>"},{"location":"guides-old/notifications/#notification-targets","title":"Notification targets","text":"<p>Notifications are configured with <code>notification targets</code>. Targets are specified in a project's configuration file (<code>config.yml</code> or <code>config.py</code>), and multiple targets can be specified for a project.</p> <p>A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each event type (e.g., notifications for <code>run</code> will be sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification target, corresponding to the two Slack notification methods and email notification. They are specified in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"guides-old/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter may be useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"guides-old/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"guides-old/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"guides-old/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n      - apply_failure\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#api-configuration","title":"API Configuration","text":"<p>If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n      - apply_end\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values directly into the configuration file:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides-old/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides-old/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, new notification targets can be configured to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>). See the definitions of these classes on Github here.</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"guides-old/projects/","title":"Projects","text":""},{"location":"guides-old/projects/#projects","title":"Projects","text":""},{"location":"guides-old/projects/#creating-a-project","title":"Creating a project","text":"<p>Before getting started, ensure that you meet the prerequisites for using Vulcan.</p> <p>To create a project from the command line, follow these steps:</p> <ol> <li> <p>Create a directory for your project:</p> <pre><code>mkdir my-project\n</code></pre> </li> <li> <p>Change directories into your new project:</p> <pre><code>cd my-project\n</code></pre> <p>From here, you can create your project structure from scratch, or Vulcan can scaffold one for you. For the purposes of this guide, we'll show you how to scaffold your project so that you can get up and running quickly.</p> </li> <li> <p>To scaffold a project, it is recommended that you use a python virtual environment by running the following commands:</p> <pre><code>python -m venv .venv\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>pip install vulcan\n</code></pre> <p>Note: When using a python virtual environment, you must ensure that it is activated first. You should see <code>(.venv)</code> in your command line; if you don't, run <code>source .venv/bin/activate</code> from your project directory to activate your environment.</p> </li> <li> <p>Once you have activated your environment, run the following command and Vulcan will build out your project:</p> <pre><code>vulcan init [SQL_DIALECT]\n</code></pre> <p>In the command above, you can use any SQL dialect supported by sqlglot, for example \"duckdb\".</p> <p>The following directories and files will be created that you can use to organize your Vulcan project:</p> <ul> <li>config.py (database configuration file)</li> <li>./models (SQL and Python models)</li> <li>./audits (shared audits)</li> <li>./tests (unit tests)</li> <li>./macros</li> </ul> </li> </ol>"},{"location":"guides-old/projects/#editing-an-existing-project","title":"Editing an existing project","text":"<p>To edit an existing project, open the project file you wish to edit in your preferred editor.</p> <p>If using CLI, you can open a file in your project for editing by using the <code>vulcan</code> command with the <code>-p</code> variable, and pointing to your project's path as follows:</p> <pre><code>vulcan -p &lt;your-project-path&gt;\n</code></pre> <p>For more details, refer to CLI</p>"},{"location":"guides-old/table_migration/","title":"Table migration","text":""},{"location":"guides-old/table_migration/#table-migration","title":"Table migration","text":"<p>Vulcan projects can read directly from tables not managed by Vulcan, but in some scenarios it may be useful to migrate an existing table into a Vulcan project.</p> <p>This guide describes two methods for migrating existing tables into a Vulcan project.</p>"},{"location":"guides-old/table_migration/#do-you-need-to-migrate","title":"Do you need to migrate?","text":"<p>Vulcan does not assume it manages all data sources: SQL models can read from any data source accessible by the SQL engine, treating them as external models that include column-level lineage or as generic sources. This approach is preferred to migrating existing tables into a Vulcan project.</p> <p>You should only migrate a table if both of the following are true:</p> <ol> <li>The table is ingesting from an upstream source that will continue generating new data</li> <li>The table is either too large to be rebuilt or cannot be rebuilt because the necessary historical data is unavailable</li> </ol> <p>If the table's upstream source will not generate more data, there is no ongoing activity for Vulcan to manage. A Vulcan model or any other downstream consumer can select directly from the table under its current name.</p> <p>If the table's upstream source is generating new data, we assume that the table is already being loaded incrementally, as there is no need for migration if the table can be fully rebuilt.</p> <p>We describe two migration methods below. The stage and union method is preferred and should be used if feasible.</p>"},{"location":"guides-old/table_migration/#migration-methods","title":"Migration methods","text":"<p>This section describes two methods for migrating tables into Vulcan.</p> <p>The method descriptions contain renaming steps that are only necessary if downstream consumers must select from the original table name (e.g., step 2 in the first example). If that is not the case, the original table can retain its name.</p> <p>The table and model names in the examples below are arbitrary - you may name them whatever is appropriate for your project.</p>"},{"location":"guides-old/table_migration/#stage-and-union","title":"Stage and union","text":"<p>The stage and union method works by treating new and historical data as separate sources.</p> <p>It requires creating an incremental staging model to ingest new records and a <code>VIEW</code> model that unions those records with the existing table's static historical records.</p>"},{"location":"guides-old/table_migration/#example","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the stage and union method consists of five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_historical</code><ul> <li>Optionally, enable column-level lineage for the table by making it an <code>EXTERNAL</code> model and adding it to the project's <code>external_models.yaml</code> file</li> </ul> </li> <li>Create a new incremental staging model named <code>my_schema.existing_table_staging</code> (see below for code)</li> <li>Create a new <code>VIEW</code> model named <code>my_schema.existing_table</code> (see below for code)</li> <li>Run <code>vulcan plan</code> to create and backfill the models</li> </ol> <p>The staging model would contain code similar to the following for an <code>INCREMENTAL_BY_TIME_RANGE</code> model. An <code>INCREMENTAL_BY_UNIQUE_KEY</code> model would have a different <code>kind</code> specification in the <code>MODEL</code> DDL and might not include the query's <code>WHERE</code> clause.</p> <pre><code>MODEL(\n  name my_schema.existing_table_staging,\n  kind INCREMENTAL_BY_TIME_RANGE ( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column\n  )\n);\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre> <p>The primary model would contain code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind VIEW\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_staging -- New data\nUNION\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  my_schema.existing_table_historical; -- Historical data\n</code></pre> <p>Changes to columns in the source data or staging model may require modifying the code selecting from the historical data so the two tables can be safely unioned.</p>"},{"location":"guides-old/table_migration/#snapshot-replacement","title":"Snapshot replacement","text":"<p>The snapshot replacement method works by renaming an existing table to a name that Vulcan recognizes as an existing Vulcan model.</p>"},{"location":"guides-old/table_migration/#background","title":"Background","text":"<p>This section briefly describes how Vulcan's virtual data environments, forward-only models, and start times work. This information is not necessary for migrating tables but is necessary for understanding why each step in the migration process is required.</p>"},{"location":"guides-old/table_migration/#virtual-data-environments","title":"Virtual data environments","text":"<p>Conceptually, Vulcan divides the database into a \"physical layer\" where data is stored and a \"virtual layer\" where data is accessed by end users. The physical layer stores materialized objects like tables, and the virtual layer contains views that point to the physical layer objects.</p> <p>Each time a Vulcan <code>plan</code> adds or modifies a model, Vulcan creates a physical layer \"snapshot\" object to which the virtual layer view points. The snapshot replacement method simply renames the migrating table to the name of the appropriate snapshot table.</p>"},{"location":"guides-old/table_migration/#forward-only-models","title":"Forward-only models","text":"<p>Sometimes a model's data may be so large that it is not feasible to rebuild either its own or its downstream models' physical tables. In those situations a  \"forward only\" model can be used. The name reflects that the change is only applied \"going forward\" in time.</p> <p>Historical data already in the migrated table should not be overwritten, so we specify that the new model is forward-only in step 3a below.</p>"},{"location":"guides-old/table_migration/#start-time","title":"Start time","text":"<p>Vulcan incremental by time models track the time periods whose data a model has loaded with the interval approach.</p> <p>The interval approach requires specifying the earliest time interval Vulcan should track - when time \"starts\" for the model. For migrated tables, Vulcan should never load data for the time intervals the table ingested before migration, so interval tracking should start immediately after the time of the last ingested record.</p> <p>In the example below, we set the model's start time in its <code>MODEL</code> DDL (step 3b) and pass it as an option to the <code>vulcan plan</code> command (step 3c). The same value must be used in both the <code>MODEL</code> DDL and the plan command. In this example, the existing table's data ingestion stopped on 2023-12-31, so the model and plan start date is the next day 2024-01-01.</p>"},{"location":"guides-old/table_migration/#example_1","title":"Example","text":"<p>Consider an existing table named <code>my_schema.existing_table</code>. Migrating this table with the snapshot replacement method involves five steps:</p> <ol> <li>Ensure <code>my_schema.existing_table</code> is up to date (has ingested all available source data)</li> <li>Rename <code>my_schema.existing_table</code> to any other name, such as <code>my_schema.existing_table_temp</code></li> <li> <p>Create and initialize an empty incremental model named <code>my_schema.existing_table</code>:</p> <p>a. Make the model forward only by setting the <code>MODEL</code> DDL <code>kind</code>'s <code>forward_only</code> key to <code>true</code></p> <p>b. Specify the start of the first time interval Vulcan should track in the <code>MODEL</code> DDL <code>start</code> key (example uses \"2024-01-01\")</p> <p>c. Create the model in the Vulcan project without backfilling any data by running <code>vulcan plan [environment name] --empty-backfill --start 2024-01-01</code>, replacing \"[environment name]\" with an environment name other than <code>prod</code> and using the same start date from the <code>MODEL</code> DDL in step 3b.</p> </li> <li> <p>Determine the name of the model's snapshot physical table by running <code>vulcan table_name --env [environment name] --prod my_schema.existing_table</code>. For example, it might return <code>vulcan__my_schema.existing_table_123456</code>.</p> </li> <li>Rename the original table <code>my_schema.existing_table_temp</code> to <code>vulcan__my_schema.existing_table_123456</code></li> </ol> <p>The model would have code similar to:</p> <pre><code>MODEL(\n  name my_schema.existing_table,\n  kind INCREMENTAL_BY_TIME_RANGE( -- or INCREMENTAL_BY_UNIQUE_KEY\n    time_column table_time_column,\n    forward_only true -- Forward-only model\n  ),\n  -- Start of first time interval Vulcan should track, immediately\n  --  after the last data point the table ingested. Must match\n  --  the value passed to the `vulcan plan --start` option.\n  start \"2024-01-01\"\n)\n\nSELECT\n  col1,\n  col2,\n  col3\nFROM\n  [your model's ongoing data source]\nWHERE\n  table_time_column BETWEEN @start_ds and @end_ds;\n</code></pre>"},{"location":"references/configuration/","title":"Vulcan configuration","text":""},{"location":"references/configuration/#vulcan-configuration","title":"Vulcan configuration","text":"<p>This page lists Vulcan configuration options and their parameters. Learn more about Vulcan configuration in the configuration guide.</p> <p>Configuration options for model definitions are listed in the model configuration reference page.</p>"},{"location":"references/configuration/#root-configurations","title":"Root configurations","text":"<p>A Vulcan project configuration consists of root level parameters within which other parameters are defined.</p> <p>Two important root level parameters are <code>gateways</code> and gateway/connection defaults, which have their own sections below.</p> <p>This section describes the other root level configuration parameters.</p>"},{"location":"references/configuration/#projects","title":"Projects","text":"<p>Configuration options for Vulcan project directories.</p> Option Description Type Required <code>ignore_patterns</code> Files that match glob patterns specified in this list are ignored when scanning the project folder (Default: <code>[]</code>) list[string] N <code>project</code> The project name of this config. Used for multi-repo setups. string N <code>cache_dir</code> The directory to store the Vulcan cache. Can be an absolute path or relative to the project directory. (Default: <code>.cache</code>) string N <code>log_limit</code> The default number of historical log files to keep (Default: <code>20</code>) int N"},{"location":"references/configuration/#database-physical-layer","title":"Database (Physical Layer)","text":"<p>Configuration options for how Vulcan manages database objects in the physical layer.</p> Option Description Type Required <code>snapshot_ttl</code> The period of time that a model snapshot not a part of any environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>physical_schema_override</code> (Deprecated) Use <code>physical_schema_mapping</code> instead. A mapping from model schema names to names of schemas in which physical tables for the corresponding models will be placed. dict[string, string] N <code>physical_schema_mapping</code> A mapping from regular expressions to names of schemas in which physical tables for the corresponding models will be placed. (Default physical schema name: <code>vulcan__[model schema]</code>) dict[string, string] N <code>physical_table_naming_convention</code> Sets which parts of the model name are included in the physical table names. Options are <code>schema_and_table</code>, <code>table_only</code> or <code>hash_md5</code> - additional details. (Default: <code>schema_and_table</code>) string N"},{"location":"references/configuration/#environments-virtual-layer","title":"Environments (Virtual Layer)","text":"<p>Configuration options for how Vulcan manages environment creation and promotion in the virtual layer.</p> Option Description Type Required <code>environment_ttl</code> The period of time that a development environment should exist before being deleted. This is defined as a string with the default <code>in 1 week</code>. Other relative dates can be used, such as <code>in 30 days</code>. (Default: <code>in 1 week</code>) string N <code>pinned_environments</code> The list of development environments that are exempt from deletion due to expiration list[string] N <code>default_target_environment</code> The name of the environment that will be the default target for the <code>vulcan plan</code> and <code>vulcan run</code> commands. (Default: <code>prod</code>) string N <code>environment_suffix_target</code> Whether Vulcan views should append their environment name to the <code>schema</code>, <code>table</code> or <code>catalog</code> - additional details. (Default: <code>schema</code>) string N <code>gateway_managed_virtual_layer</code> Whether Vulcan views of the virtual layer will be created by the default gateway or model specified gateways. (Default: False) boolean N <code>environment_catalog_mapping</code> A mapping from regular expressions to catalog names. The catalog name is used to determine the target catalog for a given environment. dict[string, string] N <code>virtual_environment_mode</code> Determines the Virtual Data Environment (VDE) mode. If set to <code>full</code>, VDE is used in both production and development environments. The <code>dev_only</code> option enables VDE only in development environments, while in production, no virtual layer is used and models are materialized directly using their original names (i.e., no versioned physical tables). (Default: <code>full</code>) string N"},{"location":"references/configuration/#models","title":"Models","text":"Option Description Type Required <code>time_column_format</code> The default format to use for all model time columns. This time format uses python format codes (Default: <code>%Y-%m-%d</code>) string N <code>infer_python_dependencies</code> Whether Vulcan will statically analyze Python code to automatically infer Python package requirements. (Default: True) boolean N <code>model_defaults</code> Default properties to set on each model. At a minimum, <code>dialect</code> must be set. dict[string, any] Y <p>The <code>model_defaults</code> key is required and must contain a value for the <code>dialect</code> key.</p> <p>See all the keys allowed in <code>model_defaults</code> at the model configuration reference page.</p>"},{"location":"references/configuration/#variables","title":"Variables","text":"<p>The <code>variables</code> key can be used to provide values for user-defined variables, accessed using the <code>@VAR</code> macro function in SQL model definitions, <code>context.var</code> method in Python model definitions, and <code>evaluator.var</code> method in Python macro functions.</p> <p>The <code>variables</code> key consists of a mapping of variable names to their values - see an example on the Vulcan macros concepts page. Note that keys are case insensitive.</p> <p>Global variable values may be any of the data types in the table below or lists or dictionaries containing those types.</p> Option Description Type Required <code>variables</code> Mapping of variable names to values dict[string, int | float | bool | string | list | dict] N"},{"location":"references/configuration/#before_all-after_all","title":"Before_all / after_all","text":"<p>The <code>before_all</code> and <code>after_all</code> keys can be used to specify lists of SQL statements and/or Vulcan macros that are executed at the start and end, respectively, of the <code>vulcan plan</code> and <code>vulcan run</code> commands. For more information and examples, see the configuration guide.</p> Option Description Type Required <code>before_all</code> List of SQL statements to be executed at the start of the <code>plan</code> and <code>run</code> commands. list[string] N <code>after_all</code> List of SQL statements to be executed at the end of the <code>plan</code> and <code>run</code> commands. list[string] N"},{"location":"references/configuration/#plan","title":"Plan","text":"<p>Configuration for the <code>vulcan plan</code> command.</p> Option Description Type Required <code>auto_categorize_changes</code> Indicates whether Vulcan should attempt to automatically categorize model changes during plan creation per each model source type (additional details) dict[string, string] N <code>include_unmodified</code> Indicates whether to create views for all models in the target development environment or only for modified ones (Default: False) boolean N <code>auto_apply</code> Indicates whether to automatically apply a new plan after creation (Default: False) boolean N <code>forward_only</code> Indicates whether the plan should be forward-only (Default: False) boolean N <code>enable_preview</code> Indicates whether to enable data preview for forward-only models when targeting a development environment (Default: True, except for dbt projects where the target engine does not support cloning) Boolean N <code>no_diff</code> Don't show diffs for changed models (Default: False) boolean N <code>no_prompts</code> Disables interactive prompts in CLI (Default: True) boolean N <code>always_recreate_environment</code> Always recreates the target environment from the environment specified in <code>create_from</code> (by default <code>prod</code>) (Default: False) boolean N"},{"location":"references/configuration/#run","title":"Run","text":"<p>Configuration for the <code>vulcan run</code> command. Please note that this is only applicable when configured with the builtin scheduler.</p> Option Description Type Required <code>environment_check_interval</code> The number of seconds to wait between attempts to check the target environment for readiness (Default: 30 seconds) int N <code>environment_check_max_wait</code> The maximum number of seconds to wait for the target environment to be ready (Default: 6 hours) int N"},{"location":"references/configuration/#format","title":"Format","text":"<p>Formatting settings for the <code>vulcan format</code> command and UI.</p> Option Description Type Required <code>normalize</code> Whether to normalize SQL (Default: False) boolean N <code>pad</code> The number of spaces to use for padding (Default: 2) int N <code>indent</code> The number of spaces to use for indentation (Default: 2) int N <code>normalize_functions</code> Whether to normalize function names. Supported values are: 'upper' and 'lower' (Default: None) string N <code>leading_comma</code> Whether to use leading commas (Default: False) boolean N <code>max_text_width</code> The maximum text width in a segment before creating new lines (Default: 80) int N <code>append_newline</code> Whether to append a newline to the end of the file (Default: False) boolean N <code>no_rewrite_casts</code> Preserve the existing casts, without rewriting them to use the :: syntax. (Default: False) boolean N"},{"location":"references/configuration/#janitor","title":"Janitor","text":"<p>Configuration for the <code>vulcan janitor</code> command.</p> Option Description Type Required <code>warn_on_delete_failure</code> Whether to warn instead of erroring if the janitor fails to delete the expired environment schema / views (Default: False) boolean N <code>expired_snapshots_batch_size</code> Maximum number of expired snapshots to clean in a single batch (Default: 200) int N"},{"location":"references/configuration/#gateways","title":"Gateways","text":"<p>The <code>gateways</code> dictionary defines how Vulcan should connect to the data warehouse, state backend, test backend, and scheduler.</p> <p>It takes one or more named <code>gateway</code> configuration keys, each of which can define its own connections. Gateway names are case-insensitive - Vulcan normalizes all gateway names to lowercase during configuration validation, allowing you to use any case when referencing gateways. A named gateway does not need to specify all four components and will use defaults if any are omitted - more information is provided about gateway defaults below.</p> <p>For example, a project might configure the <code>gate1</code> and <code>gate2</code> gateways:</p> <pre><code>gateways:\n  gate1:\n    connection:\n      ...\n    state_connection: # defaults to `connection` if omitted\n      ...\n    test_connection: # defaults to `connection` if omitted\n      ...\n    scheduler: # defaults to `builtin` if omitted\n      ...\n  gate2:\n    connection:\n      ...\n</code></pre> <p>Find additional information about gateways in the configuration guide gateways section.</p>"},{"location":"references/configuration/#gateway","title":"Gateway","text":"<p>Configuration for each named gateway.</p>"},{"location":"references/configuration/#connections","title":"Connections","text":"<p>A named gateway key may define any or all of a data warehouse connection, state backend connection, state schema name, test backend connection, and scheduler.</p> <p>Some connections use default values if not specified:</p> <ul> <li>The <code>connection</code> key may be omitted if a <code>default_connection</code> is specified.</li> <li>The state connection defaults to <code>connection</code> if omitted.</li> <li>The test connection defaults to <code>connection</code> if omitted.</li> </ul> <p>NOTE: Spark and Trino engines may not be used for the state connection.</p> Option Description Type Required <code>connection</code> The data warehouse connection for core Vulcan functions. connection configuration N (if <code>default_connection</code> specified) <code>state_connection</code> The data warehouse connection where Vulcan will store internal information about the project. (Default: <code>connection</code> if using builtin scheduler, otherwise scheduler database) connection configuration N <code>state_schema</code> The name of the schema where state information should be stored. (Default: <code>vulcan</code>) string N <code>test_connection</code> The data warehouse connection Vulcan will use to execute tests. (Default: <code>connection</code>) connection configuration N <code>scheduler</code> The scheduler Vulcan will use to execute tests. (Default: <code>builtin</code>) scheduler configuration N <code>variables</code> The gateway-specific variables which override the root-level variables by key. dict[string, int | float | bool | string | list | dict] N"},{"location":"references/configuration/#connection","title":"Connection","text":"<p>Configuration for a data warehouse connection.</p> <p>Most parameters are specific to the connection engine <code>type</code> - see below. The default data warehouse connection type is an in-memory DuckDB database.</p>"},{"location":"references/configuration/#general","title":"General","text":"Option Description Type Required <code>type</code> The engine type name, listed in engine-specific configuration pages below. str Y <code>concurrent_tasks</code> The maximum number of concurrent tasks that will be run by Vulcan. (Default: 4 for engines that support concurrent tasks.) int N <code>register_comments</code> Whether Vulcan should register model comments with the SQL engine (if the engine supports it). (Default: <code>true</code>.) bool N <code>pre_ping</code> Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive. This can only be enabled for engines with transaction support. bool N <code>pretty_sql</code> If SQL should be formatted before being executed, not recommended in a production setting. (Default: <code>false</code>.) bool N"},{"location":"references/configuration/#engine-specific","title":"Engine-specific","text":"<p>These pages describe the connection configuration options for each execution engine.</p> <ul> <li>Postgres</li> <li>Snowflake</li> </ul>"},{"location":"references/configuration/#scheduler","title":"Scheduler","text":"<p>Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing plans.</p> <p>By default, the scheduler type is set to <code>builtin</code> and uses the gateway's connection to store metadata.</p> <p>Below is the list of configuration options specific to each corresponding scheduler type. Find additional details in the configuration overview scheduler section.</p>"},{"location":"references/configuration/#builtin","title":"Builtin","text":"<p>Type: <code>builtin</code></p> <p>No configuration options are supported by this scheduler type.</p>"},{"location":"references/configuration/#gatewayconnection-defaults","title":"Gateway/connection defaults","text":"<p>The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. Find additional details in the configuration overview page gateway/connection defaults section.</p>"},{"location":"references/configuration/#default-gateway","title":"Default gateway","text":"<p>If a configuration contains multiple gateways, Vulcan will use the first one in the <code>gateways</code> dictionary by default. The <code>default_gateway</code> key is used to specify a different gateway name as the Vulcan default.</p> Option Description Type Required <code>default_gateway</code> The name of a gateway to use if one is not provided explicitly (Default: the gateway defined first in the <code>gateways</code> option). Gateway names are case-insensitive. string N"},{"location":"references/configuration/#default-connectionsscheduler","title":"Default connections/scheduler","text":"<p>The <code>default_connection</code>, <code>default_test_connection</code>, and <code>default_scheduler</code> keys are used to specify shared defaults across multiple gateways.</p> <p>For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the <code>default_test_connection</code> key.</p> Option Description Type Required <code>default_connection</code> The default connection to use if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_test_connection</code> The default connection to use when running tests if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) connection N <code>default_scheduler</code> The default scheduler configuration to use if one is not specified in a gateway (Default: built-in scheduler) scheduler N"},{"location":"references/configuration/#debug-mode","title":"Debug mode","text":"<p>Enable debug mode in one of two ways:</p> <ul> <li>Pass the <code>--debug</code> flag between the CLI command and the subcommand. For example, <code>vulcan --debug plan</code>.</li> <li>Set the <code>VULCAN_DEBUG</code> environment variable to one of the following values: \"1\", \"true\", \"t\", \"yes\" or \"y\".</li> </ul> <p>Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to <code>DEBUG</code> when this mode is enabled.</p> <p>Example enabling debug mode for the CLI command <code>vulcan plan</code>:</p> BashMS PowershellMS CMD <pre><code>$ vulcan --debug plan\n</code></pre> <pre><code>$ VULCAN_DEBUG=1 vulcan plan\n</code></pre> <pre><code>PS&gt; vulcan --debug plan\n</code></pre> <pre><code>PS&gt; $env:VULCAN_DEBUG=1\nPS&gt; vulcan plan\n</code></pre> <pre><code>C:\\&gt; vulcan --debug plan\n</code></pre> <pre><code>C:\\&gt; set VULCAN_DEBUG=1\nC:\\&gt; vulcan plan\n</code></pre>"},{"location":"references/configuration/#parallel-loading","title":"Parallel loading","text":"<p>Vulcan by default uses all of your cores when loading models and snapshots. It takes advantage of <code>fork</code> which is not available on Windows. The default is to use the same number of workers as cores on your machine if fork is available.</p> <p>You can override this setting by setting the environment variable <code>MAX_FORK_WORKERS</code>. A value of 1 will disable forking and load things sequentially.</p>"},{"location":"references/environments/","title":"Environments","text":""},{"location":"references/environments/#environments","title":"Environments","text":"<p>Environments are isolated namespaces that allow you to test and preview your changes.</p> <p>Vulcan differentiates between production and development environments. Currently, only the environment with the name <code>prod</code> is treated by Vulcan as the production one. Environments with other names are considered to be development ones.</p> <p>Models in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name <code>db.model_a</code> in the target environment <code>my_dev</code>, the <code>db__my_dev.model_a</code> table name should be used in a query. Models in the production environment are referred to by their original names.</p>"},{"location":"references/environments/#why-use-environments","title":"Why use environments","text":"<p>Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.</p> <p>Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment do not impact other environments. At the same time, any computation that was done in this environment can be safely reused in other environments.</p>"},{"location":"references/environments/#how-to-use-environments","title":"How to use environments","text":"<p>When running the plan command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is <code>prod</code>, which refers to the production environment. Environment with names other than <code>prod</code> are considered to be development environments.</p> <p>By default, the <code>vulcan plan</code> command targets the production (<code>prod</code>) environment.</p>"},{"location":"references/environments/#example","title":"Example","text":"<p>A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name <code>my_dev</code>, run:</p> <p></p><pre><code>vulcan plan my_dev\n</code></pre> A new environment is created automatically the first time a plan is applied to it.<p></p>"},{"location":"references/environments/#how-environments-work","title":"How environments work","text":"<p>Whenever a model definition changes, a new model snapshot is created with a unique fingerprint. This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model snapshots.</p> <p>Refer to plans for additional details.</p>"},{"location":"references/environments/#date-range","title":"Date range","text":"<p>A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the plan creation.</p>"},{"location":"references/glossary/","title":"Glossary","text":""},{"location":"references/glossary/#glossary","title":"Glossary","text":""},{"location":"references/glossary/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.</p>"},{"location":"references/glossary/#backfill","title":"Backfill","text":"<p>Load or refresh model data, triggered by a vulcan plan command.</p>"},{"location":"references/glossary/#catalog","title":"Catalog","text":"<p>A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.</p>"},{"location":"references/glossary/#cicd","title":"CI/CD","text":"<p>An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with tests and audits.</p>"},{"location":"references/glossary/#cte","title":"CTE","text":"<p>A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to tests.</p>"},{"location":"references/glossary/#dag","title":"DAG","text":"<p>Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.</p>"},{"location":"references/glossary/#data-modeling","title":"Data modeling","text":"<p>Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.</p>"},{"location":"references/glossary/#data-pipeline","title":"Data pipeline","text":"<p>The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.</p>"},{"location":"references/glossary/#data-transformation","title":"Data transformation","text":"<p>Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.</p>"},{"location":"references/glossary/#data-warehouse","title":"Data warehouse","text":"<p>The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.</p>"},{"location":"references/glossary/#direct-modification","title":"Direct Modification","text":"<p>A change to a model's definition from the user instead of being inherited from an upstream dependency like Indirect Modification.</p>"},{"location":"references/glossary/#elt","title":"ELT","text":"<p>Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.</p>"},{"location":"references/glossary/#etl","title":"ETL","text":"<p>Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.</p>"},{"location":"references/glossary/#full-refresh","title":"Full refresh","text":"<p>In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.</p>"},{"location":"references/glossary/#idempotency","title":"Idempotency","text":"<p>The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.</p>"},{"location":"references/glossary/#incremental-loads","title":"Incremental Loads","text":"<p>Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See Model Kinds for more information.</p>"},{"location":"references/glossary/#indirect-modification","title":"Indirect Modification","text":"<p>A change to model's upstream dependency and not to the model itself like a Direct Modification.</p>"},{"location":"references/glossary/#integration","title":"Integration","text":"<p>Combining data from various sources (such as from a data warehouse) into one unified view.</p>"},{"location":"references/glossary/#lineage","title":"Lineage","text":"<p>The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.</p>"},{"location":"references/glossary/#physical-layer","title":"Physical Layer","text":"<p>The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the Vulcan virtual layer's views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.</p>"},{"location":"references/glossary/#plan-summaries","title":"Plan Summaries","text":"<p>An upcoming feature that allows users to see a summary of changes applied to a given environment.</p>"},{"location":"references/glossary/#semantic-understanding","title":"Semantic Understanding","text":"<p>Vulcan, by leveraging SQLGlot, understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.</p>"},{"location":"references/glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.</p>"},{"location":"references/glossary/#table","title":"Table","text":"<p>A table is the visual representation of data stored in rows and columns.</p>"},{"location":"references/glossary/#user-defined-function-udf","title":"User-Defined Function (UDF)","text":"<p>Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.</p>"},{"location":"references/glossary/#view","title":"View","text":"<p>A view is the result of a SQL query on a database.</p>"},{"location":"references/glossary/#virtual-environments","title":"Virtual Environments","text":"<p>Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See plan application for more information.</p>"},{"location":"references/glossary/#virtual-layer","title":"Virtual Layer","text":"<p>The virtual layer is Vulcan's abstraction layer over the physical layer and physical data storage. While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.</p>"},{"location":"references/glossary/#virtual-update","title":"Virtual Update","text":"<p>Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See Virtual Update for more information.</p>"},{"location":"references/glossary/#virtual-preview","title":"Virtual Preview","text":"<p>Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.</p>"},{"location":"references/model_configuration/","title":"Model configuration","text":""},{"location":"references/model_configuration/#model-configuration","title":"Model configuration","text":"<p>This page lists Vulcan model configuration options and their parameters.</p> <p>Learn more about specifying Vulcan model properties in the model concepts overview page.</p>"},{"location":"references/model_configuration/#general-model-properties","title":"General model properties","text":"<p>Configuration options for Vulcan model properties. Supported by all model kinds other than <code>SEED</code> models.</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. <code>str</code> N <code>project</code> The name of the project the model belongs to - used in multi-repo deployments <code>str</code> N <code>kind</code> The model kind (Additional Details). (Default: <code>VIEW</code>) <code>str</code> | <code>dict</code> N <code>audits</code> Vulcan audits that should run against the model's output <code>array[str]</code> N <code>dialect</code> The SQL dialect in which the model's query is written. All SQL dialects supported by the SQLGlot library are allowed. <code>str</code> N <code>owner</code> The owner of a model; may be used for notification purposes <code>str</code> N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name <code>str</code> N <code>tags</code> Arbitrary strings used to organize or classify a model <code>array[str]</code> N <code>cron</code> The cron expression specifying how often the model should be refreshed. (Default: <code>@daily</code>) <code>str</code> N <code>interval_unit</code> The temporal granularity of the model's data intervals. Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code>. (Default: inferred from <code>cron</code>) <code>str</code> N <code>start</code> The date/time that determines the earliest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. (Default: <code>yesterday</code>) <code>str</code> | <code>int</code> N <code>end</code> The date/time that determines the latest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as <code>1 year ago</code>. <code>str</code> | <code>int</code> N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). <code>str</code> N <code>column_descriptions</code> A key-value mapping of column names to column comments that will be registered in the SQL engine's table COMMENT field (if supported by the engine). Specified as key-value pairs (<code>column_name = 'column comment'</code>). If present, inline column comments will not be registered in the SQL engine. <code>dict</code> N <code>grains</code> The column(s) whose combination uniquely identifies each row in the model <code>str</code> | <code>array[str]</code> N <code>profiles</code> The column(s) to profile for data quality checks using Soda profiling <code>str</code> | <code>array[str]</code> N <code>references</code> The model column(s) used to join to other models' grains <code>str</code> | <code>array[str]</code> N <code>depends_on</code> Models on which this model depends, in addition to the ones inferred from the model's query. (Default: dependencies inferred from model code) <code>array[str]</code> N <code>table_format</code> The table format that should be used to manage the physical files (eg <code>iceberg</code>, <code>hive</code>, <code>delta</code>); only applicable to engines such as Spark and Athena <code>str</code> N <code>storage_format</code> The storage format that should be used to store physical files (eg <code>parquet</code>, <code>orc</code>); only applicable to engines such as Spark and Athena <code>str</code> N <code>partitioned_by</code> The column(s) and/or column expressions used define a model's partitioning key. Required for the <code>INCREMENTAL_BY_PARTITION</code> model kind. Optional for all other model kinds; used to partition the model's physical table in engines that support partitioning. <code>str</code> | <code>array[str]</code> N <code>clustered_by</code> The column(s) and/or column expressions used to cluster the model's physical table; only applicable to engines that support clustering <code>str</code> N <code>columns</code> The column names and data types returned by the model. Disables automatic inference of column names and types from the SQL query. <code>array[str]</code> N <code>physical_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model table / view in the physical layer. Specified as key-value pairs (<code>key = value</code>). The view/table type (e.g. <code>TEMPORARY</code>, <code>TRANSIENT</code>) can be added with the <code>creatable_type</code> key. <code>dict</code> N <code>virtual_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the model view in the virtual layer. Specified as key-value pairs (<code>key = value</code>). The view type (e.g. <code>SECURE</code>) can be added with the <code>creatable_type</code> key. <code>dict</code> N <code>session_properties</code> A key-value mapping of arbitrary properties specific to the target engine that are applied to the engine session. Specified as key-value pairs (<code>key = value</code>). <code>dict</code> N <code>allow_partials</code> Whether this model can process partial (incomplete) data intervals <code>bool</code> N <code>enabled</code> Whether the model is enabled. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to ignore this model when loading the project. <code>bool</code> N <code>optimize_query</code> Whether the model's query should be optimized. This attribute is <code>true</code> by default. Setting it to <code>false</code> causes Vulcan to disable query canonicalization &amp; simplification. This should be turned off only if the optimized query leads to errors such as surpassing text limit. <code>bool</code> N <code>ignored_rules</code> A list of linter rule names (or \"ALL\") to be ignored/excluded for this model <code>str</code> | <code>array[str]</code> N <code>formatting</code> Whether the model will be formatted. All models are formatted by default. Setting this to <code>false</code> causes Vulcan to ignore this model during <code>vulcan format</code>. <code>bool</code> N"},{"location":"references/model_configuration/#model-defaults","title":"Model defaults","text":"<p>The Vulcan project-level configuration must contain the <code>model_defaults</code> key and must specify a value for its <code>dialect</code> key. Other values are set automatically unless explicitly overridden in the model definition. Learn more about project-level configuration in the configuration guide.</p> <p>In <code>physical_properties</code>, <code>virtual_properties</code>, and <code>session_properties</code>, when both project-level and model-specific properties are defined, they are merged, with model-level properties taking precedence. To unset a project-wide property for a specific model, set it to <code>None</code> in the <code>MODEL</code>'s DDL properties or within the <code>@model</code> decorator for Python models.</p> <p>For example, with the following <code>model_defaults</code> configuration:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    partition_expiration_days: 7\n    require_partition_filter: True\n    project_level_property: \"value\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"partition_expiration_days\": 7,\n      \"require_partition_filter\": True,\n      \"project_level_property\": \"value\"\n    },\n  ),\n)\n</code></pre> <p>To override <code>partition_expiration_days</code>, add a new <code>creatable_type</code> property and unset <code>project_level_property</code>, you can define the model as follows:</p> SQLPython <pre><code>MODEL (\n  ...,\n  physical_properties (\n    partition_expiration_days = 14,\n    creatable_type = TRANSIENT,\n    project_level_property = None,\n  )\n);\n</code></pre> <pre><code>@model(\n  ...,\n  physical_properties={\n    \"partition_expiration_days\": 14,\n    \"creatable_type\": \"TRANSIENT\",\n    \"project_level_property\": None\n  },\n)\n</code></pre> <p>You can also use the <code>@model_kind_name</code> variable to fine-tune control over <code>physical_properties</code> in <code>model_defaults</code>. This holds the current model's kind name and is useful for conditionally assigning a property. For example, to disable <code>creatable_type</code> for your project's <code>VIEW</code> kind models:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  start: 2022-01-01\n  physical_properties:\n    creatable_type: \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    start=\"2022-01-01\",\n    physical_properties={\n      \"creatable_type\": \"@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)\",\n    },\n  ),\n)\n</code></pre> <p>You can aso define <code>pre_statements</code>, <code>post_statements</code> and <code>on_virtual_update</code> statements at the project level that will be applied to all models. These default statements are merged with any model-specific statements, with default statements executing first, followed by model-specific statements.</p> YAMLPython <pre><code>model_defaults:\n  dialect: duckdb\n  pre_statements:\n    - \"SET timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"duckdb\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre> <p>The Vulcan project-level <code>model_defaults</code> key supports the following options, described in the general model properties table above:</p> <ul> <li>kind</li> <li>dialect</li> <li>cron</li> <li>owner</li> <li>start</li> <li>table_format</li> <li>storage_format</li> <li>physical_properties</li> <li>virtual_properties</li> <li>session_properties (on per key basis)</li> <li>on_destructive_change (described below)</li> <li>on_additive_change (described below)</li> <li>audits (described here)</li> <li>optimize_query</li> <li>allow_partials</li> <li>enabled</li> <li>interval_unit</li> <li>pre_statements (described here)</li> <li>post_statements (described here)</li> <li>on_virtual_update (described here)</li> </ul>"},{"location":"references/model_configuration/#model-naming","title":"Model Naming","text":"<p>Configuration option for name inference. Learn more in the model naming guide.</p> Option Description Type Required <code>infer_names</code> Whether to automatically infer model names based on the directory structure (Default: <code>False</code>) <code>bool</code> N"},{"location":"references/model_configuration/#model-kind-properties","title":"Model kind properties","text":"<p>Configuration options for kind-specific Vulcan model properties, in addition to the general model properties listed above.</p> <p>Learn more about model kinds at the model kind concepts page. Learn more about specifying model kind in Python models at the Python models concepts page.</p>"},{"location":"references/model_configuration/#view-models","title":"<code>VIEW</code> models","text":"<p>Configuration options for models of the <code>VIEW</code> kind (in addition to general model properties).</p> Option Description Type Required <code>materialized</code> Whether views should be materialized (for engines supporting materialized views). (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.VIEW</p>"},{"location":"references/model_configuration/#full-models","title":"<code>FULL</code> models","text":"<p>The <code>FULL</code> model kind does not support any configuration options other than the general model properties listed above.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.FULL</p>"},{"location":"references/model_configuration/#incremental-models","title":"Incremental models","text":"<p>Configuration options for all incremental models (in addition to general model properties).</p> Option Description Type Required <code>forward_only</code> Whether the model's changes should always be classified as forward-only. (Default: <code>False</code>) <code>bool</code> N <code>on_destructive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes a destructive modification to the model schema. Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>error</code>) <code>str</code> N <code>on_additive_change</code> What should happen when a change to a forward-only model or incremental model in a forward-only plan causes an additive modification to the model schema (like adding new columns). Valid values: <code>allow</code>, <code>warn</code>, <code>error</code>, <code>ignore</code>. (Default: <code>allow</code>) <code>str</code> N <code>disable_restatement</code> Whether restatements should be disabled for the model. (Default: <code>False</code>) <code>bool</code> N"},{"location":"references/model_configuration/#incremental-by-time-range","title":"Incremental by time range","text":"<p>Configuration options for <code>INCREMENTAL_BY_TIME_RANGE</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>time_column</code> The model column containing each row's timestamp. Should be UTC time zone. <code>str</code> Y <code>format</code> Argument to <code>time_column</code>. Format of the time column's data. (Default: <code>%Y-%m-%d</code>) <code>str</code> N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) <code>int</code> N <code>batch_concurrency</code> The maximum number of batches that can run concurrently for this model. (Default: the number of concurrent tasks set in the connection settings) <code>int</code> N <code>lookback</code> The number of <code>interval_unit</code>s prior to the current interval that should be processed - learn more. (Default: <code>0</code>) <code>int</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_TIME_RANGE</p>"},{"location":"references/model_configuration/#incremental-by-unique-key","title":"Incremental by unique key","text":"<p>Configuration options for <code>INCREMENTAL_BY_UNIQUE_KEY</code> models (in addition to general model properties and incremental model properties). Batch concurrency cannot be set for incremental by unique key models because they cannot safely be run in parallel.</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key <code>str</code> | <code>array[str]</code> Y <code>when_matched</code> SQL logic used to update columns when a match occurs - only available on engines that support <code>MERGE</code>. (Default: update all columns) <code>str</code> N <code>merge_filter</code> A single or a conjunction of predicates used to filter data in the ON clause of a MERGE operation - only available on engines that support <code>MERGE</code> <code>str</code> N <code>batch_size</code> The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of <code>batch_size</code> intervals. (Default: <code>None</code>) <code>int</code> N <code>lookback</code> The number of time unit intervals prior to the current interval that should be processed. (Default: <code>0</code>) <code>int</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</p>"},{"location":"references/model_configuration/#incremental-by-partition","title":"Incremental by partition","text":"<p>The <code>INCREMENTAL_BY_PARTITION</code> models kind does not support any configuration options other than the general model properties and incremental model properties.</p> <p>Python model kind <code>name</code> enum value: ModelKindName.INCREMENTAL_BY_PARTITION</p>"},{"location":"references/model_configuration/#scd-type-2-models","title":"SCD Type 2 models","text":"<p>Configuration options for <code>SCD_TYPE_2</code> models (in addition to general model properties and incremental model properties).</p> Option Description Type Required <code>unique_key</code> The model column(s) containing each row's unique key <code>array[str]</code> Y <code>valid_from_name</code> The model column containing each row's valid from date. (Default: <code>valid_from</code>) <code>str</code> N <code>valid_to_name</code> The model column containing each row's valid to date. (Default: <code>valid_to</code>) <code>str</code> N <code>invalidate_hard_deletes</code> If set to true, when a record is missing from the source table it will be marked as invalid - see here for more information. (Default: <code>True</code>) <code>bool</code> N"},{"location":"references/model_configuration/#scd-type-2-by-time","title":"SCD Type 2 By Time","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_TIME</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>updated_at_name</code> The model column containing each row's updated at date. (Default: <code>updated_at</code>) <code>str</code> N <code>updated_at_as_valid_from</code> By default, for new rows the <code>valid_from</code> column is set to 1970-01-01 00:00:00. This sets <code>valid_from</code> to the value of <code>updated_at</code> when the row is inserted. (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_TIME</p>"},{"location":"references/model_configuration/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>Configuration options for <code>SCD_TYPE_2_BY_COLUMN</code> models (in addition to general model properties, incremental model properties, and SCD Type 2 properties).</p> Option Description Type Required <code>columns</code> Columns whose changed data values indicate a data update (instead of an <code>updated_at</code> column). <code>*</code> to represent that all columns should be checked. <code>str</code> | <code>array[str]</code> Y <code>execution_time_as_valid_from</code> By default, for new rows <code>valid_from</code> is set to 1970-01-01 00:00:00. This changes the behavior to set it to the execution_time of when the pipeline ran. (Default: <code>False</code>) <code>bool</code> N <p>Python model kind <code>name</code> enum value: ModelKindName.SCD_TYPE_2_BY_COLUMN</p>"},{"location":"references/model_configuration/#seed-models","title":"<code>SEED</code> models","text":"<p>Configuration options for <code>SEED</code> models. <code>SEED</code> models do not support all the general properties supported by other models; they only support the properties listed in this table.</p> <p>Top-level options inside the MODEL DDL:</p> Option Description Type Required <code>name</code> The model name. Must include at least a qualifying schema (<code>&lt;schema&gt;.&lt;model&gt;</code>) and may include a catalog (<code>&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;</code>). Can be omitted if infer_names is set to true. <code>str</code> N <code>kind</code> The model kind. Must be <code>SEED</code>. <code>str</code> Y <code>columns</code> The column names and data types in the CSV file. Disables automatic inference of column names and types by the pandas CSV reader. NOTE: order of columns overrides the order specified in the CSV header row (if present). <code>array[str]</code> N <code>audits</code> Vulcan audits that should run against the model's output <code>array[str]</code> N <code>owner</code> The owner of a model; may be used for notification purposes <code>str</code> N <code>stamp</code> Arbitrary string used to indicate a model's version without changing the model name <code>str</code> N <code>tags</code> Arbitrary strings used to organize or classify a model <code>array[str]</code> N <code>description</code> Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine). <code>str</code> N <p>Options specified within the top-level <code>kind</code> property:</p> Option Description Type Required <code>path</code> Path to seed CSV file. <code>str</code> Y <code>batch_size</code> The maximum number of CSV rows ingested in each batch. All rows ingested in one batch if not specified. <code>int</code> N <code>csv_settings</code> Pandas CSV reader settings (overrides default values). Specified as key-value pairs (<code>key = value</code>). <code>dict</code> N <p> Options specified within the <code>kind</code> property's <code>csv_settings</code> property (overrides default Pandas CSV reader settings):</p> Option Description Type Required <code>delimiter</code> Character or regex pattern to treat as the delimiter. More information at the Pandas. <code>str</code> N <code>quotechar</code> Character used to denote the start and end of a quoted item. More information at the Pandas. <code>str</code> N <code>doublequote</code> When quotechar is specified, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. More information at the Pandas. <code>bool</code> N <code>escapechar</code> Character used to escape other characters. More information at the Pandas. <code>str</code> N <code>skipinitialspace</code> Skip spaces after delimiter. More information at the Pandas. <code>bool</code> N <code>lineterminator</code> Character used to denote a line break. More information at the Pandas. <code>str</code> N <code>encoding</code> Encoding to use for UTF when reading/writing (ex. 'utf-8'). More information at the Pandas. <code>str</code> N <code>na_values</code> An array of values that should be recognized as NA/NaN. In order to specify such an array per column, a mapping in the form of <code>(col1 = (v1, v2, ...), col2 = ...)</code> can be passed instead. These values can be integers, strings, booleans or NULL, and they are converted to their corresponding Python values. More information at the Pandas. <code>array[value]</code> | <code>array[array[key = value]]</code> N <code>keep_default_na</code> Whether or not to include the default NaN values when parsing the data. More information at the Pandas. <code>bool</code> N"},{"location":"references/overview/","title":"Overview","text":""},{"location":"references/overview/#overview","title":"Overview","text":"<p>This page provides a conceptual overview of what Vulcan does and how its components fit together.</p>"},{"location":"references/overview/#what-vulcan-is","title":"What Vulcan is","text":"<p>Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of execution engines.</p> <p>It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.</p> <p>You can use Vulcan with the CLI.</p>"},{"location":"references/overview/#how-vulcan-works","title":"How Vulcan works","text":""},{"location":"references/overview/#create-models","title":"Create models","text":"<p>You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.</p>"},{"location":"references/overview/#make-a-plan","title":"Make a plan","text":"<p>Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.</p> <p>Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change before you expend the time and resources to actually perform the computations.</p> <p>Vulcan automatically identifies all affected models and the computations a change entails by creating a \"Vulcan plan.\" When you execute the <code>plan</code> command, Vulcan generates the plan for the environment specified in the command (e.g., dev, test, prod).</p> <p>The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.</p> <p>Learn more about plans.</p>"},{"location":"references/overview/#apply-the-plan","title":"Apply the plan","text":"<p>After using <code>plan</code> to understand the impacts of a change in an environment, Vulcan offers to execute the computations by <code>apply</code>ing the plan. However, you must provide additional information that determines the scope of what computations are executed.</p> <p>The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.</p> <p>\"Backfilling\" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.</p> <p>Most business data is temporal \u2014 each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.</p> <p>The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.</p>"},{"location":"references/overview/#build-a-virtual-environment","title":"Build a Virtual Environment","text":"<p>Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.</p> <p>One challenge with using multiple data environments is that backfill and other computations must happen twice \u2014 once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.</p> <p>Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.</p> <p>Vulcan uses its knowledge of equivalent outputs to create a Virtual Environment. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but without computation or data movement.</p> <p>Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.</p>"},{"location":"references/overview/#test-your-code-and-data","title":"Test your code and data","text":"<p>Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.</p>"},{"location":"references/overview/#tests","title":"Tests","text":"<p>Vulcan \"tests\" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model code \u2014 you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.</p> <p>Vulcan automatically runs tests when you apply a <code>plan</code>, or you can run them on demand with the <code>test</code> command.</p>"},{"location":"references/overview/#audits","title":"Audits","text":"<p>In contrast to tests, Vulcan \"audits\" validate the results of model code applied to your actual data.</p> <p>You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure <code>your_field</code> has no <code>NULL</code> values would include <code>WHERE your_field IS NULL</code>. If any NULLs are detected, the query will return at least one row and the audit will fail.</p> <p>Audits are flexible \u2014 they can be tied to a specific model's contents, or you can use macros to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.</p> <p>You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.</p> <p>Vulcan automatically runs audits when you apply a <code>plan</code> to an environment, or you can run them on demand with the <code>audit</code> command.</p>"},{"location":"references/overview/#infrastructure-and-orchestration","title":"Infrastructure and orchestration","text":"<p>Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use \u2014 its only requirement is access to the target SQL/analytics engine.</p> <p>Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a <code>vulcan</code> schema in your data warehouse for its internal metadata.</p>"},{"location":"references/plans/","title":"Plans","text":""},{"location":"references/plans/#plans","title":"Plans","text":"<p>A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target environment. In order for any model changes to take effect in a target environment, a plan needs to be created and applied.</p>"},{"location":"references/plans/#plan-architecture-overview","title":"Plan Architecture Overview","text":"<p>The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:</p> <pre><code>flowchart TD\n    subgraph \"1\ufe0f\u20e3 Local Development\"\n        A[\ud83d\udc68\u200d\ud83d\udcbb Developer modifies model files&lt;br/&gt;\ud83d\udcdd Edit SQL/Python models]\n        B[\ud83d\udcc1 Local project state&lt;br/&gt;\u2728 Your changes ready]\n    end\n\n    subgraph \"2\ufe0f\u20e3 Plan Creation\"\n        C[\u26a1 vulcan plan&lt;br/&gt;\ud83d\ude80 Command execution]\n        D[\ud83d\udd0e Compare local vs environment&lt;br/&gt;\ud83d\udcca State comparison]\n        E{\ud83d\udd0d Changes detected?}\n        F[\ud83d\udccb Generate plan summary&lt;br/&gt;\u2728 Plan ready for review]\n        G[\ud83c\udff7\ufe0f Change categorization&lt;br/&gt;\ud83d\udd34 Breaking / \ud83d\udfe2 Non-breaking / \ud83d\udfe1 Forward-only]\n    end\n\n    subgraph \"3\ufe0f\u20e3 Plan Review\"\n        H[\ud83d\udc40 Review plan output&lt;br/&gt;\ud83d\udcca Check changes &amp; impacts]\n        I{\u2705 Apply plan?}\n        J[\u274c Cancel&lt;br/&gt;\ud83d\udeab No changes applied]\n    end\n\n    subgraph \"4\ufe0f\u20e3 Plan Application\"\n        K[\ud83d\udd37 Create model variants&lt;br/&gt;\ud83d\udd11 With unique fingerprints]\n        L[\ud83d\uddc4\ufe0f Create physical tables&lt;br/&gt;\ud83d\udcbe In data warehouse]\n        M[\ud83d\udd04 Backfill data&lt;br/&gt;\ud83d\udcc8 Process historical data]\n        N[\ud83d\udc41\ufe0f Update virtual layer&lt;br/&gt;\ud83d\udd0d Create/update views]\n        O[\ud83c\udf0d Update environment references&lt;br/&gt;\ud83d\udd17 Point to new variants]\n    end\n\n    subgraph \"5\ufe0f\u20e3 Result\"\n        P[\u2705 Environment updated&lt;br/&gt;\ud83c\udf89 Changes deployed]\n        Q[\ud83d\udd0d Models accessible via views&lt;br/&gt;\ud83d\udcca Ready for queries]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\u27a1\ufe0f\"| C\n    C --&gt;|\"\ud83d\udd0d\"| D\n    D --&gt;|\"\ud83d\udd0e\"| E\n    E --&gt;|\"\u2705 Yes\"| F\n    E --&gt;|\"\u274c No\"| P\n    F --&gt;|\"\ud83c\udff7\ufe0f\"| G\n    G --&gt;|\"\ud83d\udccb\"| H\n    H --&gt;|\"\ud83d\udc40\"| I\n    I --&gt;|\"\u2705 Yes\"| K\n    I --&gt;|\"\u274c No\"| J\n    K --&gt;|\"\ud83d\udd37\"| L\n    L --&gt;|\"\ud83d\udcbe\"| M\n    M --&gt;|\"\ud83d\udd04\"| N\n    N --&gt;|\"\ud83d\udc41\ufe0f\"| O\n    O --&gt;|\"\ud83d\udd17\"| P\n    P --&gt;|\"\u2728\"| Q\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre>"},{"location":"references/plans/#plan-components","title":"Plan Components","text":"<pre><code>graph LR\n    subgraph \"\ud83d\udccb Plan Contents\"\n        PC1[\u2795 Added Models&lt;br/&gt;\u2728 New models to create]\n        PC2[\u2796 Removed Models&lt;br/&gt;\ud83d\uddd1\ufe0f Models to delete]\n        PC3[\u270f\ufe0f Modified Models&lt;br/&gt;\ud83d\udcdd With diffs]\n        PC4[\ud83d\udd17 Indirectly Affected&lt;br/&gt;\ud83d\udcca Downstream models]\n        PC5[\ud83d\udcc5 Backfill Requirements&lt;br/&gt;\ud83d\udcc6 Date ranges]\n    end\n\n    subgraph \"\ud83c\udff7\ufe0f Change Types\"\n        CT1[\ud83d\udd34 Breaking Change&lt;br/&gt;\u26a0\ufe0f Requires downstream backfill&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        CT2[\ud83d\udfe2 Non-Breaking Change&lt;br/&gt;\u2705 Only direct model backfill&lt;br/&gt;\ud83c\udfaf Isolated impact]\n        CT3[\ud83d\udfe1 Forward-Only&lt;br/&gt;\u267b\ufe0f Reuses existing tables&lt;br/&gt;\ud83d\udcb0 Cost-effective]\n    end\n\n    PC3 --&gt;|\"\ud83d\udd34\"| CT1\n    PC3 --&gt;|\"\ud83d\udfe2\"| CT2\n    PC3 --&gt;|\"\ud83d\udfe1\"| CT3\n    PC4 --&gt;|\"\ud83d\udd34\"| CT1\n\n    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000</code></pre> <p>During plan creation:</p> <ul> <li>The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.</li> <li>Users may be prompted to categorize changes to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through configuration.</li> <li>Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.</li> </ul> <p>The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:</p> <ul> <li>A list of added models</li> <li>A list of removed models</li> <li>A list of directly modified models and a text diff of changes that have been made</li> <li>A list of indirectly modified models</li> <li>Missing data intervals for affected models</li> <li>A date range that will be affected by the plan application</li> </ul> <p>To create a new plan, run the following command: </p><pre><code>vulcan plan [environment name]\n</code></pre><p></p> <p>If no environment name is specified, the plan is generated for the <code>prod</code> environment.</p>"},{"location":"references/plans/#change-categories","title":"Change categories","text":"<p>Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.</p> <p>If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.</p>"},{"location":"references/plans/#change-propagation-flow","title":"Change Propagation Flow","text":"<p>The following diagram illustrates how changes propagate through the dependency graph:</p> <pre><code>graph TD\n    subgraph \"\ud83d\udcca Model Dependencies\"\n        A[\ud83d\udce5 raw.raw_orders&lt;br/&gt;\u2b06\ufe0f Upstream]\n        B[\ud83d\udcca sales.daily_sales&lt;br/&gt;\ud83d\udd04 Midstream]\n        C[\ud83d\udcc8 sales.weekly_sales&lt;br/&gt;\u2b07\ufe0f Downstream]\n        D[\ud83d\udcc9 analytics.revenue_report&lt;br/&gt;\u2b07\ufe0f Downstream]\n    end\n\n    subgraph \"\ud83d\udfe2 Scenario 1: Non-Breaking Change\"\n        NB1[\u2795 Add column to daily_sales&lt;br/&gt;\u2728 New column added]\n        NB2[\u2705 Only daily_sales backfilled&lt;br/&gt;\ud83d\udd04 Single model update]\n        NB3[\u23ed\ufe0f weekly_sales NOT affected&lt;br/&gt;\u2705 No cascade]\n        NB4[\u23ed\ufe0f revenue_report NOT affected&lt;br/&gt;\u2705 No cascade]\n    end\n\n    subgraph \"\ud83d\udd34 Scenario 2: Breaking Change\"\n        BC1[\ud83d\udd0d Add WHERE clause to daily_sales&lt;br/&gt;\u26a0\ufe0f Filter logic changed]\n        BC2[\ud83d\udd04 daily_sales backfilled&lt;br/&gt;\ud83d\udcca Data reprocessed]\n        BC3[\ud83d\udd04 weekly_sales backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n        BC4[\ud83d\udd04 revenue_report backfilled&lt;br/&gt;\ud83d\udd34 Indirect Breaking&lt;br/&gt;\ud83d\udca5 Cascading impact]\n    end\n\n    A --&gt;|\"\ud83d\udce4\"| B\n    B --&gt;|\"\ud83d\udce4\"| C\n    B --&gt;|\"\ud83d\udce4\"| D\n\n    NB1 --&gt;|\"\u270f\ufe0f\"| B\n    B --&gt;|\"\u2705\"| NB2\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| C\n    NB2 -.-&gt;|\"\u23ed\ufe0f No cascade\"| D\n\n    BC1 --&gt;|\"\u26a0\ufe0f\"| B\n    B --&gt;|\"\ud83d\udd04\"| BC2\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC3\n    BC2 --&gt;|\"\ud83d\udca5 Cascade\"| BC4\n    BC3 --&gt;|\"\ud83d\udd04\"| C\n    BC4 --&gt;|\"\ud83d\udd04\"| D\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000\n    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000</code></pre>"},{"location":"references/plans/#breaking-change","title":"Breaking change","text":"<p>If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.</p> <p>In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to backfilling).</p> <p>Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's <code>WHERE</code> clause is a breaking change because downstream models contain rows that would now be filtered out.</p>"},{"location":"references/plans/#non-breaking-change","title":"Non-breaking change","text":"<p>A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.</p> <p>This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.</p> <p>If any downstream models contain a <code>select *</code> from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.</p>"},{"location":"references/plans/#summary","title":"Summary","text":"Change Category Change Type Behaviour Breaking Direct or Indirect Backfill Non-breaking Direct Backfill Non-breaking Indirect No Backfill"},{"location":"references/plans/#forward-only-change","title":"Forward-only change","text":"<p>In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.</p> <p>A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the <code>prod</code> environment). This means that no backfill will take place.</p> <p>While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.</p> <p>In either case the data produced this way in the development environment can only be used for preview and will not be reused once the change is deployed to production. See Forward-only Plans for more details.</p> <p>This category is assigned by Vulcan automatically either when a user opts into using a forward-only plan or when a model is explicitly configured to be forward-only.</p>"},{"location":"references/plans/#plan-application","title":"Plan application","text":"<p>Once a plan has been created and reviewed, it is then applied to the target environment in order for its changes to take effect.</p> <p>Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a snapshot with a unique fingerprint is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for forward-only plans.</p> <p>When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.</p>"},{"location":"references/plans/#model-versioning-architecture","title":"Model Versioning Architecture","text":"<p>The following diagram shows how model variants, physical tables, and environments relate:</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcdd Model Definitions\"\n        M1[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 1&lt;br/&gt;\u2728 Original]\n        M2[\ud83d\udcca Model: sales.daily_sales&lt;br/&gt;\ud83d\udd22 Version 2 - Modified&lt;br/&gt;\u270f\ufe0f Updated]\n    end\n\n    subgraph \"\ud83d\udd37 Model Variants &amp; Snapshots\"\n        V1[\ud83d\udd37 Variant 1&lt;br/&gt;\ud83d\udd11 Fingerprint: abc123&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        V2[\ud83d\udd37 Variant 2&lt;br/&gt;\ud83d\udd11 Fingerprint: def456&lt;br/&gt;\ud83d\udcf8 Unique snapshot]\n        S1[\ud83d\udcf8 Snapshot 1&lt;br/&gt;\ud83d\udd10 Immutable state]\n        S2[\ud83d\udcf8 Snapshot 2&lt;br/&gt;\ud83d\udd10 Immutable state]\n    end\n\n    subgraph \"\ud83d\udcbe Physical Tables\"\n        T1[\ud83d\uddc4\ufe0f Physical Table 1&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__abc123&lt;br/&gt;\ud83d\udcbe Actual data storage]\n        T2[\ud83d\uddc4\ufe0f Physical Table 2&lt;br/&gt;\ud83d\udce6 db.vulcan__sales.daily_sales__def456&lt;br/&gt;\ud83d\udcbe Actual data storage]\n    end\n\n    subgraph \"\ud83d\udc41\ufe0f Virtual Layer Views\"\n        VL1[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 1&lt;br/&gt;\ud83d\udd17 Reference mapping]\n        VL2[\ud83d\udd0d View: sales.daily_sales&lt;br/&gt;\ud83d\udc41\ufe0f Points to Variant 2&lt;br/&gt;\ud83d\udd17 Reference mapping]\n    end\n\n    subgraph \"\ud83c\udf0d Environments\"\n        PROD[\ud83d\ude80 Production Environment&lt;br/&gt;\u2705 References Variant 1&lt;br/&gt;\ud83c\udf10 Live production data]\n        DEV[\ud83e\uddea Dev Environment&lt;br/&gt;\ud83d\udd2c References Variant 2&lt;br/&gt;\ud83e\uddea Testing environment]\n    end\n\n    M1 --&gt;|\"\u2728\"| V1\n    M2 --&gt;|\"\u270f\ufe0f\"| V2\n    V1 --&gt;|\"\ud83d\udcf8\"| S1\n    V2 --&gt;|\"\ud83d\udcf8\"| S2\n    S1 --&gt;|\"\ud83d\udcbe\"| T1\n    S2 --&gt;|\"\ud83d\udcbe\"| T2\n    T1 --&gt;|\"\ud83d\udc41\ufe0f\"| VL1\n    T2 --&gt;|\"\ud83d\udc41\ufe0f\"| VL2\n    PROD --&gt;|\"\ud83d\udd17\"| V1\n    DEV --&gt;|\"\ud83d\udd17\"| V2\n\n    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000\n    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000</code></pre> <p></p> <p>Each model variant gets its own physical table while environments only contain references to these tables.</p> <p>This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.</p> <p>Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to Virtual Update) as long as its physical table hasn't been garbage collected by the janitor process.</p> <p>Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.</p>"},{"location":"references/plans/#backfilling","title":"Backfilling","text":"<p>Despite all the benefits, the approach described above is not without trade-offs.</p> <p>When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.</p> <p>We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:</p> <ul> <li>When a VIEW model is created</li> <li>When a FULL model is built</li> <li>When an INCREMENTAL model is built for the first time</li> <li>When an INCREMENTAL model has recent data appended to it</li> <li>When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)</li> </ul> <p>Note for incremental models: despite the fact that backfilling can happen incrementally (see <code>batch_size</code> parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a forward-only plan instead.</p>"},{"location":"references/plans/#virtual-update","title":"Virtual Update","text":"<p>A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.</p> <p>With this approach, the process of promoting a change to production is reduced to reference swapping.</p> <p>If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.</p>"},{"location":"references/plans/#start-and-end-dates","title":"Start and end dates","text":"<p>The <code>plan</code> command provides two temporal options: <code>--start</code> and <code>--end</code>. These options are only applicable to plans for non-prod environments.</p> <p>For context, every model has a start date. The start can be specified in the model definition, in the project configuration's <code>model_defaults</code>, or by Vulcan's default value of yesterday.</p> <p>Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the <code>plan</code> command's <code>--start</code> and <code>--end</code> options are not supported for regular plans against prod. The options are supported for restatement plans against prod to allow re-processing a subset of existing data.</p> <p>Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the <code>--start</code> and <code>--end</code> options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.</p>"},{"location":"references/plans/#model-kind-limitations","title":"Model kind limitations","text":"<p>Some model kinds do not support backfilling a limited date range.</p> <p>For context, Vulcan strives to make models idempotent, meaning that if we ran them multiple times we would get the same correct result every time.</p> <p>However, some model kinds are inherently non-idempotent:</p> <ul> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>SCD_TYPE_2_BY_TIME</li> <li>SCD_TYPE_2_BY_COLUMN</li> <li>Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)</li> </ul> <p>Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:</p> <ul> <li>If the <code>--start</code> option date is the same as or before the model's start date, the model is fully refreshed for all of time</li> <li>If the <code>--start</code> option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production</li> </ul>"},{"location":"references/plans/#example","title":"Example","text":"<p>Consider a Vulcan project with a default start date of 2024-09-20.</p> <p>It contains the following <code>INCREMENTAL_BY_UNIQUE_KEY</code> model that specifies an explicit start date of 2024-09-23:</p> <pre><code>MODEL (\n  name vulcan_example.start_end_model,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key item_id\n  ),\n  start '2024-09-23'\n);\n\nSELECT\n  item_id,\n  num_orders\nFROM\n  vulcan_example.full_model\n</code></pre> <p>When we run the project's first plan, we see that Vulcan correctly detected a different start date for our <code>start_end_model</code> than the other models (which have the project default start of 2024-09-20):</p> <pre><code>\u276f vulcan plan\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\n`prod` environment will be initialized\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u251c\u2500\u2500 vulcan_example.full_model\n    \u251c\u2500\u2500 vulcan_example.incremental_model\n    \u251c\u2500\u2500 vulcan_example.seed_model\n    \u2514\u2500\u2500 vulcan_example.start_end_model\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example.full_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.incremental_model: 2024-09-20 - 2024-09-26\n\u251c\u2500\u2500 vulcan_example.seed_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example.start_end_model: 2024-09-23 - 2024-09-26\nApply - Backfill Tables [y/n]:\n</code></pre> <p>After executing that plan, we add columns to both the <code>incremental_model</code> and <code>start_end_model</code> queries.</p> <p>We then execute <code>vulcan plan dev</code> to create the new <code>dev</code> environment:</p> <pre><code>\u276f vulcan plan dev\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.incremental_model (Non-breaking)\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model (Indirect Non-breaking)\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26\nEnter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:\n</code></pre> <p>Note two things about the output:</p> <ol> <li>As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for <code>incremental_model</code> and 2024-09-23 for <code>start_end_model</code></li> <li>Vulcan prompted us for a backfill start date because we didn't pass the <code>--start</code> option to the <code>vulcan plan dev</code> command</li> </ol> <p>Let's cancel that plan and start a new one, passing a start date of 2024-09-24.</p> <p>The <code>start_end_model</code> is of kind <code>INCREMENTAL_BY_UNIQUE_KEY</code>, which is non-idempotent and cannot be backfilled for a limited time range.</p> <p>Because the command's <code>--start</code> of 2024-09-24 is after <code>start_end_model</code>'s start date 2024-09-23, <code>start_end_model</code> is marked as preview:</p> <pre><code>\u276f vulcan plan dev --start 2024-09-24\n======================================================================\nSuccessfully Ran 1 tests against duckdb\n----------------------------------------------------------------------\nNew environment `dev` will be created from `prod`\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 vulcan_example__dev.start_end_model\n\u2502   \u2514\u2500\u2500 vulcan_example__dev.incremental_model\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 vulcan_example__dev.full_model\n\n[...model diff omitted...]\n\nDirectly Modified: vulcan_example__dev.start_end_model (Non-breaking)\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26\n\u2514\u2500\u2500 vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)\nEnter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':\n</code></pre>"},{"location":"references/plans/#minimum-intervals","title":"Minimum intervals","text":"<p>When you run a plan with a fixed <code>--start</code> or <code>--end</code> date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.</p> <p>For example, if you have a model like so:</p> <pre><code>MODEL(\n    name vulcan_example.monthly_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n        time_column month\n    ),\n    cron '@monthly'\n);\n\nSELECT SUM(a) AS sum_a, MONTH(day) AS month\nFROM vulcan_example.upstream_model\nWHERE day BETWEEN @start_ds AND @end_ds\n</code></pre> <p>make a change to it and run the following:</p> <pre><code>$ vulcan plan dev --start '1 day ago' \n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\nSKIP: No model batches to execute\n</code></pre> <p>No data will be backfilled because <code>1 day ago</code> does not contain a complete month. However, you can use the <code>--min-intervals</code> option to override this behaviour like so:</p> <pre><code>$ vulcan plan dev --start '1 day ago' --min-intervals 1\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 vulcan_example__dev.monthly_model\nApply - Virtual Update [y/n]: y\n\n[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   \nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:00                                                             \n\n\u2714 Model batches executed\n</code></pre> <p>This will ensure that regardless of the plan <code>--start</code> date, all added or modified models will have at least <code>--min-intervals</code> intervals considered for backfill.</p> <p>Info</p> <p>If you are running plans manually you can just adjust the <code>--start</code> date to be wide enough to cover the models in question.</p> <p>The <code>--min-intervals</code> option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) \"2 weeks worth of data\" in the target environment.</p>"},{"location":"references/plans/#data-preview-for-forward-only-changes","title":"Data preview for forward-only changes","text":"<p>As mentioned earlier, the data output produced by forward-only changes in a development environment can only be used for preview and will not be reused in production.</p> <p>The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.</p> <p>Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with <code>(preview)</code> in the plan summary: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)\n\u251c\u2500\u2500 sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28\n\u251c\u2500\u2500 sushi__dev.top_waiters: 2023-12-22 - 2023-12-28\n\u2514\u2500\u2500 sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)\n</code></pre><p></p>"},{"location":"references/plans/#forward-only-plans","title":"Forward-only plans","text":"<p>Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.</p> <p>When a forward-only plan is applied to the <code>prod</code> environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.</p> <p>The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and restatements.</p> <p>Note that once a forward-only change is applied to <code>prod</code>, all development environments that referred to the previous versions of the updated models will be impacted.</p> <p>A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, <code>vulcan plan [environment name]</code> evaluates code in non-<code>prod</code> environments while targeting shallow (a.k.a. \"zero-copy\") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.</p> <p>This means that only a limited preview of changes is available in the development environment before the change is promoted to <code>prod</code>. The date range of the preview is provided as part of plan creation command.</p> <p>Engines for which table cloning is supported include:</p> <ul> <li><code>BigQuery</code></li> <li><code>Databricks</code></li> <li><code>Snowflake</code></li> </ul> <p>Note that all changes made as part of a forward-only plan automatically get a forward-only category assigned to them. These types of changes can't be mixed together with breaking and non-breaking changes within the same plan.</p> <p>To create a forward-only plan, add the <code>--forward-only</code> option to the <code>plan</code> command: </p><pre><code>vulcan plan [environment name] --forward-only\n</code></pre><p></p> <p>Note</p> <p>The <code>--forward-only</code> flag is not required when applying changes to models that have been explicitly configured as forward-only.</p> <p>Use it only if you need to provide a time range for the preview window or the effective date.</p>"},{"location":"references/plans/#destructive-changes","title":"Destructive changes","text":"<p>Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to forward-only models - learn more here.</p> <p>Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.</p> <p>Vulcan determines what to do for each model based on this setting hierarchy: </p> <ul> <li>For destructive changes: the model's <code>on_destructive_change</code> value (if present), the <code>on_destructive_change</code> model defaults value (if present), and the Vulcan global default of <code>error</code></li> <li>For additive changes: the model's <code>on_additive_change</code> value (if present), the <code>on_additive_change</code> model defaults value (if present), and the Vulcan global default of <code>allow</code></li> </ul> <p>If you want to temporarily allow destructive changes to models that don't allow them, use the <code>plan</code> command's <code>--allow-destructive-model</code> selector to specify which models.  Similarly, if you want to temporarily allow additive changes to models configured with <code>on_additive_change=error</code>, use the <code>--allow-additive-model</code> selector. </p> <p>For example, to allow destructive changes to all models in the <code>analytics</code> schema: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"analytics.*\"\n</code></pre><p></p> <p>Or to allow destructive changes to multiple specific models: </p><pre><code>vulcan plan --forward-only --allow-destructive-model \"sales.revenue_model\" --allow-destructive-model \"marketing.campaign_model\"\n</code></pre><p></p> <p>Learn more about model selectors here.</p>"},{"location":"references/plans/#effective-date","title":"Effective date","text":"<p>Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:</p> <pre><code>vulcan plan --forward-only --effective-from 2023-01-01\n</code></pre> <p>This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.</p>"},{"location":"references/plans/#restatement-plans","title":"Restatement plans","text":"<p>Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.</p> <p>For example, these scenarios all require re-evaluating model data that already exists:</p> <ul> <li>Correcting an upstream data issue by reprocessing some of a model's existing data</li> <li>Retroactively applying a forward-only plan change to some historical data</li> <li>Fully refreshing a model</li> </ul> <p>In Vulcan, reprocessing existing data is called a \"restatement.\"</p> <p>Restate one or more models' data with the <code>plan</code> command's <code>--restate-model</code> selector. The selector lets you specify which models to restate by name, wildcard, or tag (syntax below).</p> <p>No changes allowed</p> <p>Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.</p> <p>You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running <code>vulcan plan</code> without the <code>--restate-model</code> option.</p> <p>Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.</p> <p>You may restate external models. An external model is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.</p> <p>The plan's <code>--start</code> and <code>--end</code> date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more below.</p> <p>Just catching up</p> <p>Restatement plans \"catch models up\" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.</p> <p>If you pass an <code>--end</code> date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.</p> <p>To prevent models from ever being restated, set the disable_restatement attribute to <code>true</code>.</p> <p> These examples demonstrate how to select which models to restate based on model names or model tags.</p> Names OnlyUpstreamWildcardsUpstream + WildcardsSpecific Date Range <pre><code>vulcan plan --restate-model \"db.model_a\" --restate-model \"tag:expensive\"\n</code></pre> <pre><code># All selected models (including upstream models) will also include their downstream models\nvulcan plan --restate-model \"+db.model_a\" --restate-model \"+tag:expensive\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db*\" --restate-model \"tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"+db*\" --restate-model \"+tag:exp*\"\n</code></pre> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre>"},{"location":"references/plans/#restating-production-vs-development","title":"Restating production vs development","text":"<p>Restatement plans behave differently depending on if you're targeting the <code>prod</code> environment or a development environment.</p> <p>If you target a development environment by including an environment name like <code>dev</code>:</p> <pre><code>vulcan plan dev --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the requested intervals for the specified model in the <code>dev</code> environment. In other environments, the model will be unaffected.</p> <p>However, if you target the <code>prod</code> environment by omitting an environment name:</p> <pre><code>vulcan plan --restate-model \"db.model_a\" --start \"2024-01-01\" --end \"2024-01-10\"\n</code></pre> <p>the restatement plan will restate the intervals in the <code>prod</code> table and clear the model's time intervals from state in every other environment.</p> <p>The next time you do a run in <code>dev</code>, the intervals already reprocessed in <code>prod</code> are reprocessed in <code>dev</code> as well. This is to prevent old data from getting promoted to <code>prod</code> in the future.</p> <p>This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:</p> <ul> <li>Table <code>A</code> exists in <code>prod</code></li> <li>A virtual environment <code>dev</code> is created with new tables <code>B</code> and <code>C</code> downstream of <code>A</code><ul> <li>the DAG in <code>prod</code> looks like <code>A</code></li> <li>the DAG in <code>dev</code> looks like <code>A &lt;- B &lt;- C</code></li> </ul> </li> <li>A restatement plan is executed against table <code>A</code> in <code>prod</code></li> <li>Vulcan will clear the affected intervals for <code>B</code> and <code>C</code> in <code>dev</code> even though those tables do not exist in <code>prod</code></li> </ul> <p>Bringing development environments up to date</p> <p>A restatement plan against <code>prod</code> clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.</p> <p>Execute <code>vulcan run &lt;environment name&gt;</code> to trigger reprocessing in the development environment.</p> <p>This is necessary because a <code>prod</code> restatement plan only does work in the <code>prod</code> environment for speed and efficiency.</p>"},{"location":"references/state/","title":"State","text":""},{"location":"references/state/#state","title":"State","text":"<p>Vulcan stores information about your project in a state database that is usually separate from your main warehouse.</p> <p>The Vulcan state database contains:</p> <ul> <li>Information about every Model Version in your project (query, loaded intervals, dependencies)</li> <li>A list of every Virtual Data Environment in the project</li> <li>Which model versions are promoted into each Virtual Data Environment</li> <li>Information about any auto restatements present in your project</li> <li>Other metadata about your project such as current Vulcan / SQLGlot version</li> </ul> <p>The state database is how Vulcan \"remembers\" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for incremental models so you dont need to add branching logic into the model query to handle this.</p> <p>State database performance</p> <p>The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.</p> <p>For the best experience, we recommend databases designed for OLTP workloads such as PostgreSQL.</p> <p>Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and will lead to poor performance and consistency.</p> <p>For more information on engines suitable for the Vulcan state database, see the configuration guide.</p>"},{"location":"references/state/#exporting-importing-state","title":"Exporting / Importing State","text":"<p>Vulcan supports exporting the state database to a <code>.json</code> file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.</p>"},{"location":"references/state/#exporting-state","title":"Exporting state","text":"<p>Vulcan can export the state database to a file like so:</p> <pre><code>$ vulcan state export -o state.json\nExporting state to 'state.json' from the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\nContinue? [y/n]: y\n\n    Exporting versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Exporting snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nExporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState exported successfully to 'state.json'\n</code></pre> <p>This will produce a file <code>state.json</code> in the current directory containing the Vulcan state.</p> <p>The state file is a simple <code>json</code> file that looks like:</p> <pre><code>{\n    /* State export metadata */\n    \"metadata\": {\n        \"timestamp\": \"2025-03-16 23:09:00+00:00\", /* UTC timestamp of when the file was produced */\n        \"file_version\": 1, /* state export file format version */\n        \"importable\": true /* whether or not this file can be imported with `vulcan state import` */\n    },\n    /* Library versions used to produce this state export file */\n    \"versions\": {\n        \"schema_version\": 76 /* vulcan state database schema version */,\n        \"sqlglot_version\": \"26.10.1\" /* version of SQLGlot used to produce the state file */,\n        \"vulcan_version\": \"0.165.1\" /* version of Vulcan used to produce the state file */,\n    },\n    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */\n    \"snapshots\": [\n        { \"name\": \"...\" }\n    ],\n    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */\n    \"environments\": {\n        \"prod\": {\n            /* information about the environment itself */\n            \"environment\": {\n                \"...\"\n            },\n            /* information about any before_all / after_all statements for this environment */\n            \"statements\": [\n                \"...\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"references/state/#specific-environments","title":"Specific environments","text":"<p>You can export a specific environment like so:</p> <pre><code>$ vulcan state export --environment my_dev -o my_dev_state.json\n</code></pre> <p>Note that every snapshot that is part of the environment will be exported, not just the differences from <code>prod</code>. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.</p>"},{"location":"references/state/#local-state","title":"Local state","text":"<p>You can export local state like so:</p> <pre><code>$ vulcan state export --local -o local_state.json\n</code></pre> <p>This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.</p> <p>Therefore, a local state export will only have <code>snapshots</code> populated. <code>environments</code> will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as not importable so it cannot be used with a subsequent <code>vulcan state import</code> command.</p>"},{"location":"references/state/#importing-state","title":"Importing state","text":"<p>Back up your state database first!</p> <p>Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.</p> <p>Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means a import error has the potential to leave the state database in an inconsistent state.</p> <p>Vulcan can import a state file into the state database like so:</p> <pre><code>$ vulcan state import -i state.json --replace\nLoading state from 'state.json' into the following connection:\n\nGateway: dev\nState Connection:\n\u251c\u2500\u2500 Type: postgres\n\u251c\u2500\u2500 Catalog: sushi_dev\n\u2514\u2500\u2500 Dialect: postgres\n\n[WARNING] This destructive operation will delete all existing state against the 'dev' gateway\nand replace it with what\\'s in the 'state.json' file.\n\nAre you sure? [y/n]: y\n\nState File Information:\n\u251c\u2500\u2500 Creation Timestamp: 2025-03-31 02:15:00+00:00\n\u251c\u2500\u2500 File Version: 1\n\u251c\u2500\u2500 Vulcan version: 0.170.1.dev0\n\u251c\u2500\u2500 Vulcan migration version: 76\n\u2514\u2500\u2500 SQLGlot version: 26.12.0\n\n    Importing versions \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3   \u2022 0:00:00\n   Importing snapshots \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 17/17 \u2022 0:00:00\nImporting environments \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1   \u2022 0:00:00\n\nState imported successfully from 'state.json'\n</code></pre> <p>Note that the state database structure needs to be present and up to date, so run <code>vulcan migrate</code> before running <code>vulcan state import</code> if you get a version mismatch error.</p> <p>If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the <code>--replace</code> parameter:</p> <pre><code>$ vulcan state import -i state.json\n...\n\n[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.\nMatching snapshots or environments will be replaced.\nNon-matching snapshots or environments will be ignored.\n\nAre you sure? [y/n]: y\n\n...\nState imported successfully from 'state.json'\n</code></pre>"},{"location":"references/state/#specific-gateways","title":"Specific gateways","text":"<p>If your project has multiple gateways with different state connections per gateway, you can target the state_connection of a specific gateway like so:</p> <pre><code># state export\n$ vulcan --gateway &lt;gateway&gt; state export -o state.json\n\n# state import\n$ vulcan --gateway &lt;gateway&gt; state import -i state.json\n</code></pre>"},{"location":"references/state/#version-compatibility","title":"Version Compatibility","text":"<p>When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.</p> <p>If you attempt to import state with an incompatible version, you will get the following error:</p> <pre><code>$ vulcan state import -i state.json\n...SNIP...\n\nState import failed!\nError: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.\nPlease upgrade/downgrade your Vulcan version to match the state file before performing the import.\n</code></pre>"},{"location":"references/state/#upgrading-a-state-file","title":"Upgrading a state file","text":"<p>You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:</p> <ul> <li>Loading it into a local database using the older Vulcan version</li> <li>Installing the newer Vulcan version</li> <li>Running <code>vulcan migrate</code> to upgrade the state within the local database</li> <li>Running <code>vulcan state export</code> to export it back out again. The new export is now compatible with the newer version of Vulcan.</li> </ul> <p>Below is an example of how to upgrade a state file created with Vulcan <code>0.164.1</code> to be compatible with Vulcan <code>0.165.1</code>.</p> <p>First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:</p> <pre><code>$ python -m venv migration-env\n\n$ . ./migration-env/bin/activate\n\n(migration-env)$\n</code></pre> <p>Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg <code>the state file was created with '0.164.1'</code> means you need to install Vulcan <code>0.164.1</code>:</p> <pre><code>(migration-env)$ pip install \"vulcan==0.164.1\"\n</code></pre> <p>Add a gateway to your <code>config.yaml</code> like so:</p> <pre><code>gateways:\n  migration:\n    connection:\n      type: duckdb\n      database: ./state-migration.duckdb\n</code></pre> <p>The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the <code>model_defaults</code> from your project in order to migrate state correctly which is why we have not used an isolated directory.</p> <p>Warning</p> <p>From here on, be sure to specify <code>--gateway migration</code> to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway</p> <p>You can now import your state export using the same version of Vulcan it was created with:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n\n(migration-env)$ vulcan --gateway migration state import -i state.json\n...\nState imported successfully from 'state.json'\n</code></pre> <p>Now we have the state imported, we can upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, eg <code>You are running '0.165.1'</code></p> <p>To upgrade Vulcan, simply install the new version:</p> <pre><code>(migration-env)$ pip install --upgrade \"vulcan==0.165.1\"\n</code></pre> <p>Migrate the state to the new version:</p> <pre><code>(migration-env)$ vulcan --gateway migration migrate\n</code></pre> <p>And finally, create a new state file which is now compatible with the new Vulcan version:</p> <pre><code> (migration-env)$ vulcan --gateway migration state export -o state-migrated.json\n</code></pre> <p>The <code>state-migrated.json</code> file is now compatible with the newer version of Vulcan. You can then transfer it to the place you originally needed it and import it in:</p> <pre><code>$ vulcan state import -i state-migrated.json\n...\nState imported successfully from 'state-migrated.json'\n</code></pre>"},{"location":"references/integrations/engines/athena/","title":"Athena","text":""},{"location":"references/integrations/engines/athena/#athena","title":"Athena","text":""},{"location":"references/integrations/engines/athena/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[athena]\"\n</code></pre>"},{"location":"references/integrations/engines/athena/#connection-options","title":"Connection options","text":""},{"location":"references/integrations/engines/athena/#pyathena-connection-options","title":"PyAthena connection options","text":"<p>Vulcan leverages the PyAthena DBAPI driver to connect to Athena. Therefore, the connection options relate to the PyAthena connection options. Note that PyAthena uses boto3 under the hood so you can also use boto3 environment variables for configuration.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>athena</code> string Y <code>aws_access_key_id</code> The access key for your AWS user string N <code>aws_secret_access_key</code> The secret key for your AWS user string N <code>role_arn</code> The ARN of a role to assume once authenticated string N <code>role_session_name</code> The session name to use when assuming <code>role_arn</code> string N <code>region_name</code> The AWS region to use string N <code>work_group</code> The Athena workgroup to send queries to string N <code>s3_staging_dir</code> The S3 location for Athena to write query results. Only required if not using <code>work_group</code> OR the configured <code>work_group</code> doesnt have a results location set string N <code>schema_name</code> The default schema to place objects in if a schema isnt specified. Defaults to <code>default</code> string N <code>catalog_name</code> The default catalog to place schemas in. Defaults to <code>AwsDataCatalog</code> string N"},{"location":"references/integrations/engines/athena/#vulcan-connection-options","title":"Vulcan connection options","text":"<p>These options are specific to Vulcan itself and are not passed to PyAthena</p> Option Description Type Required <code>s3_warehouse_location</code> Set the base path in S3 where Vulcan will instruct Athena to place table data. Only required if you arent specifying the location in the model itself. See S3 Locations below. string N"},{"location":"references/integrations/engines/athena/#model-properties","title":"Model properties","text":"<p>The Athena adapter utilises the following model top-level properties:</p> Name Description Type Required <code>table_format</code> Sets the table_type Athena uses when creating the table. Valid values are <code>hive</code> or <code>iceberg</code>. string N <code>storage_format</code> Configures the file format to be used by the <code>table_format</code>. For Hive tables, this sets the STORED AS option. For Iceberg tables, this sets format property. string N <p>The Athena adapter recognises the following model physical_properties:</p> Name Description Type Default <code>s3_base_location</code> <code>s3://</code> base URI of where the snapshot tables for this model should be written. Overrides <code>s3_warehouse_location</code> if one is configured. string"},{"location":"references/integrations/engines/athena/#s3-locations","title":"S3 Locations","text":"<p>When creating tables, Athena needs to know where in S3 the table data is located. You cannot issue a <code>CREATE TABLE</code> statement without specifying a <code>LOCATION</code> for the table data.</p> <p>In addition, unlike other engines such as Trino, Athena will not infer a table location if you set a schema location via <code>CREATE SCHEMA &lt;schema&gt; LOCATION 's3://schema/location'</code>.</p> <p>Therefore, in order for Vulcan to issue correct <code>CREATE TABLE</code> statements to Athena, you need to configure where the tables should be stored. There are two options for this:</p> <ul> <li>Project-wide: set <code>s3_warehouse_location</code> in the connection config. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_warehouse_location&gt;/&lt;schema_name&gt;/&lt;snapshot_table_name&gt;</code> when it creates a snapshot of your model.</li> <li>Per-model: set <code>s3_base_location</code> in the model <code>physical_properties</code>. Vulcan will set the table <code>LOCATION</code> to be <code>&lt;s3_base_location&gt;/&lt;snapshot_table_name&gt;</code> every time it creates a snapshot of your model. This takes precedence over any <code>s3_warehouse_location</code> set in the connection config.</li> </ul>"},{"location":"references/integrations/engines/athena/#limitations","title":"Limitations","text":"<p>Athena was initially designed to read data stored in S3 and to do so without changing that data. This means that it does not have good support for mutating tables. In particular, it will not delete data from Hive tables.</p> <p>Consequently, forward only changes that mutate the schemas of existing tables have a high chance of failure because Athena supports very limited schema modifications on Hive tables.</p> <p>However, Athena does support Apache Iceberg tables which allow a full range of operations. These can be used for more complex model types such as <code>INCREMENTAL_BY_UNIQUE_KEY</code> and <code>SCD_TYPE_2</code>.</p> <p>To use an Iceberg table for a model, set <code>table_format iceberg</code> in the model properties.</p> <p>In general, Iceberg tables offer the most flexibility and you'll run into the least Vulcan limitations when using them. However, we create Hive tables by default because Athena creates Hive tables by default, so Iceberg tables are opt-in rather than opt-out.</p>"},{"location":"references/integrations/engines/azuresql/","title":"Azure SQL","text":""},{"location":"references/integrations/engines/azuresql/#azure-sql","title":"Azure SQL","text":"<p>Azure SQL is \"a family of managed, secure, and intelligent products that use the SQL Server database engine in the Azure cloud.\"</p>"},{"location":"references/integrations/engines/azuresql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>azuresql</code></p>"},{"location":"references/integrations/engines/azuresql/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/azuresql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[azuresql]\"\n</code></pre>"},{"location":"references/integrations/engines/azuresql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[azuresql-odbc]\"\n</code></pre>"},{"location":"references/integrations/engines/azuresql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>azuresql</code> string Y <code>host</code> The hostname of the Azure SQL server string Y <code>user</code> The username / client ID to use for authentication with the Azure SQL server string N <code>password</code> The password / client secret to use for authentication with the Azure SQL server string N <code>port</code> The port number of the Azure SQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"references/integrations/engines/bigquery/","title":"BigQuery","text":""},{"location":"references/integrations/engines/bigquery/#bigquery","title":"BigQuery","text":""},{"location":"references/integrations/engines/bigquery/#introduction","title":"Introduction","text":"<p>This guide provides step-by-step instructions on how to connect Vulcan to the BigQuery SQL engine.</p> <p>It will walk you through the steps of installing Vulcan and BigQuery connection libraries locally, configuring the connection in Vulcan, and running the quickstart project.</p>"},{"location":"references/integrations/engines/bigquery/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes the following about the BigQuery project being used with Vulcan:</p> <ul> <li>The project already exists</li> <li>Project CLI/API access is enabled</li> <li>Project billing is configured (i.e. it's not a sandbox project)</li> <li>Vulcan can authenticate using an account with permissions to execute commands against the project</li> </ul>"},{"location":"references/integrations/engines/bigquery/#installation","title":"Installation","text":"<p>Follow the quickstart guide to set up Vulcan, then install the necessary BigQuery libraries.</p> <p>Instead of installing just Vulcan core, we will also include the BigQuery engine libraries:</p> <pre><code>&gt; pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#install-google-cloud-sdk","title":"Install Google Cloud SDK","text":"<p>Vulcan connects to BigQuery via the Python <code>google-cloud-bigquery</code> library, which uses the Google Cloud SDK <code>gcloud</code> tool for authenticating with BigQuery.</p> <p>Follow these steps to install and configure the Google Cloud SDK on your computer:</p> <ul> <li>Download the appropriate installer for your system from the Google Cloud installation guide</li> <li> <p>Unpack the downloaded file with the <code>tar</code> command:</p> <pre><code>&gt; tar -xzvf google-cloud-cli-{SYSTEM_SPECIFIC_INFO}.tar.gz\n</code></pre> </li> <li> <p>Run the installation script:</p> <pre><code>&gt; ./google-cloud-sdk/install.sh\n</code></pre> </li> <li> <p>Reload your shell profile (e.g., for zsh):</p> <pre><code>&gt; source $HOME/.zshrc\n</code></pre> </li> <li> <p>Run <code>gcloud init</code> to setup authentication</p> </li> </ul>"},{"location":"references/integrations/engines/bigquery/#configuration","title":"Configuration","text":""},{"location":"references/integrations/engines/bigquery/#configure-vulcan-for-bigquery","title":"Configure Vulcan for BigQuery","text":"<p>Add the following gateway specification to your Vulcan project's <code>config.yaml</code> file:</p> <pre><code>bigquery:\n  connection:\n    type: bigquery\n    project: &lt;your_project_id&gt;\n\ndefault_gateway: bigquery\n</code></pre> <p>This creates a gateway named <code>bigquery</code> and makes it your project's default gateway.</p> <p>It uses the <code>oauth</code> authentication method, which does not specify a username or other information directly in the connection configuration. Other authentication methods are described below.</p> <p>In BigQuery, navigate to the dashboard and select the BigQuery project your Vulcan project will use. From the Google Cloud dashboard, use the arrow to open the pop-up menu:</p> <p></p> <p>Now we can identify the project ID needed in the <code>config.yaml</code> gateway specification above. Select the project that you want to work with, the project ID that you need to add to your yaml file is the ID label from the pop-up menu.</p> <p></p> <p>For this guide, the Docs-Demo is the one we will use, thus the project ID for this example is <code>healthy-life-440919-s0</code>.</p>"},{"location":"references/integrations/engines/bigquery/#usage","title":"Usage","text":""},{"location":"references/integrations/engines/bigquery/#test-the-connection","title":"Test the connection","text":"<p>Run the following command to verify that Vulcan can connect to BigQuery:</p> <pre><code>&gt; vulcan info\n</code></pre> <p>The output will look something like this:</p> <p></p> <ul> <li> <p>Set quota project (optional)</p> <p>You may see warnings like this when you run <code>vulcan info</code>:</p> <p></p> <p>You can avoid these warnings about quota projects by running:</p> <pre><code>&gt; gcloud auth application-default set-quota-project &lt;your_project_id&gt;\n&gt; gcloud config set project &lt;your_project_id&gt;\n</code></pre> </li> </ul>"},{"location":"references/integrations/engines/bigquery/#create-and-run-a-plan","title":"Create and run a plan","text":"<p>We've verified our connection, so we're ready to create and execute a plan in BigQuery:</p> <pre><code>&gt; vulcan plan\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#view-results-in-bigquery-console","title":"View results in BigQuery Console","text":"<p>Let's confirm that our project models are as expected.</p> <p>First, navigate to the BigQuery Studio Console:</p> <p></p> <p>Then use the left sidebar to find your project and the newly created models:</p> <p></p> <p>We have confirmed that our Vulcan project is running properly in BigQuery!</p>"},{"location":"references/integrations/engines/bigquery/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>bigquery</code></p>"},{"location":"references/integrations/engines/bigquery/#installation_1","title":"Installation","text":"<pre><code>pip install \"vulcan[bigquery]\"\n</code></pre>"},{"location":"references/integrations/engines/bigquery/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>bigquery</code> string Y <code>method</code> Connection methods - see allowed values below. Default: <code>oauth</code>. string N <code>project</code> The ID of the GCP project string N <code>location</code> The location of for the datasets (can be regional or multi-regional) string N <code>execution_project</code> The name of the GCP project to bill for the execution of the models. If not set, the project associated with the model will be used. string N <code>quota_project</code> The name of the GCP project used for the quota. If not set, the <code>quota_project_id</code> set within the credentials of the account is used to authenticate to BigQuery. string N <code>keyfile</code> Path to the keyfile to be used with service-account method string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>token</code> OAuth 2.0 access token string N <code>refresh_token</code> OAuth 2.0 refresh token string N <code>client_id</code> OAuth 2.0 client ID string N <code>client_secret</code> OAuth 2.0 client secret string N <code>token_uri</code> OAuth 2.0 authorization server's token endpoint URI string N <code>scopes</code> The scopes used to obtain authorization list N <code>impersonated_service_account</code> If set, Vulcan will attempt to impersonate this service account string N <code>job_creation_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to be created. int N <code>job_execution_timeout_seconds</code> The maximum amount of time, in seconds, to wait for the underlying job to complete. int N <code>job_retries</code> The number of times to retry the underlying job if it fails. (Default: <code>1</code>) int N <code>priority</code> The priority of the underlying job. (Default: <code>INTERACTIVE</code>) string N <code>maximum_bytes_billed</code> The maximum number of bytes to be billed for the underlying job. int N"},{"location":"references/integrations/engines/bigquery/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>oauth (default)<ul> <li>Related Credential Configuration:<ul> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>oauth-secrets<ul> <li>Related Credential Configuration:<ul> <li><code>token</code> (Optional): Can be None if refresh information is provided.</li> <li><code>refresh_token</code> (Optional): If specified, credentials can be refreshed.</li> <li><code>client_id</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>client_secret</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>token_uri</code> (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.</li> <li><code>scopes</code> (Optional): OAuth 2.0 credentials can not request additional scopes after authorization. The scopes must be derivable from the refresh token if refresh information is provided (e.g. The refresh token scopes are a superset of this or contain a wild card scope like 'https://www.googleapis.com/auth/any-api')</li> </ul> </li> </ul> </li> <li>service-account<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> <li>service-account-json<ul> <li>Related Credential Configuration:<ul> <li><code>keyfile_json</code> (Required)</li> <li><code>scopes</code> (Optional)</li> </ul> </li> </ul> </li> </ul> <p>If the <code>impersonated_service_account</code> argument is set, Vulcan will:</p> <ol> <li>Authenticate user account credentials with one of the methods above</li> <li>Attempt to impersonate the service account with those credentials</li> </ol> <p>The user account must have sufficient permissions to impersonate the service account.</p>"},{"location":"references/integrations/engines/bigquery/#permissions-required","title":"Permissions Required","text":"<p>With any of the above connection methods, ensure these BigQuery permissions are enabled to allow Vulcan to work correctly.</p> <ul> <li><code>BigQuery Data Owner</code></li> <li><code>BigQuery User</code></li> </ul>"},{"location":"references/integrations/engines/clickhouse/","title":"ClickHouse","text":""},{"location":"references/integrations/engines/clickhouse/#clickhouse","title":"ClickHouse","text":"<p>This page describes Vulcan support for the ClickHouse engine, including configuration options specific to ClickHouse.</p> <p>Note</p> <p>ClickHouse may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/clickhouse/#background","title":"Background","text":"<p>ClickHouse is a distributed, column-oriented SQL engine designed to rapidly execute analytical workloads.</p> <p>It provides users fine-grained control of its behavior, but that control comes at the cost of complex configuration.</p> <p>This section provides background information about ClickHouse, providing context for how to use Vulcan with the ClickHouse engine.</p>"},{"location":"references/integrations/engines/clickhouse/#object-naming","title":"Object naming","text":"<p>Most SQL engines use a three-level hierarchical naming scheme: tables/views are nested within schemas, and schemas are nested within catalogs. For example, the full name of a table might be <code>my_catalog.my_schema.my_table</code>.</p> <p>ClickHouse instead uses a two-level hierarchical naming scheme that has no counterpart to catalog. In addition, it calls the second level in the hierarchy \"databases.\" Vulcan and its documentation refer to this second level as \"schemas.\"</p> <p>Vulcan fully supports ClickHouse's two-level naming scheme without user action.</p>"},{"location":"references/integrations/engines/clickhouse/#table-engines","title":"Table engines","text":"<p>Every ClickHouse table is created with a \"table engine\" that controls how the table's data is stored and queried. ClickHouse's (and Vulcan's) default table engine is <code>MergeTree</code>.</p> <p>The <code>MergeTree</code> engine family requires that every table be created with an <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY</code> clause into every <code>MergeTree</code> family table's <code>CREATE</code> statement, or you can specify the columns/expressions by which the table should be ordered.</p>"},{"location":"references/integrations/engines/clickhouse/#clickhouse-modes-of-operation","title":"ClickHouse modes of operation","text":"<p>Conceptually, it may be helpful to view ClickHouse as having three modes of operation: single server, cluster, and ClickHouse Cloud. Vulcan supports all three modes.</p>"},{"location":"references/integrations/engines/clickhouse/#single-server-mode","title":"Single server mode","text":"<p>Single server mode is similar to other SQL engines: aside from choosing each table's engine, you do not need to worry about how computations are executed. You issue standard SQL commands/queries, and ClickHouse executes them.</p>"},{"location":"references/integrations/engines/clickhouse/#cluster-mode","title":"Cluster mode","text":"<p>Cluster mode allows you to scale your ClickHouse engine to any number of networked servers. This enables massive workloads, but requires that you specify how computations are executed by the networked servers.</p> <p>ClickHouse coordinates the computations on the networked servers with ClickHouse Keeper (it also supports Apache ZooKeeper).</p> <p>You specify named virtual clusters of servers in the Keeper configuration, and those clusters provide namespaces for data objects and computations. For example, you might include all networked servers in the cluster you name <code>MyCluster</code>.</p> <p>In general, you must be connected to a ClickHouse server to execute commands. By default, each command you execute runs in single-server mode on the server you are connected to.</p> <p>To associate an object with a cluster, DDL commands that create or modify it must include the text <code>ON CLUSTER [your cluster name]</code>.</p> <p>If you provide a cluster name in your Vulcan connection configuration, Vulcan will automatically inject the <code>ON CLUSTER</code> statement into the DDL commands for all objects created while executing the project. We provide more information about clusters in Vulcan below.</p>"},{"location":"references/integrations/engines/clickhouse/#clickhouse-cloud-mode","title":"ClickHouse Cloud mode","text":"<p>ClickHouse Cloud is a managed ClickHouse platform. It allows you to scale ClickHouse without administering a cluster yourself or modifying your SQL commands to run on the cluster.</p> <p>ClickHouse Cloud automates ClickHouse's cluster controls, which sometimes constrains ClickHouse's flexibility or how you execute SQL commands. For example, creating a table with a <code>SELECT</code> command must occur in two steps on ClickHouse Cloud. Vulcan handles this limitation for you.</p> <p>Aside from those constraints, ClickHouse Cloud mode is similar to single server mode - you run standard SQL commands/queries, and ClickHouse Cloud executes them.</p>"},{"location":"references/integrations/engines/clickhouse/#permissions","title":"Permissions","text":"<p>In the default Vulcan configuration, users must have sufficient permissions to create new ClickHouse databases.</p> <p>Alternatively, you can configure specific databases where Vulcan should create table and view objects.</p>"},{"location":"references/integrations/engines/clickhouse/#environment-views","title":"Environment views","text":"<p>Use the <code>environment_suffix_target</code> key in your project configuration to specify that environment views should be created within the model's database instead of in a new database:</p> <pre><code>environment_suffix_target: table\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#physical-tables","title":"Physical tables","text":"<p>Use the <code>physical_schema_mapping</code> key in your project configuration to specify the databases where physical tables should be created.</p> <p>The key accepts a dictionary of regular expressions that map model database names to the corresponding databases where physical tables should be created.</p> <p>Vulcan will compare a model's database name to each regular expression and use the first match to determine which database a physical table should be created in.</p> <p>For example, this configuration places every model's physical table in the <code>model_physical_tables</code> database because the regular expression <code>.*</code> matches any database name:</p> <pre><code>physical_schema_mapping:\n  '.*': model_physical_tables\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#cluster-specification","title":"Cluster specification","text":"<p>A ClickHouse cluster allows multiple networked ClickHouse servers to operate on the same data object. Every cluster must be named in the ClickHouse configuration files, and that name is passed to a table's DDL statements in the <code>ON CLUSTER</code> clause.</p> <p>For example, we could create a table <code>my_schema.my_table</code> on cluster <code>TheCluster</code> like this: <code>CREATE TABLE my_schema.my_table ON CLUSTER TheCluster (col1 Int8)</code>.</p> <p>To create Vulcan objects on a cluster, provide the cluster name to the <code>cluster</code> key in the Vulcan connection definition (see all connection parameters below).</p> <p>Vulcan will automatically inject the <code>ON CLUSTER</code> clause and cluster name you provide into all project DDL statements.</p>"},{"location":"references/integrations/engines/clickhouse/#model-definition","title":"Model definition","text":"<p>This section describes how you control a table's engine and other ClickHouse-specific functionality in Vulcan models.</p>"},{"location":"references/integrations/engines/clickhouse/#table-engine","title":"Table engine","text":"<p>Vulcan uses the <code>MergeTree</code> table engine with an empty <code>ORDER BY</code> clause by default.</p> <p>Specify a different table engine by passing the table engine definition to the model DDL's <code>storage_format</code> parameter. For example, you could specify the <code>Log</code> table engine like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    storage_format Log,\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>You may also specify more complex table engine definitions. For example:</p> <pre><code>MODEL (\n    name my_schema.my_rep_table,\n    kind full,\n    storage_format ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name', '{replica}', ver),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#order-by","title":"ORDER BY","text":"<p><code>MergeTree</code> family engines require that a table's <code>CREATE</code> statement include the <code>ORDER BY</code> clause.</p> <p>Vulcan will automatically inject an empty <code>ORDER BY ()</code> when creating a table with an engine in the <code>MergeTree</code> family. This creates the table without any ordering.</p> <p>You may specify columns/expressions to <code>ORDER BY</code> by passing them to the model <code>physical_properties</code> dictionary's <code>order_by</code> key.</p> <p>For example, you could order by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2)\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>order_by</code> key name and value <code>(col1, col2)</code>.</p> <p>Complex <code>ORDER BY</code> expressions may need to be passed in single quotes, with interior single quotes escaped by the <code>\\</code> character.</p>"},{"location":"references/integrations/engines/clickhouse/#primary-key","title":"PRIMARY KEY","text":"<p>Table engines may also accept a <code>PRIMARY KEY</code> specification. Similar to <code>ORDER BY</code>, specify a primary key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>primary_key</code> key name and value <code>col1</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#ttl","title":"TTL","text":"<p>ClickHouse tables accept a TTL expression that triggers actions like deleting rows after a certain amount of time has passed.</p> <p>Similar to <code>ORDER_BY</code> and <code>PRIMARY_KEY</code>, specify a TTL key in the model DDL's <code>physical_properties</code> dictionary. For example:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        order_by = (col1, col2),\n        primary_key = col1,\n        ttl = timestamp + INTERVAL 1 WEEK\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>ttl</code> key name and value <code>timestamp + INTERVAL 1 WEEK</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#partitioning","title":"Partitioning","text":"<p>Some ClickHouse table engines support partitioning. Specify the partitioning columns/expressions in the model DDL's <code>partitioned_by</code> key.</p> <p>For example, you could partition by columns <code>col1</code> and <code>col2</code> like this:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    partitioned_by (col1, col2),\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Learn more below about how Vulcan uses partitioned tables to improve performance.</p>"},{"location":"references/integrations/engines/clickhouse/#settings","title":"Settings","text":"<p>ClickHouse supports an immense number of settings, many of which can be altered in multiple places: ClickHouse configuration files, Python client connection arguments, DDL statements, SQL queries, and others.</p> <p>This section discusses how to control ClickHouse settings in Vulcan.</p>"},{"location":"references/integrations/engines/clickhouse/#connection-settings","title":"Connection settings","text":"<p>Vulcan connects to Python with the <code>clickhouse-connect</code> library. Its connection method accepts a dictionary of arbitrary settings that are passed to ClickHouse.</p> <p>Specify these settings in the <code>connection_settings</code> key. This example demonstrates how to set the <code>distributed_ddl_task_timeout</code> setting to <code>300</code>:</p> <pre><code>clickhouse_gateway:\n  connection:\n    type: clickhouse\n    host: localhost\n    port: 8123\n    username: user\n    password: pw\n    connection_settings:\n      distributed_ddl_task_timeout: 300\n  state_connection:\n    type: duckdb\n</code></pre>"},{"location":"references/integrations/engines/clickhouse/#ddl-settings","title":"DDL settings","text":"<p>ClickHouse settings may also be specified in DDL commands like <code>CREATE</code>.</p> <p>Specify these settings in a model DDL's <code>physical_properties</code> key (where the <code>order_by</code> and <code>primary_key</code> values are specified, if present).</p> <p>This example demonstrates how to set the <code>index_granularity</code> setting to <code>128</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n    physical_properties (\n        index_granularity = 128\n    )\n);\n\nselect\n    *\nfrom other_schema.other_table;\n</code></pre> <p>Note that there is an <code>=</code> between the <code>index_granularity</code> key name and value <code>128</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#query-settings","title":"Query settings","text":"<p>ClickHouse settings may be specified directly in a model's query with the <code>SETTINGS</code> keyword.</p> <p>This example demonstrates setting the <code>join_use_nulls</code> setting to <code>1</code>:</p> <pre><code>MODEL (\n    name my_schema.my_log_table,\n    kind full,\n);\n\nselect\n    *\nfrom other_schema.other_table\nSETTINGS join_use_nulls = 1;\n</code></pre> <p>Multiple settings may be specified in a query with repeated use of the <code>SETTINGS</code> keyword: <code>SELECT * FROM other_table SETTINGS first_setting = 1 SETTINGS second_setting = 2;</code>.</p>"},{"location":"references/integrations/engines/clickhouse/#usage-by-vulcan","title":"Usage by Vulcan","text":"<p>The ClickHouse setting <code>join_use_nulls</code> affects the behavior of Vulcan SCD models and table diffs. This section describes how Vulcan uses query settings to control that behavior.</p> <p>Background</p> <p>In general, table <code>JOIN</code>s can return empty cells for rows not present in both tables.</p> <p>For example, consider <code>LEFT JOIN</code>ing two tables <code>left</code> and <code>right</code>, where the column <code>right_column</code> is only present in the <code>right</code> table. Any rows only present in the <code>left</code> table will have no value for <code>right_column</code> in the joined table.</p> <p>In other SQL engines, those empty cells are filled with <code>NULL</code>s.</p> <p>In contrast, ClickHouse fills the empty cells with data type-specific default values (e.g., 0 for integer column types). It will instead fill the cells with <code>NULL</code>s if you set the <code>join_use_nulls</code> setting to <code>1</code>.</p> <p>Vulcan</p> <p>Vulcan automatically generates SQL queries for both SCD Type 2 models and table diff comparisons. These queries include table <code>JOIN</code>s and calculations based on the presence of <code>NULL</code> values.</p> <p>Because those queries expect <code>NULL</code> values in empty cells, Vulcan automatically adds <code>SETTINGS join_use_nulls = 1</code> to the generated SCD and table diff SQL code.</p> <p>The SCD model definition query is embedded as a CTE in the full Vulcan-generated query. If run alone, the model definition query would use the ClickHouse server's current <code>join_use_nulls</code> value.</p> <p>If that value is not <code>1</code>, the Vulcan setting on the outer query would override the server value and produce incorrect results.</p> <p>Therefore, Vulcan uses the following procedure to ensure the model definition query runs with the correct <code>join_use_nulls</code> value:</p> <ul> <li>If the model query sets <code>join_use_nulls</code> itself, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>1</code>, do nothing</li> <li>If the model query does not set <code>join_use_nulls</code> and the current server <code>join_use_nulls</code> value is <code>0</code>, add <code>SETTINGS join_use_nulls = 0</code> to the CTE model query<ul> <li>All other CTEs and the outer query will still execute with a <code>join_use_nulls</code> value of <code>1</code></li> </ul> </li> </ul>"},{"location":"references/integrations/engines/clickhouse/#performance-considerations","title":"Performance considerations","text":"<p>ClickHouse is optimized for writing/reading records, so deleting/replacing records can be extremely slow.</p> <p>This section describes why Vulcan needs to delete/replace records and how the ClickHouse engine adapter works around the limitations.</p>"},{"location":"references/integrations/engines/clickhouse/#why-delete-or-replace","title":"Why delete or replace?","text":"<p>Vulcan \"materializes\" model kinds in a number of ways, such as:</p> <ul> <li>Replacing an entire table (<code>FULL</code> models)</li> <li>Replacing records in a specific time range (<code>INCREMENTAL_BY_TIME_RANGE</code> models)</li> <li>Replacing records with specific key values (<code>INCREMENTAL_BY_UNIQUE_KEY</code> models)</li> <li>Replacing records in specific partitions (<code>INCREMENTAL_BY_PARTITION</code> models)</li> </ul> <p>Different SQL engines provide different methods for performing record replacement.</p> <p>Some engines natively support updating or inserting (\"upserting\") records. For example, in some engines you can <code>merge</code> a new table into an existing table based on a key. Records in the new table whose keys are already in the existing table will update/replace the existing records. Records in the new table without keys in the existing table will be inserted into the existing table.</p> <p>Other engines do not natively support upserts, so Vulcan replaces records in two steps: delete the records to update/replace from the existing table, then insert the new records.</p> <p>ClickHouse does not support upserts, and it performs the two step delete/insert operation so slowly as to be unusable. Therefore, Vulcan uses a different method for replacing records.</p>"},{"location":"references/integrations/engines/clickhouse/#temp-table-swap","title":"Temp table swap","text":"<p>Vulcan uses what we call the \"temp table swap\" method of replacing records in ClickHouse.</p> <p>Because ClickHouse is optimized for writing and reading records, it is often faster to copy most of a table than to delete a small portion of its records. That is the approach used by the temp table swap method (with optional performance improvements for partitioned tables).</p> <p>The temp table swap has four steps:</p> <ol> <li>Make an empty temp copy of the existing table that has the same structure (columns, data types, table engine, etc.)</li> <li>Insert new records into the temp table</li> <li>Insert the existing records that should be kept into the temp table</li> <li>Swap the table names, such that the temp table now has the existing table's name</li> </ol> <p>Figure 1 illustrates these four steps: </p> <p> Figure 1: steps to execute a temp table swap </p> <p>The weakness of this method is that it requires copying all existing rows to keep (step three), which can be problematic for large tables.</p> <p>To address this weakness, Vulcan instead uses partition swapping if a table is partitioned.</p>"},{"location":"references/integrations/engines/clickhouse/#partition-swap","title":"Partition swap","text":"<p>ClickHouse supports partitioned tables, which store groups of records in separate files, or \"partitions.\"</p> <p>A table is partitioned based on a table column or SQL expression - the \"partitioning key.\" All records with the same value for the partitioning key are stored together in a partition.</p> <p>For example, consider a table containing each record's creation date in a datetime column. If we partition the table by month, all the records whose timestamp was in January will be stored in one partition, records from February in another partition, and so on.</p> <p>Table partitioning provides a major benefit for improving swap performance: records can be inserted, updated, or deleted in individual partitions.</p> <p>Vulcan leverages this to avoid copying large numbers of existing records into a temp table. Instead, it only copies the records that are in partitions affected by a load's newly ingested records.</p> <p>Vulcan automatically uses partition swapping for any incremental model that specifies the <code>partitioned_by</code> key.</p>"},{"location":"references/integrations/engines/clickhouse/#choosing-a-partitioning-key","title":"Choosing a partitioning key","text":"<p>The first step of partitioning a table is choosing its partitioning key (columns or expression). The primary consideration for a key is the total number of partitions it will generate, which affects table performance.</p> <p>Too many partitions can drastically decrease performance because the overhead of handling partition files swamps the benefits of copying fewer records. Too few partitions decreases swap performance because many existing records must still be copied in each incremental load.</p> <p>How many partitions is too many?</p> <p>ClickHouse's documentation specifically warns against tables having too many partitions, suggesting a maximum of 1000.</p> <p>The total number of partitions in a table is determined by the actual data in the table, not by the partition column/expression alone.</p> <p>For example, consider a table partitioned by date. If we insert records created on <code>2024-10-23</code>, the table will have one partition. If we then insert records from <code>2024-10-24</code>, the table will have two partitions. One partition is created for each unique value of the key.</p> <p>For each partitioned table in your project, carefully consider the number of partitions created by the combination of your partitioning expression and the characteristics of your data.</p>"},{"location":"references/integrations/engines/clickhouse/#incremental-by-time-models","title":"Incremental by time models","text":"<p><code>INCREMENTAL_BY_TIME_RANGE</code> kind models must be partitioned by time. If the model's <code>time_column</code> is not present in any <code>partitioned_by</code> expression, Vulcan will automatically add it as the first partitioning expression.</p> <p>By default, <code>INCREMENTAL_BY_TIME_RANGE</code> models partition by week, so the maximum recommended 1000 partitions corresponds to about 19 years of data. Vulcan projects have widely varying time ranges and data sizes, so you should choose a model's partitioning key based on the data your system will process.</p> <p>If a model has many records in each partition, you may see additional performance benefits by including the time column in the model's <code>ORDER_BY</code> expression.</p> <p>Partitioning by time</p> <p><code>INCREMENTAL_BY_TIME_RANGE</code> models must be partitioned by time.</p> <p>Vulcan will automatically partition them by week unless the <code>partitioned_by</code> configuration key includes the time column or an expression based on it.</p> <p>Choose a model's time partitioning granularity based on the characteristics of the data it will process, making sure the total number of partitions is 1000 or fewer.</p>"},{"location":"references/integrations/engines/clickhouse/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>clickhouse</code></p> Option Description Type Required <code>type</code> Engine type name - must be <code>clickhouse</code> string Y <code>host</code> ClickHouse server hostname or IP address string Y <code>username</code> ClickHouse user name string Y <code>password</code> ClickHouse user password string N <code>port</code> The ClickHouse HTTP or HTTPS port (Default: <code>8123</code>) int N <code>cluster</code> ClickHouse cluster name string N <code>connect_timeout</code> Connection timeout in seconds (Default: <code>10</code>) int N <code>send_receive_timeout</code> Send/receive timeout in seconds (Default: <code>300</code>) int N <code>query_limit</code> Query result limit (Default: <code>0</code> - no limit) int N <code>use_compression</code> Whether to use compression (Default: <code>True</code>) bool N <code>compression_method</code> Compression method to use string N <code>http_proxy</code> HTTP proxy address (equivalent to setting the HTTP_PROXY environment variable) string N <code>verify</code> Verify server TLS/SSL certificate (Default: <code>True</code>) bool N <code>ca_cert</code> Ignored if verify is <code>False</code>. If verify is <code>True</code>, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Not necessary if the ClickHouse server certificate is a globally trusted root as verified by the operating system. string N <code>client_cert</code> File path to a TLS Client certificate in .pem format (for mutual TLS authentication). The file should contain a full certificate chain, including any intermediate certificates. string N <code>client_cert_key</code> File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file. string N <code>https_proxy</code> HTTPS proxy address (equivalent to setting the HTTPS_PROXY environment variable) string N <code>server_host_name</code> The ClickHouse server hostname as identified by the CN or SNI of its TLS certificate. Set this to avoid SSL errors when connecting through a proxy or tunnel with a different hostname. string N <code>tls_mode</code> Controls advanced TLS behavior. proxy and strict do not invoke ClickHouse mutual TLS connection, but do send client cert and key. mutual assumes ClickHouse mutual TLS auth with a client certificate. string N <code>connection_settings</code> Additional connection settings dict N <code>connection_pool_options</code> Additional options                                                                                                                                         for the HTTP connection pool dict N"},{"location":"references/integrations/engines/coming_soon/","title":"Engines Coming Soon","text":""},{"location":"references/integrations/engines/coming_soon/#engines-coming-soon","title":"Engines Coming Soon","text":"<p>The following execution engines are coming soon:</p> <ul> <li>Athena</li> <li>Azure SQL</li> <li>BigQuery</li> <li>ClickHouse</li> <li>Databricks</li> <li>DuckDB</li> <li>Fabric</li> <li>MotherDuck</li> <li>MSSQL</li> <li>MySQL</li> <li>GCP Postgres</li> <li>Redshift</li> <li>RisingWave</li> <li>Snowflake</li> <li>Spark</li> <li>Trino</li> </ul>"},{"location":"references/integrations/engines/databricks/","title":"Databricks","text":""},{"location":"references/integrations/engines/databricks/#databricks","title":"Databricks","text":"<p>This page provides information about how to use Vulcan with the Databricks SQL engine. It begins with a description of the three methods for connecting Vulcan to Databricks.</p> <p>After that is a Connection Quickstart that demonstrates how to connect to Databricks, or you can skip directly to information about using Databricks with the built-in.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connection-methods","title":"Databricks connection methods","text":"<p>Databricks provides multiple computing options and connection methods. This section describes the three methods for connecting with Vulcan.</p>"},{"location":"references/integrations/engines/databricks/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Vulcan connects to Databricks with the Databricks SQL Connector library by default.</p> <p>The SQL Connector is bundled with Vulcan and automatically installed when you include the <code>databricks</code> extra in the command <code>pip install \"vulcan[databricks]\"</code>.</p> <p>The SQL Connector has all the functionality needed for Vulcan to execute SQL models on Databricks and Python models that do not return PySpark DataFrames.</p> <p>If you have Python models returning PySpark DataFrames, check out the Databricks Connect section.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connect","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan must use the Databricks Connect library to connect to Databricks (instead of the Databricks SQL Connector library).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>Find more configuration details below.</p>"},{"location":"references/integrations/engines/databricks/#databricks-notebook-interface","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly in a Databricks Cluster interface, the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>Find more configuration details below.</p>"},{"location":"references/integrations/engines/databricks/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Databricks.</p> <p>It demonstrates connecting to a Databricks All-Purpose Compute instance with the <code>databricks-sql-connector</code> Python library bundled with Vulcan.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"references/integrations/engines/databricks/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Databricks account with access to an appropriate Databricks Workspace<ul> <li>The Workspace must support authenticating with personal access tokens (Databricks Community Edition workspaces do not)</li> <li>Your account must have Workspace Access and Create Compute permissions (these permissions are enabled by default)</li> </ul> </li> <li>Your Databricks compute resources have Unity Catalog activated</li> <li>Your computer has Vulcan installed with the Databricks extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[databricks]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init duckdb</code></li> </ul> </li> </ol> <p>Unity Catalog required</p> <p>Databricks compute resources used by Vulcan must have Unity Catalog activated.</p>"},{"location":"references/integrations/engines/databricks/#get-connection-info","title":"Get connection info","text":"<p>The first step to configuring a Databricks connection is gathering the necessary information from your Databricks compute instance.</p>"},{"location":"references/integrations/engines/databricks/#create-compute","title":"Create Compute","text":"<p>We must have something to connect to, so we first create and activate a Databricks compute instance. If you already have one running, skip to the next section.</p> <p>We begin in the default view for our Databricks Workspace. Access the Compute view by clicking the <code>Compute</code> entry in the left-hand menu:</p> <p></p> <p>In the Compute view, click the <code>Create compute</code> button:</p> <p></p> <p>Modify compute cluster options if desired and click the <code>Create compute</code> button:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#get-jdbcodbc-info","title":"Get JDBC/ODBC info","text":"<p>Scroll to the bottom of the view and click the open the <code>Advanced Options</code> view:</p> <p></p> <p>Click the <code>JDBC/ODBC</code> tab:</p> <p></p> <p>Open your project's <code>config.yaml</code> configuration file in a text editor and add a new gateway named <code>databricks</code> below the existing <code>local</code> gateway:</p> <p></p> <p>Copy the <code>server_hostname</code> and <code>http_path</code> connection values from the Databricks JDBC/ODBC tab to the <code>config.yaml</code> file:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#get-personal-access-token","title":"Get personal access token","text":"<p>The final piece of information we need for the <code>config.yaml</code> file is your personal access token.</p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre> <p> To create a personal access token, click on your profile logo and go to your profile's <code>Settings</code> page:</p> <p></p> <p>Go to the <code>Developer</code> view in the User menu. Depending on your account's role, your page may not display the Workspace Admin section of the page.</p> <p></p> <p>Click the <code>Manage</code> button in the Access Tokens section:</p> <p></p> <p>Click the <code>Generate new token</code> button:</p> <p></p> <p>Name your token in the <code>Comment</code> field, and click the <code>Generate</code> button:</p> <p></p> <p>Click the copy button and paste the token into the <code>access_token</code> key:</p> <p></p> <p>Warning</p> <p>Do not share your personal access token with anyone.</p> <p>Best practice for storing secrets like access tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>DATABRICKS_ACCESS_TOKEN</code> for the configuration's <code>access_token</code> parameter:</p> <pre><code>gateways:\n  databricks:\n      connection:\n        type: databricks\n        access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}\n</code></pre>"},{"location":"references/integrations/engines/databricks/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>databricks</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Databricks. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan --gateway databricks info</code>.</p> <p>We manually specify the <code>databricks</code> gateway because it is not our project's default gateway:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Databricks SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Databricks is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"references/integrations/engines/databricks/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>databricks</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>databricks_state.db</code> file:</p> <p></p> <p>Now we no longer see the warning when running <code>vulcan --gateway databricks info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"references/integrations/engines/databricks/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>For convenience, we can omit the <code>--gateway</code> option from our CLI commands by specifying <code>databricks</code> as our project's <code>default_gateway</code>:</p> <p></p> <p>And run a <code>vulcan plan</code> in Databricks:</p> <p></p> <p>And confirm that our schemas and objects exist in the Databricks catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Databricks!</p> <p>Tip</p> <p>Vulcan connects to your Databricks Cluster's default catalog by default. Connect to a different catalog by specifying its name in the connection configuration's <code>catalog</code> parameter.</p>"},{"location":"references/integrations/engines/databricks/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>databricks</code></p>"},{"location":"references/integrations/engines/databricks/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[databricks]\"\n</code></pre>"},{"location":"references/integrations/engines/databricks/#connection-method-details","title":"Connection method details","text":"<p>Databricks provides multiple computing options and connection methods. The section above explains how to use them with Vulcan, and this section provides additional configuration details.</p>"},{"location":"references/integrations/engines/databricks/#databricks-sql-connector_1","title":"Databricks SQL Connector","text":"<p>Vulcan uses the Databricks SQL Connector to connect to Databricks by default. Learn more above.</p>"},{"location":"references/integrations/engines/databricks/#databricks-connect_1","title":"Databricks Connect","text":"<p>If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan needs to use the Databricks Connect to connect to Databricks (instead of the Databricks SQL Connector).</p> <p>Vulcan DOES NOT include/bundle the Databricks Connect library. You must install the version of Databricks Connect that matches the Databricks Runtime used in your Databricks cluster.</p> <p>If Vulcan detects that you have Databricks Connect installed, then it will automatically configure the connection and use it for all Python models that return a Pandas or PySpark DataFrame.</p> <p>To have databricks-connect installed but ignored by Vulcan, set <code>disable_databricks_connect</code> to <code>true</code> in the connection configuration.</p> <p>Databricks Connect can execute SQL and DataFrame operations on different clusters by setting the Vulcan <code>databricks_connect_*</code> connection options. For example, these options could configure Vulcan to run SQL on a Databricks SQL Warehouse while still routing DataFrame operations to a normal Databricks Cluster.</p> <p>Note</p> <p>If using Databricks Connect, make sure to learn about the Databricks requirements and limitations.</p>"},{"location":"references/integrations/engines/databricks/#databricks-notebook-interface_1","title":"Databricks notebook interface","text":"<p>If you are always running Vulcan commands directly on a Databricks Cluster (like in a Databricks Notebook), the SparkSession provided by Databricks is used to execute all Vulcan commands.</p> <p>The only relevant Vulcan configuration parameter is the optional <code>catalog</code> parameter.</p>"},{"location":"references/integrations/engines/databricks/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>databricks</code> string Y <code>server_hostname</code> Databricks instance host name string N <code>http_path</code> HTTP path, either to a DBSQL endpoint (such as <code>/sql/1.0/endpoints/1234567890abcdef</code>) or to an All-Purpose cluster (such as <code>/sql/protocolv1/o/1234567890123456/1234-123456-slid123</code>) string N <code>access_token</code> HTTP Bearer access token, such as Databricks Personal Access Token string N <code>catalog</code> The name of the catalog to use for the connection. Defaults to use Databricks cluster default. string N <code>auth_type</code> SQL Connector Only: Set to 'databricks-oauth' or 'azure-oauth' to trigger OAuth (or dont set at all to use <code>access_token</code>) string N <code>oauth_client_id</code> SQL Connector Only: Optional M2M OAuth Client ID to use when <code>auth_type</code> is set string N <code>oauth_client_secret</code> SQL Connector Only: Optional M2M OAuth Client Secret to use when <code>auth_type</code> is set string N <code>http_headers</code> SQL Connector Only: An optional dictionary of HTTP headers that will be set on every request dict N <code>session_configuration</code> SQL Connector Only: An optional dictionary of Spark session parameters. Execute the SQL command <code>SET -v</code> to get a full list of available commands. dict N <code>databricks_connect_server_hostname</code> Databricks Connect Only: Databricks Connect server hostname. Uses <code>server_hostname</code> if not set. string N <code>databricks_connect_access_token</code> Databricks Connect Only: Databricks Connect access token. Uses <code>access_token</code> if not set. string N <code>databricks_connect_cluster_id</code> Databricks Connect Only: Databricks Connect cluster ID. Uses <code>http_path</code> if not set. Cannot be a Databricks SQL Warehouse. string N <code>databricks_connect_use_serverless</code> Databricks Connect Only: Use a serverless cluster for Databricks Connect instead of <code>databricks_connect_cluster_id</code>. bool N <code>force_databricks_connect</code> When running locally, force the use of Databricks Connect for all model operations (so don't use SQL Connector for SQL models) bool N <code>disable_databricks_connect</code> When running locally, disable the use of Databricks Connect for all model operations (so use SQL Connector for all models) bool N <code>disable_spark_session</code> Do not use SparkSession if it is available (like when running in a notebook). bool N"},{"location":"references/integrations/engines/databricks/#model-table-properties-to-support-altering-tables","title":"Model table properties to support altering tables","text":"<p>If you are making a change to the structure of a table that is forward only, then you may need to add the following to your model's <code>physical_properties</code>:</p> <pre><code>MODEL (\n    name vulcan_example.new_model,\n    ...\n    physical_properties (\n        'delta.columnMapping.mode' = 'name'\n    ),\n)\n</code></pre> <p>If you attempt to alter without having this property set, you will get an error similar to <code>databricks.sql.exc.ServerOperationError: [DELTA_UNSUPPORTED_DROP_COLUMN] DROP COLUMN is not supported for your Delta table.</code>. Databricks Documentation for more details.</p>"},{"location":"references/integrations/engines/duckdb/","title":"DuckDB","text":""},{"location":"references/integrations/engines/duckdb/#duckdb","title":"DuckDB","text":"<p>DuckDB state connection limitations</p> <p>DuckDB is a single user database. Using it for a state connection in your Vulcan project limits you to a single workstation. This means your project cannot be shared amongst your team members or your CI/CD infrastructure. This is usually fine for proof of concept or test projects but it will not scale to production usage.</p> <p>For production projects, use Tobiko Cloud or a more robust state database such as Postgres.</p>"},{"location":"references/integrations/engines/duckdb/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>duckdb</code></p>"},{"location":"references/integrations/engines/duckdb/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>duckdb</code> string Y <code>database</code> The optional database name. If not specified, the in-memory database is used. Cannot be defined if using <code>catalogs</code>. string N <code>catalogs</code> Mapping to define multiple catalogs. Can attach DuckDB catalogs or catalogs for other connections. First entry is the default catalog. Cannot be defined if using <code>database</code>. dict N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g., S3) using DuckDB secrets. Can be a list of secret configurations or a dictionary with custom secret names. list/dict N <code>filesystems</code> Configuration for registering <code>fsspec</code> filesystems to the DuckDB connection. dict N"},{"location":"references/integrations/engines/duckdb/#duckdb-catalogs-example","title":"DuckDB Catalogs Example","text":"<p>This example specifies two catalogs. The first catalog is named \"persistent\" and maps to the DuckDB file database <code>local.duckdb</code>. The second catalog is named \"ephemeral\" and maps to the DuckDB in-memory database.</p> <p><code>persistent</code> is the default catalog since it is the first entry in the dictionary. Vulcan will place models without an explicit catalog, such as <code>my_schema.my_model</code>, into the <code>persistent</code> catalog <code>local.duckdb</code> DuckDB file database.</p> <p>Vulcan will place models with the explicit catalog \"ephemeral\", such as <code>ephemeral.other_schema.other_model</code>, into the <code>ephemeral</code> catalog DuckDB in-memory database.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        persistent: 'local.duckdb'\n        ephemeral: ':memory:'\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"persistent\": \"local.duckdb\"\n                    \"ephemeral\": \":memory:\"\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#ducklake-catalog-example","title":"DuckLake Catalog Example","text":"YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: 'catalog.ducklake'\n          data_path: data/ducklake\n          encrypted: True\n          data_inlining_row_limit: 10\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"ducklake\": DuckDBAttachOptions(\n                        type=\"ducklake\",\n                        path=\"catalog.ducklake\",\n                        data_path=\"data/ducklake\",\n                        encrypted=True,\n                        data_inlining_row_limit=10,\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#other-connection-catalogs-example","title":"Other Connection Catalogs Example","text":"<p>Catalogs can also be defined to connect to anything that DuckDB can be attached to.</p> <p>Below are examples of connecting to a SQLite database and a PostgreSQL database. The SQLite database is read-write, while the PostgreSQL database is read-only.</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: duckdb\n      catalogs:\n        memory: ':memory:'\n        sqlite:\n          type: sqlite\n          path: 'test.db'\n        postgres:\n          type: postgres\n          path: 'dbname=postgres user=postgres host=127.0.0.1'\n          read_only: true\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\nfrom vulcan.core.config.connection import DuckDBAttachOptions\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=&lt;dialect&gt;),\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"memory\": \":memory:\",\n                    \"sqlite\": DuckDBAttachOptions(\n                        type=\"sqlite\",\n                        path=\"test.db\"\n                    ),\n                    \"postgres\": DuckDBAttachOptions(\n                        type=\"postgres\",\n                        path=\"dbname=postgres user=postgres host=127.0.0.1\",\n                        read_only=True\n                    ),\n                }\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#catalogs-for-postgresql","title":"Catalogs for PostgreSQL","text":"<p>In PostgreSQL, the catalog name must match the actual catalog name it is associated with, as shown in the example above, where the database name (<code>dbname</code> in the path) is the same as the catalog name.</p>"},{"location":"references/integrations/engines/duckdb/#connectors-without-schemas","title":"Connectors without schemas","text":"<p>Some connections, like SQLite, do not support schema names and therefore objects will be attached under the default schema name of <code>main</code>.</p> <p>Example: mounting a SQLite database with the name <code>sqlite</code> that has a table <code>example_table</code> will be accessible as <code>sqlite.main.example_table</code>.</p>"},{"location":"references/integrations/engines/duckdb/#sensitive-fields-in-paths","title":"Sensitive fields in paths","text":"<p>If a connector, like Postgres, requires sensitive information in the path, it might support defining environment variables instead. See DuckDB Documentation for more information.</p>"},{"location":"references/integrations/engines/duckdb/#cloud-service-authentication","title":"Cloud service authentication","text":"<p>DuckDB can read data directly from cloud services via extensions (e.g., httpfs, azure).</p> <p>The <code>secrets</code> option allows you to configure DuckDB's Secrets Manager to authenticate with external services like S3. This is the recommended approach for cloud storage authentication in DuckDB v0.10.0 and newer, replacing the legacy authentication method via variables.</p>"},{"location":"references/integrations/engines/duckdb/#secrets-configuration","title":"Secrets Configuration","text":"<p>The <code>secrets</code> option supports two formats:</p> <ol> <li>List format (default secrets): A list of secret configurations where each secret uses DuckDB's default naming</li> <li>Dictionary format (named secrets): A dictionary where keys are custom secret names and values are the secret configurations</li> </ol> <p>This flexibility allows you to organize multiple secrets of the same type or reference specific secrets by name in your SQL queries.</p>"},{"location":"references/integrations/engines/duckdb/#list-format-example-default-secrets","title":"List Format Example (Default Secrets)","text":"<p>Using a list creates secrets with DuckDB's default naming:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        - type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets=[\n                    {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    }\n                ]\n            )\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/duckdb/#dictionary-format-example-named-secrets","title":"Dictionary Format Example (Named Secrets)","text":"<p>Using a dictionary allows you to assign custom names to your secrets for better organization and reference:</p> YAMLPython <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        local: local.db\n        remote: \"s3://bucket/data/remote.duckdb\"\n      extensions:\n        - name: httpfs\n      secrets:\n        my_s3_secret:\n          type: s3\n          region: \"YOUR_AWS_REGION\"\n          key_id: \"YOUR_AWS_ACCESS_KEY\"\n          secret: \"YOUR_AWS_SECRET_KEY\"\n        my_azure_secret:\n          type: azure\n          account_name: \"YOUR_AZURE_ACCOUNT\"\n          account_key: \"YOUR_AZURE_KEY\"\n</code></pre> <pre><code>from vulcan.core.config import (\n    Config,\n    ModelDefaultsConfig,\n    GatewayConfig,\n    DuckDBConnectionConfig\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"duckdb\"),\n    gateways={\n        \"duckdb\": GatewayConfig(\n            connection=DuckDBConnectionConfig(\n                catalogs={\n                    \"local\": \"local.db\",\n                    \"remote\": \"s3://bucket/data/remote.duckdb\"\n                },\n                extensions=[\n                    {\"name\": \"httpfs\"},\n                ],\n                secrets={\n                    \"my_s3_secret\": {\n                        \"type\": \"s3\",\n                        \"region\": \"YOUR_AWS_REGION\",\n                        \"key_id\": \"YOUR_AWS_ACCESS_KEY\",\n                        \"secret\": \"YOUR_AWS_SECRET_KEY\"\n                    },\n                    \"my_azure_secret\": {\n                        \"type\": \"azure\",\n                        \"account_name\": \"YOUR_AZURE_ACCOUNT\",\n                        \"account_key\": \"YOUR_AZURE_KEY\"\n                    }\n                }\n            )\n        ),\n    }\n)\n</code></pre> <p>After configuring the secrets, you can directly reference S3 paths in your catalogs or in SQL queries without additional authentication steps.</p> <p>Refer to the official DuckDB documentation for the full list of supported S3 secret parameters and for more information on the Secrets Manager configuration.</p> <p>Note: Loading credentials at runtime using <code>load_aws_credentials()</code> or similar deprecated functions may fail when using Vulcan.</p>"},{"location":"references/integrations/engines/duckdb/#file-system-configuration-example-for-microsoft-onelake","title":"File system configuration example for Microsoft Onelake","text":"<p>The <code>filesystems</code> accepts a list of file systems to register in the DuckDB connection. This is especially useful for Azure Storage Accounts, as it adds write support for DuckDB which is not natively supported by DuckDB (yet).</p> YAML <pre><code>gateways:\n  ducklake:\n    connection:\n      type: duckdb\n      catalogs:\n        ducklake:\n          type: ducklake\n          path: myducklakecatalog.duckdb\n          data_path: abfs://MyFabricWorkspace/MyFabricLakehouse.Lakehouse/Files/DuckLake.Files\n    extensions:\n      - ducklake\n    filesystems:\n      - fs: abfs\n        account_name: onelake\n        account_host: onelake.blob.fabric.microsoft.com\n        client_id: {{ env_var('AZURE_CLIENT_ID') }}\n        client_secret: {{ env_var('AZURE_CLIENT_SECRET') }}\n        tenant_id: {{ env_var('AZURE_TENANT_ID') }}\n        # anon: False # To use azure.identity.DefaultAzureCredential authentication \n</code></pre> <p>Refer to the documentation for <code>fsspec</code> fsspec.filesystem and <code>adlfs</code> adlfs.AzureBlobFileSystem for a full list of storage options. </p>"},{"location":"references/integrations/engines/fabric/","title":"Fabric","text":""},{"location":"references/integrations/engines/fabric/#fabric","title":"Fabric","text":"<p>Info</p> <p>The Fabric engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"references/integrations/engines/fabric/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>fabric</code></p> <p>NOTE: Fabric Warehouse is not recommended to be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/fabric/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/fabric/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[fabric]\"\n</code></pre>"},{"location":"references/integrations/engines/fabric/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>fabric</code> string Y <code>host</code> The hostname of the Fabric Warehouse server string Y <code>user</code> The client id to use for authentication with the Fabric Warehouse server string N <code>password</code> The client secret to use for authentication with the Fabric Warehouse server string N <code>port</code> The port number of the Fabric Warehouse server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pyodbc string N <code>driver_name</code> The driver name to use for the connection. E.g., ODBC Driver 18 for SQL Server string N <code>tenant_id</code> The Azure / Entra tenant UUID string Y <code>workspace_id</code> The Fabric workspace UUID. The preferred way to retrieve it is by running <code>notebookutils.runtime.context.get(\"currentWorkspaceId\")</code> in a python notebook. string Y <code>odbc_properties</code> The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more here. dict N"},{"location":"references/integrations/engines/gcp-postgres/","title":"GCP Postgres","text":""},{"location":"references/integrations/engines/gcp-postgres/#gcp-postgres","title":"GCP Postgres","text":""},{"location":"references/integrations/engines/gcp-postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>gcp_postgres</code></p>"},{"location":"references/integrations/engines/gcp-postgres/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[gcppostgres]\"\n</code></pre>"},{"location":"references/integrations/engines/gcp-postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>gcp_postgres</code> string Y <code>instance_connection_string</code> Connection name for the postgres instance string Y <code>user</code> The username (postgres or IAM) to use for authentication string Y <code>password</code> The password to use for authentication. Required when connecting as a Postgres user string N <code>enable_iam_auth</code> Enables IAM authentication. Required when connecting as an IAM user boolean N <code>keyfile</code> Path to the keyfile to be used with enable_iam_auth instead of ADC string N <code>keyfile_json</code> Keyfile information provided inline (not recommended) dict N <code>db</code> The name of the database instance to connect to string Y <code>ip_type</code> The IP type to use for the connection. Must be one of <code>public</code>, <code>private</code>, or <code>psc</code>. Default: <code>public</code> string N <code>timeout</code> The connection timeout in seconds. Default: <code>30</code> integer N <code>scopes</code> The scopes to use for the connection. Default: <code>(https://www.googleapis.com/auth/sqlservice.admin,)</code> tuple[str] N <code>driver</code> The driver to use for the connection. Default: <code>pg8000</code>. Note: only <code>pg8000</code> is tested string N"},{"location":"references/integrations/engines/motherduck/","title":"MotherDuck","text":""},{"location":"references/integrations/engines/motherduck/#motherduck","title":"MotherDuck","text":"<p>This page provides information about how to use Vulcan with MotherDuck.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to MotherDuck, or you can skip directly to information about using MotherDuck with the built-in scheduler.</p>"},{"location":"references/integrations/engines/motherduck/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with MotherDuck.</p> <p>It demonstrates connecting to MotherDuck with the <code>duckdb</code> library bundled with Vulcan.</p> <p>MotherDuck provides a single way to authorize a connection. This quickstart demonstrates authenticating with a token.</p> <p>Tip</p> <p>This quick start assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not familiar, work through the Vulcan Quickstart before continuing.</p>"},{"location":"references/integrations/engines/motherduck/#prerequisites","title":"Prerequisites","text":"<p>Before working through this quickstart guide, ensure that:</p> <ol> <li>You have a motherduck account and an access token.</li> <li>Your computer has Vulcan installed with the DuckDB extra available.</li> <li>Install from command line with the command <code>pip install \u201cvulcan[duckdb]\u201d</code></li> <li>You have initialized a Vulcan example project on your computer</li> <li>Open a command line interface and navigate to the directory where the project files should go.</li> <li>Initialize the project with the command <code>vulcan init duckdb</code>, since <code>duckdb</code> is the dialect.</li> </ol>"},{"location":"references/integrations/engines/motherduck/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access your MotherDuck databases. Since permission is granted to specific databases for a specific user, you should create a service account for Vulcan that will contain the credentials for writing to MotherDuck.</p>"},{"location":"references/integrations/engines/motherduck/#configure-the-connection","title":"Configure the connection","text":"<p>We now have what is required to configure Vulcan\u2019s connection to MotherDuck.</p> <p>We start the configuration by adding a gateway named <code>motherduck</code> to our example project\u2019s config.yaml file and making it our <code>default gateway</code>, as well as adding our token, persistent, and ephemeral catalogs.</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n        catalogs:\n          persistent: \"md:\"\n          ephemeral: \":memory:\"\n      token: &lt;your_token&gt;\n\ndefault_gateway: motherduck\n</code></pre> <p>Catalogs can be defined to connect to anything that DuckDB can be attached to.</p> <p>Warning</p> <p>Best practice for storing secrets like tokens is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>MOTHERDUCK_TOKEN</code> for the configuration's <code>token</code> parameter:</p> <pre><code>gateways:\n  motherduck:\n    connection:\n      type: motherduck\n      token: {{ env_var('MOTHERDUCK_TOKEN') }}\n</code></pre>"},{"location":"references/integrations/engines/motherduck/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>motherduck</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to MotherDuck. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p>"},{"location":"references/integrations/engines/motherduck/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in MotherDuck:</p> <p></p> <p>And confirm that our schemas and objects exist in the MotherDuck catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on MotherDuck!</p>"},{"location":"references/integrations/engines/motherduck/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>motherduck</code></p>"},{"location":"references/integrations/engines/motherduck/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>motherduck</code> string Y <code>database</code> The database name. string Y <code>token</code> The optional MotherDuck token. If not specified, the user will be prompted to login with their web browser. string N <code>extensions</code> Extension to load into duckdb. Only autoloadable extensions are supported. list N <code>connector_config</code> Configuration to pass into the duckdb connector. dict N <code>secrets</code> Configuration for authenticating external sources (e.g. S3) using DuckDB secrets. dict N"},{"location":"references/integrations/engines/mssql/","title":"MSSQL","text":""},{"location":"references/integrations/engines/mssql/#mssql","title":"MSSQL","text":""},{"location":"references/integrations/engines/mssql/#installation","title":"Installation","text":""},{"location":"references/integrations/engines/mssql/#user-password-authentication","title":"User / Password Authentication:","text":"<pre><code>pip install \"vulcan[mssql]\"\n</code></pre>"},{"location":"references/integrations/engines/mssql/#microsoft-entra-id-azure-active-directory-authentication","title":"Microsoft Entra ID / Azure Active Directory Authentication:","text":"<pre><code>pip install \"vulcan[mssql-odbc]\"\n</code></pre>"},{"location":"references/integrations/engines/mssql/#incremental-by-unique-key-merge","title":"Incremental by unique key <code>MERGE</code>","text":"<p>Vulcan executes a <code>MERGE</code> statement to insert rows for incremental by unique key model kinds.</p> <p>By default, the <code>MERGE</code> statement updates all non-key columns of an existing row when a new row with the same key values is inserted. If all column values match between the two rows, those updates are unnecessary.</p> <p>Vulcan provides an optional performance optimization that skips unnecessary updates by comparing column values with the <code>EXISTS</code> and <code>EXCEPT</code> operators.</p> <p>Enable the optimization by setting the <code>mssql_merge_exists</code> key to <code>true</code> in the <code>physical_properties</code> section of the <code>MODEL</code> statement.</p> <p>For example:</p> <pre><code>MODEL (\n    name vulcan_example.unique_key,\n    kind INCREMENTAL_BY_UNIQUE_KEY (\n        unique_key id\n    ),\n    cron '@daily',\n    physical_properties (\n        mssql_merge_exists = true\n    )\n);\n</code></pre> <p>Not all column types supported</p> <p>The <code>mssql_merge_exists</code> optimization is not supported for all column types, including <code>GEOMETRY</code>, <code>XML</code>, <code>TEXT</code>, <code>NTEXT</code>, <code>IMAGE</code>, and most user-defined types.</p> <p>Learn more in the MSSQL <code>EXCEPT</code> statement documentation.</p>"},{"location":"references/integrations/engines/mssql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mssql</code></p>"},{"location":"references/integrations/engines/mssql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mssql</code> string Y <code>host</code> The hostname of the MSSQL server string Y <code>user</code> The username / client id to use for authentication with the MSSQL server string N <code>password</code> The password / client secret to use for authentication with the MSSQL server string N <code>port</code> The port number of the MSSQL server int N <code>database</code> The target database string N <code>charset</code> The character set used for the connection string N <code>timeout</code> The query timeout in seconds. Default: no timeout int N <code>login_timeout</code> The timeout for connection and login in seconds. Default: 60 int N <code>appname</code> The application name to use for the connection string N <code>conn_properties</code> The list of connection properties list[string] N <code>autocommit</code> Is autocommit mode enabled. Default: false bool N <code>driver</code> The driver to use for the connection. Default: pymssql string N <code>driver_name</code> The driver name to use for the connection (e.g., ODBC Driver 18 for SQL Server). string N <code>odbc_properties</code> ODBC connection properties (e.g., authentication: ActiveDirectoryServicePrincipal). See more here. dict N"},{"location":"references/integrations/engines/mysql/","title":"MySQL","text":""},{"location":"references/integrations/engines/mysql/#mysql","title":"MySQL","text":""},{"location":"references/integrations/engines/mysql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mysql</code></p>"},{"location":"references/integrations/engines/mysql/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[mysql]\"\n</code></pre>"},{"location":"references/integrations/engines/mysql/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>mysql</code> string Y <code>host</code> The hostname of the MysQL server string Y <code>user</code> The username to use for authentication with the MySQL server string Y <code>password</code> The password to use for authentication with the MySQL server string Y <code>port</code> The port number of the MySQL server int N <code>charset</code> The character set used for the connection string N <code>ssl_disabled</code> Is SSL disabled bool N"},{"location":"references/integrations/engines/postgres/","title":"Postgres","text":""},{"location":"references/integrations/engines/postgres/#postgres","title":"Postgres","text":""},{"location":"references/integrations/engines/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"references/integrations/engines/postgres/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"references/integrations/engines/redshift/","title":"Redshift","text":""},{"location":"references/integrations/engines/redshift/#redshift","title":"Redshift","text":""},{"location":"references/integrations/engines/redshift/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>redshift</code></p>"},{"location":"references/integrations/engines/redshift/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[redshift]\"\n</code></pre>"},{"location":"references/integrations/engines/redshift/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>redshift</code> string Y <code>user</code> The username to use for authentication with the Amazon Redshift cluster string N <code>password</code> The password to use for authentication with the Amazon Redshift cluster string N <code>database</code> The name of the database instance to connect to string N <code>host</code> The hostname of the Amazon Redshift cluster string N <code>port</code> The port number of the Amazon Redshift cluster int N <code>ssl</code> Is SSL enabled. SSL must be enabled when authenticating using IAM (Default: <code>True</code>) bool N <code>sslmode</code> The security of the connection to the Amazon Redshift cluster. <code>verify-ca</code> and <code>verify-full</code> are supported. string N <code>timeout</code> The number of seconds before the connection to the server will timeout. int N <code>tcp_keepalive</code> Is TCP keepalive used. (Default: <code>True</code>) bool N <code>application_name</code> The name of the application string N <code>preferred_role</code> The IAM role preferred for the current connection string N <code>principal_arn</code> The ARN of the IAM entity (user or role) for which you are generating a policy string N <code>credentials_provider</code> The class name of the IdP that will be used for authenticating with the Amazon Redshift cluster string N <code>region</code> The AWS region of the Amazon Redshift cluster string N <code>cluster_identifier</code> The cluster identifier of the Amazon Redshift cluster string N <code>iam</code> If IAM authentication is enabled. IAM must be True when authenticating using an IdP dict N <code>is_serverless</code> If the Amazon Redshift cluster is serverless (Default: <code>False</code>) bool N <code>serverless_acct_id</code> The account ID of the serverless cluster string N <code>serverless_work_group</code> The name of work group for serverless end point string N <code>enable_merge</code> Whether the incremental_by_unique_key model kind will use the native Redshift MERGE operation or Vulcan's logical merge. (Default: <code>False</code>) bool N"},{"location":"references/integrations/engines/redshift/#performance-considerations","title":"Performance Considerations","text":""},{"location":"references/integrations/engines/redshift/#timestamp-macro-variables-and-sort-keys","title":"Timestamp Macro Variables and Sort Keys","text":"<p>When working with Redshift tables that have a <code>TIMESTAMP</code> sort key, using the standard <code>@start_dt</code> and <code>@end_dt</code> macro variables may lead to performance issues. These macros render as <code>TIMESTAMP WITH TIME ZONE</code> values in SQL queries, which prevents Redshift from performing efficient pruning when filtering against <code>TIMESTAMP</code> (without timezone) sort keys.</p> <p>This can result in full table scans instead, causing significant performance degradation.</p> <p>Solution: Use the <code>_dtntz</code> (datetime no timezone) variants of macro variables:</p> <ul> <li><code>@start_dtntz</code> instead of <code>@start_dt</code></li> <li><code>@end_dtntz</code> instead of <code>@end_dt</code></li> </ul> <p>These variants render as <code>TIMESTAMP WITHOUT TIME ZONE</code>, allowing Redshift to properly utilize sort key optimizations.</p> <p>Example:</p> <pre><code>-- Inefficient: May cause full table scan\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dt\n  AND timestamp_column &lt; @end_dt\n\n-- Efficient: Uses sort key optimization\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_dtntz\n  AND timestamp_column &lt; @end_dtntz\n\n-- Alternative: Cast to timestamp\nSELECT * FROM my_table\nWHERE timestamp_column &gt;= @start_ts::timestamp\n  AND timestamp_column &lt; @end_ts::timestamp\n</code></pre>"},{"location":"references/integrations/engines/risingwave/","title":"RisingWave","text":""},{"location":"references/integrations/engines/risingwave/#risingwave","title":"RisingWave","text":"<p>This page provides information about how to use Vulcan with the RisingWave streaming database engine.</p> <p>Info</p> <p>The RisingWave engine adapter is a community contribution. Due to this, only limited community support is available.</p>"},{"location":"references/integrations/engines/risingwave/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>risingwave</code></p>"},{"location":"references/integrations/engines/risingwave/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[risingwave]\"\n</code></pre>"},{"location":"references/integrations/engines/risingwave/#connection-options","title":"Connection options","text":"<p>RisingWave is based on Postgres and uses the same <code>psycopg2</code> connection library. Therefore, the connection parameters are very similar to Postgres.</p> Option Description Type Required <code>type</code> Engine type name - must be <code>risingwave</code> string Y <code>host</code> The hostname of the RisingWave server string Y <code>user</code> The username to use for authentication with the RisingWave server string Y <code>password</code> The password to use for authentication with the RisingWave server string N <code>port</code> The port number of the RisingWave engine server int Y <code>database</code> The name of the database instance to connect to string Y <code>role</code> The role to use for authentication with the RisingWave server string N <code>sslmode</code> The security of the connection to the RisingWave server string N"},{"location":"references/integrations/engines/risingwave/#extra-features","title":"Extra Features","text":"<p>As a streaming database engine, RisingWave contains some extra features tailored specifically to streaming usecases.</p> <p>Primarily, these are:  - Sources which are used to stream records into RisingWave from streaming sources like Kafka  - Sinks which are used to write the results of data processed by RisingWave to an external target, such as an Apache Iceberg table in object storage.</p> <p>RisingWave exposes these features via normal SQL statements, namely <code>CREATE SOURCE</code> and <code>CREATE SINK</code>. To utilize these in Vulcan, you can use them in pre / post statements.</p> <p>Here is an example of creating a Sink from a Vulcan model using a post statement:</p> <pre><code>MODEL (\n    name vulcan_example.view_model,\n    kind VIEW (\n      materialized true\n    )\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id;\n\nCREATE\n  SINK IF NOT EXISTS kafka_sink\nFROM\n  @this_model\nWITH (\n  connector='kafka',\n  \"properties.bootstrap.server\"='localhost:9092',\n  topic='test1',\n)\nFORMAT PLAIN\nENCODE JSON (force_append_only=true);\n</code></pre> <p>@this_model</p> <p>The <code>@this_model</code> macro resolves to the physical table for the current version of the model. See here for more information.</p>"},{"location":"references/integrations/engines/snowflake/","title":"Snowflake","text":""},{"location":"references/integrations/engines/snowflake/#snowflake","title":"Snowflake","text":"<p>This page provides information about how to use Vulcan with the Snowflake SQL engine.</p> <p>It begins with a Connection Quickstart that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the built-in.</p>"},{"location":"references/integrations/engines/snowflake/#connection-quickstart","title":"Connection quickstart","text":"<p>Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.</p> <p>It demonstrates connecting to Snowflake with the <code>snowflake-connector-python</code> library bundled with Vulcan.</p> <p>Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are described below.</p> <p>Tip</p> <p>This quickstart assumes you are familiar with basic Vulcan commands and functionality.</p> <p>If you're not, work through the Vulcan Quickstart before continuing!</p>"},{"location":"references/integrations/engines/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before working through this connection quickstart, ensure that:</p> <ol> <li>You have a Snowflake account and know your username and password</li> <li>Your Snowflake account has at least one warehouse available for running computations</li> <li>Your computer has Vulcan installed with the Snowflake extra available<ul> <li>Install from the command line with the command <code>pip install \"vulcan[snowflake]\"</code></li> </ul> </li> <li>You have initialized a Vulcan example project on your computer<ul> <li>Open a command line interface and navigate to the directory where the project files should go</li> <li>Initialize the project with the command <code>vulcan init snowflake</code></li> </ul> </li> </ol>"},{"location":"references/integrations/engines/snowflake/#access-control-permissions","title":"Access control permissions","text":"<p>Vulcan must have sufficient permissions to create and access different types of database objects.</p> <p>Vulcan's core functionality requires relatively broad permissions, including:</p> <ol> <li>Ability to create and delete schemas in a database</li> <li>Ability to create, modify, delete, and query tables and views in the schemas it creates</li> </ol> <p>If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.</p> <p>We now describe how to grant Vulcan appropriate permissions.</p>"},{"location":"references/integrations/engines/snowflake/#snowflake-roles","title":"Snowflake roles","text":"<p>Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a \"role\" that you then grant to the user.</p> <p>Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.</p> <p>The role must be granted <code>USAGE</code> on a warehouse so it can execute computations. We describe other permissions below.</p>"},{"location":"references/integrations/engines/snowflake/#database-permissions","title":"Database permissions","text":"<p>The top-level object container in Snowflake is a \"database\" (often called a \"catalog\" in other engines). Vulcan does not need permission to create databases; it may use an existing one.</p> <p>The simplest way to grant Vulcan sufficient permissions for a database is to give it <code>OWNERSHIP</code> of the database, which includes all the necessary permissions.</p> <p>Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.</p>"},{"location":"references/integrations/engines/snowflake/#granting-the-permissions","title":"Granting the permissions","text":"<p>This section provides example code for creating a <code>vulcan</code> role, granting it sufficient permissions, and granting it to a user.</p> <p>The code must be executed by a user with <code>USERADMIN</code> level permissions or higher. We provide two versions of the code, one that grants database <code>OWNERSHIP</code> to the role and another that does not.</p> <p>Both examples create a role named <code>vulcan</code>, grant it usage of the warehouse <code>compute_wh</code>, create a database named <code>demo_db</code>, and assign the role to the user <code>demo_user</code>. The step that creates the database can be omitted if the database already exists.</p> With database ownershipWithout database ownership <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\n\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\nGRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre> <pre><code>USE ROLE useradmin; -- This code requires USERADMIN privileges or higher\n\nCREATE ROLE vulcan; -- Create role for permissions\nCREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)\n\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse\nGRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database\n\nGRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates\nGRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas\nGRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas\nGRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas\nGRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas\n\nGRANT ROLE vulcan TO USER demo_user; -- Grant role to user\nALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#get-connection-info","title":"Get connection info","text":"<p>Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.</p>"},{"location":"references/integrations/engines/snowflake/#account-name","title":"Account name","text":"<p>Snowflake connection configurations require the <code>account</code> parameter that identifies the Snowflake account Vulcan should connect to.</p> <p>Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a <code>/</code>.</p> <p>This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:</p> <p></p> <p>In this example, our organization name is <code>idapznw</code>, and our account name is <code>wq29399</code>.</p> <p>We concatenate the two components, separated by a <code>-</code>, for the Vulcan <code>account</code> parameter: <code>idapznw-wq29399</code>.</p>"},{"location":"references/integrations/engines/snowflake/#warehouse-name","title":"Warehouse name","text":"<p>Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.</p> <p>Some Snowflake user accounts may have a default warehouse they automatically use when connecting.</p> <p>The connection configuration's <code>warehouse</code> parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.</p>"},{"location":"references/integrations/engines/snowflake/#database-name","title":"Database name","text":"<p>Snowflake user accounts may have a \"Default Namespace\" that includes a default database they automatically use when connecting.</p> <p>The connection configuration's <code>database</code> parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.</p>"},{"location":"references/integrations/engines/snowflake/#configure-the-connection","title":"Configure the connection","text":"<p>We now have the information we need to configure Vulcan's connection to Snowflake.</p> <p>We start the configuration by adding a gateway named <code>snowflake</code> to our example project's config.yaml file and making it our <code>default_gateway</code>:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>And we specify the <code>account</code>, <code>user</code>, <code>password</code>, <code>database</code>, and <code>warehouse</code> connection parameters using the information from above:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Warning</p> <p>Best practice for storing secrets like passwords is placing them in environment variables that the configuration file loads dynamically. For simplicity, this guide instead places the value directly in the configuration file.</p> <p>This code demonstrates how to use the environment variable <code>SNOWFLAKE_PASSWORD</code> for the configuration's <code>password</code> parameter:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      password: {{ env_var('SNOWFLAKE_PASSWORD') }}\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#check-connection","title":"Check connection","text":"<p>We have now specified the <code>snowflake</code> gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the <code>vulcan info</code> command.</p> <p>First, open a command line terminal. Now enter the command <code>vulcan info</code>:</p> <p></p> <p>The output shows that our data warehouse connection succeeded:</p> <p></p> <p>However, the output includes a <code>WARNING</code> about using the Snowflake SQL engine for storing Vulcan state:</p> <p></p> <p>Warning</p> <p>Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.</p> <p>Learn more about storing Vulcan state here.</p>"},{"location":"references/integrations/engines/snowflake/#specify-state-connection","title":"Specify state connection","text":"<p>We can store Vulcan state in a different SQL engine by specifying a <code>state_connection</code> in our <code>snowflake</code> gateway.</p> <p>This example uses the DuckDB engine to store state in the local <code>snowflake_state.db</code> file:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: idapznw-wq29399\n      user: DEMO_USER\n      password: &lt;&lt; your password here &gt;&gt;\n      database: DEMO_DB\n      warehouse: COMPUTE_WH\n    state_connection:\n      type: duckdb\n      database: snowflake_state.db\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2024-07-24\n</code></pre> <p>Now we no longer see the warning when running <code>vulcan info</code>, and we see a new entry <code>State backend connection succeeded</code>:</p> <p></p>"},{"location":"references/integrations/engines/snowflake/#run-a-vulcan-plan","title":"Run a <code>vulcan plan</code>","text":"<p>Now we're ready to run a <code>vulcan plan</code> in Snowflake:</p> <p></p> <p>And confirm that our schemas and objects exist in the Snowflake catalog:</p> <p></p> <p>Congratulations - your Vulcan project is up and running on Snowflake!</p>"},{"location":"references/integrations/engines/snowflake/#where-are-the-row-counts","title":"Where are the row counts?","text":"<p>Vulcan reports the number of rows processed by each model in its <code>plan</code> and <code>run</code> terminal output.</p> <p>However, due to limitations in the Snowflake Python connector, row counts cannot be determined for <code>CREATE TABLE AS</code> statements. Therefore, Vulcan does not report row counts for certain model kinds, such as <code>FULL</code> models.</p> <p>Learn more about the connector limitation on Github.</p>"},{"location":"references/integrations/engines/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"references/integrations/engines/snowflake/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[snowflake]\"\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account name string Y <code>user</code> The Snowflake username string N <code>password</code> The Snowflake password string N <code>authenticator</code> The Snowflake authenticator method string N <code>warehouse</code> The Snowflake warehouse name string N <code>database</code> The Snowflake database name string N <code>role</code> The Snowflake role name string N <code>token</code> The Snowflake OAuth 2.0 access token string N <code>private_key</code> The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). string N <code>private_key_path</code> The optional path to the private key to use for authentication. This would be used instead of <code>private_key</code>. string N <code>private_key_passphrase</code> The optional passphrase to use to decrypt <code>private_key</code> (if in PEM format) or <code>private_key_path</code>. Keys can be created without encryption so only provide this if needed. string N <code>session_parameters</code> The optional session parameters to set for the connection. dict N"},{"location":"references/integrations/engines/snowflake/#lowercase-object-names","title":"Lowercase object names","text":"<p>Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command <code>CREATE SCHEMA vulcan</code> will generate a schema named <code>VULCAN</code> in Snowflake.</p> <p>If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.</p> <p>For example, a connection to the database <code>\"my_db\"</code> would include:</p> <pre><code>connection:\n  type: snowflake\n  &lt;other connection options&gt;\n  database: '\"my_db\"' # outer single and inner double quotes\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-authorization-methods","title":"Snowflake authorization methods","text":"<p>The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.</p> <p>This section describes how to configure other authorization methods.</p>"},{"location":"references/integrations/engines/snowflake/#snowflake-sso-authorization","title":"Snowflake SSO Authorization","text":"<p>Vulcan supports Snowflake SSO authorization connections using the <code>externalbrowser</code> authenticator method. For example:</p> <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: ************\n      user: ************\n      authenticator: externalbrowser\n      warehouse: ************\n      database: ************\n      role: ************\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-oauth-authorization","title":"Snowflake OAuth Authorization","text":"<p>Vulcan supports Snowflake OAuth authorization connections using the <code>oauth</code> authenticator method. For example:</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      authenticator: oauth\n      token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                authenticator=\"oauth\",\n                token=\"eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#snowflake-private-key-authorization","title":"Snowflake Private Key Authorization","text":"<p>Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).</p> <p>The <code>account</code> and <code>user</code> parameters are required for each of these methods.</p> <p>Private Key Path</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key_path: '/path/to/key.key'\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key_path=\"/path/to/key.key\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key PEM</p> <p>Note: <code>private_key_passphrase</code> is only needed if the key was encrypted with a passphrase.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: |\n        -----BEGIN PRIVATE KEY-----\n        ...\n        -----END PRIVATE KEY-----\n      private_key_passphrase: supersecret\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"\"\"\n                -----BEGIN PRIVATE KEY-----\n                ...\n                -----END PRIVATE KEY-----\"\"\",\n                private_key_passphrase=\"supersecret\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#private-key-base64","title":"Private Key Base64","text":"<p>Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.</p> YAMLPython <pre><code>gateways:\n  snowflake:\n    connection:\n      type: snowflake\n      account: account\n      user: user\n      private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='\n</code></pre> <pre><code>config = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=\"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=\",\n            ),\n        ),\n    }\n)\n</code></pre> <p>Private Key Bytes</p> YAMLPython <p>Base64 encode the bytes and follow Private Key Base64 instructions.</p> <pre><code>from vulcan.core.config import (\n    Config,\n    GatewayConfig,\n    ModelDefaultsConfig,\n    SnowflakeConnectionConfig,\n)\n\nfrom cryptography.hazmat.primitives import serialization\n\nkey = \"\"\"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\"\"\".encode()\n\np_key= serialization.load_pem_private_key(key, password=None)\n\npkb = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(dialect=\"snowflake\"),\n    gateways={\n       \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"user\",\n                account=\"account\",\n                private_key=pkb,\n            ),\n        ),\n    }\n)\n</code></pre> <p>The authenticator method is assumed to be <code>snowflake_jwt</code> when <code>private_key</code> is provided, but it can also be explicitly provided in the connection configuration.</p>"},{"location":"references/integrations/engines/snowflake/#configuring-virtual-warehouses","title":"Configuring Virtual Warehouses","text":"<p>The Snowflake Virtual Warehouse a model should use can be specified in the <code>session_properties</code> attribute of the model definition:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  session_properties (\n    'warehouse' = TEST_WAREHOUSE,\n  ),\n);\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#custom-view-and-table-types","title":"Custom View and Table types","text":"<p>Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the <code>physical_properties</code> and <code>virtual_properties</code> attributes respectively. For example:</p>"},{"location":"references/integrations/engines/snowflake/#secure-views","title":"Secure Views","text":"<p>A table can be exposed through a <code>SECURE</code> view in the virtual layer by specifying the <code>creatable_type</code> property and setting it to <code>SECURE</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  virtual_properties (\n      creatable_type = SECURE\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#transient-tables","title":"Transient Tables","text":"<p>A model can use a <code>TRANSIENT</code> table in the physical layer by specifying the <code>creatable_type</code> property and setting it to <code>TRANSIENT</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  physical_properties (\n      creatable_type = TRANSIENT\n  )\n);\n\nSELECT a FROM schema_name.model_b;\n</code></pre>"},{"location":"references/integrations/engines/snowflake/#iceberg-tables","title":"Iceberg Tables","text":"<p>In order for Snowflake to be able to create an Iceberg table, there must be an External Volume configured to store the Iceberg table data on.</p> <p>Once that is configured, you can create a model backed by an Iceberg table by using <code>table_format iceberg</code> like so:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = '&lt;external volume name&gt;'\n  )\n);\n</code></pre> <p>To prevent having to specify <code>catalog = 'snowflake'</code> and <code>external_volume = '&lt;external volume name&gt;'</code> on every model, see the Snowflake documentation for:</p> <ul> <li>Configuring a default Catalog</li> <li>Configuring a default External Volume</li> </ul> <p>Alternatively you can also use model defaults to set defaults at the Vulcan level instead.</p> <p>To utilize the wide variety of optional properties that Snowflake makes available for Iceberg tables, simply specify them as <code>physical_properties</code>:</p> <pre><code>MODEL (\n  name schema_name.model_name,\n  kind FULL,\n  table_format iceberg,\n  physical_properties (\n    catalog = 'snowflake',\n    external_volume = 'my_external_volume',\n    base_location = 'my/product_reviews/'\n  )\n);\n</code></pre> <p>External catalogs</p> <p>Setting <code>catalog = 'snowflake'</code> to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake does not support writing to Iceberg tables configured under external catalogs.</p> <p>You can however still reference a table from an external catalog in your model as a normal external table.</p>"},{"location":"references/integrations/engines/snowflake/#troubleshooting","title":"Troubleshooting","text":""},{"location":"references/integrations/engines/snowflake/#frequent-authentication-prompts","title":"Frequent Authentication Prompts","text":"<p>When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.</p> <p>To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:</p> <ul> <li>For general authentication, see Connection Caching Documentation</li> <li>For MFA specifically, see MFA Token Caching Documentation.</li> </ul>"},{"location":"references/integrations/engines/spark/","title":"Spark","text":""},{"location":"references/integrations/engines/spark/#spark","title":"Spark","text":""},{"location":"references/integrations/engines/spark/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>spark</code></p> <p>NOTE: Spark may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/spark/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>spark</code> string Y <code>config_dir</code> Value to set for <code>SPARK_CONFIG_DIR</code> string N <code>catalog</code> The catalog to use when issuing commands. See Catalog Support for details string N <code>config</code> Key/value pairs to set for the Spark Configuration. dict N"},{"location":"references/integrations/engines/spark/#catalog-support","title":"Catalog Support","text":"<p>Vulcan's Spark integration is only designed/tested with a single catalog usage in mind.  Therefore all Vulcan models must be defined with a single catalog.</p> <p>If <code>catalog</code> is not set, then the behavior changes based on spark release:</p> <ul> <li>If &gt;=3.4, then the default catalog is determined at runtime</li> <li>If &lt;3.4, then the default catalog is <code>spark_catalog</code> </li> </ul>"},{"location":"references/integrations/engines/trino/","title":"Trino","text":""},{"location":"references/integrations/engines/trino/#trino","title":"Trino","text":""},{"location":"references/integrations/engines/trino/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>trino</code></p> <p>NOTE: Trino may not be used for the Vulcan state connection.</p>"},{"location":"references/integrations/engines/trino/#installation","title":"Installation","text":"<pre><code>pip install \"vulcan[trino]\"\n</code></pre> <p>If you are using Oauth for Authentication, it is recommended to install keyring cache: </p><pre><code>pip install \"trino[external-authentication-token-cache]\"\n</code></pre><p></p>"},{"location":"references/integrations/engines/trino/#trino-connector-support","title":"Trino Connector Support","text":"<p>The trino engine adapter has been tested against the Hive Connector, Iceberg Connector, and Delta Lake Connector.</p> <p>Please let us know on Slack if you are wanting to use another connector or have tried another connector.</p>"},{"location":"references/integrations/engines/trino/#hive-connector-configuration","title":"Hive Connector Configuration","text":"<p>Recommended hive catalog properties configuration (<code>&lt;catalog_name&gt;.properties</code>):</p> <pre><code>hive.metastore-cache-ttl=0s\nhive.metastore-refresh-interval=5s\nhive.metastore-timeout=10s\nhive.allow-drop-table=true\nhive.allow-add-column=true\nhive.allow-drop-column=true\nhive.allow-rename-column=true\nhive.allow-rename-table=true\n</code></pre>"},{"location":"references/integrations/engines/trino/#iceberg-connector-configuration","title":"Iceberg Connector Configuration","text":"<p>If you're using a hive metastore for the Iceberg catalog, the properties are mostly the same as the Hive connector.</p> <pre><code>iceberg.catalog.type=hive_metastore\n# metastore properties as per the Hive Connector Configuration above\n</code></pre> <p>Note: The Trino Iceberg Connector must be configured with an <code>iceberg.catalog.type</code> that supports views. At the time of this writing, this is <code>hive_metastore</code>, <code>glue</code>, and <code>rest</code>.</p> <p>The <code>jdbc</code> and <code>nessie</code> iceberg catalog types do not support views and are thus incompatible with Vulcan.</p> <p>Nessie</p> <p>Nessie is supported when used as an Iceberg REST Catalog (<code>iceberg.catalog.type=rest</code>). For more information on how to configure the Trino Iceberg connector for this, see the Nessie documentation.</p>"},{"location":"references/integrations/engines/trino/#delta-lake-connector-configuration","title":"Delta Lake Connector Configuration","text":"<p>The Trino adapter Delta Lake connector has only been tested with the Hive metastore catalog type.</p> <p>The properties file must include the Hive metastore URI and catalog name in addition to any other general properties.</p> <pre><code>hive.metastore.uri=thrift://example.net:9083\ndelta.hive-catalog-name=datalake_delta # example catalog name, can be any valid string\n</code></pre>"},{"location":"references/integrations/engines/trino/#aws-glue","title":"AWS Glue","text":"<p>AWS Glue provides an implementation of the Hive metastore catalog.</p> <p>Your Trino project's physical data objects are stored in a specific location, such as an AWS S3 bucket. Hive provides a default location, which you can override in its configuration file.</p> <p>Set the default location for your project's tables in the Hive catalog configuration's <code>hive.metastore.glue.default-warehouse-dir</code> parameter.</p> <p>For example:</p> <pre><code>hive.metastore=glue\nhive.metastore.glue.default-warehouse-dir=s3://my-bucket/\n</code></pre>"},{"location":"references/integrations/engines/trino/#connection-options","title":"Connection options","text":"Option Description Type Required <code>type</code> Engine type name - must be <code>trino</code> string Y <code>user</code> The username (of the account) to log in to your cluster. When connecting to Starburst Galaxy clusters, you must include the role of the user as a suffix to the username. string Y <code>host</code> The hostname of your cluster. Don't include the <code>http://</code> or <code>https://</code> prefix. string Y <code>catalog</code> The name of a catalog in your cluster. string Y <code>http_scheme</code> The HTTP scheme to use when connecting to your cluster. By default, it's <code>https</code> and can only be <code>http</code> for no-auth or basic auth. string N <code>port</code> The port to connect to your cluster. By default, it's <code>443</code> for <code>https</code> scheme and <code>80</code> for <code>http</code> int N <code>roles</code> Mapping of catalog name to a role dict N <code>http_headers</code> Additional HTTP headers to send with each request. dict N <code>session_properties</code> Trino session properties. Run <code>SHOW SESSION</code> to see all options. dict N <code>retries</code> Number of retries to attempt when a request fails. Default: <code>3</code> int N <code>timezone</code> Timezone to use for the connection. Default: client-side local timezone string N <code>schema_location_mapping</code> A mapping of regex patterns to S3 locations to use for the <code>LOCATION</code> property when creating schemas. See Table and Schema locations for more details. dict N <code>catalog_type_overrides</code> A mapping of catalog names to their connector type. This is used to enable/disable connector specific behavior. See Catalog Type Overrides for more details. dict N"},{"location":"references/integrations/engines/trino/#table-and-schema-locations","title":"Table and Schema locations","text":"<p>When using connectors that are decoupled from their storage (such as the Iceberg, Hive or Delta connectors), when creating new tables Trino needs to know the location in the physical storage it should write the table data to.</p> <p>This location gets stored against the table in the metastore so that any engine trying to read the data knows where to look.</p>"},{"location":"references/integrations/engines/trino/#default-behaviour","title":"Default behaviour","text":"<p>Trino allows you to optionally configure a <code>default-warehouse-dir</code> property at the Metastore level. When creating objects, Trino will infer schema locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;</code> and table locations to be <code>&lt;default warehouse dir&gt;/&lt;schema name&gt;/&lt;table name&gt;</code>.</p> <p>However, if you dont set this property, Trino can still infer table locations if a schema location is explicitly set.</p> <p>For example, if you specify the <code>LOCATION</code> property when creating a schema like so:</p> <pre><code>CREATE SCHEMA staging_data\nWITH (LOCATION = 's3://warehouse/production/staging_data')\n</code></pre> <p>Then any tables created under that schema will have their location inferred as <code>&lt;schema location&gt;/&lt;table name&gt;</code>.</p> <p>If you specify neither a <code>default-warehouse-dir</code> in the metastore config nor a schema location when creating the schema, you must specify an explicit table location when creating the table or Trino will produce an error.</p> <p>Creating a table in a specific location is very similar to creating a schema in a specific location:</p> <pre><code>CREATE TABLE staging_data.customers (customer_id INT)\nWITH (LOCATION = 's3://warehouse/production/staging_data/customers')\n</code></pre>"},{"location":"references/integrations/engines/trino/#configuring-in-vulcan","title":"Configuring in Vulcan","text":"<p>Within Vulcan, you can configure the value to use for the <code>LOCATION</code> property when Vulcan creates tables and schemas. This overrides what Trino would have inferred based on the cluster configuration.</p>"},{"location":"references/integrations/engines/trino/#schemas","title":"Schemas","text":"<p>To configure the <code>LOCATION</code> property that Vulcan will specify when issuing <code>CREATE SCHEMA</code> statements, you can use the <code>schema_location_mapping</code> connection property. This applies to all schemas that Vulcan creates, including its internal ones.</p> <p>The simplest example is to emulate a <code>default-warehouse-dir</code>:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '.*': 's3://warehouse/production/@{schema_name}'\n</code></pre> <p>This will cause all schemas to get created with their location set to <code>s3://warehouse/production/&lt;schema name&gt;</code>. The table locations will be inferred by Trino as <code>s3://warehouse/production/&lt;schema name&gt;/&lt;table name&gt;</code> so all objects will effectively be created under <code>s3://warehouse/production/</code>.</p> <p>It's worth mentioning that if your models are using fully qualified three part names, eg <code>&lt;catalog&gt;.&lt;schema&gt;.&lt;name&gt;</code> then string being matched against the <code>schema_location_mapping</code> regex will be <code>&lt;catalog&gt;.&lt;schema&gt;</code> and not just the <code>&lt;schema&gt;</code> itself. This allows you to set different locations for the same schema name if that schema name is used across multiple catalogs.</p> <p>If your models are using two part names, eg <code>&lt;schema&gt;.&lt;table&gt;</code> then only the <code>&lt;schema&gt;</code> part will be matched against the regex.</p> <p>Here's an example:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      schema_location_mapping:\n        '^utils$': 's3://utils-bucket/@{schema_name}'\n        '^landing\\..*$': 's3://raw-data/@{catalog_name}/@{schema_name}'\n        '^staging.*$': 's3://bucket/@{schema_name}_dev'\n        '^vulcan.*$': 's3://vulcan-internal/dev/@{schema_name}'\n</code></pre> <p>This would perform the following mappings:</p> <ul> <li>a schema called <code>sales</code> would not be mapped to a location at all because it doesnt match any of the patterns. It would be created without a <code>LOCATION</code> property</li> <li>a schema called <code>utils</code> would be mapped to the location <code>s3://utils-bucket/utils</code> because it directly matches the <code>^utils$</code> pattern</li> <li>a schema called <code>transactions</code> in a catalog called <code>landing</code> would be mapped to the location <code>s3://raw-data/landing/transactions</code> because the string <code>landing.transactions</code> matches the <code>^landing\\..*$</code> pattern</li> <li>schemas called <code>staging_customers</code> and <code>staging_accounts</code> would be mapped to the locations <code>s3://bucket/staging_customers_dev</code> and <code>s3://bucket/staging_accounts_dev</code> respectively because they match the <code>^staging.*$</code> pattern</li> <li>a schema called <code>accounts</code> in a catalog called <code>staging</code> would be mapped to the location <code>s3://bucket/accounts_dev</code> because the string <code>staging.accounts</code> matches the <code>^staging.*$</code> pattern</li> <li>schemas called <code>vulcan__staging_customers</code> and <code>vulcan__staging_utils</code> would be mapped to the locations <code>s3://vulcan-internal/dev/vulcan__staging_customers</code> and <code>s3://vulcan-internal/dev/vulcan__staging_utils</code> respectively because they match the <code>^vulcan.*$</code> pattern</li> </ul> <p>Placeholders</p> <p>You may use the <code>@{catalog_name}</code> and <code>@{schema_name}</code> placeholders in the mapping value.</p> <p>If there is a match on one of the patterns then the catalog / schema that Vulcan is about to use in the <code>CREATE SCHEMA</code> statement will be substituted into these placeholders.</p> <p>Note the use of curly brace syntax <code>@{}</code> when referencing these placeholders - learn more here.</p>"},{"location":"references/integrations/engines/trino/#tables","title":"Tables","text":"<p>Often, you don't need to configure an explicit table location because if you have configured explicit schema locations, table locations are automatically inferred by Trino to be a subdirectory under the schema location.</p> <p>However, if you need to, you can configure an explicit table location by adding a <code>location</code> property to the model <code>physical_properties</code>.</p> <p>Note that you need to use the @resolve_template macro to generate a unique table location for each model version. Otherwise, all model versions will be written to the same location and clobber each other.</p> <pre><code>MODEL (\n  name staging.customers,\n  kind FULL,\n  physical_properties (\n    location = @resolve_template('s3://warehouse/@{catalog_name}/@{schema_name}/@{table_name}')\n  )\n);\n\nSELECT ...\n</code></pre> <p>This will cause Vulcan to set the specified <code>LOCATION</code> when issuing a <code>CREATE TABLE</code> statement.</p>"},{"location":"references/integrations/engines/trino/#catalog-type-overrides","title":"Catalog Type Overrides","text":"<p>Vulcan attempts to determine the connector type of a catalog by querying the <code>system.metadata.catalogs</code> table and checking the <code>connector_name</code> column. It checks if the connector name is <code>hive</code> for Hive connector behavior or contains <code>iceberg</code> or <code>delta_lake</code> for Iceberg or Delta Lake connector behavior respectively. However, the connector name may not always be a reliable way to determine the connector type, for example when using a custom connector or a fork of an existing connector. To handle such cases, you can use the <code>catalog_type_overrides</code> connection property to explicitly specify the connector type for specific catalogs. For example, to specify that the <code>datalake</code> catalog is using the Iceberg connector and the <code>analytics</code> catalog is using the Hive connector, you can configure the connection as follows:</p> config.yaml<pre><code>gateways:\n  trino:\n    connection:\n      type: trino\n      ...\n      catalog_type_overrides:\n        datalake: iceberg\n        analytics: hive\n</code></pre>"},{"location":"references/integrations/engines/trino/#authentication","title":"Authentication","text":"No AuthBasic AuthLDAPKerberosJWTCertificateOauth Option Description Type Required <code>method</code> <code>no-auth</code> (Default) string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    user: [user]\n    host: [host]\n    catalog: [catalog]\n    # Most likely you will want http for scheme when not using auth\n    http_scheme: http\n</code></pre> Option Description Type Required <code>method</code> <code>basic</code> string Y <code>password</code> The password to use when authenticating. string Y <code>verify</code> Boolean flag for whether SSL verification should occur. Default: trinodb Python client default (<code>true</code> as of this writing) bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: basic\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on Basic Authentication</li> <li>Python Client Basic Authentication</li> </ul> Option Description Type Required <code>method</code> <code>ldap</code> string Y <code>password</code> The password to use when authenticating. string Y <code>impersonation_user</code> Override the provided username. This lets you impersonate another user. string N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: ldap\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on LDAP Authentication</li> <li>Python Client LDAP Authentication</li> </ul> Option Description Type Required <code>method</code> <code>kerberos</code> string Y <code>keytab</code> Path to keytab. Ex: <code>/tmp/trino.keytab</code> string Y <code>krb5_config</code> Path to config. Ex: <code>/tmp/krb5.conf</code> string Y <code>principal</code> Principal.  Ex: <code>user@company.com</code> string Y <code>service_name</code> Service name (default is <code>trino</code>) string N <code>hostname_override</code> Kerberos hostname for a host whose DNS name doesn't match string N <code>mutual_authentication</code> Boolean flag for mutual authentication. Default: <code>false</code> bool N <code>force_preemptive</code> Boolean flag to preemptively initiate the Kerberos GSS exchange. Default: <code>false</code> bool N <code>sanitize_mutual_error_response</code> Boolean flag to strip content and headers from error responses. Default: <code>true</code> bool N <code>delegate</code> Boolean flag for credential delegation (<code>GSS_C_DELEG_FLAG</code>). Default: <code>false</code> bool N <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: kerberos\n    user: user\n    keytab: /tmp/trino.keytab\n    krb5_config: /tmp/krb5.conf\n    principal: trino@company.com\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Kerberos Authentication</li> <li>Python Client Kerberos Authentication</li> </ul> Option Description Type Required <code>method</code> <code>jwt</code> string Y <code>jwt_token</code> The JWT string. string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: jwt\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n</code></pre> <ul> <li>Trino Documentation on JWT Authentication</li> <li>Python Client JWT Authentication</li> </ul> Option Description Type Required <code>method</code> <code>certificate</code> string Y <code>cert</code> The full path to a certificate file string Y <code>client_certificate</code> Path to client certificate. Ex: <code>/tmp/client.crt</code> string Y <code>client_private_key</code> Path to client private key. Ex: <code>/tmp/client.key</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: certificate\n    user: [user]\n    password: [password]\n    host: [host]\n    catalog: [catalog]\n    cert: [path/to/cert_file]\n    client_certificate: [path/to/client/cert]\n    client_private_key: [path/to/client/key]\n</code></pre> Option Description Type Required <code>method</code> <code>oauth</code> string Y <pre><code>gateway_name:\n  connection:\n    type: trino\n    method: oauth\n    host: trino.company.com\n    catalog: datalake\n</code></pre> <ul> <li>Trino Documentation on Oauth Authentication</li> <li>Python Client Oauth Authentication</li> </ul>"}]}