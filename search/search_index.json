{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/_]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About","text":""},{"location":"#about","title":"About","text":"<p>Stop guessing why your production pipeline failed at 3 AM. Vulcan is a complete stack for building data products that gives you visibility into what's happening at every step, with built-in validation that blocks bad data before it hits production.</p> <p>You write SQL or Python models in your Vulcan project. Most teams start with SQL for transformations, then add Python models when they hit logic that's painful to express in SQL: calling external APIs, running machine learning models, or handling complex business rules. Mix both in the same project. Vulcan handles CI/CD, testing, data quality, and API generation. Same stack whether you're processing thousands or billions of rows.</p> <pre><code>graph LR\n    subgraph VulcanStack [THE VULCAN STACK]\n        Start[Write SQL or Python Models] --&gt; Linter[Linter: Code Safety]\n\n        Linter --&gt; ModelCore((Model Definition))\n\n        ModelCore --&gt; Tests[Automated Tests&lt;br/&gt;validate if desired output&lt;br/&gt;can be achieved]\n        Tests --&gt; Signals[Signals&lt;br/&gt;validate if dependencies&lt;br/&gt;are met]\n        Signals --&gt; ModelCore\n\n        ModelCore --&gt; AC[Assertions &amp; Checks&lt;br/&gt;validate data quality]\n        AC --&gt; Semantics[Semantics: &lt;br&gt; Dimensions &amp; Metrics]\n    end\n\n    Semantics --&gt; APIs[Automatic APIs: &lt;br&gt; REST &amp; Graph]\n    Semantics --&gt; Warehouse[(Postgres or Snowflake)]\n\n    APIs --&gt; Products[Production Data Products]\n    Warehouse --&gt; Products\n\n    style VulcanStack fill:#f0f4f8,stroke:#01579b,stroke-width:2px\n    style Products fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n    style Semantics fill:#fff9c4,stroke:#fbc02d,stroke-width:2px\n    style ModelCore fill:#e1f5fe,stroke:#0277bd,stroke-width:2px</code></pre> How to read this diagram  Read the diagram left to right. Here's what each part does:      - Write SQL or Python models in your `models/` directory. Most teams start with SQL, then add Python when they hit logic that's painful in SQL: API calls, machine learning, or complex business rules.      - Linter catches syntax errors before your code runs. This stops bad SQL or Python from hitting your warehouse.      - Model Definition is where your validated model becomes a defined entity. This is the central hub where the validation loop happens.      - Automated Tests run first to check if your model can produce the desired output. This happens before any data processing.      - Signals check if all dependencies are met. If a model depends on upstream data or external services, signals verify they're available. Only then does the loop feed back to Model Definition.      - Assertions and Checks validate data quality. Bad data gets blocked here, before it reaches your semantic layer or warehouse.      - Semantics layer only powers up after models pass validation. This is where you define business metrics and dimensions. It's your single source of truth, but only because everything feeding it has been validated.      - Automatic APIs generate from your semantic layer. REST, Python, and Graph APIs appear without manual code. Business users get self-service analytics, developers get programmatic access.      - Postgres or Snowflake receives error-free data. The validation loop ensures only clean data reaches production.      - Production Data Products are what business users and developers actually access. Everything flows here after passing through the validation gauntlet."},{"location":"#what-you-get","title":"What you get","text":"<p>SQL or Python: Write transformations in either language, or mix both in the same project. Use SQL for heavy-lifting transformations, switch to Python for complex logic. No new languages to learn.</p> <p>Visibility and control: The linter catches syntax errors before deployment. CLI commands show you exactly what's running, what's queued, and what failed. You intercept errors in your Postgres or Snowflake environment before they trigger notifications. See what breaks before you break it.</p> <p>Ship without errors: Assertions block bad data at the door. Tests run locally without touching your warehouse, so you get fast feedback without warehouse costs. CI/CD plans changes and shows you the full impact before deploying. Review what changed, approve when ready, roll back if something goes wrong.</p> <p>Why bother with Semantic Models? Because it's the difference between a messy data warehouse and a single source of truth for your entire business metrics. Define metrics and dimensions once in your semantic layer. Vulcan generates REST, Python, and Graph APIs automatically. No manual API code needed. Business users get self-service analytics through the semantic layer, developers get programmatic access through the APIs.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>The quickstart guide walks you through setting up Vulcan and creating your first project. By the end, you'll have implemented your first audit or data quality check. That's the difference between just installing Vulcan and actually getting value from it.</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#vulcan-terminology-guide","title":"Vulcan Terminology Guide","text":"<p>When you're working with Vulcan, you'll encounter terms that might be unfamiliar. This guide explains what they mean and why they matter. Each term links to detailed documentation where you can dive deeper.</p>"},{"location":"glossary/#model-terms","title":"Model Terms","text":"<p>These control how your models behave and how Vulcan processes them.</p> Term Definition Documentation Grain The column or columns that make each row unique. Like declaring a primary key, but Vulcan uses this for table comparisons and semantic layer joins. Model Properties References Foreign key columns that tell Vulcan how models relate to each other. The semantic layer uses these to detect joins automatically. Model Properties Model Kind How Vulcan processes your data. FULL rebuilds everything, INCREMENTAL only processes new intervals, VIEW computes on-demand. Model Kinds Cron Schedule expression that sets when your model runs. Use <code>@daily</code> for nightly refreshes or standard cron syntax for custom schedules. Model Properties Interval A time period Vulcan tracks for incremental models. Each day, week, or hour becomes an interval that gets processed independently. Incremental Models Backfill Loading historical data when you first create a model or after changing logic. Vulcan figures out which intervals need processing. Plans Lookback How many intervals to reprocess for late-arriving data. Handles events that show up after their time window already passed. Model Kinds Forward-only Models that never rebuild historical data. Use when past data is immutable or reprocessing costs too much. Model Kinds Owner Team or person responsible for a model. Used for notifications and knowing who to contact when something breaks. Model Properties Description Human-readable explanation of what your model does. Vulcan registers this as a table comment, so it shows up in BI tools. Model Properties Depends On Explicit dependency list. Required for Python models since Vulcan can't auto-detect dependencies from Python code. Model Properties"},{"location":"glossary/#execution-terms","title":"Execution Terms","text":"<p>These describe how Vulcan applies changes and processes data.</p> Term Definition Documentation Plan Summary of what will change before you deploy. Shows affected models, intervals that need processing, and the full impact of your changes. Plans Run Scheduled execution that processes new data intervals. Different from plan: plan applies code changes, run handles regular data processing. Run and Scheduling Virtual Environment Isolated testing space that can share tables when safe. Test changes without touching production, roll back if needed. Plans Virtual Layer Views that point to physical tables. This is what you interact with most of the time, not the raw storage underneath. Plans Physical Layer Actual database tables and materialized views where your data lives. Vulcan manages this automatically. Plans"},{"location":"glossary/#semantic-layer-terms","title":"Semantic Layer Terms","text":"<p>These transform your technical tables into something business users can query without SQL.</p> Term Definition Documentation Semantic Layer Translation layer that maps physical models to business concepts. Turns <code>analytics.daily_revenue_metrics</code> into \"Monthly Revenue by Customer Tier\" that anyone can query. Semantic Layer Overview Semantic Model Maps a physical Vulcan model to a semantic representation. Provides business aliases and exposes dimensions, measures, segments, and joins. Semantic Models Dimensions Attributes for grouping and filtering. Answer \"by what?\" questions. Your model columns become dimensions automatically. Semantic Models Measures Aggregated calculations. Answer \"how much?\" or \"how many?\" using SQL expressions like <code>SUM(amount)</code> or <code>COUNT(*)</code>. Semantic Models Segments Reusable filter conditions. Define subsets like \"active customers\" or \"high-value orders\" that you can reuse across queries. Semantic Models Joins Relationships between semantic models. Enable cross-model analysis, like combining order data with customer data automatically. Semantic Models Business Metrics Complete analytical definitions that combine measures with dimensions and time. Ready for time-series analysis and automatic API generation. Business Metrics Proxy Dimensions Expose measures from joined models as dimensions. Lets you filter or group by aggregated values from other models. Semantic Models Transpilation Convert semantic queries to executable SQL. Validate and debug business-friendly queries before they hit your database. Transpiling Semantics"},{"location":"glossary/#data-quality-terms","title":"Data Quality Terms","text":"<p>These ensure data integrity. The difference between catching problems early and discovering them at 3 AM.</p> Term Definition Documentation Assertions / Audits SQL queries that validate data after execution. Always blocking: if they find bad data, execution stops immediately. Audits Checks Quality monitoring rules configured in YAML. Non-blocking validation that tracks trends and detects anomalies over time. Data Quality"},{"location":"glossary/#architecture-terms","title":"Architecture Terms","text":"<p>These describe how Vulcan structures and tracks your pipeline.</p> Term Definition Documentation DAG Directed Acyclic Graph. Structure Vulcan uses to track model dependencies and figure out the correct execution order. DAG Lineage Visualization of how data flows from sources to consumption. See how changes propagate through your pipeline. Lineage"},{"location":"glossary/#how-these-terms-fit-together","title":"How These Terms Fit Together","text":"<p>You don't need to memorize everything. Here's how these concepts connect in practice:</p> <p>\u2192 Models define your logic</p> <p>\u2192 Grain documents structure</p> <p>\u2192 Model Kind determines processing</p> <p>\u2192 Cron sets schedule</p> <p>\u2192 Plan reviews changes</p> <p>\u2192 Run processes data</p> <p>\u2192 Assertions validate quality</p> <p>\u2192 Semantic Layer exposes business interface</p> <p>\u2192 Dimensions and Measures enable self-service analytics</p> <p>Each term solves a specific problem. Grain helps with comparisons. References enable automatic joins. Model Kinds optimize performance. The Semantic Layer makes data accessible to non-technical users.</p> <p>Click any documentation link above for detailed guides with examples, best practices, and advanced patterns.</p>"},{"location":"ci-cd/ci-cd/","title":"CI/CD","text":""},{"location":"ci-cd/ci-cd/#cicd","title":"CI/CD","text":"<p>Coming soon...</p>"},{"location":"cli-commands/cli/","title":"CLI Commands","text":""},{"location":"cli-commands/cli/#cli-commands","title":"CLI Commands","text":"<p>The Vulcan CLI is your primary interface for working with your data pipeline. You'll use it to plan changes, run models, check data quality, and manage your project. Here's everything you need to know about the commands available to you.</p> <pre><code>Usage: vulcan [OPTIONS] COMMAND [ARGS]...\n\n  Vulcan command line tool.\n\nOptions:\n  --version            Show the version and exit.\n  -p, --paths TEXT     Path(s) to the Vulcan config/project.\n  --config TEXT        Name of the config object. Only applicable to\n                       configuration defined using Python script.\n  --gateway TEXT       The name of the gateway.\n  --ignore-warnings    Ignore warnings.\n  --debug              Enable debug mode.\n  --log-to-stdout      Display logs in stdout.\n  --log-file-dir TEXT  The directory to write log files to.\n  --dotenv PATH        Path to a custom .env file to load environment\n                       variables.\n  --help               Show this message and exit.\n\nCommands:\n  api                     Start the Vulcan API server (models, metrics,...\n  audit                   Run audits for the target model(s).\n  check_intervals         Show missing intervals in an environment,...\n  clean                   Clears the Vulcan cache and any build artifacts.\n  create_external_models  Create a schema file containing external model...\n  create_test             Generate a unit test fixture for a given model.\n  dag                     Render the DAG as an html file.\n  destroy                 The destroy command removes all project resources.\n  diff                    Show the diff between the local state and the...\n  dlt_refresh             Attaches to a DLT pipeline with the option to...\n  environments            Prints the list of Vulcan environments with its...\n  evaluate                Evaluate a model and return a dataframe with a...\n  fetchdf                 Run a SQL query and display the results.\n  format                  Format all SQL models and audits.\n  graphql                 Manage the GraphQL service (subcommands: up,...\n  info                    Print information about a Vulcan project.\n  invalidate              Invalidate the target environment, forcing its...\n  janitor                 Run the janitor process on-demand.\n  lint                    Run the linter for the target model(s).\n  migrate                 Migrate Vulcan to the current running version.\n  plan                    Apply local changes to the target environment.\n  render                  Render a model's query, optionally expanding...\n  rollback                Rollback Vulcan to the previous migration.\n  run                     Evaluate missing intervals for the target...\n  semantic                Semantic layer operations.\n  state                   Commands for interacting with state\n  table_diff              Show the diff between two tables or a selection...\n  table_name              Prints the name of the physical table for the...\n  test                    Run model unit tests.\n  transpile               Transpile a semantic SQL or REST-style semantic...\n  transpiler              Manage the Transpiler service (subcommands: up,...\n</code></pre>"},{"location":"cli-commands/cli/#audit","title":"audit","text":"<p>Run data quality audits for your models. This command executes all the audits you've defined in your models and reports which ones pass or fail. It's perfect for validating data quality before deploying changes.</p> <pre><code>Usage: vulcan audit [OPTIONS]\n\n  Run audits for the target model(s).\n\nOptions:\n  --model TEXT           A model to audit. Multiple models can be audited.\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan audit\n  Found 11 audit(s).\n  unique_values on model sales.daily_sales \u2705 PASS.\n  not_null on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  positive_values on model sales.daily_sales \u2705 PASS.\n  unique_values on model raw.raw_products \u2705 PASS.\n  not_null on model raw.raw_products \u2705 PASS.\n  unique_values on model raw.raw_customers \u2705 PASS.\n  not_null on model raw.raw_customers \u2705 PASS.\n  unique_values on model raw.raw_orders \u2705 PASS.\n  not_null on model raw.raw_orders \u2705 PASS.\n  positive_values on model raw.raw_orders \u2705 PASS.\n\n  Finished with 0 audit errors and 0 audits skipped.\n  Done.\n</code></pre>"},{"location":"cli-commands/cli/#check_intervals","title":"check_intervals","text":"<p>Check which time intervals are missing for your models in a given environment. This is super useful for understanding what data needs to be backfilled or processed. By default, it respects signals (like upstream dependencies), but you can disable that if you want to see all missing intervals.</p> <pre><code>Usage: vulcan check_intervals [OPTIONS] [ENVIRONMENT]\n\n  Show missing intervals in an environment, respecting signals.\n\nOptions:\n  --no-signals         Disable signal checks and only show missing intervals.\n  --select-model TEXT  Select specific models to show missing intervals for.\n  -s, --start TEXT     The start datetime of the interval for which this\n                       command will be applied.\n  -e, --end TEXT       The end datetime of the interval for which this command\n                       will be applied.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#clean","title":"clean","text":"<p>Clear out Vulcan's cache and any build artifacts. This is handy when you're troubleshooting issues or want to start fresh. Don't worry, it won't delete your models or data, just the cached files.</p> <pre><code>Usage: vulcan clean [OPTIONS]\n\n  Clears the Vulcan cache and any build artifacts.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#create_external_models","title":"create_external_models","text":"<p>Generate a schema file for external models that Vulcan can reference. This is useful when you're working with tables or views that exist outside of your Vulcan project but need to be referenced in your models.</p> <pre><code>Usage: vulcan create_external_models [OPTIONS]\n\n  Create a schema file containing external model schemas.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_external_models\n</code></pre>"},{"location":"cli-commands/cli/#create_test","title":"create_test","text":"<p>Generate a unit test fixture for a model. This command creates the test file structure and can even generate sample data based on queries you provide. It's a great way to quickly set up tests for your models without writing all the boilerplate yourself.</p> <pre><code>Usage: vulcan create_test [OPTIONS] MODEL\n\n  Generate a unit test fixture for a given model.\n\nOptions:\n  -q, --query &lt;TEXT TEXT&gt;...  Queries that will be used to generate data for\n                              the model's dependencies.\n  -o, --overwrite             When true, the fixture file will be overwritten\n                              in case it already exists.\n  -v, --var &lt;TEXT TEXT&gt;...    Key-value pairs that will define variables\n                              needed by the model.\n  -p, --path TEXT             The file path corresponding to the fixture,\n                              relative to the test directory. By default, the\n                              fixture will be created under the test directory\n                              and the file name will be inferred based on the\n                              test's name.\n  -n, --name TEXT             The name of the test that will be created. By\n                              default, it's inferred based on the model's\n                              name.\n  --include-ctes              When true, CTE fixtures will also be generated.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan create_test sales.daily_sales --query raw.raw_orders \"SELECT * FROM raw.raw_orders\"\n</code></pre>"},{"location":"cli-commands/cli/#dag","title":"dag","text":"<p>Generate a visual representation of your data pipeline's dependency graph (DAG) as an HTML file. This is super helpful for understanding how your models connect and visualizing the flow of data through your pipeline. You can open the HTML file in any browser to explore the graph interactively.</p> <pre><code>Usage: vulcan dag [OPTIONS] FILE\n\n  Render the DAG as an html file.\n\nOptions:\n  --select-model TEXT  Select specific models to include in the dag.\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan dag ./dag.html\n</code></pre>"},{"location":"cli-commands/cli/#destroy","title":"destroy","text":"<p>\u26a0\ufe0f Use with caution! This command permanently removes all Vulcan-managed resources from your data warehouse, including state tables, the cache, and all project resources. It will delete all tables, views, and schemas that Vulcan manages, as well as any external resources created by other tools within those schemas. This is a destructive operation that can't be undone, so make sure you really want to do this before running it.</p> <pre><code>Usage: vulcan destroy\n\n  Removes all state tables, the Vulcan cache and all project resources, including warehouse objects. This includes all tables, views and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.\n\nOptions:\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan destroy\n[WARNING] This will permanently delete all engine-managed objects, state tables and Vulcan cache.\nThe operation may disrupt any currently running or scheduled plans.\n\nSchemas to be deleted:\n  \u2022 warehouse.raw\n  \u2022 warehouse.sales\n\nSnapshot tables to be deleted:\n  \u2022 warehouse.vulcan__raw.raw__raw_customers__1474975870\n  \u2022 warehouse.vulcan__raw.raw__raw_orders__1032938324\n  \u2022 warehouse.vulcan__raw.raw__raw_products__3337559381\n  \u2022 warehouse.vulcan__sales.sales__daily_sales__2671854529\n\nThis action will DELETE ALL the above resources managed by Vulcan AND\npotentially external resources created by other tools in these schemas.\n\nAre you ABSOLUTELY SURE you want to proceed with deletion? [y/n]: y\nEnvironment 'prod' invalidated.\n\nDeleted object warehouse.raw\nDeleted object warehouse.sales\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381__dev\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529__dev\nDeleted object warehouse.vulcan__sales.sales__daily_sales__2671854529\nDeleted object warehouse.vulcan__raw.raw__raw_customers__1474975870\nDeleted object warehouse.vulcan__raw.raw__raw_products__3337559381\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324__dev\nDeleted object warehouse.vulcan__raw.raw__raw_orders__1032938324\nState tables removed.\nDestroy completed successfully.\n</code></pre>"},{"location":"cli-commands/cli/#dlt_refresh","title":"dlt_refresh","text":"<pre><code>Usage: dlt_refresh PIPELINE [OPTIONS]\n\n  Attaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.\n\nOptions:\n  -t, --table TEXT  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.\n  -f, --force       If set it will overwrite existing models with the new generated models from the DLT tables.\n  --help            Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#diff","title":"diff","text":"<p>See exactly what's different between your local project state and a target environment. This is super useful for understanding what changes you're about to deploy before running a plan. It shows you model changes, semantic layer changes, and quality check modifications in a clear diff format.</p> <pre><code>Usage: vulcan diff [OPTIONS] ENVIRONMENT\n\n  Show the diff between the local state and the target environment.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan diff prod\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n        --- .../daily_sales.sql\n\n        +++ .../daily_sales.sql\n\n        @@ -20,10 +20,11 @@\n\n          grains (order_date)\n        )\n        SELECT\n          CAST(order_date AS TIMESTAMP) AS order_date,\n          CAST(COUNT(order_id) AS INT) AS total_orders,\n          CAST(SUM(total_amount) AS DOUBLE PRECISION) AS total_revenue,\n        -  CAST(MAX(order_id) AS VARCHAR) AS last_order_id\n        +  CAST(MAX(order_id) AS VARCHAR) AS last_order_id,\n        +  COUNT(DISTINCT product_id) AS total_products\n        FROM raw.raw_orders\n        GROUP BY\n          order_date\nSemantics:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 semantic-model:sales.daily_sales\n    \u251c\u2500\u2500 semantic-metric:order_volume\n    \u2514\u2500\u2500 semantic-metric:revenue_trends\nQuality Checks:\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:accuracy\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:timeliness\n    \u251c\u2500\u2500 check-suite:sales.daily_sales:completeness\n    \u2514\u2500\u2500 check-suite:sales.daily_sales:validity\n</code></pre>"},{"location":"cli-commands/cli/#environments","title":"environments","text":"<p>List all your Vulcan environments and see when they expire. This is helpful for managing development environments and understanding which ones might need cleanup.</p> <pre><code>Usage: vulcan environments [OPTIONS]\n\n  Prints the list of Vulcan environments with its expiry datetime.\n\nOptions:\n  --help             Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan environments\nNumber of Vulcan environments are: 2\nprod - No Expiry\ndev - 2025-12-23 00:00:00\n</code></pre>"},{"location":"cli-commands/cli/#evaluate","title":"evaluate","text":"<p>Preview a model's output without actually materializing it. This is perfect for debugging and quick iteration, you can see what your model would produce without running a full plan or run. By default, it returns up to 1000 rows, but you can adjust that limit.</p> <pre><code>Usage: vulcan evaluate [OPTIONS] MODEL\n\n  Evaluate a model and return a dataframe with a default limit of 1000.\n\nOptions:\n  -s, --start TEXT       The start datetime of the interval for which this\n                         command will be applied.\n  -e, --end TEXT         The end datetime of the interval for which this\n                         command will be applied.\n  --execution-time TEXT  The execution time (defaults to now).\n  --limit INTEGER        The number of rows which the query should be limited\n                         to.\n  --help                 Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan evaluate sales.daily_sales\n   order_date  total_orders  total_revenue last_order_id  total_products\n0  2024-01-05             1          70.77          O001               1\n1  2024-01-10             1          44.22          O002               1\n2  2024-01-15             1          65.52          O003               1\n3  2024-01-20             1          79.42          O004               1\n4  2024-02-01             1          91.35          O005               1\n....\n19 2024-05-15             1          38.38          O020               1\n</code></pre>"},{"location":"cli-commands/cli/#fetchdf","title":"fetchdf","text":"<p>Run a raw SQL query against your data warehouse and see the results. This is handy for quick data exploration or debugging queries without opening a separate database client.</p> <pre><code>Usage: vulcan fetchdf [OPTIONS] SQL\n\n  Run a SQL query and display the results.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan fetchdf \"select count(*) from sales.daily_sales\"\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 count \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 20    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli-commands/cli/#format","title":"format","text":"<p>Automatically format all your SQL models and audits according to your formatting preferences. This helps keep your codebase consistent and readable. You can customize formatting options like indentation, comma placement, and function name casing.</p> <pre><code>Usage: vulcan format [OPTIONS]\n\n  Format all SQL models and audits.\n\nOptions:\n  -t, --transpile TEXT        Transpile project models to the specified\n                              dialect.\n  --append-newline            Include a newline at the end of each file.\n  --no-rewrite-casts          Preserve the existing casts, without rewriting\n                              them to use the :: syntax.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --check                     Whether or not to check formatting (but not\n                              actually format anything).\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#info","title":"info","text":"<p>Get a quick overview of your Vulcan project. This command shows you how many models and macros you have, and it tests your connections to both the data warehouse and state backend. It's a great first command to run when setting up a new project or troubleshooting connection issues.</p> <pre><code>Usage: vulcan info [OPTIONS]\n\n  Print information about a Vulcan project.\n\n  Includes counts of project models and macros and connection tests for the\n  data warehouse.\n\nOptions:\n  --skip-connection  Skip the connection test.\n  -v, --verbose      Verbose output.\n  --help  Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan info\nModels: 4\nMacros: 0\nData warehouse connection succeeded\nState backend connection succeeded\n</code></pre>"},{"location":"cli-commands/cli/#init","title":"init","text":"<p>Initialize a new Vulcan project. This sets up the basic project structure and configuration files you'll need to get started. You can choose from different templates (like dbt or DLT) or start with an empty project.</p> <pre><code>Usage: vulcan init [OPTIONS] [ENGINE]\n\n  Create a new Vulcan repository.\n\nOptions:\n  -t, --template TEXT  Project template. Supported values: dbt, dlt, default,\n                       empty.\n  --dlt-pipeline TEXT  DLT pipeline for which to generate a Vulcan project.\n                       Use alongside template: dlt\n  --dlt-path TEXT      The directory where the DLT pipeline resides. Use\n                       alongside template: dlt\n  --help               Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan init postgres\n</code></pre>"},{"location":"cli-commands/cli/#invalidate","title":"invalidate","text":"<p>Mark an environment for deletion. The janitor process will clean it up on its next run. This is useful when you want to remove a development environment that's no longer needed. By default, the deletion happens asynchronously, but you can use <code>--sync</code> to wait for it to complete immediately.</p> <pre><code>Usage: vulcan invalidate [OPTIONS] ENVIRONMENT\n\n  Invalidate the target environment, forcing its removal during the next run\n  of the janitor process.\n\nOptions:\n  -s, --sync  Wait for the environment to be deleted before returning. If not\n              specified, the environment will be deleted asynchronously by the\n              janitor process. This option requires a connection to the data\n              warehouse.\n  --help      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan invalidate dev\nEnvironment 'dev' invalidated.\n</code></pre>"},{"location":"cli-commands/cli/#janitor","title":"janitor","text":"<p>Run the janitor process manually to clean up old environments and expired snapshots. Normally, the janitor runs automatically, but sometimes you might want to trigger it immediately to free up space or clean up resources right away.</p> <pre><code>Usage: vulcan janitor [OPTIONS]\n\n  Run the janitor process on-demand.\n\n  The janitor cleans up old environments and expired snapshots.\n\nOptions:\n  --ignore-ttl  Cleanup snapshots that are not referenced in any environment,\n                regardless of when they're set to expire\n  --help        Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan janitor\nDeleted object warehouse.sales__dev\nDeleted object warehouse.raw__dev\nCleanup complete.\n</code></pre>"},{"location":"cli-commands/cli/#migrate","title":"migrate","text":"<p>Upgrade Vulcan's internal state to match the current version you're running. This is typically needed when you upgrade Vulcan itself. Important: This command affects all Vulcan users, so make sure to coordinate with your team and contact your Vulcan administrator before running it.</p> <pre><code>Usage: vulcan migrate [OPTIONS]\n\n  Migrate Vulcan to the current running version.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>migrate</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-commands/cli/#plan","title":"plan","text":"<p>Create and apply a plan that compares your local project state with a target environment and determines what changes need to be made. This is one of the most important commands, it's how you deploy model changes, add new models, and backfill data. The plan shows you exactly what will happen before it happens, so you can review changes before they're applied.</p> <pre><code>Usage: vulcan plan [OPTIONS] [ENVIRONMENT]\n\n  Apply local changes to the target environment.\n\nOptions:\n  -s, --start TEXT                The start datetime of the interval for which\n                                  this command will be applied.\n  -e, --end TEXT                  The end datetime of the interval for which\n                                  this command will be applied.\n  --execution-time TEXT           The execution time (defaults to now).\n  --create-from TEXT              The environment to create the target\n                                  environment from if it doesn't exist.\n                                  Default: prod.\n  --skip-tests                    Skip tests prior to generating the plan if\n                                  they are defined.\n  --skip-linter                   Skip linting prior to generating the plan if\n                                  the linter is enabled.\n  -r, --restate-model TEXT        Restate data for specified models and models\n                                  downstream from the one specified. For\n                                  production environment, all related model\n                                  versions will have their intervals wiped,\n                                  but only the current versions will be\n                                  backfilled. For development environment,\n                                  only the current model versions will be\n                                  affected.\n  --no-gaps                       Ensure that new snapshots have no data gaps\n                                  when comparing to existing snapshots for\n                                  matching models in the target environment.\n  --skip-backfill, --dry-run      Skip the backfill step and only create a\n                                  virtual update for the plan.\n  --empty-backfill                Produce empty backfill. Like --skip-backfill\n                                  no models will be backfilled, unlike --skip-\n                                  backfill missing intervals will be recorded\n                                  as if they were backfilled.\n  --forward-only                  Create a plan for forward-only changes.\n  --allow-destructive-model TEXT  Allow destructive forward-only changes to\n                                  models whose names match the expression.\n  --allow-additive-model TEXT     Allow additive forward-only changes to\n                                  models whose names match the expression.\n  --effective-from TEXT           The effective date from which to apply\n                                  forward-only changes on production.\n  --no-prompts                    Disable interactive prompts for the backfill\n                                  time range. Please note that if this flag is\n                                  set and there are uncategorized changes,\n                                  plan creation will fail.\n  --auto-apply                    Automatically apply the new plan after\n                                  creation.\n  --no-auto-categorization        Disable automatic change categorization.\n  --include-unmodified            Include unmodified models in the target\n                                  environment.\n  --select-model TEXT             Select specific model changes that should be\n                                  included in the plan.\n  --backfill-model TEXT           Backfill only the models whose names match\n                                  the expression.\n  --no-diff                       Hide text differences for changed models.\n  --run                           Run latest intervals as part of the plan\n                                  application (prod environment only).\n  --enable-preview                Enable preview for forward-only models when\n                                  targeting a development environment.\n  --diff-rendered                 Output text differences for the rendered\n                                  versions of the models and standalone\n                                  audits.\n  --explain                       Explain the plan instead of applying it.\n  --ignore-cron                   Run all missing intervals, ignoring\n                                  individual cron schedules. Only applies if\n                                  --run is set.\n  --min-intervals INTEGER         For every model, ensure at least this many\n                                  intervals are covered by a missing intervals\n                                  check regardless of the plan start date\n  -v, --verbose                   Verbose output. Use -vv for very verbose\n                                  output.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#api","title":"api","text":"<p>Start Vulcan's API server, which provides programmatic access to models, metrics, lineage information, and telemetry. This is useful if you want to integrate Vulcan with other tools or build custom dashboards and applications on top of your data pipeline.</p> <pre><code>Usage: vulcan api [OPTIONS]\n\n  Start the Vulcan API server (models, metrics, lineage, telemetry).\n\nOptions:\n  --host TEXT        Bind socket to this host. Default: 0.0.0.0\n  --port INTEGER     Bind socket to this port. Default: 8000\n  --reload           Enable auto-reload on file changes. Default: False\n  --workers INTEGER  Number of worker processes. Default: 1\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#render","title":"render","text":"<p>See the actual SQL that Vulcan will execute for a model. This is super helpful for debugging and understanding how Vulcan transforms your model definitions into executable SQL. You can optionally expand referenced models to see the full query with all dependencies inlined.</p> <pre><code>Usage: vulcan render [OPTIONS] MODEL\n\n  Render a model's query, optionally expanding referenced models.\n\nOptions:\n  -s, --start TEXT            The start datetime of the interval for which\n                              this command will be applied.\n  -e, --end TEXT              The end datetime of the interval for which this\n                              command will be applied.\n  --execution-time TEXT       The execution time (defaults to now).\n  --expand TEXT               Whether or not to expand materialized models\n                              (defaults to False). If True, all referenced\n                              models are expanded as raw queries. Multiple\n                              model names can also be specified, in which case\n                              only they will be expanded as raw queries.\n  --dialect TEXT              The SQL dialect to render the query as.\n  --no-format                 Disable fancy formatting of the query.\n  --max-text-width INTEGER    The max number of characters in a segment before\n                              creating new lines in pretty mode.\n  --leading-comma             Determines whether or not the comma is leading\n                              or trailing in select expressions. Default is\n                              trailing.\n  --normalize-functions TEXT  Whether or not to normalize all function names.\n                              Possible values are: 'upper', 'lower'\n  --indent INTEGER            Determines the indentation size in a formatted\n                              string.\n  --pad INTEGER               Determines the pad size in a formatted string.\n  --normalize                 Whether or not to normalize identifiers to\n                              lowercase.\n  --help                      Show this message and exit.\n</code></pre> Example <pre><code>$ vulcan render sales.daily_sales\n\nSELECT\n  CAST(\"raw_orders\".\"order_date\" AS TIMESTAMP) AS \"order_date\",\n  CAST(COUNT(\"raw_orders\".\"order_id\") AS INT) AS \"total_orders\",\n  CAST(SUM(\"raw_orders\".\"total_amount\") AS DOUBLE PRECISION) AS \"total_revenue\",\n  CAST(MAX(\"raw_orders\".\"order_id\") AS VARCHAR) AS \"last_order_id\",\n  COUNT(DISTINCT \"raw_orders\".\"product_id\") AS \"total_products\"\nFROM \"warehouse\".\"vulcan__raw\".\"raw__raw_orders__1032938324\" AS \"raw_orders\" /* warehouse.raw.raw_orders */\nGROUP BY\n  \"raw_orders\".\"order_date\"\nORDER BY\n  \"order_date\"\n</code></pre>"},{"location":"cli-commands/cli/#rollback","title":"rollback","text":"<p>Revert Vulcan's internal state to the previous migration version. This is useful if a migration caused issues and you need to go back. Important: Like <code>migrate</code>, this command affects all Vulcan users, so coordinate with your team and contact your Vulcan administrator before running it.</p> <pre><code>Usage: vulcan rollback [OPTIONS]\n\n  Rollback Vulcan to the previous migration.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Caution</p> <p>The <code>rollback</code> command affects all Vulcan users. Contact your Vulcan administrator before running.</p>"},{"location":"cli-commands/cli/#run","title":"run","text":"<p>Process missing time intervals for your models in a target environment. This is different from <code>plan</code>, <code>run</code> focuses on executing scheduled work based on cron schedules, while <code>plan</code> handles deploying changes. Use <code>run</code> when you want to process new or missing data without making any model definition changes.</p> <pre><code>Usage: vulcan run [OPTIONS] [ENVIRONMENT]\n\n  Evaluate missing intervals for the target environment.\n\nOptions:\n  -s, --start TEXT              The start datetime of the interval for which\n                                this command will be applied.\n  -e, --end TEXT                The end datetime of the interval for which\n                                this command will be applied.\n  --skip-janitor                Skip the janitor task.\n  --ignore-cron                 Run for all missing intervals, ignoring\n                                individual cron schedules.\n  --select-model TEXT           Select specific models to run. Note: this\n                                always includes upstream dependencies.\n  --exit-on-env-update INTEGER  If set, the command will exit with the\n                                specified code if the run is interrupted by an\n                                update to the target environment.\n  --no-auto-upstream            Do not automatically include upstream models.\n                                Only applicable when --select-model is used.\n                                Note: this may result in missing / invalid\n                                data for the selected models.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#state","title":"state","text":"<p>Manage Vulcan's state database. This includes exporting state for backup or migration purposes, and importing state from another environment. These commands are useful for disaster recovery, environment cloning, or moving state between systems.</p> <pre><code>Usage: vulcan state [OPTIONS] COMMAND [ARGS]...\n\n  Commands for interacting with state\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  export  Export the state database to a file\n  import  Import a state export file back into the state database\n</code></pre>"},{"location":"cli-commands/cli/#export","title":"export","text":"<p>Export Vulcan's state database to a file. This creates a backup of your state that you can use for recovery or to move state between environments. You can export specific environments or all of them.</p> <pre><code>Usage: vulcan state export [OPTIONS]\n\n  Export the state database to a file\n\nOptions:\n  -o, --output-file FILE  Path to write the state export to  [required]\n  --environment TEXT      Name of environment to export. Specify multiple\n                          --environment arguments to export multiple\n                          environments\n  --local                 Export local state only. Note that the resulting\n                          file will not be importable\n  --no-confirm            Do not prompt for confirmation before exporting\n                          existing state\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#import","title":"import","text":"<p>Import a previously exported state file back into the state database. This is useful for restoring from backups or copying state from one environment to another. By default, it merges with existing state, but you can use <code>--replace</code> to completely replace it.</p> <pre><code>Usage: vulcan state import [OPTIONS]\n\n  Import a state export file back into the state database\n\nOptions:\n  -i, --input-file FILE  Path to the state file  [required]\n  --replace              Clear the remote state before loading the file. If\n                         omitted, a merge is performed instead\n  --no-confirm           Do not prompt for confirmation before updating\n                         existing state\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#table_diff","title":"table_diff","text":"<p>Compare data between two tables or models to see what's different. This is super useful for validating that changes produce the expected results, comparing environments, or debugging data discrepancies. You can compare entire tables or specific models, and customize how the comparison works.</p> <pre><code>Usage: vulcan table_diff [OPTIONS] SOURCE:TARGET [MODEL]\n\n  Show the diff between two tables or a selection of models when they are\n  specified.\n\nOptions:\n  -o, --on TEXT            The column to join on. Can be specified multiple\n                           times. The model grain will be used if not\n                           specified.\n  -s, --skip-columns TEXT  The column(s) to skip when comparing the source and\n                           target table.\n  --where TEXT             An optional where statement to filter results.\n  --limit INTEGER          The limit of the sample dataframe.\n  --show-sample            Show a sample of the rows that differ. With many\n                           columns, the output can be very wide.\n  -d, --decimals INTEGER   The number of decimal places to keep when comparing\n                           floating point columns. Default: 3\n  --skip-grain-check       Disable the check for a primary key (grain) that is\n                           missing or is not unique.\n  --warn-grain-check       Warn if any selected model is missing a grain,\n                           and compute diffs for the remaining models.\n  --temp-schema TEXT       Schema used for temporary tables. It can be\n                           `CATALOG.SCHEMA` or `SCHEMA`. Default:\n                           `vulcan_temp`\n  -m, --select-model TEXT  Specify one or more models to data diff. Use\n                           wildcards to diff multiple models. Ex: '*' (all\n                           models with applied plan diffs), 'demo.model+'\n                           (this and downstream models),\n                           'git:feature_branch' (models with direct\n                           modifications in this branch only)\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#table_name","title":"table_name","text":"<p>Get the actual physical table name that Vulcan uses for a model. This is helpful when you need to reference the table directly in SQL or other tools, since Vulcan's internal naming might differ from your model name.</p> <pre><code>Usage: vulcan table_name [OPTIONS] MODEL_NAME\n\n  Prints the name of the physical table for the given model.\n\nOptions:\n  --environment, --env TEXT  The environment to source the model version from.\n  --prod                     If set, return the name of the physical table\n                             that will be used in production for the model\n                             version promoted in the target environment.\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#test","title":"test","text":"<p>Run unit tests for your models. These tests validate that your SQL logic works correctly with the test fixtures you've defined. It's a great way to catch bugs before deploying to production.</p> <pre><code>Usage: vulcan test [OPTIONS] [TESTS]...\n\n  Run model unit tests.\n\nOptions:\n  -k TEXT              Only run tests that match the pattern of substring.\n  -v, --verbose        Verbose output.\n  --preserve-fixtures  Preserve the fixture tables in the testing database,\n                       useful for debugging.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#semantic","title":"semantic","text":"<p>Work with Vulcan's semantic layer. Currently, this includes exporting your semantic models and metrics to CubeJS-compatible YAML schemas, which lets you use your Vulcan semantic layer with other tools that support CubeJS.</p> <pre><code>Usage: vulcan semantic [OPTIONS] {export} [ENVIRONMENT]\n\n  Semantic layer operations.\n\n  This command provides semantic layer export functionality, allowing users to\n  convert semantic models and metrics into CubeJS-compatible YAML schemas.\n\nOptions:\n  -o, --output PATH   Output file path for the CubeJS schema.  [required]\n  --strict            Strict mode: export only explicitly defined semantic\n                      models.\n  --no-auto-measures  Disable automatic generation of measures (e.g., _count)\n                      for models with grains.\n  --no-confirm        Do not prompt for confirmation before overwriting\n                      existing output file.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#transpile","title":"transpile","text":"<p>Convert semantic SQL queries or REST-style semantic query payloads into executable, database-specific SQL. This is useful for debugging semantic queries, validating them, or understanding how Vulcan translates business-friendly queries into actual SQL.</p> <pre><code>Usage: vulcan transpile [OPTIONS] [QUERY]\n\n  Transpile a semantic SQL or REST-style semantic query to executable SQL.\n\nOptions:\n  --format [sql|rest]        Input type: semantic SQL ('sql') or REST-style\n                             semantic payload ('rest').  [required]\n  --file TEXT                Read query or REST payload from file. Use '-' to\n                             read from stdin.\n  --user TEXT                User id to propagate in the X-User header\n                             (defaults to 'cli').\n  --disable-post-processing  Disable post-processing in the Transpiler.\n  --style [pretty|compact]   SQL output style: 'pretty' (formatted with\n                             indentation), 'compact' (unformatted but\n                             processed),\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#transpiler","title":"transpiler","text":"<pre><code>Usage: vulcan transpiler [OPTIONS] {up|down}\n\n  Manage the Transpiler service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#graphql","title":"graphql","text":"<pre><code>Usage: vulcan graphql [OPTIONS] {up|down}\n\n  Manage the GraphQL service (subcommands: up, down).\n\nOptions:\n  --no-detach  Run docker compose in the foreground (omit -d).\n  --help       Show this message and exit.\n</code></pre>"},{"location":"cli-commands/cli/#lint","title":"lint","text":"<p>Run linting rules on your models to catch potential issues and enforce code quality standards. You can lint specific models or all models in your project. This is super helpful for maintaining consistent code quality and catching common mistakes early.</p> <pre><code>Usage: vulcan lint [OPTIONS]\n  Run linter for the target model(s).\n\nOptions:\n  --model TEXT           A model to lint. Multiple models can be linted.  If no models are specified, every model will be linted.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/","title":"Custom materializations","text":""},{"location":"components/advanced-features/custom_materializations/#custom-materializations","title":"Custom materializations","text":"<p>Vulcan comes with a variety of model kinds that handle the most common ways to evaluate and materialize your data transformations. But what if you need something different?</p> <p>Sometimes, your specific use case doesn't quite fit any of the built-in model kinds. Maybe you need custom logic for how data gets inserted, or you want to implement a materialization strategy that's unique to your workflow. That's where custom materializations come in, they let you write your own Python code to control exactly how your models get materialized.</p> <p>Advanced Feature</p> <p>Custom materializations are powerful, but they're also advanced. Before diving in, make sure you've exhausted all other options. If an existing model kind can solve your problem, we want to improve our docs; if a built-in kind is almost what you need, we might be able to enhance it for everyone.</p>"},{"location":"components/advanced-features/custom_materializations/#what-is-a-materialization","title":"What is a materialization?","text":"<p>A materialization is the \"how\" behind your model execution. When Vulcan runs a model, it needs to figure out how to get that data into your database. The materialization is the set of methods that handle executing your transformation logic and managing the resulting data.</p> <p>Some materializations are straightforward. For example, a <code>FULL</code> model kind completely replaces the table each time it runs, so its materialization is essentially just <code>CREATE OR REPLACE TABLE [name] AS [your query]</code>.</p> <p>Other materializations are more complex. An <code>INCREMENTAL_BY_TIME_RANGE</code> model needs to figure out which time intervals to process, query only that data, and then merge it into the existing table. That requires more logic.</p> <p>The materialization logic can also vary by SQL engine. PostgreSQL doesn't support <code>CREATE OR REPLACE TABLE</code>, so <code>FULL</code> models on Postgres use <code>DROP</code> then <code>CREATE</code> instead. Vulcan handles all these engine-specific details for built-in model kinds, but with custom materializations, you're in control.</p>"},{"location":"components/advanced-features/custom_materializations/#how-custom-materializations-work","title":"How custom materializations work","text":"<p>Custom materializations are like creating your own model kind. You define them in Python, give them a name, and then reference that name in your model's <code>MODEL</code> block. They can accept configuration arguments that you pass in from your model definition.</p> <p>Here's what every custom materialization needs:</p> <ul> <li> <p>Python code: Written as a Python class</p> </li> <li> <p>Base class: Must inherit from Vulcan's <code>CustomMaterialization</code> class</p> </li> <li> <p>Insert method: At minimum, you need to implement the <code>insert</code> method</p> </li> <li> <p>Auto-loading: Vulcan automatically discovers materializations in your <code>materializations/</code> directory</p> </li> </ul> <p>You can also:</p> <ul> <li> <p>Override other methods from <code>MaterializableStrategy</code> or <code>EngineAdapter</code> classes</p> </li> <li> <p>Execute arbitrary SQL using the engine adapter</p> </li> <li> <p>Perform Python processing with Pandas or other libraries (though for most cases, you'd want that logic in a Python model instead)</p> </li> </ul> <p>Vulcan will automatically load any Python files in your project's <code>materializations/</code> directory. Or, if you prefer, you can package your materialization as a Python package and install it like any other dependency.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-materialization","title":"Creating a custom materialization","text":"<p>To create a custom materialization, just add a <code>.py</code> file to your project's <code>materializations/</code> folder. Vulcan will automatically import all Python modules in this folder when your project loads, so your materializations will be ready to use.</p> <p>Your materialization class needs to inherit from <code>CustomMaterialization</code> and implement at least the <code>insert</code> method. Let's look at some examples to see how this works.</p>"},{"location":"components/advanced-features/custom_materializations/#simple-example","title":"Simple example","text":"<p>Here's a complete example that shows custom insert logic with some helpful logging:</p> <pre><code>import typing as t\nfrom sqlalchemy import text\nfrom vulcan import CustomMaterialization\nfrom vulcan import Model\n\nclass SimpleCustomMaterialization(CustomMaterialization):\n    \"\"\"Simple custom materialization - demonstrates custom insert logic\"\"\"\n\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Union[str, t.Any],\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        \"\"\"Custom insert logic for tables\"\"\"\n\n        print(f\"Custom materialization: Processing table {table_name}\")\n        print(f\"Model: {model.name}\")\n        print(f\"Is first insert: {is_first_insert}\")\n\n        if is_first_insert:\n            print(\"Creating table for the first time\")\n            # Create the table normally using the adapter\n            self.adapter.create_table(\n                table_name,\n                columns=model.columns_to_types,\n                target_columns_to_types=model.columns_to_types,\n                partitioned_by=model.partitioned_by,\n            )\n\n        # Insert data with custom logic\n        if isinstance(query_or_df, str):\n            print(\"Executing SQL query\")\n            # Execute the query - Vulcan provides the INSERT INTO ... SELECT query\n            self.adapter.execute(text(query_or_df))\n        else:\n            print(\"Inserting DataFrame\")\n            # Insert DataFrame normally - useful for Python models that return DataFrames\n            self.adapter.insert_append(table_name, query_or_df)\n\n        print(f\"Custom materialization completed for {table_name}\")\n</code></pre> <p>Let's break down what's happening here:</p> Component What It Does <code>NAME</code> The identifier you'll use in your model definition (like <code>simple_custom</code>) <code>table_name</code> The target table where your data will be inserted <code>query_or_df</code> Either a SQL query string or a DataFrame (works with Pandas, PySpark, Snowpark) <code>model</code> The full model definition object, gives you access to all model properties <code>is_first_insert</code> <code>True</code> if this is the first time inserting data for this model version <code>render_kwargs</code> Dictionary of arguments used to render the model query <code>self.adapter</code> The engine adapter, your interface to execute SQL and interact with the database"},{"location":"components/advanced-features/custom_materializations/#minimal-example","title":"Minimal example","text":"<p>If you just want a simple full-refresh materialization, here's the minimal version:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre> <p>That's it! This will completely replace the table contents each time the model runs, just like a <code>FULL</code> model kind.</p>"},{"location":"components/advanced-features/custom_materializations/#controlling-table-creation-and-deletion","title":"Controlling table creation and deletion","text":"<p>You can also customize how tables and views are created and deleted by overriding the <code>create</code> and <code>delete</code> methods:</p> <pre><code>from vulcan import CustomMaterialization\nfrom vulcan import Model\nimport typing as t\n\nclass CustomFullMaterialization(CustomMaterialization):\n    NAME = \"my_custom_full\"\n\n    def insert(self, table_name: str, query_or_df: t.Any, model: Model, \n               is_first_insert: bool, render_kwargs: t.Dict[str, t.Any], **kwargs: t.Any) -&gt; None:\n        self.adapter.replace_query(table_name, query_or_df)\n\n    def create(\n        self,\n        table_name: str,\n        model: Model,\n        is_table_deployable: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Custom table/view creation logic\n        # Uses self.adapter methods like create_table, create_view, or ctas\n        self.adapter.create_table(\n            table_name,\n            columns=model.columns_to_types,\n            target_columns_to_types=model.columns_to_types,\n        )\n\n    def delete(self, name: str, **kwargs: t.Any) -&gt; None:\n        # Custom table/view deletion logic\n        self.adapter.drop_table(name)\n</code></pre> <p>This gives you full control over the lifecycle of your data objects.</p>"},{"location":"components/advanced-features/custom_materializations/#using-a-custom-materialization","title":"Using a custom materialization","text":"<p>Once you've created your materialization, using it is straightforward. In your model definition, set the <code>kind</code> to <code>CUSTOM</code> and specify the <code>materialization</code> name (the <code>NAME</code> from your Python class):</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom'\n  ),\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers c\nLEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name\nORDER BY total_spent DESC\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.custom_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.CUSTOM,\n        materialization=\"simple_custom\",\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Python model using custom materialization with dynamic dependencies\"\"\"\n\n    # Simple customer summary\n    query = \"\"\"\n    SELECT \n        c.customer_id,\n        c.name as customer_name,\n        COUNT(DISTINCT o.order_id) as total_orders,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_spent\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id, c.name\n    ORDER BY total_spent DESC\n    \"\"\"\n\n    # Execute query and return results\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#passing-properties-to-the-materialization","title":"Passing properties to the materialization","text":"<p>You can pass configuration to your materialization using <code>materialization_properties</code>. This is useful when you want to customize behavior per model:</p> <pre><code>MODEL (\n  name vulcan_demo.custom_model,\n  kind CUSTOM (\n    materialization 'simple_custom',\n    materialization_properties (\n      'config_key' = 'config_value',\n      'batch_size' = 1000\n    )\n  )\n);\n</code></pre> <p>Then access these properties in your materialization code via <code>model.custom_materialization_properties</code>:</p> <pre><code>class SimpleCustomMaterialization(CustomMaterialization):\n    NAME = \"simple_custom\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        # Access custom properties\n        config_value = model.custom_materialization_properties.get(\"config_key\")\n        batch_size = model.custom_materialization_properties.get(\"batch_size\", 500)\n\n        print(f\"Config value: {config_value}, Batch size: {batch_size}\")\n\n        # Proceed with insert logic\n        self.adapter.replace_query(table_name, query_or_df)\n</code></pre> <p>This lets you create flexible materializations that can adapt to different use cases.</p>"},{"location":"components/advanced-features/custom_materializations/#extending-customkind","title":"Extending <code>CustomKind</code>","text":"<p>Warning</p> <p>This is advanced territory. You're working with Vulcan's internals here, so there's extra complexity involved. If the basic custom materialization approach works for you, stick with that. Only dive into this if you really need the extra control.</p> <p>Most of the time, the standard custom materialization approach is all you need. But sometimes you want tighter integration with Vulcan's internals, maybe you need to validate custom properties before any database connections are made, or you want to leverage functionality that depends on specific properties being present.</p> <p>In those cases, you can create a subclass of <code>CustomKind</code> that Vulcan will use instead of the default. When your project loads, Vulcan will detect your subclass and use it instead of the standard <code>CustomKind</code>.</p>"},{"location":"components/advanced-features/custom_materializations/#creating-a-custom-kind","title":"Creating a custom kind","text":"<p>Here's how you'd create a custom kind that validates a <code>primary_key</code> property:</p> <pre><code>import typing as t\nfrom typing_extensions import Self\nfrom pydantic import model_validator\nfrom sqlglot import exp\nfrom vulcan import CustomKind\nfrom vulcan.utils.pydantic import list_of_fields_validator\nfrom vulcan.utils.errors import ConfigError\n\nclass MyCustomKind(CustomKind):\n\n    _primary_key: t.List[exp.Expression]\n\n    @model_validator(mode=\"after\")\n    def _validate_model(self) -&gt; Self:\n        self._primary_key = list_of_fields_validator(\n            self.materialization_properties.get(\"primary_key\"),\n            {\"dialect\": self.dialect}\n        )\n        if not self.primary_key:\n            raise ConfigError(\"primary_key must be specified\")\n        return self\n\n    @property\n    def primary_key(self) -&gt; t.List[exp.Expression]:\n        return self._primary_key\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#using-the-custom-kind-in-a-model","title":"Using the custom kind in a model","text":"<p>Use it in your model like this:</p> <pre><code>MODEL (\n  name vulcan_demo.my_model,\n  kind CUSTOM (\n    materialization 'my_custom_full',\n    materialization_properties (\n      primary_key = (col1, col2)\n    )\n  )\n);\n</code></pre>"},{"location":"components/advanced-features/custom_materializations/#linking-to-your-materialization","title":"Linking to your materialization","text":"<p>To connect your custom kind to your materialization, specify it as a generic type parameter:</p> <pre><code>class CustomFullMaterialization(CustomMaterialization[MyCustomKind]):\n    NAME = \"my_custom_full\"\n\n    def insert(\n        self,\n        table_name: str,\n        query_or_df: t.Any,\n        model: Model,\n        is_first_insert: bool,\n        render_kwargs: t.Dict[str, t.Any],\n        **kwargs: t.Any,\n    ) -&gt; None:\n        assert isinstance(model.kind, MyCustomKind)\n\n        self.adapter.merge(\n            ...,\n            unique_key=model.kind.primary_key\n        )\n</code></pre> <p>When Vulcan loads your materialization, it inspects the type signature for generic parameters that are subclasses of <code>CustomKind</code>. If it finds one, it uses your subclass when building <code>model.kind</code> instead of the default.</p> <p>Why would you want this? Two main benefits:</p> <ul> <li> <p>Early validation: Your <code>primary_key</code> validation happens at load time, not evaluation time. Issues get caught before you even create a plan.</p> </li> <li> <p>Type safety: <code>model.kind</code> resolves to your custom kind object, so you get access to extra properties without additional validation.</p> </li> </ul>"},{"location":"components/advanced-features/custom_materializations/#sharing-custom-materializations","title":"Sharing custom materializations","text":"<p>Once you've built a custom materialization, you'll probably want to use it across multiple projects. You have a couple of options.</p>"},{"location":"components/advanced-features/custom_materializations/#copying-files","title":"Copying files","text":"<p>The simplest approach is to copy the materialization code into each project's <code>materializations/</code> directory. It works, but it's not the most maintainable approach, you'll need to manually update each copy when you make changes.</p> <p>If you go this route, we strongly recommend keeping the materialization code in version control and setting up a reliable way to notify users when updates are available.</p>"},{"location":"components/advanced-features/custom_materializations/#python-packaging","title":"Python packaging","text":"<p>A more robust approach is to package your materialization as a Python package. This is especially useful if you're using Airflow or other external schedulers where the scheduler cluster doesn't have direct access to your project's <code>materializations/</code> folder.</p> <p>Package your materialization using setuptools entrypoints:</p> pyproject.tomlsetup.py <pre><code>[project.entry-points.\"vulcan.materializations\"]\nmy_materialization = \"my_package.my_materialization:CustomFullMaterialization\"\n</code></pre> <pre><code>setup(\n    ...,\n    entry_points={\n        \"vulcan.materializations\": [\n            \"my_materialization = my_package.my_materialization:CustomFullMaterialization\",\n        ],\n    },\n)\n</code></pre> <p>Once the package is installed, Vulcan automatically discovers and loads your materialization from the entrypoint list. No manual configuration needed!</p>"},{"location":"components/advanced-features/signals/","title":"Signals","text":""},{"location":"components/advanced-features/signals/#signals","title":"Signals","text":"<p>Vulcan's built-in scheduler knows when to run your models based on their <code>cron</code> schedules. If you have a model set to run <code>@daily</code>, it checks whether a day has passed since the last run and evaluates the model if needed.</p> <p>But here's the thing: real-world data doesn't always follow our schedules. Sometimes data arrives late, maybe your upstream system had an issue, or a batch job ran behind schedule. When that happens, your daily model might have already run for the day, and that late data won't get processed until tomorrow's scheduled run.</p> <p>Signals solve this problem by letting you add custom conditions that must be met before a model runs. They are extra gates that the scheduler checks, beyond \"has enough time passed?\" and \"are upstream dependencies done?\"</p>"},{"location":"components/advanced-features/signals/#what-is-a-signal","title":"What is a signal?","text":"<p>By default, Vulcan's scheduler uses two criteria to decide if a model should run:</p> <ol> <li>Has the model's <code>cron</code> interval elapsed since the last evaluation?</li> <li>Have all upstream dependencies finished running?</li> </ol> <p>Signals let you add a third criterion: your own custom check. A signal is just a Python function that examines a batch of time intervals and decides whether they're ready for evaluation.</p> <p>Here's how it works under the hood: The scheduler doesn't actually evaluate \"a model\", it evaluates a model over specific time intervals. For incremental models, this is obvious (you're processing a date range). But even non-temporal models like <code>FULL</code> and <code>VIEW</code> are evaluated based on time intervals, their <code>cron</code> frequency determines the interval.</p> <p>The scheduler looks at candidate intervals, groups them into batches (controlled by your model's <code>batch_size</code> parameter), and then checks signals to see if those batches are ready. Your signal function gets called with a batch of time intervals and can return:</p> <ul> <li> <p><code>True</code> if all intervals in the batch are ready</p> </li> <li> <p><code>False</code> if none are ready</p> </li> <li> <p>A list of specific intervals if only some are ready</p> </li> </ul> <p>One model, multiple signals</p> <p>You can specify multiple signals for a single model. When you do, Vulcan requires that all signal functions agree an interval is ready before it gets evaluated. It works like an AND gate: every signal must give the green light.</p>"},{"location":"components/advanced-features/signals/#defining-a-signal","title":"Defining a signal","text":"<p>To create a signal, add a <code>signals</code> directory to your project and create your signal function in <code>__init__.py</code> (you can organize signals across multiple Python files if you prefer).</p> <p>A signal function needs to:</p> <ul> <li> <p>Accept a batch of time intervals (<code>DateTimeRanges: t.List[t.Tuple[datetime, datetime]]</code>)</p> </li> <li> <p>Return either a boolean or a list of intervals</p> </li> <li> <p>Use the <code>@signal</code> decorator</p> </li> </ul> <p>Let's look at some examples, starting simple and building up to more complex use cases.</p>"},{"location":"components/advanced-features/signals/#simple-example","title":"Simple example","text":"<p>Here's a basic signal that randomly decides whether intervals are ready (useful for testing, maybe not so much for production!):</p> <pre><code>import random\nimport typing as t\nfrom vulcan import signal, DatetimeRanges\n\n\n@signal()\ndef random_signal(batch: DatetimeRanges, threshold: float) -&gt; t.Union[bool, DatetimeRanges]:\n    return random.random() &gt; threshold\n</code></pre> <p>This signal takes a <code>threshold</code> argument (you'll pass this from your model definition) and returns <code>True</code> if a random number exceeds that threshold. Notice how the function signature includes <code>threshold: float</code>, Vulcan will automatically extract this from your model definition and pass it to the function. The type inference works the same way as Vulcan macros.</p> <p>To use this signal in a model, add it to the <code>signals</code> key in your <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind FULL,\n  signals (\n    random_signal(threshold := 0.5), # specify threshold value\n  )\n);\n\nSELECT 1\n</code></pre> <p>The <code>signals</code> key accepts a list of signal calls, each with its own arguments. When you run <code>vulcan run</code>, this signal checks if a random number is greater than 0.5. If it is, the model runs; otherwise, it waits.</p>"},{"location":"components/advanced-features/signals/#advanced-example","title":"Advanced example","text":"<p>Sometimes you want more fine-grained control. Instead of saying \"all intervals are ready\" or \"none are ready,\" you can return specific intervals from the batch. Here's an example that only allows intervals from at least one week ago:</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges\nfrom vulcan.utils.date import to_datetime\n\n\n# signal that returns only intervals that are &lt;= 1 week ago\n@signal()\ndef one_week_ago(batch: DatetimeRanges) -&gt; t.Union[bool, DatetimeRanges]:\n    dt = to_datetime(\"1 week ago\")\n\n    return [\n        (start, end)\n        for start, end in batch\n        if start &lt;= dt\n    ]\n</code></pre> <p>Instead of returning <code>True</code> or <code>False</code> for the entire batch, this function filters the batch and returns only the intervals that meet the criteria. It compares each interval's start time to \"1 week ago\" and includes only those that are old enough.</p> <p>Use it in an incremental model like this:</p> <pre><code>MODEL (\n  name example.signal_model,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column ds,\n  ),\n  start '2 week ago',\n  signals (\n    one_week_ago(),\n  )\n);\n\nSELECT @start_ds AS ds\n</code></pre> <p>This ensures that only data from at least a week ago gets processed, useful if you want to wait for late-arriving data to stabilize before processing it.</p>"},{"location":"components/advanced-features/signals/#accessing-execution-context","title":"Accessing execution context","text":"<p>Sometimes you need to check something in your database or access the execution context. You can do that by adding a <code>context</code> parameter to your signal function:</p> <pre><code>import typing as t\n\nfrom vulcan import signal, DatetimeRanges, ExecutionContext\n\n\n# add the context argument to your function\n@signal()\ndef one_week_ago(batch: DatetimeRanges, context: ExecutionContext) -&gt; t.Union[bool, DatetimeRanges]:\n    return len(context.engine_adapter.fetchdf(\"SELECT 1\")) &gt; 1\n</code></pre> <p>The <code>context</code> parameter gives you access to the engine adapter, so you can query your warehouse, check if certain tables exist, verify data freshness, or perform any other checks you need.</p>"},{"location":"components/advanced-features/signals/#testing-signals","title":"Testing signals","text":"<p>Signals only evaluate when you run <code>vulcan run</code> or use the <code>check_intervals</code> command. To test your signals without actually running models:</p> <ol> <li>Deploy your changes to an environment: <code>vulcan plan my_dev</code></li> <li>Check which intervals would be evaluated: <code>vulcan check_intervals my_dev</code>    - Use <code>--select-model</code> to check specific models</li> </ol> <ul> <li>Use <code>--no-signals</code> to see what would run without signal checks 3. Iterate by making changes to your signal and redeploying</li> </ul> <p>Note</p> <p>The <code>check_intervals</code> command only works with remote models that have been deployed to an environment. Local signal changes won't be tested until you deploy them.</p> <p>This workflow lets you verify your signal logic before it affects your actual model runs.</p>"},{"location":"components/advanced-features/macros/built_in/","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#built-in","title":"Built In","text":""},{"location":"components/advanced-features/macros/built_in/#macro-systems-two-approaches","title":"Macro systems: two approaches","text":"<p>Vulcan macros work differently than templating systems like Jinja. Here's the key difference: templating systems are all about string substitution, they scan your code, find special characters, and replace them with other text. That's their whole job.</p> <p>Templating systems are intentionally language-agnostic. They work for blog posts, HTML, SQL, or pretty much anything. They have control flow (if-then, loops) and other features, but those are just tools to help them substitute the right strings.</p> <p>Vulcan macros are different. They're built specifically for SQL, and they understand what your SQL actually means. Instead of just swapping strings, Vulcan macros analyze your SQL using the sqlglot library to build a semantic representation of your query. Then they modify that representation. This means they can do things templating systems can't, like knowing whether something is a column name or a string literal, or understanding the structure of your query.</p> <p>Plus, you can write macro logic in Python, which gives you way more power than simple string substitution.</p>"},{"location":"components/advanced-features/macros/built_in/#how-vulcan-macros-work","title":"How Vulcan macros work","text":"<p>This section explains what happens under the hood when Vulcan processes your macros. You don't need to read this to use macros, but it's helpful when you're debugging something that's not working as expected.</p> <p>The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.</p> <p>In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. Vulcan macros work by building and modifying the semantic representation of the SQL query.</p> <p>After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.</p> <p>It uses the following five step approach to accomplish this:</p> <ol> <li> <p>Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol <code>@</code> to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as \"placeholder\" values to use in subsequent steps.</p> </li> <li> <p>Examine the placeholder values to classify them as one of the following types:</p> <ul> <li> <p>Creation of user-defined macro variables with the <code>@DEF</code> operator (see more about user-defined macro variables)</p> </li> <li> <p>Macro variables: Vulcan pre-defined, user-defined local, and user-defined global</p> </li> <li> <p>Macro functions, both Vulcan's and user-defined</p> </li> </ul> </li> <li> <p>Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.</p> </li> <li> <p>Execute any macro functions and substitute the returned values.</p> </li> <li> <p>Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).</p> </li> </ol>"},{"location":"components/advanced-features/macros/built_in/#embedding-variables-in-strings","title":"Embedding variables in strings","text":"<p>Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.</p> <p>For context, two commonly used types of string in SQL are:</p> <ul> <li> <p>String literals, which represent text values and are surrounded by single quotes, such as <code>'the_string'</code></p> </li> <li> <p>Identifiers, which reference database objects like column, table, alias, and function names</p> <ul> <li>They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect</li> </ul> </li> </ul> <p>In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the <code>MODEL</code> block (and not the query itself).</p> <p>For example, consider a project that defines a gateway variable named <code>gateway_var</code>. The project includes a model that references <code>@gateway_var</code> as part of the schema in the model's <code>name</code>, which is a SQL identifier.</p> <p>This is how we might try to write the model:</p> Incorrectly rendered to string literal<pre><code>MODEL (\n  name the_@gateway_var_schema.table\n);\n</code></pre> <p>From Vulcan's perspective, the model schema is the combination of three sub-strings: <code>the_</code>, the value of <code>@gateway_var</code>, and <code>_schema</code>.</p> <p>Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.</p> <p>To provide the context Vulcan needs, you must add curly braces to the macro variable reference: <code>@{gateway_var}</code> instead of <code>@gateway_var</code>:</p> Correctly rendered to identifier<pre><code>MODEL (\n  name the_@{gateway_var}_schema.table\n);\n</code></pre> <p>The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.</p> <p>The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable <code>my_variable</code> whose value is <code>col</code>.</p> <p>If we <code>SELECT</code> this value with regular macro syntax, it will render to a string literal:</p> <pre><code>SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column\n</code></pre> <p><code>'col'</code> is surrounded with single quotes, and the SQL engine will use that string as the column's data value.</p> <p>If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:</p> <pre><code>SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column\n</code></pre> <p><code>col</code> is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named <code>col</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#user-defined-variables","title":"User-defined variables","text":"<p>Vulcan supports four kinds of user-defined macro variables: global, gateway, blueprint, and local.</p> <p>Here's how they're organized:</p> <ul> <li> <p>Global and gateway variables are defined in your project configuration file and can be used in any model</p> </li> <li> <p>Blueprint and local variables are defined in a specific model and only work in that model</p> </li> </ul> <p>What happens if you have variables with the same name at different levels? The most specific one wins. Local variables override blueprint or gateway variables, gateway variables override global variables, and so on. This lets you set defaults globally but override them when needed.</p>"},{"location":"components/advanced-features/macros/built_in/#global-variables","title":"Global variables","text":"<p>Global variables live in your project configuration file under the <code>variables</code> key. They're perfect for values you want to use across multiple models.</p> <p>You can store numbers (<code>int</code>, <code>float</code>), booleans (<code>bool</code>), strings (<code>str</code>), or even lists and dictionaries containing these types.</p> <p>Access them in your models using either <code>@VAR_NAME</code> (the simple syntax) or <code>@VAR('var_name')</code> (the function syntax). The function syntax lets you provide a default value as the second argument, useful if the variable might not be defined.</p> <p>For example, this Vulcan configuration key defines six variables of different data types:</p> YAMLPython <pre><code>variables:\n  int_var: 1\n  float_var: 2.0\n  bool_var: true\n  str_var: \"cat\"\n  list_var: [1, 2, 3]\n  dict_var:\n    key1: 1\n    key2: 2\n</code></pre> <pre><code>variables = {\n    \"int_var\": 1,\n    \"float_var\": 2.0,\n    \"bool_var\": True,\n    \"str_var\": \"cat\",\n    \"list_var\": [1, 2, 3],\n    \"dict_var\": {\"key1\": 1, \"key2\": 2},\n}\n\nconfig = Config(\n    variables=variables,\n    ... # other Config arguments\n)\n</code></pre> <p>A model definition could access the <code>int_var</code> value in a <code>WHERE</code> clause like this:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @INT_VAR\n</code></pre> <p>Alternatively, the same variable can be accessed by passing the variable name into the <code>@VAR()</code> macro function. Note that the variable name is in single quotes in the call <code>@VAR('int_var')</code>:</p> <pre><code>SELECT *\nFROM table\nWHERE int_variable = @VAR('int_var')\n</code></pre> <p>A default value can be passed as a second argument to the <code>@VAR()</code> macro function, which will be used as a fallback value if the variable is missing from the configuration file.</p> <p>In this example, the <code>WHERE</code> clause would render to <code>WHERE some_value = 0</code> because no variable named <code>missing_var</code> was defined in the project configuration file:</p> <pre><code>SELECT *\nFROM table\nWHERE some_value = @VAR('missing_var', 0)\n</code></pre> <p>A similar API is available for Python macro functions via the <code>evaluator.var</code> method and Python models via the <code>context.var</code> method.</p>"},{"location":"components/advanced-features/macros/built_in/#gateway-variables","title":"Gateway variables","text":"<p>Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's <code>variables</code> key:</p> YAMLPython <pre><code>gateways:\n  my_gateway:\n    variables:\n      int_var: 1\n    ...\n</code></pre> <pre><code>gateway_variables = {\n  \"int_var\": 1\n}\n\nconfig = Config(\n    gateways={\n      \"my_gateway\": GatewayConfig(\n        variables=gateway_variables\n        ... # other GatewayConfig arguments\n        ),\n      }\n)\n</code></pre> <p>Access them in models using the same methods as global variables.</p> <p>Gateway-specific variable values take precedence over variables with the same name specified in the root <code>variables</code> key.</p>"},{"location":"components/advanced-features/macros/built_in/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint macro variables are defined in a model. Blueprint variable values take precedence over global or gateway-specific variables with the same name.</p> <p>Blueprint variables are defined as a property of the <code>MODEL</code> statement, and serve as a mechanism for creating model templates:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),\n    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')\n  )\n);\n\nSELECT\n  @field_a,\n  @{field_b} AS field_b,\n  @field_c AS @{field_c}\nFROM @customer.some_source\n\n/*\nWhen rendered for customer1.some_table:\nSELECT\n  x,\n  y AS field_b,\n  'foo' AS foo\nFROM customer1.some_source\n\nWhen rendered for customer2.some_table:\nSELECT\n  z,\n  w AS field_b,\n  'bar' AS bar\nFROM customer2.some_source\n*/\n</code></pre> <p>Note the use of both regular <code>@field_a</code> and curly brace syntax <code>@{field_b}</code> macro variable references in the model query. Both of these will be rendered as identifiers. In the case of <code>field_c</code>, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax <code>@field_c</code> and if we want to use the string as an identifier then we use the curly braces <code>@{field_c}</code>. Learn more above</p> <p>Blueprint variables can be accessed using the syntax shown above, or through the <code>@BLUEPRINT_VAR()</code> macro function, which also supports specifying default values in case the variable is undefined (similar to <code>@VAR()</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#local-variables","title":"Local variables","text":"<p>Local macro variables are defined in a model. Local variable values take precedence over global, blueprint, or gateway-specific variables with the same name.</p> <p>Define your own local macro variables with the <code>@DEF</code> macro operator. For example, you could set the macro variable <code>macro_var</code> to the value <code>1</code> with:</p> <pre><code>@DEF(macro_var, 1);\n</code></pre> <p>Vulcan has three basic requirements for using the <code>@DEF</code> operator:</p> <ol> <li>The <code>MODEL</code> statement must end with a semi-colon <code>;</code></li> <li>All <code>@DEF</code> uses must come after the <code>MODEL</code> statement and before the SQL query</li> <li>Each <code>@DEF</code> use must end with a semi-colon <code>;</code></li> </ol> <p>For example, consider the following model <code>vulcan_example.full_model</code> from the Vulcan quickstart guide:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n</code></pre> <p>This model could be extended with a user-defined macro variable to filter the query results based on <code>item_size</code> like this:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n); -- NOTE: semi-colon at end of MODEL statement\n\n@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE\n  item_size &gt; @size -- Reference to macro variable `@size` defined above with `@DEF()`\nGROUP BY item_id\n</code></pre> <p>This example defines the macro variable <code>size</code> with <code>@DEF(size, 1)</code>. When the model is run, Vulcan will substitute in the number <code>1</code> where <code>@size</code> appears in the <code>WHERE</code> clause.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-functions","title":"Macro functions","text":"<p>In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:</p> <pre><code>MODEL(...);\n\n@DEF(\n  rank_to_int,\n  x -&gt; case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end\n);\n\nSELECT\n  id,\n  cust_rank_1,\n  cust_rank_2,\n  cust_rank_3\n  @rank_to_int(cust_rank_1) as cust_rank_1_int,\n  @rank_to_int(cust_rank_2) as cust_rank_2_int,\n  @rank_to_int(cust_rank_3) as cust_rank_3_int\nFROM\n  some.model\n</code></pre> <p>Multiple arguments can be expressed in a macro function as well:</p> <pre><code>@DEF(pythag, (x,y) -&gt; sqrt(pow(x, 2) + pow(y, 2)));\n\nSELECT\n  sideA,\n  sideB,\n  @pythag(sideA, sideB) AS sideC\nFROM\n  some.triangle\n</code></pre> <pre><code>@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -&gt; (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);\n\nSELECT\n  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,\n  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,\n  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,\nFROM\n  some.revenue\n</code></pre> <p>You can nest macro functions like so:</p> <pre><code>MODEL (\n  name dummy.model,\n  kind FULL\n);\n\n@DEF(area, r -&gt; pi() * r * r);\n@DEF(container_volume, (r, h) -&gt; @area(@r) * h);\n\nSELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#macro-operators","title":"Macro operators","text":"<p>Vulcan's macro system comes with a bunch of operators that let you add dynamic behavior to your models. These are the built-in tools that make your SQL adapt to different situations.</p>"},{"location":"components/advanced-features/macros/built_in/#each","title":"@EACH","text":"<p><code>@EACH</code> is like a <code>for</code> loop for your SQL. It takes a list of items and applies a function to each one, transforming them into whatever you need.</p> Learn more about <code>for</code> loops and <code>@EACH</code> <p>Before diving into the <code>@EACH</code> operator, let's dissect a <code>for</code> loop to understand its components.</p> <p><code>for</code> loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a <code>for</code> loop in Python:</p> <pre><code>for number in [4, 5, 6]:\n    print(number)\n</code></pre> <p>This for loop prints each number present in the brackets:</p> <pre><code>4\n5\n6\n</code></pre> <p>The first line of the example sets up the loop, doing two things:</p> <ol> <li>Telling Python that code inside the loop will refer to each item as <code>number</code></li> <li>Telling Python to step through the list of items in brackets</li> </ol> <p>The second line tells Python what action should be taken for each item. In this case, it prints the item.</p> <p>The loop executes one time for each item in the list, substituting in the item for the word <code>number</code> in the code. For example, the first time through the loop the code would execute as <code>print(4)</code> and the second time as <code>print(5)</code>.</p> <p>The Vulcan <code>@EACH</code> operator is used to implement the equivalent of a <code>for</code> loop in Vulcan macros.</p> <p><code>@EACH</code> gets its name from the fact that a loop performs the action \"for each\" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.</p> <p><code>@EACH</code> takes two arguments: a list of items and a function definition.</p> <pre><code>@EACH([list of items], [function definition])\n</code></pre> <p>The function definition is specified inline. This example specifies the identity function, returning the input unmodified:</p> <pre><code>SELECT\n  @EACH([4, 5, 6], number -&gt; number)\nFROM table\n</code></pre> <p>The loop is set up by the first argument: <code>@EACH([4, 5, 6]</code> tells Vulcan to step through the list of items in brackets.</p> <p>The second argument <code>number -&gt; number</code> tells Vulcan what action should be taken for each item using an \"anonymous\" function (aka \"lambda\" function). The left side of the arrow states what name the code on the right side will refer to each item as (like <code>name</code> in <code>for [name] in [items]</code> in a Python <code>for</code> loop).</p> <p>The right side of the arrow specifies what should be done to each item in the list. <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should return that item (e.g., <code>1</code>).</p> <p>Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If <code>@EACH</code> is used in the <code>SELECT</code> clause of a SQL statement:</p> <ol> <li>It prints the item</li> <li>It knows fields are separated by commas in <code>SELECT</code>, so it automatically separates the printed items with commas</li> </ol> <p>Because of the automatic print and comma-separation, the anonymous function <code>number -&gt; number</code> tells <code>@EACH</code> that for each item <code>number</code> it should print the item and separate the items with commas. Therefore, the complete output from the example is:</p> <pre><code>SELECT\n  4,\n  5,\n  6\nFROM table\n</code></pre> <p>This basic example is too simple to be useful. Many uses of <code>@EACH</code> will involve using the values as one or both of a literal value and an identifier.</p> <p>For example, a column <code>favorite_number</code> in our data might contain values <code>4</code>, <code>5</code>, and <code>6</code>, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6\nFROM table\n</code></pre> <p>In that SQL query each number is being used in two distinct ways. For example, <code>4</code> is being used:</p> <ol> <li>As a literal numeric value in <code>favorite_number = 4</code></li> <li>As part of a column name in <code>favorite_4</code></li> </ol> <p>We describe each of these uses separately.</p> <p>For the literal numeric value, <code>@EACH</code> substitutes in the exact value that is passed in the brackets, including quotes. For example, consider this query similar to the <code>CASE WHEN</code> example above:</p> <pre><code>SELECT\n  @EACH([4,5,6], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>It renders to this SQL:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>Note that the number <code>4</code>, <code>5</code>, and <code>6</code> are unquoted in both the input <code>@EACH</code> array in brackets and the resulting SQL query.</p> <p>We can instead quote them in the input <code>@EACH</code> array:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)\nFROM table\n</code></pre> <p>And they will be quoted in the resulting SQL query:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column\nFROM table\n</code></pre> <p>We can place the array values at the end of a column name by using the Vulcan macro operator <code>@</code> inside the <code>@EACH</code> function definition:</p> <pre><code>SELECT\n  @EACH(['4','5','6'], x -&gt; CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)\nFROM table\n</code></pre> <p>This query will render to:</p> <pre><code>SELECT\n  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,\n  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,\n  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6\nFROM table\n</code></pre> <p>This syntax works regardless of whether the array values are quoted or not.</p> <p>Embedding macros in strings</p> <p>You can put macro values at the end of a column name using <code>column_@x</code>, but if you want to put the variable anywhere else in the identifier, use curly braces <code>@{}</code> to avoid confusion. For example: <code>@{x}_column</code> or <code>my_@{x}_column</code> work great.</p> <p>Learn more about embedding macros in strings above</p>"},{"location":"components/advanced-features/macros/built_in/#if","title":"@IF","text":"<p><code>@IF</code> lets you conditionally include parts of your SQL based on a logical condition. It's like an if-then statement, but for your query.</p> <p>It has three parts:</p> <ol> <li>A condition that evaluates to <code>TRUE</code> or <code>FALSE</code> (written in SQL)</li> <li>What to return if the condition is <code>TRUE</code></li> <li>What to return if the condition is <code>FALSE</code> (this is optional, if you omit it and the condition is false, nothing gets included)</li> </ol> <p>These elements are specified as:</p> <pre><code>@IF([logical condition], [value if TRUE], [value if FALSE])\n</code></pre> <p>The value to return if the condition is <code>FALSE</code> is optional - if it is not provided and the condition is <code>FALSE</code>, then the macro has no effect on the resulting query.</p> <p>The logical condition should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports the following operators:</p> <ul> <li> <p>Equality: <code>=</code> for equals, <code>!=</code> or <code>&lt;&gt;</code> for not equals</p> </li> <li> <p>Comparison: <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>,</p> </li> <li> <p>Between: <code>[number] BETWEEN [low number] AND [high number]</code></p> </li> <li> <p>Membership: <code>[item] IN ([comma-separated list of items])</code></p> </li> </ul> <p>For example, the following simple conditions are all valid SQL and evaluate to <code>TRUE</code>:</p> <ul> <li> <p><code>'a' = 'a'</code></p> </li> <li> <p><code>'a' != 'b'</code></p> </li> <li> <p><code>0 &lt; 1</code></p> </li> <li> <p><code>1 &gt;= 1</code></p> </li> <li> <p><code>2 BETWEEN 1 AND 3</code></p> </li> <li> <p><code>'a' IN ('a', 'b')</code></p> </li> </ul> <p><code>@IF</code> can be used to modify any part of a SQL query. For example, this query conditionally includes <code>sensitive_col</code> in the query results:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 0, sensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 0</code> evaluates to <code>TRUE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  sensitive_col\nFROM table\n</code></pre> <p>Note that <code>@IF(1 &gt; 0, sensitive_col)</code> does not include the third argument specifying a value if <code>FALSE</code>. Had the condition evaluated to <code>FALSE</code>, <code>@IF</code> would return nothing and only <code>col1</code> would be selected.</p> <p>Alternatively, we could specify that <code>nonsensitive_col</code> be returned if the condition evaluates to <code>FALSE</code>:</p> <pre><code>SELECT\n  col1,\n  @IF(1 &gt; 2, sensitive_col, nonsensitive_col)\nFROM table\n</code></pre> <p>Because <code>1 &gt; 2</code> evaluates to <code>FALSE</code>, the query is rendered as:</p> <pre><code>SELECT\n  col1,\n  nonsensitive_col\nFROM table\n</code></pre> <p>Macro rendering occurs before the <code>@IF</code> condition is evaluated. For example, Vulcan doesn't evaluate the condition <code>my_column &gt; @my_value</code> until it has first substituted the number <code>@my_value</code> represents.</p> <p>Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro \"has side effects\"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.</p>"},{"location":"components/advanced-features/macros/built_in/#prepost-statements","title":"Pre/post-statements","text":"<p><code>@IF</code> may be used to conditionally execute pre/post-statements:</p> <pre><code>@IF([logical condition], [statement to execute if TRUE]);\n</code></pre> <p>The <code>@IF</code> statement itself must end with a semi-colon, but the inner statement argument must not.</p> <p>This example conditionally executes a pre/post-statement depending on the model's runtime stage, accessed via the pre-defined macro variable <code>@runtime_stage</code>. The <code>@IF</code> post-statement will only be executed at model evaluation time:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  grains (item_id),\n  audits (assert_positive_order_ids),\n);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\nORDER BY item_id;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR\n);\n</code></pre> <p>NOTE: alternatively, we could alter a column's type if the <code>@runtime_stage = 'creating'</code>, but that would only be useful if the model is incremental and the alteration would persist. <code>FULL</code> models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.</p>"},{"location":"components/advanced-features/macros/built_in/#eval","title":"@EVAL","text":"<p><code>@EVAL</code> evaluates its arguments with SQLGlot's SQL executor.</p> <p>It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the <code>@IF</code> operator, but it is not limited to logical conditions.</p> <p>For example, consider a query adding 5 to a macro variable:</p> <pre><code>MODEL (\n  ...\n);\n\n@DEF(x, 1);\n\nSELECT\n  @EVAL(5 + @x) as my_six\nFROM table\n</code></pre> <p>After macro variable substitution, this would render as <code>@EVAL(5 + 1)</code> and be evaluated to <code>6</code>, resulting in the final rendered query:</p> <pre><code>SELECT\n  6 as my_six\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#filter","title":"@FILTER","text":"<p><code>@FILTER</code> is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as <code>@EACH</code> or <code>@REDUCE</code>.</p> <p>The user-specified anonymous function must evaluate to <code>TRUE</code> or <code>FALSE</code>. <code>@FILTER</code> applies the function to each item in the array, only including the item in the output array if it meets the condition.</p> <p>The anonymous function should be written in SQL and is evaluated with SQLGlot's SQL executor. It supports standard SQL equality and comparison operators - see <code>@IF</code> above for more information about supported operators.</p> <p>For example, consider this <code>@FILTER</code> call:</p> <pre><code>@FILTER([1,2,3], x -&gt; x &gt; 1)\n</code></pre> <p>It applies the condition <code>x &gt; 1</code> to each item in the input array <code>[1,2,3]</code> and returns <code>[2,3]</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#reduce","title":"@REDUCE","text":"<p><code>@REDUCE</code> is used to combine the items in an array.</p> <p>The anonymous function specifies how the items in the input array should be combined. In contrast to <code>@EACH</code> and <code>@FILTER</code>, the anonymous function takes two arguments whose values are named in parentheses.</p> <p>For example, an anonymous function for <code>@EACH</code> might be specified <code>x -&gt; x + 1</code>. The <code>x</code> to the left of the arrow tells Vulcan that the array items will be referred to as <code>x</code> in the code to the right of the arrow.</p> <p>Because the <code>@REDUCE</code> anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, <code>(x, y) -&gt; x + y</code> tells Vulcan that items will be referred to as <code>x</code> and <code>y</code> in the code to the right of the arrow.</p> <p>Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.</p> <p>Consider the anonymous function <code>(x, y) -&gt; x + y</code>. Conceptually, only the <code>y</code> argument corresponds to items in the array; the <code>x</code> argument is a temporary value created when the function is evaluated.</p> <p>For the call <code>@REDUCE([1,2,3,4], (x, y) -&gt; x + y)</code>, the anonymous function is applied to the array in the following steps:</p> <ol> <li>Take the first two items in the array as <code>x</code> and <code>y</code>. Apply the function to them: <code>1 + 2</code> = <code>3</code>.</li> <li>Take the output of step (1) as <code>x</code> and the next item in the array <code>3</code> as <code>y</code>. Apply the function to them: <code>3 + 3</code> = <code>6</code>.</li> <li>Take the output of step (2) as <code>x</code> and the next item in the array <code>4</code> as <code>y</code>. Apply the function to them: <code>6 + 4</code> = <code>10</code>.</li> <li>No items remain. Return value from step (3): <code>10</code>.</li> </ol> <p><code>@REDUCE</code> will almost always be used with another macro operator. For example, we might want to build a <code>WHERE</code> clause from multiple column names:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  col1 = 1 and col2 = 1 and col3 = 1\n</code></pre> <p>We can use <code>@EACH</code> to build each column's predicate (e.g., <code>col1 = 1</code>) and <code>@REDUCE</code> to combine them into a single statement:</p> <pre><code>SELECT\n  my_column\nFROM\n  table\nWHERE\n  @REDUCE(\n    @EACH([col1, col2, col3], x -&gt; x = 1), -- Builds each individual predicate `col1 = 1`\n    (x, y) -&gt; x AND y -- Combines individual predicates with `AND`\n  )\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#star","title":"@STAR","text":"<p><code>@STAR</code> is used to return a set of column selections in a query.</p> <p><code>@STAR</code> is named after SQL's star operator <code>*</code>, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one <code>@STAR</code> and may also include explicit column selections.</p> <p><code>@STAR</code> uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.</p> <p>If the column data types are known, the resulting query <code>CAST</code>s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.</p> <p><code>@STAR</code> supports the following arguments, in this order:</p> <ul> <li> <p><code>relation</code>: The relation/table whose columns are being selected</p> </li> <li> <p><code>alias</code> (optional): The alias of the relation (if it has one)</p> </li> <li> <p><code>exclude</code> (optional): A list of columns to exclude</p> </li> <li> <p><code>prefix</code> (optional): A string to use as a prefix for all selected column names</p> </li> <li> <p><code>suffix</code> (optional): A string to use as a suffix for all selected column names</p> </li> <li> <p><code>quote_identifiers</code> (optional): Whether to quote the resulting identifiers, defaults to true</p> </li> </ul> <p>NOTE: the <code>exclude</code> argument used to be named <code>except_</code>. The latter is still supported but we discourage its use because it will be deprecated in the future.</p> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@STAR</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>alias</code> argument with <code>@STAR(foo, exclude := [c])</code>. Learn more about macro function arguments below.</p> <p>As a <code>@STAR</code> example, consider the following query:</p> <pre><code>SELECT\n  @STAR(foo, bar, [c], 'baz_', '_qux')\nFROM foo AS bar\n</code></pre> <p>The arguments to <code>@STAR</code> are:</p> <ol> <li>The name of the table <code>foo</code> (from the query's <code>FROM foo</code>)</li> <li>The table alias <code>bar</code> (from the query's <code>AS bar</code>)</li> <li>A list of columns to exclude from the selection, containing one column <code>c</code></li> <li>A string <code>baz_</code> to use as a prefix for all column names</li> <li>A string <code>_qux</code> to use as a suffix for all column names</li> </ol> <p><code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, if the column types are known the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"baz_a_qux\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"baz_b_qux\",\n  CAST(\"bar\".\"d\" AS INT) AS \"baz_d_qux\"\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li> <p>Each column is <code>CAST</code> to its data type in the table <code>foo</code> (e.g., <code>a</code> to <code>TEXT</code>)</p> </li> <li> <p>Each column selection uses the alias <code>bar</code> (e.g., <code>\"bar\".\"a\"</code>)</p> </li> <li> <p>Column <code>c</code> is not present because it was passed to <code>@STAR</code>'s <code>exclude</code> argument</p> </li> <li> <p>Each column alias is prefixed with <code>baz_</code> and suffixed with <code>_qux</code> (e.g., <code>\"baz_a_qux\"</code>)</p> </li> </ul> <p>Now consider a more complex example that provides different prefixes to <code>a</code> and <code>b</code> than to <code>d</code> and includes an explicit column <code>my_column</code>:</p> <pre><code>SELECT\n  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),\n  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),\n  my_column\nFROM foo AS bar\n</code></pre> <p>As before, <code>foo</code> is a table that contains four columns: <code>a</code> (<code>TEXT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>) and <code>d</code> (<code>INT</code>). After macro expansion, the query would be rendered as:</p> <pre><code>SELECT\n  CAST(\"bar\".\"a\" AS TEXT) AS \"ab_pre_a\",\n  CAST(\"bar\".\"b\" AS TEXT) AS \"ab_pre_b\",\n  CAST(\"bar\".\"d\" AS INT) AS \"d_pre_d\",\n  my_column\nFROM foo AS bar\n</code></pre> <p>Note these aspects of the rendered query:</p> <ul> <li> <p>Columns <code>a</code> and <code>b</code> have the prefix <code>\"ab_pre_\"</code> , while column <code>d</code> has the prefix <code>\"d_pre_\"</code></p> </li> <li> <p>Column <code>c</code> is not present because it was passed to the <code>exclude</code> argument in both <code>@STAR</code> calls</p> </li> <li> <p><code>my_column</code> is present in the query</p> </li> </ul>"},{"location":"components/advanced-features/macros/built_in/#generate_surrogate_key","title":"@GENERATE_SURROGATE_KEY","text":"<p><code>@GENERATE_SURROGATE_KEY</code> generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as <code>MD5</code>, on the concatenated column values.</p> <p>The surrogate key is created by:</p> <ol> <li><code>CAST</code>ing each column's value to <code>TEXT</code> (or the SQL engine's equivalent type)</li> <li>Replacing <code>NULL</code> values with the text <code>'_vulcan_surrogate_key_null_'</code> for each column</li> <li>Concatenating the column values after steps (1) and (2)</li> <li>Applying the <code>MD5()</code> hash function to the concatenated value returned by step (3)</li> </ol> <p>For example, the following query:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c) AS col\nFROM foo\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  MD5(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre> <p>By default, the <code>MD5</code> function is used, but this behavior can change by setting the <code>hash_function</code> argument as follows:</p> <pre><code>SELECT\n  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col\nFROM foo\n</code></pre> <p>This query will similarly be rendered as:</p> <pre><code>SELECT\n  SHA256(\n    CONCAT(\n      COALESCE(CAST(\"a\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"b\" AS TEXT), '_vulcan_surrogate_key_null_'),\n      '|',\n      COALESCE(CAST(\"c\" AS TEXT), '_vulcan_surrogate_key_null_')\n    )\n  ) AS \"col\"\nFROM \"foo\" AS \"foo\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_add","title":"@SAFE_ADD","text":"<p><code>@SAFE_ADD</code> adds two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_ADD(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_sub","title":"@SAFE_SUB","text":"<p><code>@SAFE_SUB</code> subtracts two or more operands, substituting <code>NULL</code>s with <code>0</code>s. It returns <code>NULL</code> if all operands are <code>NULL</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_SUB(a, b, c)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#safe_div","title":"@SAFE_DIV","text":"<p><code>@SAFE_DIV</code> divides two numbers, returning <code>NULL</code> if the denominator is <code>0</code>.</p> <p>For example, the following query:</p> <p></p><pre><code>SELECT\n  @SAFE_DIV(a, b)\nFROM foo\n</code></pre> would be rendered as:<p></p> <pre><code>SELECT\n  a / NULLIF(b, 0)\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#union","title":"@UNION","text":"<p><code>@UNION</code> returns a <code>UNION</code> query that selects all columns with matching names and data types from the tables.</p> <p>Its first argument can be either a condition or the <code>UNION</code> \"type\". If the first argument evaluates to a boolean (<code>TRUE</code> or <code>FALSE</code>), it's treated as a condition. If the condition is <code>FALSE</code>, only the first table is returned. If it's <code>TRUE</code>, the union operation is performed.</p> <p>If the first argument is not a boolean condition, it's treated as the <code>UNION</code> \"type\": either <code>'DISTINCT'</code> (removing duplicated rows) or <code>'ALL'</code> (returning all rows). Subsequent arguments are the tables to be combined.</p> <p>Let's assume that:</p> <ul> <li> <p><code>foo</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>TEXT</code>), <code>c</code> (<code>TEXT</code>)</p> </li> <li> <p><code>bar</code> is a table that contains three columns: <code>a</code> (<code>INT</code>), <code>b</code> (<code>INT</code>), <code>c</code> (<code>TEXT</code>)</p> </li> </ul> <p>Then, the following expression:</p> <pre><code>@UNION('distinct', foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>If the union type is omitted, <code>'ALL'</code> is used as the default. So the following expression:</p> <pre><code>@UNION(foo, bar)\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\nUNION ALL\nSELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM bar\n</code></pre> <p>You can also use a condition to control whether the union happens:</p> <pre><code>@UNION(1 &gt; 0, 'all', foo, bar)\n</code></pre> <p>This would render the same as above. However, if the condition is <code>FALSE</code>:</p> <pre><code>@UNION(1 &gt; 2, 'all', foo, bar)\n</code></pre> <p>Only the first table would be selected:</p> <pre><code>SELECT\n  CAST(a AS INT) AS a,\n  CAST(c AS TEXT) AS c\nFROM foo\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#haversine_distance","title":"@HAVERSINE_DISTANCE","text":"<p><code>@HAVERSINE_DISTANCE</code> returns the haversine distance between two geographic points.</p> <p>It supports the following arguments, in this order:</p> <ul> <li> <p><code>lat1</code>: Latitude of the first point</p> </li> <li> <p><code>lon1</code>: Longitude of the first point</p> </li> <li> <p><code>lat2</code>: Latitude of the second point</p> </li> <li> <p><code>lon2</code>: Longitude of the second point</p> </li> <li> <p><code>unit</code> (optional): The measurement unit, currently only <code>'mi'</code> (miles, default) and <code>'km'</code> (kilometers) are supported</p> </li> </ul> <p>Vulcan macro operators do not accept named arguments. For example, <code>@HAVERSINE_DISTANCE(lat1=lat_column)</code> will error.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist\nFROM rides\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist\nFROM rides\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#pivot","title":"@PIVOT","text":"<p><code>@PIVOT</code> returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a \"long\" format (multiple values in a single column) to a \"wide\" format (one value in each of multiple columns).</p> <p>It supports the following arguments, in this order:</p> <ul> <li> <p><code>column</code>: The column to pivot</p> </li> <li> <p><code>values</code>: The values to use for pivoting (one column is created for each value in <code>values</code>)</p> </li> <li> <p><code>alias</code> (optional): Whether to create aliases for the resulting columns, defaults to true</p> </li> <li> <p><code>agg</code> (optional): The aggregation function to use, defaults to <code>SUM</code></p> </li> <li> <p><code>cmp</code> (optional): The comparison operator to use for comparing the column values, defaults to <code>=</code></p> </li> <li> <p><code>prefix</code> (optional): A prefix to use for all aliases</p> </li> <li> <p><code>suffix</code> (optional): A suffix to use for all aliases</p> </li> <li> <p><code>then_value</code> (optional): The value to be used if the comparison succeeds, defaults to <code>1</code></p> </li> <li> <p><code>else_value</code> (optional): The value to be used if the comparison fails, defaults to <code>0</code></p> </li> <li> <p><code>quote</code> (optional): Whether to quote the resulting aliases, defaults to true</p> </li> <li> <p><code>distinct</code> (optional): Whether to apply a <code>DISTINCT</code> clause for the aggregation function, defaults to false</p> </li> </ul> <p>Like all Vulcan macro functions, omitting an argument when calling <code>@PIVOT</code> requires passing subsequent arguments with their name and the special <code>:=</code> keyword operator. For example, we might omit the <code>agg</code> argument with <code>@PIVOT(status, ['cancelled', 'completed'], cmp := '&lt;')</code>. Learn more about macro function arguments below.</p> <p>For example, the following query:</p> <pre><code>SELECT\n  date_day,\n  @PIVOT(status, ['cancelled', 'completed'])\nFROM rides\nGROUP BY 1\n</code></pre> <p>would be rendered as:</p> <pre><code>SELECT\n  date_day,\n  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS \"'cancelled'\",\n  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS \"'completed'\"\nFROM rides\nGROUP BY 1\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#deduplicate","title":"@DEDUPLICATE","text":"<p><code>@DEDUPLICATE</code> is used to deduplicate rows in a table based on the specified partition and order columns with a window function.</p> <p>It supports the following arguments, in this order:</p> <ul> <li> <p><code>relation</code>: The table or CTE name to deduplicate</p> </li> <li> <p><code>partition_by</code>: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row</p> </li> <li> <p><code>order_by</code>: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: [' desc nulls last']</p> </li> </ul> <p>For example, the following query: </p><pre><code>with raw_data as (\n@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])\n)\n\nselect * from raw_data\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"raw_data\" AS (\n  SELECT\n    *\n  FROM \"my_table\" AS \"my_table\"\n  QUALIFY\n    ROW_NUMBER() OVER (PARTITION BY \"id\", CAST(\"event_date\" AS DATE) ORDER BY \"event_date\" DESC, \"status\" ASC) = 1\n)\nSELECT\n  *\nFROM \"raw_data\" AS \"raw_data\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#date_spine","title":"@DATE_SPINE","text":"<p><code>@DATE_SPINE</code> returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the <code>date_spine</code> macro in <code>dbt-utils</code> which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in <code>where</code> clauses across many SQL models.</p> <p>It supports the following arguments, in this order:</p> <ul> <li> <p><code>datepart</code>: The datepart to use for the date spine - day, week, month, quarter, year</p> </li> <li> <p><code>start_date</code>: The start date for the date spine in format YYYY-MM-DD</p> </li> <li> <p><code>end_date</code>: The end date for the date spine in format YYYY-MM-DD</p> </li> </ul> <p>For example, the following query: </p><pre><code>WITH discount_promotion_dates AS (\n  @date_spine('day', '2024-01-01', '2024-01-16')\n)\n\nSELECT * FROM discount_promotion_dates\n</code></pre><p></p> <p>would be rendered as:</p> <pre><code>WITH \"discount_promotion_dates\" AS (\n  SELECT\n    \"_exploded\".\"date_day\" AS \"date_day\"\n  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS\nDATE[])) AS \"_exploded\"(\"date_day\")\n)\nSELECT\n  \"discount_promotion_dates\".\"date_day\" AS \"date_day\"\nFROM \"discount_promotion_dates\" AS \"discount_promotion_dates\"\n</code></pre> <p>Note: This is DuckDB SQL and other dialects will be transpiled accordingly. - Recursive CTEs (common table expressions) will be used for <code>Redshift / MySQL / MSSQL</code>.</p> <ul> <li>For <code>MSSQL</code> in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an <code>OPTION (MAXRECURSION 0)</code> clause after the date spine macro logic to remove the limit. This applies for long date ranges.</li> </ul>"},{"location":"components/advanced-features/macros/built_in/#resolve_template","title":"@RESOLVE_TEMPLATE","text":"<p><code>@resolve_template</code> is a helper macro intended to be used in situations where you need to gain access to the components of the physical object name. It's intended for use in the following situations:</p> <ul> <li> <p>Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)</p> </li> <li> <p>Generating references to engine-specific metadata tables that are derived from the physical table name, such as the <code>&lt;table&gt;$properties</code> metadata table in Trino.</p> </li> </ul> <p>Under the hood, it uses the <code>@this_model</code> variable so it can only be used during the <code>creating</code> and <code>evaluation</code> runtime stages. Attempting to use it at the <code>loading</code> runtime stage will result in a no-op.</p> <p>The <code>@resolve_template</code> macro supports the following arguments:</p> <ul> <li> <p><code>template</code> - The string template to render into an AST node</p> </li> <li> <p><code>mode</code> - What type of SQLGlot AST node to return after rendering the template. Valid values are <code>literal</code> or <code>table</code>. Defaults to <code>literal</code>.</p> </li> </ul> <p>The <code>template</code> can contain the following placeholders that will be substituted:</p> <ul> <li> <p><code>@{catalog_name}</code> - The name of the catalog, eg <code>datalake</code></p> </li> <li> <p><code>@{schema_name}</code> - The name of the physical schema that Vulcan is using for the model version table, eg <code>vulcan__landing</code></p> </li> <li> <p><code>@{table_name}</code> - The name of the physical table that Vulcan is using for the model version, eg <code>landing__customers__2517971505</code></p> </li> </ul> <p>Note the use of the curly brace syntax <code>@{}</code> in the template placeholders - learn more above.</p> <p>The <code>@resolve_template</code> macro can be used in a <code>MODEL</code> block:</p> <pre><code>MODEL (\n  name datalake.landing.customers,\n  ...\n  physical_properties (\n    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')\n  )\n);\n-- CREATE TABLE \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505\" ...\n\n-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')\n</code></pre> <p>And also within a query, using <code>mode := 'table'</code>:</p> <pre><code>SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')\n-- SELECT * FROM \"datalake\".\"vulcan__landing\".\"landing__customers__2517971505$properties\"\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#and","title":"@AND","text":"<p><code>@AND</code> combines a sequence of operands using the <code>AND</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@AND(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#or","title":"@OR","text":"<p><code>@OR</code> combines a sequence of operands using the <code>OR</code> operator, filtering out any NULL expressions.</p> <p>For example, the following expression:</p> <pre><code>@OR(TRUE, NULL)\n</code></pre> <p>would be rendered as:</p> <pre><code>TRUE\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operators","title":"SQL clause operators","text":"<p>Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:</p> <ul> <li> <p><code>@WITH</code>: common table expression <code>WITH</code> clause</p> </li> <li> <p><code>@JOIN</code>: table <code>JOIN</code> clause(s)</p> </li> <li> <p><code>@WHERE</code>: filtering <code>WHERE</code> clause</p> </li> <li> <p><code>@GROUP_BY</code>: grouping <code>GROUP BY</code> clause</p> </li> <li> <p><code>@HAVING</code>: group by filtering <code>HAVING</code> clause</p> </li> <li> <p><code>@ORDER_BY</code>: ordering <code>ORDER BY</code> clause</p> </li> <li> <p><code>@LIMIT</code>: limiting <code>LIMIT</code> clause</p> </li> </ul> <p>Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.</p>"},{"location":"components/advanced-features/macros/built_in/#how-sql-clause-operators-work","title":"How SQL clause operators work","text":"<p>The SQL clause operators take a single argument that determines whether the clause is generated.</p> <p>If the argument is <code>TRUE</code> the clause code is generated, if <code>FALSE</code> the code is not. The argument should be written in SQL and its value is evaluated with SQLGlot's SQL engine.</p> <p>Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.</p> <p>As an example of SQL clause operators, let's revisit the example model from the User-defined Variables section above.</p> <p>As written, the model will always include the <code>WHERE</code> clause. We could make its presence dynamic by using the <code>@WHERE</code> macro operator:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(TRUE) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The <code>@WHERE</code> argument is set to <code>TRUE</code>, so the WHERE code is included in the rendered model:</p> <pre><code>SELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\nWHERE item_id &gt; 1\nGROUP BY item_id\n</code></pre> <p>If the <code>@WHERE</code> argument were instead set to <code>FALSE</code> the <code>WHERE</code> clause would be omitted from the query.</p> <p>These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.</p> <p>For example, the <code>WHERE</code> clause will be included in this query because 1 less than 2:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(1 &lt; 2) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The operator's argument code can include macro variables.</p> <p>In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\n@DEF(left_number, 1);\n@DEF(right_number, 2);\n@DEF(size, 1);\n\nSELECT\n  item_id,\n  count(distinct id) AS num_orders,\nFROM\n  vulcan_example.incremental_model\n@WHERE(@left_number &lt; @right_number) item_id &gt; @size\nGROUP BY item_id\n</code></pre> <p>The argument to <code>@WHERE</code> will be \"1 &lt; 2\" as in the previous hard-coded example after the macro variables <code>left_number</code> and <code>right_number</code> are substituted in.</p>"},{"location":"components/advanced-features/macros/built_in/#sql-clause-operator-examples","title":"SQL clause operator examples","text":"<p>This section provides brief examples of each SQL clause operator's usage.</p> <p>The examples use variants of this simple select statement:</p> <pre><code>SELECT *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#with-operator","title":"@WITH operator","text":"<p>The <code>@WITH</code> operator is used to create common table expressions, or \"CTEs.\"</p> <p>CTEs are typically used in place of derived tables (subqueries in the <code>FROM</code> clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.</p> <pre><code>@WITH(True) all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre> <p>renders to</p> <pre><code>WITH all_cities as (select * from city)\nselect *\nFROM all_cities\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#join-operator","title":"@JOIN operator","text":"<p>The <code>@JOIN</code> operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER @JOIN(True) country\n  ON city.country = country.name\n</code></pre> <p>renders to</p> <pre><code>select *\nFROM all_cities\nLEFT OUTER JOIN country\n  ON city.country = country.name\n</code></pre> <p>The <code>@JOIN</code> operator recognizes that <code>LEFT OUTER</code> is a component of the <code>JOIN</code> specification and will omit it if the <code>@JOIN</code> argument evaluates to False.</p>"},{"location":"components/advanced-features/macros/built_in/#where-operator","title":"@WHERE operator","text":"<p>The <code>@WHERE</code> operator adds a filtering <code>WHERE</code> clause(s) to the query when its argument evaluates to True.</p> <pre><code>SELECT *\nFROM all_cities\n@WHERE(True) city_name = 'Toronto'\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nWHERE city_name = 'Toronto'\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#group_by-operator","title":"@GROUP_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@GROUP_BY(True) city_id\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nGROUP BY city_id\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#having-operator","title":"@HAVING operator","text":"<pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\n@HAVING(True) population &gt; 1000\n</code></pre> <p>renders to</p> <pre><code>SELECT\ncount(city_pop) as population\nFROM all_cities\nGROUP BY city_id\nHAVING population &gt; 1000\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#order_by-operator","title":"@ORDER_BY operator","text":"<pre><code>SELECT *\nFROM all_cities\n@ORDER_BY(True) city_pop\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nORDER BY city_pop\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#limit-operator","title":"@LIMIT operator","text":"<pre><code>SELECT *\nFROM all_cities\n@LIMIT(True) 10\n</code></pre> <p>renders to</p> <pre><code>SELECT *\nFROM all_cities\nLIMIT 10\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>Macro functions let you write reusable logic that you can call from multiple models. Instead of copying the same code everywhere, you define it once and reuse it.</p> <p>Vulcan supports macro functions in two languages:</p> <ul> <li> <p>SQL functions use the Jinja templating system</p> </li> <li> <p>Python functions use SQLGlot and give you way more power, you can do complex operations that go beyond what variables and operators can handle alone</p> </li> </ul>"},{"location":"components/advanced-features/macros/built_in/#python-macro-functions","title":"Python macro functions","text":""},{"location":"components/advanced-features/macros/built_in/#setup","title":"Setup","text":"<p>Python macro functions should be placed in <code>.py</code> files in the Vulcan project's <code>macros</code> directory. Multiple functions can be defined in one <code>.py</code> file, or they can be distributed across multiple files.</p> <p>An empty <code>__init__.py</code> file must be present in the Vulcan project's <code>macros</code> directory. It will be created automatically when the project scaffold is created with <code>vulcan init</code>.</p> <p>Each <code>.py</code> file containing a macro definition must import Vulcan's <code>macro</code> decorator with <code>from vulcan import macro</code>.</p> <p>Python macros are defined as regular python functions adorned with the Vulcan <code>@macro()</code> decorator. The first argument to the function must be <code>evaluator</code>, which provides the macro evaluation context in which the macro function will run.</p>"},{"location":"components/advanced-features/macros/built_in/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless argument type annotations are provided in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.</p> <p>Python macro functions may return values of either <code>string</code> or SQLGlot <code>expression</code> types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.</p> <p>Macro functions may return a list of strings or expressions that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple <code>CASE WHEN</code> statements would be incorporated into the query properly, but a list containing both <code>CASE WHEN</code> statements and a <code>WHERE</code> clause would not.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-function-basics","title":"Macro function basics","text":"<p>This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string <code>text</code>.</p> <pre><code>from vulcan import macro\n\n@macro() # Note parentheses at end of `@macro()` decorator\ndef print_text(evaluator):\n  return 'text'\n</code></pre> <p>We could use this in a Vulcan SQL model like this:</p> <pre><code>SELECT\n  @print_text() as my_text\nFROM table\n</code></pre> <p>After processing, it will render to this:</p> <pre><code>SELECT\n  text as my_text\nFROM table\n</code></pre> <p>Note that the python function returned a string <code>'text'</code>, but the rendered query uses <code>text</code> as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.</p> <p>The rendered query will treat <code>text</code> as a string if we double-quote the single-quoted value in the function definition as <code>\"'text'\"</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef print_text(evaluator):\n    return \"'text'\"\n</code></pre> <p>When run in the same model query as before, this will render to:</p> <pre><code>SELECT\n  'text' as my_text\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#argument-data-types","title":"Argument data types","text":"<p>Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information below.</p> <p>As mentioned above, argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).</p> <p>If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).</p> <p>For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg):\n    return my_arg in [1,2,3]\n</code></pre> <p>When this macro is called, it will return <code>FALSE</code> even if an integer was passed in the call. Consider this macro call:</p> <pre><code>SELECT\n  @arg_in_123(1)\n</code></pre> <p>It returns <code>SELECT FALSE</code> because:</p> <ol> <li>The passed value <code>1</code> is parsed by SQLGlot into a SQLGlot expression before the function code executes and</li> <li>There is no matching SQLGlot expression in <code>[1,2,3]</code></li> </ol> <p>However, the macro will treat the argument like a normal Python function does if we annotate <code>my_arg</code> with the integer <code>int</code> type in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`\n    return my_arg in [1,2,3]\n</code></pre> <p>Now the macro call will return <code>SELECT TRUE</code> because the value is coerced to a Python integer before the function code executes and <code>1</code> is in <code>[1,2,3]</code>.</p> <p>If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value <code>None</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#positional-and-keyword-arguments","title":"Positional and keyword arguments","text":"<p>In a macro call, the arguments may be provided by position if none are skipped.</p> <p>For example, consider the <code>add_args()</code> function - it has three arguments with default values provided in the function definition:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(\n    evaluator,\n    argument_1: int = 1,\n    argument_2: int = 2,\n    argument_3: int = 3\n):\n    return argument_1 + argument_2 + argument_3\n</code></pre> <p>An <code>@add_args</code> call providing values for all arguments accepts positional arguments like this: <code>@add_args(5, 6, 7)</code> (which returns 5 + 6 + 7 = <code>18</code>). A call omitting and using the default value for the the final <code>argument_3</code> can also use positional arguments: <code>@add_args(5, 6)</code> (which returns 5 + 6 + 3 = <code>14</code>).</p> <p>However, skipping an argument requires specifying the names of subsequent arguments (i.e., using \"keyword arguments\"). For example, skipping the second argument above by just omitting it - <code>@add_args(5, , 7)</code> - results in an error.</p> <p>Unlike Python, Vulcan keyword arguments must use the special operator <code>:=</code>. To skip and use the default value for the second argument above, the call must name the third argument: <code>@add_args(5, argument_3 := 8)</code> (which returns 5 + 2 + 8 = <code>15</code>).</p>"},{"location":"components/advanced-features/macros/built_in/#variable-length-arguments","title":"Variable-length arguments","text":"<p>The <code>add_args()</code> macro defined in the previous section accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.</p> <p>The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's \"variable-length arguments\" to accomplish this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`\n    return sum(args)\n</code></pre> <p>This macro can be called with one or more arguments. For example:</p> <ul> <li> <p><code>@add_args(1)</code> returns 1</p> </li> <li> <p><code>@add_args(1, 2)</code> returns 3</p> </li> <li> <p><code>@add_args(1, 2, 3)</code> returns 6</p> </li> </ul>"},{"location":"components/advanced-features/macros/built_in/#returning-more-than-one-value","title":"Returning more than one value","text":"<p>Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.</p> <p>For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into <code>CASE WHEN</code> statements.</p> <p>Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code cannot treat the input list as a regular Python list.</p> <p>Two things will happen to the input Python list before the function code is executed:</p> <ol> <li> <p>Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:</p> <ul> <li> <p>Numbers are parsed into <code>Literal</code> expressions</p> </li> <li> <p>Quoted strings are parsed into <code>Literal</code> expressions</p> </li> <li> <p>Unquoted strings are parsed into <code>Column</code> expressions</p> </li> </ul> </li> <li> <p>The parsed entries will be contained in a SQLGlot <code>Array</code> expression, the SQL entity analogous to a Python list</p> </li> </ol> <p>Because the input  <code>Array</code> expression named <code>values</code> is not a Python list, we cannot iterate over it directly - instead, we iterate over its <code>expressions</code> attribute with <code>values.expressions</code>:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef make_indicators(evaluator, string_column, values):\n    cases = []\n\n    for value in values.expressions: # Iterate over `values.expressions`\n        cases.append(f\"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}\")\n\n    return cases\n</code></pre> <p>We call this function in a model query to create <code>CASE WHEN</code> statements for the <code>vehicle</code> column values <code>truck</code> and <code>bus</code> like this:</p> <pre><code>SELECT\n  @make_indicators(vehicle, [truck, bus])\nFROM table\n</code></pre> <p>Which renders to:</p> <pre><code>SELECT\n  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,\n  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,\nFROM table\n</code></pre> <p>Note that in the call <code>@make_indicators(vehicle, [truck, bus])</code> none of the three values is quoted.</p> <p>Because they are unquoted, SQLGlot will parse them all as <code>Column</code> expressions. In the places we used single quotes when building the string (<code>'{value}'</code>), they will be single-quoted in the output. In the places we did not quote them (<code>{string_column} =</code> and <code>{string_column}_{value}</code>), they will not.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-predefined-and-local-variable-values","title":"Accessing predefined and local variable values","text":"<p>Pre-defined variables and user-defined local variables can be accessed within the macro's body via the <code>evaluator.locals</code> attribute.</p> <p>The first argument to every macro function, the macro evaluation context <code>evaluator</code>, contains macro variable values in its <code>locals</code> attribute. <code>evaluator.locals</code> is a dictionary whose key:value pairs are macro variables names and the associated values.</p> <p>For example, a function could access the predefined <code>execution_epoch</code> variable containing the epoch timestamp of when the execution started.</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_execution_epoch(evaluator):\n    return evaluator.locals['execution_epoch']\n</code></pre> <p>The function would return the <code>execution_epoch</code> value when called in a model query:</p> <pre><code>SELECT\n  @get_execution_epoch() as execution_epoch\nFROM table\n</code></pre> <p>The same approach works for user-defined local macro variables, where the key <code>\"execution_epoch\"</code> would be replaced with the name of the user-defined variable to be accessed.</p> <p>One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef get_macro_var(evaluator, macro_var):\n    return evaluator.locals[macro_var]\n</code></pre> <p>We could define a local macro variable <code>my_macro_var</code> with a value of 1 and pass it to the <code>get_macro_var</code> function like this:</p> <pre><code>MODEL (...);\n\n@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'\n\nSELECT\n  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function\nFROM table\n</code></pre> <p>The model query would render to:</p> <pre><code>SELECT\n  1 as macro_var_value\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-global-variable-values","title":"Accessing global variable values","text":"<p>User-defined global variables can be accessed within the macro's body using the <code>evaluator.var</code> method.</p> <p>If a global variable is not defined, the method will return a Python <code>None</code> value. You may provide a different default value as the method's second argument.</p> <p>For example:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    var_value = evaluator.var(\"&lt;var_name&gt;\") # Default value is `None`\n    another_var_value = evaluator.var(\"&lt;another_var_name&gt;\", \"default_value\") # Default value is `\"default_value\"`\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-physical-table-and-virtual-layer-view-names","title":"Accessing model, physical table, and virtual layer view names","text":"<p>All Vulcan models have a name in their <code>MODEL</code> specification. We refer to that as the model's \"unresolved\" name because it may not correspond to any specific object in the SQL engine.</p> <p>When Vulcan renders and executes a model, it converts the model name into three forms at different stages:</p> <ol> <li> <p>The fully qualified name</p> <ul> <li> <p>If the model name is of the form <code>schema.table</code>, Vulcan determines the correct catalog and adds it, like <code>catalog.schema.table</code></p> </li> <li> <p>Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like <code>\"catalog\".\"schema\".\"table\"</code></p> </li> </ul> </li> <li> <p>The resolved physical table name</p> <ul> <li>The qualified name of the model's underlying physical table</li> </ul> </li> <li> <p>The resolved virtual layer view name</p> <ul> <li>The qualified name of the model's virtual layer view in the environment where the model is being executed</li> </ul> </li> </ol> <p>You can access any of these three forms in a Python macro through properties of the <code>evaluation</code> context object.</p> <p>Access the unresolved, fully-qualified name through the <code>this_model_fqn</code> property.</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    # Example:\n    # Name in model definition: landing.customers\n    # Value returned here: '\"datalake\".\"landing\".\"customers\"'\n    unresolved_model_fqn = evaluator.this_model_fqn\n    ...\n</code></pre> <p>Access the resolved physical table and virtual layer view names through the <code>this_model</code> property.</p> <p>The <code>this_model</code> property returns different names depending on the runtime stage:</p> <ul> <li> <p><code>promoting</code> runtime stage: <code>this_model</code> resolves to the virtual layer view name</p> <ul> <li> <p>Example</p> <ul> <li> <p>Model name is <code>db.test_model</code></p> </li> <li> <p><code>plan</code> is running in the <code>dev</code> environment</p> </li> <li> <p><code>this_model</code> resolves to <code>\"catalog\".\"db__dev\".\"test_model\"</code> (note the <code>__dev</code> suffix in the schema name)</p> </li> </ul> </li> </ul> </li> <li> <p>All other runtime stages: <code>this_model</code> resolves to the physical table name</p> <ul> <li> <p>Example</p> <ul> <li> <p>Model name is <code>db.test_model</code></p> </li> <li> <p><code>plan</code> is running in any environment</p> </li> <li> <p><code>this_model</code> resolves to <code>\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"</code></p> </li> </ul> </li> </ul> </li> </ul> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"promoting\":\n        # virtual layer view name '\"catalog\".\"db__dev\".\"test_model\"'\n        resolved_name = evaluator.this_model\n    else:\n        # physical table name '\"catalog\".\"vulcan__project\".\"project__test_model__684351896\"'\n        resolved_name = evaluator.this_model\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#accessing-model-schemas","title":"Accessing model schemas","text":"<p>Model schemas can be accessed within a Python macro function through its evaluation context's <code>column_to_types()</code> method, if the column types can be statically determined. For instance, a schema of an external model can be accessed only after the <code>vulcan create_external_models</code> command has been executed.</p> <p>This macro function renames the columns of an upstream model by adding a prefix to them:</p> <pre><code>from sqlglot import exp\nfrom vulcan.core.macros import macro\n\n@macro()\ndef prefix_columns(evaluator, model_name, prefix: str):\n    renamed_projections = []\n\n    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,\n    # assuming that it does not contain quotes. If it did, we would have to generate SQL for\n    # each part of `model_name` separately and then concatenate these parts, because in that\n    # case `model_name.sql()` would produce an invalid lookup key.\n    model_name_sql = model_name.sql()\n\n    for name in evaluator.columns_to_types(model_name_sql):\n        new_name = prefix + name\n        renamed_projections.append(exp.column(name).as_(new_name))\n\n    return renamed_projections\n</code></pre> <p>This can then be used in a SQL model like this:</p> <pre><code>MODEL (\n  name schema.child,\n  kind FULL\n);\n\nSELECT\n  @prefix_columns(schema.parent, 'stg_')\nFROM\n  schema.parent\n</code></pre> <p>Note that <code>columns_to_types</code> expects an unquoted model name, such as <code>schema.parent</code>. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for <code>model_name</code> using the <code>sql()</code> method.</p> <p>Accessing the schema of an upstream model can be useful for various reasons. For example:</p> <ul> <li> <p>Renaming columns so that downstream consumers are not tightly coupled to external or source tables</p> </li> <li> <p>Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)</p> </li> <li> <p>Applying transformations to columns, such as masking PII or computing various statistics based on the column types</p> </li> </ul> <p>Thus, leveraging <code>columns_to_types</code> can also enable one to write code according to the DRY principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.</p> <p>Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: <code>__schema_unavailable_at_load__</code>. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.</p>"},{"location":"components/advanced-features/macros/built_in/#accessing-snapshots","title":"Accessing snapshots","text":"<p>After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the <code>get_snapshot</code> method of <code>MacroEvaluator</code>.</p> <p>This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:</p> <pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef some_macro(evaluator):\n    if evaluator.runtime_stage == \"evaluating\":\n        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly\n        intervals = evaluator.get_snapshot(\"some_model_name\").intervals\n        ...\n    ...\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#using-sqlglot-expressions","title":"Using SQLGlot expressions","text":"<p>Vulcan automatically parses strings returned by Python macro functions into SQLGlot expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.</p> <p>For example, consider a macro function that uses the <code>BETWEEN</code> operator in the predicate of a <code>WHERE</code> clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:</p> <pre><code>from vulcan import macro, SQL\n\n@macro()\ndef between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):\n    return f\"{column_name} BETWEEN {low_val} AND {high_val}\"\n</code></pre> <p>The function could then be called in a query:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE @between_where(a, 1, 3)\n</code></pre> <p>And it would render to:</p> <pre><code>SELECT\n  a\nFROM table\nWHERE a BETWEEN 1 and 3\n</code></pre> <p>Alternatively, the function could return a SQLGLot expression equivalent to that string by using SQLGlot's expression methods for building semantic representations:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef between_where(evaluator, column, low_val, high_val):\n    return column.between(low_val, high_val)\n</code></pre> <p>The methods are available because the <code>column</code> argument is parsed as a SQLGlot Column expression when the macro function is executed.</p> <p>Column expressions are sub-classes of the Condition class, so they have builder methods like <code>between</code> and <code>like</code>.</p>"},{"location":"components/advanced-features/macros/built_in/#macro-prepost-statements","title":"Macro pre/post-statements","text":"<p>Macro functions may be used to generate pre/post-statements in a model.</p> <p>By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.</p> <p>If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.</p> <p>Specify that a macro only affects a model's metadata by setting the <code>@macro()</code> decorator's <code>metadata_only</code> argument to <code>True</code>. For example:</p> <pre><code>from vulcan import macro\n\n@macro(metadata_only=True)\ndef print_message(evaluator, message):\n  print(message)\n</code></pre>"},{"location":"components/advanced-features/macros/built_in/#typed-macros","title":"Typed macros","text":"<p>Typed macros bring Python's type hinting to your SQL macros. By specifying what types your macro expects, you make your code more readable, easier to maintain, and less prone to errors. Plus, IDEs can give you better autocomplete and catch mistakes before you run your code.</p>"},{"location":"components/advanced-features/macros/built_in/#benefits-of-typed-macros","title":"Benefits of Typed Macros","text":"<ol> <li>Improved Readability: By specifying types, the intent of the macro is clearer to other developers or future you.</li> <li>Reduced Boilerplate: No need for manual type conversion within the macro function, allowing you to focus on the core logic.</li> <li>Enhanced Autocompletion: IDEs can provide better autocompletion and documentation based on the specified types.</li> </ol>"},{"location":"components/advanced-features/macros/built_in/#defining-a-typed-macro","title":"Defining a Typed Macro","text":"<p>Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return text * count\n</code></pre> <p>This macro takes two arguments: <code>text</code> of type <code>str</code> and <code>count</code> of type <code>int</code>, and it returns a string.</p> <p>Without type hints, the inputs are two SQLGlot <code>exp.Literal</code> objects you would need to manually convert to Python <code>str</code> and <code>int</code> types. With type hints, you can work with them as string and integer types directly.</p> <p>Let's try to use the macro in a Vulcan model:</p> <pre><code>SELECT\n  @repeat_string('Vulcan ', 3) as repeated_string\nFROM some_table;\n</code></pre> <p>Unfortunately, this model generates an error when rendered:</p> <pre><code>Error: Invalid expression / Unexpected token. Line 1, Col: 23.\n  Vulcan Vulcan Vulcan\n</code></pre> <p>Why? The macro returned <code>Vulcan Vulcan Vulcan</code> as expected, but that string is not valid SQL in the rendered query:</p> <pre><code>SELECT\n  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code\nFROM some_table;\n</code></pre> <p>The problem is a mismatch between our macro's Python return type <code>str</code> and the type expected by the parsed SQL query.</p> <p>Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.</p> <p>Therefore, we must explicitly convert the output with SQLGlot's <code>exp.Literal.string()</code> method:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef repeat_string(evaluator, text: str, count: int):\n    return exp.Literal.string(text * count)\n</code></pre> <p>Now the query will render with a valid single-quoted string literal:</p> <pre><code>SELECT\n  'Vulcan Vulcan Vulcan ' AS \"repeated_string\"\nFROM \"some_table\" AS \"some_table\"\n</code></pre> <p>Typed macros coerce the inputs to a macro function, but the macro code is responsible for coercing the output to the type expected by the query's semantic representation.</p>"},{"location":"components/advanced-features/macros/built_in/#supported-types","title":"Supported Types","text":"<p>Vulcan supports common Python types for typed macros including:</p> <ul> <li> <p><code>str</code> -- This handles string literals and basic identifiers, but won't coerce anything more complicated.</p> </li> <li> <p><code>int</code></p> </li> <li> <p><code>float</code></p> </li> <li> <p><code>bool</code></p> </li> <li> <p><code>datetime.datetime</code></p> </li> <li> <p><code>datetime.date</code></p> </li> <li> <p><code>SQL</code> -- When you want the SQL string representation of the argument that's passed in</p> </li> <li> <p><code>list[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</p> </li> <li> <p><code>tuple[T]</code> - where <code>T</code> is any supported type including sqlglot expressions</p> </li> <li> <p><code>T1 | T2 | ...</code> - where <code>T1</code>, <code>T2</code>, etc. are any supported types including sqlglot expressions</p> </li> </ul> <p>We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:</p> <ul> <li> <p><code>exp.Table</code></p> </li> <li> <p><code>exp.Column</code></p> </li> <li> <p><code>exp.Literal</code></p> </li> <li> <p><code>exp.Identifier</code></p> </li> </ul> <p>While these might be obvious examples, you can effectively coerce an input into any SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.</p> <pre><code>@macro()\ndef stamped(evaluator, query: exp.Select) -&gt; exp.Subquery:\n    return query.select(exp.Literal.string(str(datetime.now())).as_(\"stamp\")).subquery()\n\n# Coercing to a complex node like `exp.Select` works as expected given a string literal input\n# SELECT * FROM @stamped('SELECT a, b, c')\n</code></pre> <p>When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an <code>assert</code> statement. For example:</p> <pre><code>@macro()\ndef my_macro(evaluator, table: exp.Table) -&gt; exp.Column:\n    assert isinstance(table, exp.Table)\n    table.set(\"catalog\", \"dev\")\n    return table\n\n# Works\n# SELECT * FROM @my_macro('some.table')\n# SELECT * FROM @my_macro(some.table)\n\n# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning\n# SELECT * FROM @my_macro('SELECT 1 + 1')\n</code></pre> <p>In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you also get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.</p>"},{"location":"components/advanced-features/macros/built_in/#advanced-typed-macros","title":"Advanced Typed Macros","text":"<p>You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:</p> <pre><code>from typing import List\nfrom vulcan import macro\n\n@macro()\ndef sum_integers(evaluator, numbers: List[int]) -&gt; int:\n    return sum(numbers)\n</code></pre> <p>Usage in Vulcan:</p> <pre><code>SELECT\n  @sum_integers([1, 2, 3, 4, 5]) as total\nFROM some_table;\n</code></pre> <p>Generics can be nested and are resolved recursively allowing for fairly robust type hinting.</p>"},{"location":"components/advanced-features/macros/built_in/#conclusion","title":"Conclusion","text":"<p>Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.</p>"},{"location":"components/advanced-features/macros/built_in/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Vulcan macros and Jinja macros, but we strongly recommend picking one system per model. If you mix them, things can get confusing or break in unexpected ways. Pick the one that fits your needs and stick with it.</p>"},{"location":"components/advanced-features/macros/jinja/","title":"Jinja","text":""},{"location":"components/advanced-features/macros/jinja/#jinja","title":"Jinja","text":"<p>Vulcan supports macros from the Jinja templating system. If you're already familiar with Jinja (maybe from dbt or other tools), you can use it here.</p> <p>Jinja works differently than Vulcan's native macros. While Vulcan macros understand the semantic structure of your SQL, Jinja macros are pure string substitution, they assemble SQL text by replacing placeholders, without building a semantic representation of the query.</p> <p>dbt compatibility</p> <p>Vulcan supports the standard Jinja function library, but not dbt-specific functions like <code>{{ ref() }}</code>. If you're working with a dbt project using the Vulcan adapter, dbt-specific functions will work there, but not in native Vulcan projects.</p>"},{"location":"components/advanced-features/macros/jinja/#the-basics","title":"The basics","text":"<p>Jinja uses curly braces <code>{}</code> to mark macro code. The second character after the opening brace tells Jinja what to do:</p> <ul> <li> <p><code>{{...}}</code> - Expressions: These get replaced with values in your rendered SQL. Use them for variables and function calls.</p> </li> <li> <p><code>{%...%}</code> - Statements: These control flow and logic. Use them for <code>if</code> statements, <code>for</code> loops, and setting variables.</p> </li> <li> <p><code>{#...#}</code> - Comments: These are stripped out and won't appear in your final SQL.</p> </li> </ul> <p>Since Jinja syntax isn't valid SQL, you need to wrap your Jinja queries in special blocks so Vulcan knows to process them. For queries, use <code>JINJA_QUERY_BEGIN; ...; JINJA_END;</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_QUERY_BEGIN;\n\nSELECT {{ 1 + 1 }};\n\nJINJA_END;\n</code></pre> <p>For pre/post-statements (code that runs before or after your query), use <code>JINJA_STATEMENT_BEGIN; ...; JINJA_END;</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model\n);\n\nJINJA_STATEMENT_BEGIN;\n{{ pre_hook() }}\nJINJA_END;\n\nJINJA_QUERY_BEGIN;\nSELECT {{ 1 + 1 }};\nJINJA_END;\n\nJINJA_STATEMENT_BEGIN;\n{{ post_hook() }}\nJINJA_END;\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#using-vulcans-predefined-variables","title":"Using Vulcan's predefined variables","text":"<p>You can use all of Vulcan's predefined macro variables in your Jinja code. Some give you information about the Vulcan project itself (like <code>runtime_stage</code> or <code>this_model</code>), while others are temporal (like <code>start_ds</code> and <code>execution_date</code> for incremental models).</p> <p>Access them by putting the variable name (unquoted) inside curly braces:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';\n\nJINJA_END;\n</code></pre> <p>Notice the single quotes around the variable references? That's because <code>start_ds</code> and <code>end_ds</code> return string values. For numeric variables like <code>start_epoch</code>, you wouldn't need the quotes.</p> <p>One special case: the <code>gateway</code> variable is a function call, so you need parentheses: <code>{{ gateway() }}</code> instead of just <code>{{ gateway }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#user-defined-variables","title":"User-defined variables","text":"<p>Beyond the predefined variables, you can create your own. Vulcan supports global variables (defined in your project config) and local variables (defined in a specific model).</p>"},{"location":"components/advanced-features/macros/jinja/#global-variables","title":"Global variables","text":"<p>Global variables are defined in your project configuration file and can be used in any model. Learn more about setting them up in the Vulcan macros documentation.</p> <p>Access them using the <code>{{ var() }}</code> function. Pass the variable name (in single quotes) as the first argument, and optionally a default value as the second:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE int_variable = {{ var('int_var') }};\n\nJINJA_END;\n</code></pre> <p>If the variable might not exist, provide a default:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT *\nFROM table\nWHERE some_value = {{ var('missing_var', 0) }};\n\nJINJA_END;\n</code></pre> <p>If <code>missing_var</code> isn't defined, this will use <code>0</code> as the fallback value.</p>"},{"location":"components/advanced-features/macros/jinja/#gateway-variables","title":"Gateway variables","text":"<p>Gateway variables work just like global variables, but they're defined in a specific gateway's configuration. They take precedence over global variables with the same name. Learn more in the Vulcan macros documentation.</p> <p>Access them the same way as global variables using <code>{{ var() }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#blueprint-variables","title":"Blueprint variables","text":"<p>Blueprint variables let you create model templates. They're defined in the <code>MODEL</code> block and can be used to generate multiple models from one template:</p> <pre><code>MODEL (\n  name @customer.some_table,\n  kind FULL,\n  blueprints (\n    (customer := customer1, field_a := x, field_b := y),\n    (customer := customer2, field_a := z)\n  )\n);\n\nJINJA_QUERY_BEGIN;\nSELECT\n  {{ blueprint_var('field_a') }}\n  {{ blueprint_var('field_b', 'default_b') }} AS field_b\nFROM {{ blueprint_var('customer') }}.some_source\nJINJA_END;\n</code></pre> <p>Use <code>{{ blueprint_var() }}</code> to access them, with an optional default value just like <code>{{ var() }}</code>.</p>"},{"location":"components/advanced-features/macros/jinja/#local-variables","title":"Local variables","text":"<p>Define variables that are only available in the current model using <code>{% set ... %}</code>:</p> <pre><code>MODEL (\n  name vulcan_example.full_model,\n  kind FULL,\n  cron '@daily',\n  audits (assert_positive_order_ids),\n);\n\nJINJA_QUERY_BEGIN;\n\n{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`\n\nSELECT\n  item_id,\n  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}\nFROM\n  vulcan_example.incremental_model\nGROUP BY item_id\n\nJINJA_END;\n</code></pre> <p>The <code>{% set %}</code> statement goes after the <code>MODEL</code> block and before your SQL query.</p> <p>Jinja variables can be strings, numbers, or even complex data structures like lists, tuples, or dictionaries. They support Python methods too, so you can call <code>.upper()</code> on strings, iterate over lists, and so on.</p>"},{"location":"components/advanced-features/macros/jinja/#control-flow","title":"Control flow","text":"<p>Jinja gives you control flow operators to make your SQL dynamic.</p>"},{"location":"components/advanced-features/macros/jinja/#for-loops","title":"For loops","text":"<p>For loops let you iterate over collections to generate repetitive SQL. They start with <code>{% for ... %}</code> and end with <code>{% endfor %}</code>.</p> <p>Here's an example that creates indicator columns for different vehicle types:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  {% for vehicle_type in ['car', 'truck', 'bus'] %}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>A few things to notice:</p> <ul> <li> <p>The values in the list are quoted: <code>['car', 'truck', 'bus']</code></p> </li> <li> <p>When you use <code>{{ vehicle_type }}</code> in the <code>CASE WHEN</code>, you need quotes around it: <code>'{{ vehicle_type }}'</code></p> </li> <li> <p>When you use it in an identifier name like <code>vehicle_{{ vehicle_type }}</code>, no quotes needed</p> </li> <li> <p>There's a trailing comma after the <code>CASE WHEN</code> line, Vulcan's semantic understanding will remove it automatically</p> </li> </ul> <p>This renders to:</p> <pre><code>SELECT\n  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,\n  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,\n  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus\nFROM table\n</code></pre> <p>It's usually better to define your lists separately:</p> <pre><code>JINJA_QUERY_BEGIN;\n\n{% set vehicle_types = ['car', 'truck', 'bus'] %}\n\nSELECT\n  {% for vehicle_type in vehicle_types %}\n    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},\n  {% endfor %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>Same result, but easier to maintain.</p>"},{"location":"components/advanced-features/macros/jinja/#if-statements","title":"If statements","text":"<p>If statements let you conditionally include SQL based on some condition. They start with <code>{% if ... %}</code> and end with <code>{% endif %}</code>.</p> <p>The condition needs to evaluate to <code>True</code> or <code>False</code>. Things like <code>True</code>, <code>1 + 1 == 2</code>, or <code>'a' in ['a', 'b']</code> all work.</p> <p>Here's an example that conditionally includes a testing column:</p> <pre><code>JINJA_QUERY_BEGIN;\n\n{% set testing = True %}\n\nSELECT\n  normal_column,\n  {% if testing %}\n    testing_column\n  {% endif %}\nFROM table\n\nJINJA_END;\n</code></pre> <p>Since <code>testing</code> is <code>True</code>, this renders to:</p> <pre><code>SELECT\n  normal_column,\n  testing_column\nFROM table\n</code></pre>"},{"location":"components/advanced-features/macros/jinja/#user-defined-macro-functions","title":"User-defined macro functions","text":"<p>Macro functions let you reuse code across multiple models. Define them in <code>.sql</code> files in your project's <code>macros</code> directory (you can put multiple functions in one file or split them up).</p> <p>Define a function with <code>{% macro %}</code> and <code>{% endmacro %}</code>:</p> <pre><code>{% macro print_text() %}\ntext\n{% endmacro %}\n</code></pre> <p>Call it in your model with <code>{{ print_text() }}</code>, and it gets replaced with <code>text</code>.</p> <p>Functions can take arguments:</p> <pre><code>{% macro alias(expression, alias) %}\n  {{ expression }} AS {{ alias }}\n{% endmacro %}\n</code></pre> <p>Use it like this:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  item_id,\n  {{ alias('item_id', 'item_id2')}}\nFROM table\n\nJINJA_END;\n</code></pre> <p>This renders to:</p> <pre><code>SELECT\n  item_id,\n  item_id AS item_id2\nFROM table\n</code></pre> <p>Notice that even though you quoted the arguments in the function call, they're not quoted in the output. Vulcan's semantic understanding recognizes that <code>item_id</code> is a column name and handles it appropriately.</p> <p>If you want to select a string literal instead of a column, use double quotes around the string in the function call:</p> <pre><code>JINJA_QUERY_BEGIN;\n\nSELECT\n  item_id,\n  {{ alias(\"'item_id'\", 'item_id2')}}\nFROM table\n\nJINJA_END;\n</code></pre> <p>This renders to:</p> <pre><code>SELECT\n  item_id,\n  'item_id' AS item_id2\nFROM table\n</code></pre> <p>The double quotes tell Vulcan \"this is a string literal, not a column name.\" You can also use <code>'\"item_id\"'</code> if you want double quotes in the output (useful for some SQL dialects).</p>"},{"location":"components/advanced-features/macros/jinja/#mixing-macro-systems","title":"Mixing macro systems","text":"<p>Vulcan supports both Jinja and Vulcan macros, but we strongly recommend picking one system per model. Mixing them can lead to confusing behavior or errors.</p> <p>You can use predefined Vulcan macro variables in Jinja queries, but if you're passing them as arguments to a Jinja macro function, use the Jinja syntax <code>{{ start_ds }}</code> instead of the Vulcan <code>@start_ds</code> syntax. You may need to add quotes depending on what you're doing.</p>"},{"location":"components/advanced-features/macros/overview/","title":"Overview","text":""},{"location":"components/advanced-features/macros/overview/#overview","title":"Overview","text":"<p>SQL is a declarative language, which means you describe what you want, not how to get it. This provides clarity, but SQL doesn't have built-in features like variables or control flow (if-then statements, loops) that let your queries adapt to different situations.</p> <p>The problem? Data models are dynamic. You need different behavior depending on context, maybe filter by a different date each day, or include different columns based on configuration. That's where macros come in.</p> <p>Macros let you make your SQL dynamic. Instead of hardcoding values, you can use variables that get substituted at runtime. Instead of writing repetitive code, you can use functions that generate SQL for you.</p> <p>Vulcan supports two macro systems, each with its own strengths:</p> <ul> <li> <p>Vulcan macros: Built specifically for SQL, with semantic understanding of your queries</p> </li> <li> <p>Jinja macros: The popular templating system, useful if you're already familiar with it</p> </li> </ul> <p>Both systems can use the same pre-defined variables that Vulcan provides, like <code>@execution_ds</code> for the current execution date or <code>@this_model</code> for the current model name.</p> <p>Next steps:</p> <ul> <li> <p>Pre-defined macro variables - Built-in variables available in both systems</p> </li> <li> <p>Vulcan macros - Vulcan's native macro system with SQL-aware features</p> </li> <li> <p>Jinja macros - The Jinja templating system for SQL</p> </li> </ul>"},{"location":"components/advanced-features/macros/variables/","title":"Variables","text":""},{"location":"components/advanced-features/macros/variables/#variables","title":"Variables","text":"<p>Macro variables are placeholders that get replaced with actual values when Vulcan renders your SQL. They're what make your queries dynamic, instead of hardcoding values, you use variables that change based on context.</p> <p>Instead of writing <code>WHERE date &gt; '2023-01-01'</code> and manually updating it every day, you write <code>WHERE date &gt; @execution_ds</code> and it automatically uses today's date.</p> <p>Note</p> <p>This page covers Vulcan's built-in macro variables, the ones that come pre-configured and ready to use. If you want to create your own custom variables, check out the Vulcan macros page or Jinja macros page.</p>"},{"location":"components/advanced-features/macros/variables/#a-quick-example","title":"A quick example","text":"<p>Let's say you have a query that filters by date. Without macros, you'd write something like this:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-01-01'\n</code></pre> <p>Every time you want to change the date, you have to edit the query. That's tedious and error-prone.</p> <p>With a macro variable, you can make it dynamic:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; @execution_ds\n</code></pre> <p>The <code>@</code> symbol tells Vulcan \"this is a macro variable, replace it with a value before executing.\" The <code>@execution_ds</code> variable is predefined, so Vulcan automatically sets it to the date when execution started.</p> <p>If you run this model on February 1, 2023, Vulcan renders it as:</p> <pre><code>SELECT *\nFROM table\nWHERE my_date &gt; '2023-02-01'\n</code></pre> <p>The date updates automatically every time you run it. No manual editing needed!</p> <p>Vulcan comes with a bunch of predefined variables like this. You can also create your own custom variables if you need something specific, we'll cover those in the macro system pages.</p>"},{"location":"components/advanced-features/macros/variables/#predefined-variables","title":"Predefined variables","text":"<p>Vulcan provides a set of predefined variables that are automatically available in your models. Most of them are related to time (dates, timestamps, etc.), since time-based logic is common in data models.</p> <p>The time variables follow a consistent naming pattern: they combine a prefix (like <code>start</code>, <code>end</code>, or <code>execution</code>) with a postfix (like <code>ds</code>, <code>ts</code>, or <code>epoch</code>) to create variables like <code>@start_ds</code> or <code>@execution_epoch</code>.</p>"},{"location":"components/advanced-features/macros/variables/#temporal-variables","title":"Temporal variables","text":"<p>Vulcan uses Python's datetime module under the hood and follows the standard Unix epoch (starting January 1, 1970).</p> <p>Important</p> <p>All time-related predefined variables use UTC time zone. If you need to work with other timezones, you'll handle that in your query logic.</p> <p>Learn more about timezones and incremental models here.</p> <p>Prefixes tell you what time period the variable represents:</p> <ul> <li> <p><code>start</code> - The beginning of the time interval for this model run (inclusive)</p> </li> <li> <p><code>end</code> - The end of the time interval for this model run (inclusive)</p> </li> <li> <p><code>execution</code> - The exact timestamp when the execution started</p> </li> </ul> <p>Postfixes tell you what format the value is in:</p> <ul> <li> <p><code>dt</code> - A Python datetime object that becomes a SQL <code>TIMESTAMP</code></p> </li> <li> <p><code>dtntz</code> - A Python datetime object that becomes a SQL <code>TIMESTAMP WITHOUT TIME ZONE</code></p> </li> <li> <p><code>date</code> - A Python date object that becomes a SQL <code>DATE</code></p> </li> <li> <p><code>ds</code> - A date string formatted as <code>'YYYY-MM-DD'</code> (like <code>'2023-02-01'</code>)</p> </li> <li> <p><code>ts</code> - An ISO 8601 datetime string: <code>'YYYY-MM-DD HH:MM:SS'</code></p> </li> <li> <p><code>tstz</code> - An ISO 8601 datetime string with timezone: <code>'YYYY-MM-DD HH:MM:SS+00:00'</code></p> </li> <li> <p><code>hour</code> - An integer from 0-23 representing the hour of the day</p> </li> <li> <p><code>epoch</code> - An integer representing seconds since Unix epoch</p> </li> <li> <p><code>millis</code> - An integer representing milliseconds since Unix epoch</p> </li> </ul> <p>Here are all the temporal variables you can use:</p> <p>dt (datetime objects):</p> <ul> <li> <p><code>@start_dt</code></p> </li> <li> <p><code>@end_dt</code></p> </li> <li> <p><code>@execution_dt</code></p> </li> </ul> <p>dtntz (datetime without timezone):</p> <ul> <li> <p><code>@start_dtntz</code></p> </li> <li> <p><code>@end_dtntz</code></p> </li> <li> <p><code>@execution_dtntz</code></p> </li> </ul> <p>date (date objects):</p> <ul> <li> <p><code>@start_date</code></p> </li> <li> <p><code>@end_date</code></p> </li> <li> <p><code>@execution_date</code></p> </li> </ul> <p>ds (date strings):</p> <ul> <li> <p><code>@start_ds</code></p> </li> <li> <p><code>@end_ds</code></p> </li> <li> <p><code>@execution_ds</code></p> </li> </ul> <p>ts (timestamp strings):</p> <ul> <li> <p><code>@start_ts</code></p> </li> <li> <p><code>@end_ts</code></p> </li> <li> <p><code>@execution_ts</code></p> </li> </ul> <p>tstz (timestamp strings with timezone):</p> <ul> <li> <p><code>@start_tstz</code></p> </li> <li> <p><code>@end_tstz</code></p> </li> <li> <p><code>@execution_tstz</code></p> </li> </ul> <p>hour (hour integers):</p> <ul> <li> <p><code>@start_hour</code></p> </li> <li> <p><code>@end_hour</code></p> </li> <li> <p><code>@execution_hour</code></p> </li> </ul> <p>epoch (Unix epoch seconds):</p> <ul> <li> <p><code>@start_epoch</code></p> </li> <li> <p><code>@end_epoch</code></p> </li> <li> <p><code>@execution_epoch</code></p> </li> </ul> <p>millis (Unix epoch milliseconds):</p> <ul> <li> <p><code>@start_millis</code></p> </li> <li> <p><code>@end_millis</code></p> </li> <li> <p><code>@execution_millis</code></p> </li> </ul>"},{"location":"components/advanced-features/macros/variables/#runtime-variables","title":"Runtime variables","text":"<p>Beyond time, Vulcan provides variables that give you information about the current execution context:</p> <ul> <li> <p><code>@runtime_stage</code> - A string telling you what stage Vulcan is currently in. Useful for conditionally running code based on whether you're creating tables, evaluating queries, or promoting views. Possible values:</p> </li> <li> <p><code>'loading'</code> - Project is being loaded into Vulcan's runtime</p> </li> <li> <p><code>'creating'</code> - Model tables are being created for the first time</p> </li> <li> <p><code>'evaluating'</code> - Model query is being evaluated and data inserted</p> </li> <li> <p><code>'promoting'</code> - Model is being promoted (view created in virtual layer)</p> </li> <li> <p><code>'demoting'</code> - Model is being demoted (view dropped from virtual layer)</p> </li> <li> <p><code>'auditing'</code> - Audit is being run</p> </li> <li> <p><code>'testing'</code> - Model is being evaluated in a unit test context</p> </li> </ul> <p>Learn more about using this in pre/post-statements.</p> <ul> <li> <p><code>@gateway</code> - The name of the current gateway (your database connection)</p> </li> <li> <p><code>@this_model</code> - The physical table name that the model's view selects from. Useful for creating generic audits. When used in on_virtual_update statements, it contains the qualified view name instead.</p> </li> <li> <p><code>@model_kind_name</code> - The name of the current model kind (like <code>'FULL'</code> or <code>'INCREMENTAL_BY_TIME_RANGE'</code>). Useful when you need to control physical properties in model defaults based on the model kind.</p> </li> </ul> <p>Embedding variables in strings</p> <p>Sometimes you'll see variables written with curly braces like <code>@{variable}</code> instead of just <code>@variable</code>. They do different things!</p> <p>The curly brace syntax tells Vulcan to treat the rendered value as a SQL identifier (like a table or column name), not a string literal. So if <code>variable</code> contains <code>foo.bar</code>, then:</p> <ul> <li> <p><code>@variable</code> produces <code>foo.bar</code> (as a literal value)</p> </li> <li> <p><code>@{variable}</code> produces <code>\"foo.bar\"</code> (as an identifier, with quotes)</p> </li> </ul> <p>You'll most often use <code>@{variable}</code> when you want to interpolate a value into an identifier name, like <code>@{schema}_table</code>. The regular <code>@variable</code> syntax is for plain value substitution.</p> <p>Learn more in the Vulcan macros documentation.</p>"},{"location":"components/advanced-features/macros/variables/#before-all-and-after-all-variables","title":"Before all and after all variables","text":"<p>These variables are available in <code>before_all</code> and <code>after_all</code> statements, as well as in any macros called within those statements:</p> <ul> <li> <p><code>@this_env</code> - The name of the current environment</p> </li> <li> <p><code>@schemas</code> - A list of schema names in the virtual layer for the current environment</p> </li> <li> <p><code>@views</code> - A list of view names in the virtual layer for the current environment</p> </li> </ul> <p>These are useful when you need to perform setup or cleanup operations that depend on the environment context.</p>"},{"location":"components/audits/audits/","title":"Audits","text":""},{"location":"components/audits/audits/#audits","title":"Audits","text":"<p>Audits stop bad data before it causes problems downstream. They run after every model execution and halt your models if something's wrong.</p> <p>Unlike tests (which you run manually to verify logic), audits run automatically whenever you apply a plan. They catch data quality issues early, whether they come from external vendors, upstream teams, or your own model changes.</p> <p>All audits in Vulcan are blocking. When an audit fails, Vulcan stops everything: no plan application, no run execution. This prevents bad data from propagating through your entire pipeline.</p> <p>A comprehensive suite of audits helps you catch problems upstream, builds trust in your data across the organization, and lets your team work with confidence knowing that invalid data won't slip through.</p> <p>Note: For incremental by time range models, audits only run on the intervals being processed, not the entire table. This keeps things fast and focused on what actually changed.</p>"},{"location":"components/audits/audits/#terminology-audits-and-assertions","title":"Terminology: Audits and Assertions","text":"<p>Before we dive in, let's clear up some terminology. Vulcan uses two related but distinct concepts:</p> <ul> <li> <p>AUDIT - The validation rule itself (the SQL query that checks for problems)</p> </li> <li> <p>ASSERTION - Attaching an audit to a model (claiming it should pass)</p> </li> </ul> <p>An audit is the rule (\"prices must be positive\"), and an assertion is you saying \"this model follows that rule.\"</p> <p>In MODEL definitions:</p> <pre><code>-- Define the AUDIT (the rule)\nAUDIT (name check_positive_price);\nSELECT * FROM @this_model WHERE price &lt;= 0;\n\n-- Make ASSERTIONS about your model (attach the audit)\nMODEL (\n  name products,\n  assertions (check_positive_price)  -- Declaring this audit should pass\n);\n</code></pre> <p>Note: You might see older code using <code>audits</code> instead of <code>assertions</code> in MODEL definitions. Both work identically, but <code>assertions</code> is clearer, you're asserting that your model passes these audits. This documentation uses <code>assertions</code> throughout.</p>"},{"location":"components/audits/audits/#how-audits-work","title":"How Audits Work","text":"<p>When an audit fails, Vulcan stops everything. No ifs, ands, or buts. This is by design, it's better to catch problems early than to let bad data flow downstream and cause bigger issues.</p> <p>Here's what happens when you run a model:</p> <ol> <li>Evaluate the model - Vulcan runs your model SQL (inserts new data, rebuilds the table, etc.)</li> <li>Run the audit query - Vulcan executes your audit SQL against the newly updated table. For incremental models, this only checks the intervals you're processing (keeps things fast!)</li> <li>Check the results - If the query returns any rows, the audit fails and everything stops</li> </ol> <p>Why this matters: Audits query for bad data. If your audit finds bad data (returns rows), that's a problem. If it finds nothing (returns zero rows), you're good to go.</p>"},{"location":"components/audits/audits/#plan-vs-run","title":"Plan vs. Run","text":"<p>The difference between <code>plan</code> and <code>run</code> matters a lot when it comes to audits:</p> <p><code>plan</code> - The safe way: - Vulcan evaluates and audits all modified models before promoting them to production</p> <ul> <li> <p>If an audit fails, the plan stops and your production table is untouched</p> </li> <li> <p>Invalid data stays in an isolated table and never reaches production</p> </li> <li> <p>This is like testing in a sandbox before deploying</p> </li> </ul> <p><code>run</code> - The direct way: - Vulcan evaluates and audits models directly against the production environment</p> <ul> <li> <p>If an audit fails, the run stops, but the invalid data is already in production</p> </li> <li> <p>The blocking prevents this bad data from being used to build downstream models</p> </li> <li> <p>This is like deploying directly, faster, but riskier</p> </li> </ul> <p>Which should you use? For production changes, use <code>plan</code>. It's safer and gives you a chance to fix issues before they hit production. Use <code>run</code> when you're confident or doing quick iterations.</p>"},{"location":"components/audits/audits/#fixing-a-failed-audit","title":"Fixing a Failed Audit","text":"<p>So an audit failed. Don't panic! Here's how to fix it:</p> <ol> <li> <p>Find the root cause - Look at the audit query results. What data failed? Check upstream models and data sources.</p> </li> <li> <p>Fix the source - This depends on where the problem came from:    - External data source? Fix it at the source, then run a restatement plan on the first Vulcan model that ingests it. This will restate all downstream models automatically.</p> </li> </ol> <ul> <li>Vulcan model? Update the model's logic, then apply the change with a <code>plan</code>. Vulcan will automatically re-evaluate all downstream models.</li> </ul> <p>The key is fixing the root cause, not just the symptom. If bad data is coming from upstream, fixing it downstream won't help long-term.</p>"},{"location":"components/audits/audits/#user-defined-audits","title":"User-Defined Audits","text":"<p>You can write your own audits! They're just SQL queries that should return zero rows. If they return rows, that means they found bad data and the audit fails.</p> <p>Audits live in <code>.sql</code> files in an <code>audits</code> directory in your project. You can put multiple audits in one file (organize them however makes sense) or define them inline in your model files.</p>"},{"location":"components/audits/audits/#your-first-audit","title":"Your First Audit","text":"<p>Let's create a simple audit. Here's the basic structure:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  dialect spark\n);\nSELECT * from sushi.items\nWHERE\n  ds BETWEEN @start_ds AND @end_ds\n  AND price IS NULL;\n</code></pre> <p>This audit checks that every sushi item has a price. If any items are missing prices (the query returns rows), the audit fails.</p> <p>A few things to note:</p> <ul> <li> <p>The <code>name</code> is what you'll reference when attaching it to a model</p> </li> <li> <p>If your query uses a different SQL dialect than your project, specify it with <code>dialect</code> (like <code>spark</code> in the example)</p> </li> <li> <p>The <code>@start_ds</code> and <code>@end_ds</code> macros are automatically filled in for incremental models</p> </li> </ul> <p>To actually use this audit, attach it to a model:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (assert_item_price_is_not_null)\n);\n</code></pre> <p>This audit runs every time the <code>sushi.items</code> model runs.</p>"},{"location":"components/audits/audits/#generic-audits","title":"Generic Audits","text":"<p>Here's where audits get really powerful. You can create parameterized audits that work across multiple models. This saves you from writing the same audit over and over.</p> <p>Consider this audit that checks if a column exceeds a threshold:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>This uses macros to make it flexible:</p> <ul> <li> <p><code>@this_model</code> is a special macro that refers to the model being audited (and handles incremental models correctly)</p> </li> <li> <p><code>@column</code> and <code>@threshold</code> are parameters you'll specify when you use the audit</p> </li> </ul> <p>Now you can use this same audit for different columns and thresholds:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    does_not_exceed_threshold(column := id, threshold := 1000),\n    does_not_exceed_threshold(column := price, threshold := 100)\n  )\n);\n</code></pre> <p>You can use the same audit multiple times on the same model with different parameters.</p> <p>Default values:</p> <p>You can set default values for parameters:</p> <pre><code>AUDIT (\n  name does_not_exceed_threshold,\n  defaults (\n    threshold = 10,\n    column = id\n  )\n);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n</code></pre> <p>If someone uses the audit without specifying parameters, it uses these defaults.</p> <p>Global audits:</p> <p>You can also apply audits globally using model defaults:</p> <pre><code>model_defaults:\n  assertions:\n    - assert_positive_order_ids\n\n    - does_not_exceed_threshold(column := id, threshold := 1000)\n</code></pre> <p>This applies these audits to all models by default.</p> <p>Note: In <code>model_defaults</code>, you can use either <code>audits</code> or <code>assertions</code>, both work for backward compatibility.</p>"},{"location":"components/audits/audits/#naming","title":"Naming","text":"<p>Avoid SQL keywords when naming audit parameters. If you must use a keyword, quote it.</p> <p>For example, if your audit uses a <code>values</code> parameter (which is a SQL keyword), you'll need quotes:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    my_audit(column := a, \"values\" := (1,2,3))\n  )\n)\n</code></pre> <p>It's easier to just avoid keywords in the first place, but if you need them, quotes work fine.</p>"},{"location":"components/audits/audits/#inline-audits","title":"Inline Audits","text":"<p>You can also define audits right in your model file. This is useful when an audit is specific to one model:</p> <pre><code>MODEL (\n    name sushi.items,\n    assertions(does_not_exceed_threshold(column := id, threshold := 1000), price_is_not_null)\n);\nSELECT id, price\nFROM sushi.seed;\n\nAUDIT (name does_not_exceed_threshold);\nSELECT * FROM @this_model\nWHERE @column &gt;= @threshold;\n\nAUDIT (name price_is_not_null);\nSELECT * FROM @this_model\nWHERE price IS NULL;\n</code></pre> <p>You can define multiple audits in the same file. Just make sure they're defined before (or alongside) the MODEL that uses them.</p>"},{"location":"components/audits/audits/#built-in-audits","title":"Built-in Audits","text":"<p>Vulcan comes with a whole suite of built-in audits that cover most common use cases. These are ready to use, no need to write SQL yourself for these scenarios.</p> <p>All built-in audits are blocking (they stop execution when they fail), and they're grouped by what they check. Let's walk through them:</p>"},{"location":"components/audits/audits/#generic-assertion-audit","title":"Generic Assertion Audit","text":""},{"location":"components/audits/audits/#forall","title":"<code>forall</code>","text":"<p>The most flexible built-in audit. It lets you write arbitrary boolean SQL expressions:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    forall(criteria := (\n      price &gt; 0,\n      LENGTH(name) &gt; 0\n    ))\n  )\n);\n</code></pre> <p>This checks that all rows have a <code>price</code> greater than 0 AND a <code>name</code> with at least one character. You can add as many criteria as you want, they all need to pass.</p>"},{"location":"components/audits/audits/#row-counts-and-null-value-audits","title":"Row Counts and NULL Value Audits","text":"<p>These audits check that you have enough data and that required fields aren't missing.</p>"},{"location":"components/audits/audits/#number_of_rows","title":"<code>number_of_rows</code>","text":"<p>Make sure you have enough rows. Useful for catching cases where a model didn't run properly or data didn't load:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    number_of_rows(threshold := 10)\n  )\n);\n</code></pre> <p>This ensures your model has more than 10 rows. If you have 10 or fewer, something's probably wrong.</p>"},{"location":"components/audits/audits/#not_null","title":"<code>not_null</code>","text":"<p>The classic \"required field\" check. Ensures specified columns don't have NULL values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    not_null(columns := (id, customer_id, waiter_id))\n  )\n);\n</code></pre> <p>This checks that <code>id</code>, <code>customer_id</code>, and <code>waiter_id</code> are never NULL. If any of them are NULL, the audit fails.</p>"},{"location":"components/audits/audits/#at_least_one","title":"<code>at_least_one</code>","text":"<p>Sometimes you just need at least one non-NULL value, not all of them. This is useful for optional fields that should have some data:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    at_least_one(column := zip)\n    )\n);\n</code></pre> <p>This ensures the <code>zip</code> column has at least one non-NULL value. Maybe most customers don't have zip codes, but at least some should.</p>"},{"location":"components/audits/audits/#not_null_proportion","title":"<code>not_null_proportion</code>","text":"<p>Check that NULL values don't exceed a certain percentage. Useful when some NULLs are okay, but too many is a problem:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    not_null_proportion(column := zip, threshold := 0.8)\n    )\n);\n</code></pre> <p>This ensures that at least 80% of rows have a zip code. The other 20% can be NULL, but if more than 20% are missing, that's a problem.</p>"},{"location":"components/audits/audits/#specific-data-values-audits","title":"Specific Data Values Audits","text":"<p>These audits check the actual values in your data, not just whether they exist.</p>"},{"location":"components/audits/audits/#not_constant","title":"<code>not_constant</code>","text":"<p>Make sure a column has variety. If every row has the same value, something might be wrong:</p> <pre><code>MODEL (\n  name sushi.customer_revenue_by_day,\n  assertions (\n    not_constant(column := customer_id)\n    )\n);\n</code></pre> <p>This ensures <code>customer_id</code> has at least two different non-NULL values. If every row has the same customer ID, that's suspicious.</p>"},{"location":"components/audits/audits/#unique_values","title":"<code>unique_values</code>","text":"<p>The classic uniqueness check. Ensures no duplicate values:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_values(columns := (id, item_id))\n  )\n);\n</code></pre> <p>This checks that <code>id</code> and <code>item_id</code> each have unique values. No duplicates allowed!</p>"},{"location":"components/audits/audits/#unique_combination_of_columns","title":"<code>unique_combination_of_columns</code>","text":"<p>Check uniqueness across multiple columns. Maybe individual columns can repeat, but combinations must be unique:</p> <pre><code>MODEL (\n  name sushi.orders,\n  assertions (\n    unique_combination_of_columns(columns := (id, ds))\n  )\n);\n</code></pre> <p>This ensures that the combination of <code>id</code> and <code>ds</code> is unique. So <code>id</code> can repeat across different dates, but the same <code>id</code> can't appear twice on the same date.</p>"},{"location":"components/audits/audits/#accepted_values","title":"<code>accepted_values</code>","text":"<p>Make sure values are in an allowed set. Like an enum check:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_values(column := name, is_in := ('Hamachi', 'Unagi', 'Sake'))\n  )\n);\n</code></pre> <p>This ensures that <code>name</code> is one of the three allowed values. Anything else fails the audit.</p> <p>Note</p> <p>Rows with <code>NULL</code> values will pass this audit in most databases. If you want to reject NULLs, combine this with a <code>not_null</code> audit.</p>"},{"location":"components/audits/audits/#not_accepted_values","title":"<code>not_accepted_values</code>","text":"<p>The opposite, make sure certain values are NOT present:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_accepted_values(column := name, is_in := ('Hamburger', 'French fries'))\n  )\n);\n</code></pre> <p>This ensures that <code>name</code> is never 'Hamburger' or 'French fries'. Useful for catching data that shouldn't be there.</p> <p>Note</p> <p>This audit doesn't support rejecting <code>NULL</code> values. Use <code>not_null</code> if you need to ensure no NULLs.</p>"},{"location":"components/audits/audits/#numeric-data-audits","title":"Numeric Data Audits","text":"<p>These audits check numeric ranges and distributions.</p>"},{"location":"components/audits/audits/#sequential_values","title":"<code>sequential_values</code>","text":"<p>Check that values are sequential. Useful for IDs or sequence numbers:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    sequential_values(column := item_id, interval := 1)\n  )\n);\n</code></pre> <p>This ensures that <code>item_id</code> values are sequential (1, 2, 3, 4...). If you have gaps or duplicates, the audit fails.</p>"},{"location":"components/audits/audits/#accepted_range","title":"<code>accepted_range</code>","text":"<p>Check that values are within a numeric range:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 1, max_v := 100)\n  )\n);\n</code></pre> <p>This ensures all prices are between 1 and 100 (inclusive). Values outside this range fail the audit.</p> <p>Exclusive ranges:</p> <p>You can make the range exclusive (not including the boundaries):</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    accepted_range(column := price, min_v := 0, max_v := 100, inclusive := false)\n  )\n);\n</code></pre> <p>Now prices must be greater than 0 and less than 100 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#mutually_exclusive_ranges","title":"<code>mutually_exclusive_ranges</code>","text":"<p>Check that ranges don't overlap. Useful for pricing tiers or time slots:</p> <pre><code>MODEL (\n  name pricing.tier_ranges,\n  assertions (\n    mutually_exclusive_ranges(lower_bound_column := min_price, upper_bound_column := max_price)\n  )\n);\n</code></pre> <p>This ensures that each row's price range [min_price, max_price] doesn't overlap with any other row's range. Perfect for ensuring pricing tiers don't conflict.</p>"},{"location":"components/audits/audits/#character-data-audits","title":"Character Data Audits","text":"<p>These audits check string formats and patterns.</p> <p>Warning</p> <p>Different databases may behave differently with character sets or languages. Test your audits!</p>"},{"location":"components/audits/audits/#not_empty_string","title":"<code>not_empty_string</code>","text":"<p>Make sure strings aren't empty. NULL is okay, but empty strings <code>''</code> are not:</p> <pre><code>MODEL (\n  name sushi.items,\n  assertions (\n    not_empty_string(column := name)\n  )\n);\n</code></pre> <p>This ensures no <code>name</code> is an empty string. NULL values pass, but <code>''</code> fails.</p>"},{"location":"components/audits/audits/#string_length_equal","title":"<code>string_length_equal</code>","text":"<p>Check that all strings have the exact same length:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_equal(column := zip, v := 5)\n    )\n);\n</code></pre> <p>This ensures all <code>zip</code> values are exactly 5 characters. Useful for fixed-length codes.</p>"},{"location":"components/audits/audits/#string_length_between","title":"<code>string_length_between</code>","text":"<p>Check that string lengths are within a range:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := name, min_v := 5, max_v := 50)\n    )\n);\n</code></pre> <p>This ensures all <code>name</code> values are between 5 and 50 characters (inclusive).</p> <p>Exclusive ranges:</p> <p>You can make the range exclusive:</p> <pre><code>MODEL (\n  name sushi.customers,\n  assertions (\n    string_length_between(column := zip, min_v := 4, max_v := 60, inclusive := false)\n    )\n);\n</code></pre> <p>Now names must be longer than 4 characters and shorter than 60 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#valid_uuid","title":"<code>valid_uuid</code>","text":"<p>Check that values match UUID format:</p> <pre><code>MODEL (\n  name events.user_sessions,\n  assertions (\n    valid_uuid(column := uuid)\n    )\n);\n</code></pre> <p>This ensures all <code>uuid</code> values match the UUID structure (like <code>550e8400-e29b-41d4-a716-446655440000</code>).</p>"},{"location":"components/audits/audits/#valid_email","title":"<code>valid_email</code>","text":"<p>Check email format:</p> <pre><code>MODEL (\n  name dim.users,\n  assertions (\n    valid_email(column := email)\n    )\n);\n</code></pre> <p>This ensures all <code>email</code> values look like valid email addresses (has <code>@</code>, has domain, etc.).</p>"},{"location":"components/audits/audits/#valid_url","title":"<code>valid_url</code>","text":"<p>Check URL format:</p> <pre><code>MODEL (\n  name dim.products,\n  assertions (\n    valid_url(column := url)\n    )\n);\n</code></pre> <p>This ensures all <code>url</code> values are valid URLs (starts with <code>http://</code>, <code>https://</code>, or <code>ftp://</code>, etc.).</p>"},{"location":"components/audits/audits/#valid_http_method","title":"<code>valid_http_method</code>","text":"<p>Check that values are valid HTTP methods:</p> <pre><code>MODEL (\n  name logs.api_requests,\n  assertions (\n    valid_http_method(column := http_method)\n  )\n);\n</code></pre> <p>This ensures <code>http_method</code> is one of: <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>PATCH</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.</p>"},{"location":"components/audits/audits/#match_regex_pattern_list","title":"<code>match_regex_pattern_list</code>","text":"<p>Check that values match at least one regex pattern:</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    match_regex_pattern_list(column := todo, patterns := ('^\\d.*', '.*!$'))\n  )\n);\n</code></pre> <p>This ensures all <code>todo</code> values match at least one pattern: either start with a digit (<code>^\\d.*</code>) or end with an exclamation mark (<code>.*!$</code>).</p>"},{"location":"components/audits/audits/#not_match_regex_pattern_list","title":"<code>not_match_regex_pattern_list</code>","text":"<p>The opposite, make sure values don't match any pattern:</p> <pre><code>MODEL (\n  name products.inventory,\n  assertions (\n    not_match_regex_pattern_list(column := todo, patterns := ('^!.*', '.*\\d$'))\n  )\n);\n</code></pre> <p>This ensures no <code>todo</code> values start with <code>!</code> or end with a digit.</p>"},{"location":"components/audits/audits/#match_like_pattern_list","title":"<code>match_like_pattern_list</code>","text":"<p>Check that values match at least one SQL LIKE pattern:</p> <pre><code>MODEL (\n  name sales.customers,\n  assertions (\n    match_like_pattern_list(column := name, patterns := ('jim%', 'pam%'))\n  )\n);\n</code></pre> <p>This ensures all <code>name</code> values start with 'jim' or 'pam'. Uses SQL LIKE syntax, so <code>%</code> matches any characters.</p>"},{"location":"components/audits/audits/#not_match_like_pattern_list","title":"<code>not_match_like_pattern_list</code>","text":"<p>Make sure values don't match any LIKE pattern:</p> <pre><code>MODEL (\n  name products.catalog,\n  assertions (\n    not_match_like_pattern_list(column := name, patterns := ('%doe', '%smith'))\n  )\n);\n</code></pre> <p>This ensures no <code>name</code> values end with 'doe' or 'smith'.</p>"},{"location":"components/audits/audits/#statistical-audits","title":"Statistical Audits","text":"<p>These audits check statistical properties of your data. They're powerful but require some tuning to get the thresholds right.</p> <p>Note</p> <p>Statistical audit thresholds usually need fine-tuning through trial and error. Start with wide ranges and tighten them as you learn what's normal for your data.</p>"},{"location":"components/audits/audits/#mean_in_range","title":"<code>mean_in_range</code>","text":"<p>Check that a column's average is within a range:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 21, max_v := 50)\n    )\n);\n</code></pre> <p>This ensures the average <code>age</code> is between 21 and 50. Useful for catching when your data distribution shifts unexpectedly.</p> <p>Exclusive ranges:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    mean_in_range(column := age, min_v := 18, max_v := 65, inclusive := false)\n    )\n);\n</code></pre> <p>Now the mean must be greater than 18 and less than 65 (not equal to the boundaries).</p>"},{"location":"components/audits/audits/#stddev_in_range","title":"<code>stddev_in_range</code>","text":"<p>Check that standard deviation is within a range:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 2, max_v := 5)\n  )\n);\n</code></pre> <p>This ensures the standard deviation of <code>age</code> is between 2 and 5. Useful for detecting when your data becomes more or less spread out than expected.</p> <p>Exclusive ranges:</p> <pre><code>MODEL (\n  name analytics.customer_metrics,\n  assertions (\n    stddev_in_range(column := age, min_v := 3, max_v := 6, inclusive := false)\n  )\n);\n</code></pre> <p>Now the standard deviation must be greater than 3 and less than 6.</p>"},{"location":"components/audits/audits/#z_score","title":"<code>z_score</code>","text":"<p>Check for statistical outliers. Values with high z-scores are far from the mean:</p> <pre><code>MODEL (\n  name sales.transactions,\n  assertions (\n    z_score(column := age, threshold := 3)\n    )\n);\n</code></pre> <p>This ensures no <code>age</code> values have a z-score greater than 3 (meaning they're more than 3 standard deviations from the mean). Useful for catching outliers that might indicate data quality issues.</p> <p>The z-score is calculated as: <code>ABS(([row value] - [column mean]) / NULLIF([column standard deviation], 0))</code></p>"},{"location":"components/audits/audits/#kl_divergence","title":"<code>kl_divergence</code>","text":"<p>Check how different two distributions are. Useful for comparing current data to a reference:</p> <pre><code>MODEL (\n  name analytics.cohort_comparison,\n  assertions (\n    kl_divergence(column := age, target_column := reference_age, threshold := 0.1)\n    )\n);\n</code></pre> <p>This ensures the symmetrised Kullback-Leibler divergence (also called \"Jeffreys divergence\" or \"Population Stability Index\") between <code>age</code> and <code>reference_age</code> is less than or equal to 0.1.</p> <p>Lower values mean the distributions are more similar. This is great for detecting when your data distribution has shifted significantly from a known good reference.</p>"},{"location":"components/audits/audits/#chi_square","title":"<code>chi_square</code>","text":"<p>Check the relationship between two categorical columns:</p> <pre><code>MODEL (\n  name analytics.user_segments,\n  assertions (\n    chi_square(column := user_state, target_column := user_type, critical_value := 6.635)\n    )\n);\n</code></pre> <p>This ensures the chi-square statistic for <code>user_state</code> and <code>user_type</code> doesn't exceed 6.635.</p> <p>Finding critical values:</p> <p>You can look up critical values in a chi-square table or calculate them with Python:</p> <pre><code>from scipy.stats import chi2\n\n# critical value for p-value := 0.95 and degrees of freedom := 1\nchi2.ppf(0.95, 1)\n</code></pre> <p>This is useful for detecting when the relationship between two categorical variables has changed unexpectedly.</p>"},{"location":"components/audits/audits/#running-audits","title":"Running Audits","text":""},{"location":"components/audits/audits/#the-cli-audit-command","title":"The CLI Audit Command","text":"<p>You can run audits manually with the <code>vulcan audit</code> command:</p> <pre><code>$ vulcan -p project audit --start 2022-01-01 --end 2022-01-02\nFound 1 audit(s).\nassert_item_price_is_not_null FAIL.\n\nFinished with 1 audit error(s).\n\nFailure in audit assert_item_price_is_not_null for model sushi.items (audits/items.sql).\nGot 3 results, expected 0.\nSELECT * FROM vulcan.sushi__items__1836721418_83893210 WHERE ds BETWEEN '2022-01-01' AND '2022-01-02' AND price IS NULL\nDone.\n</code></pre> <p>This is useful for testing audits before running a full plan, or for debugging why an audit is failing. The output shows you exactly what query failed and how many rows it found.</p>"},{"location":"components/audits/audits/#automated-auditing","title":"Automated Auditing","text":"<p>When you apply a plan, Vulcan automatically runs all audits for models being evaluated. You don't need to do anything special, just run your plan and audits happen automatically.</p> <p>If any audit fails, Vulcan halts the models immediately. This prevents bad data from propagating downstream and causing bigger problems. It might be annoying when it happens, but trust us, it's better than finding out later that bad data made it into production.</p>"},{"location":"components/audits/audits/#advanced-usage","title":"Advanced Usage","text":""},{"location":"components/audits/audits/#skipping-audits","title":"Skipping Audits","text":"<p>Sometimes you need to temporarily disable an audit. Maybe you're debugging, or you know there's a temporary data issue you're working on fixing. You can skip audits by setting <code>skip</code> to <code>true</code>:</p> <pre><code>AUDIT (\n  name assert_item_price_is_not_null,\n  skip true\n);\nSELECT * from sushi.items\nWHERE ds BETWEEN @start_ds AND @end_ds AND\n   price IS NULL;\n</code></pre> <p>Use this sparingly! Skipped audits won't run, which means they won't catch problems. It's better to fix the underlying issue than to skip the audit. But sometimes you need it for debugging or temporary situations.</p>"},{"location":"components/audits/audits/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/audits/audits/#audit-fails-unexpectedly","title":"Audit Fails Unexpectedly","text":"<p>Problem: Your audit is failing, but you're not sure why.</p> <p>Solution: Run the audit query manually to see what it's finding:</p> <pre><code>vulcan -p project audit --start 2022-01-01 --end 2022-01-02 --verbose\n</code></pre> <p>This will show you the exact query and the rows that failed. Once you see what data is causing the failure, you can either fix the data or adjust the audit.</p>"},{"location":"components/audits/audits/#audit-too-strict","title":"Audit Too Strict","text":"<p>Problem: Your audit is failing during normal operation, even though the data is actually fine.</p> <p>Solution: Review your thresholds. Maybe your <code>accepted_range</code> is too narrow, or your <code>number_of_rows</code> threshold is too high. Statistical audits especially need tuning, start with wide ranges and tighten them as you learn what's normal.</p>"},{"location":"components/audits/audits/#performance-issues","title":"Performance Issues","text":"<p>Problem: Audits are slowing down your plan execution.</p> <p>Solution:</p> <ul> <li> <p>Make sure your audit queries use indexes on the columns they're checking</p> </li> <li> <p>For incremental models, audits only run on processed intervals (which helps), but you can also add date filters to your audit queries</p> </li> <li> <p>Consider if you really need all those audits, sometimes less is more</p> </li> </ul>"},{"location":"components/audits/audits/#understanding-audit-results","title":"Understanding Audit Results","text":"<p>When an audit fails, Vulcan shows you:</p> <ul> <li> <p>Which audit failed</p> </li> <li> <p>Which model it was attached to</p> </li> <li> <p>The exact query that was run</p> </li> <li> <p>How many rows were returned (when it expected 0)</p> </li> </ul> <p>Use this information to understand what went wrong. The query results tell you exactly what data failed the check.</p>"},{"location":"components/checks/checks/","title":"Checks","text":""},{"location":"components/checks/checks/#checks","title":"Checks","text":"<p>Quality checks are validation rules that monitor your data quality over time without blocking your models. They warn you when something looks off, but they don't stop execution.</p> <p>Unlike audits (which block models execution when they fail), checks run separately or alongside your models and provide non-blocking validation. They're perfect for tracking trends, detecting anomalies, and building up a historical picture of your data quality.</p> <p>What makes checks special:</p> <ul> <li> <p>Configured in simple YAML files in the <code>checks/</code> directory</p> </li> <li> <p>Don't block models (your models keep running even if checks fail)</p> </li> <li> <p>Track historical patterns and trends</p> </li> <li> <p>Support complex statistical analysis</p> </li> <li> <p>Integrate with Activity API for monitoring and alerting</p> </li> </ul>"},{"location":"components/checks/checks/#checks-vs-audits-vs-profiles","title":"Checks vs Audits vs Profiles","text":"<p>Before we dive in, let's clear up the confusion around these three data quality mechanisms. They all serve different purposes, and understanding when to use each one will save you headaches later.</p> Feature Audits Checks Profiles Purpose Critical validation Monitoring &amp; analysis Observation &amp; tracking When runs With model (inline) Separately or with models With model Blocks models? Yes (always) No No Configuration In MODEL DDL or .sql files YAML files (<code>checks/</code>) In MODEL DDL Output Pass/fail Pass/fail + samples Statistical metrics Best for Business rules, data integrity Trend monitoring, anomalies Understanding data Historical tracking No Yes (Activity API) Yes (<code>_check_profiles</code>) <p>The Three-Layer Strategy:</p> <p>A layered approach to data quality:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUDITS (Critical - Blocks models)   \u2502\n\u2502  \u2022 Primary keys must be unique          \u2502\n\u2502  \u2022 Revenue must be non-negative         \u2502\n\u2502  \u2022 Foreign key relationships valid      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CHECKS (Monitoring - Non-Blocking)     \u2502\n\u2502  \u2022 Row count within expected range      \u2502\n\u2502  \u2022 Anomaly detection on metrics         \u2502\n\u2502  \u2022 Cross-table consistency              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PROFILES (Observation - Metrics)       \u2502\n\u2502  \u2022 Track null percentages               \u2502\n\u2502  \u2022 Monitor column distributions         \u2502\n\u2502  \u2022 Detect data drift                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Audits stop bad data at the door. Checks watch for problems but don't interfere. Profiles observe patterns and help you understand what's normal.</p>"},{"location":"components/checks/checks/#when-to-use-checks","title":"When to Use Checks","text":"<p>Use Quality Checks for:</p> <ul> <li> <p>Monitoring data quality trends over time (is completeness getting worse?)</p> </li> <li> <p>Statistical anomaly detection (did revenue suddenly spike?)</p> </li> <li> <p>Cross-model validation (do orders match customers?)</p> </li> <li> <p>Non-critical validation (warnings, not blockers)</p> </li> <li> <p>Complex validation requiring historical context</p> </li> <li> <p>Building data quality dashboards</p> </li> </ul> <p>Use Audits Instead for:</p> <ul> <li> <p>Critical business rules that must pass (revenue can't be negative)</p> </li> <li> <p>Model-specific validation (runs inline with the model)</p> </li> <li> <p>Simple SQL assertions</p> </li> <li> <p>Blocking invalid data from flowing downstream</p> </li> </ul> <p>Use Profiles Instead for:</p> <ul> <li> <p>Understanding data characteristics (what does this column look like?)</p> </li> <li> <p>Discovering patterns (not validation)</p> </li> <li> <p>Detecting data drift over time</p> </li> <li> <p>Informing which checks/audits to add</p> </li> </ul> <p>Example: Revenue validation strategy</p> <p>Here's how you'd layer all three for a revenue table:</p> <pre><code>-- AUDIT (Critical - blocks if fails)\n\n-- This stops the models if revenue is invalid\nMODEL (\n  name analytics.revenue,\n  assertions (\n    not_null(columns := (customer_id, revenue)),\n    accepted_range(column := revenue, min_v := 0, max_v := 100000000)\n  )\n);\n</code></pre> <pre><code># CHECK (Monitoring - warns if unusual)\n# This watches for anomalies but doesn't block\nchecks:\n  analytics.revenue:\n    accuracy:\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n      - change for row_count &gt;= -30%:\n          name: row_count_drop_alert\n</code></pre> <pre><code>-- PROFILE (Observation - tracks over time)\n\n-- This just watches and records what it sees\nMODEL (\n  name analytics.revenue,\n  profiles (revenue, order_count, customer_tier)\n);\n</code></pre>"},{"location":"components/checks/checks/#quick-start","title":"Quick Start","text":""},{"location":"components/checks/checks/#your-first-check","title":"Your First Check","text":"<p>Let's create your first check. It's simpler than you might think!</p> <p>Create a file <code>checks/customers.yml</code>:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_emails\n          attributes:\n            description: \"All customers must have an email address\"\n</code></pre> <p>That's it! This check ensures that every customer has an email address. When you run your models, this check will run automatically and warn you if any emails are missing.</p> <p>What happens when it runs:</p> <p>Checks and profiles run automatically when models are executed, either through a plan or run command. Here's what the execution output looks like:</p> <pre><code>Check Executions (1 Models)\n\u2514\u2500\u2500 hello.subscriptions\n    \u251c\u2500\u2500 completeness (4/4)\n    \u251c\u2500\u2500 uniqueness (1/1)\n    \u2514\u2500\u2500 validity (3/3)\n\nProfiled 1 model (3 columns):\n  warehouse.hello.subscriptions: 3 columns\n</code></pre> <p>Here are common patterns you'll use:</p>"},{"location":"components/checks/checks/#common-check-patterns","title":"Common Check Patterns","text":"<p>Here are the patterns you'll use most often. Copy these, tweak them for your tables, and you're good to go!</p>"},{"location":"components/checks/checks/#pattern-1-completeness-checks","title":"Pattern 1: Completeness Checks","text":"<p>Make sure required data is present:</p> <pre><code>checks:\n  analytics.orders:\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: customer_id_required\n\n      - missing_percent(email) &lt; 5:\n          name: email_mostly_complete\n\n      - row_count &gt; 1000:\n          name: sufficient_orders\n</code></pre> <p>The first check ensures every order has a customer ID (zero tolerance). The second allows up to 5% missing emails (sometimes that's okay). The third makes sure you have enough data to work with.</p>"},{"location":"components/checks/checks/#pattern-2-validity-checks","title":"Pattern 2: Validity Checks","text":"<p>Validate data format and values:</p> <pre><code>checks:\n  analytics.users:\n    validity:\n      - failed rows:\n          name: invalid_emails\n          fail query: |\n            SELECT user_id, email\n            FROM analytics.users\n            WHERE email NOT LIKE '%@%'\n          samples limit: 10\n\n      - failed rows:\n          name: invalid_ages\n          fail query: |\n            SELECT user_id, age\n            FROM analytics.users\n            WHERE age &lt; 0 OR age &gt; 120\n</code></pre> <p>The <code>failed rows</code> check type is flexible. You can write any SQL query. If it returns rows, the check fails and captures those rows as samples.</p>"},{"location":"components/checks/checks/#pattern-3-uniqueness-checks","title":"Pattern 3: Uniqueness Checks","text":"<p>Ensure no duplicates:</p> <pre><code>checks:\n  analytics.customers:\n    uniqueness:\n      - duplicate_count(email) = 0:\n          name: unique_emails\n\n      - duplicate_count(customer_id, order_date) = 0:\n          name: unique_customer_date_combination\n</code></pre> <p>The second example shows composite keys, maybe a customer can have multiple orders, but only one per day.</p>"},{"location":"components/checks/checks/#pattern-4-anomaly-detection","title":"Pattern 4: Anomaly Detection","text":"<p>Detect unusual patterns automatically:</p> <pre><code>checks:\n  analytics.daily_revenue:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly\n</code></pre> <p>Anomaly detection learns from historical data and flags when something looks unusual. It needs to run a few times first to build up a baseline, then it detects problems.</p>"},{"location":"components/checks/checks/#pattern-5-change-monitoring","title":"Pattern 5: Change Monitoring","text":"<p>Track changes over time:</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - change for row_count &gt;= -50%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if row count drops more than 50%\"\n</code></pre> <p>This compares the current value to the previous run and alerts you if it changes too much. Perfect for catching sudden drops or spikes.</p>"},{"location":"components/checks/checks/#check-configuration","title":"Check Configuration","text":""},{"location":"components/checks/checks/#file-structure","title":"File Structure","text":"<p>Checks live in YAML files in the <code>checks/</code> directory. You can organize them however makes sense for your project:</p> <pre><code>project/\n\u251c\u2500\u2500 models/\n\u251c\u2500\u2500 checks/\n\u2502   \u251c\u2500\u2500 users.yml           # Checks for user tables\n\u2502   \u251c\u2500\u2500 orders.yml          # Checks for order tables\n\u2502   \u251c\u2500\u2500 revenue.yml         # Checks for revenue tables\n\u2502   \u2514\u2500\u2500 cross_model.yml     # Checks spanning multiple tables\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming:</p> <ul> <li> <p>Must end with <code>.yml</code> or <code>.yaml</code></p> </li> <li> <p>The name doesn't matter (Vulcan reads all files in the directory)</p> </li> <li> <p>Organize by domain or table for clarity, whatever helps you find things</p> </li> </ul>"},{"location":"components/checks/checks/#basic-check-syntax","title":"Basic Check Syntax","text":"<p>Here's the basic structure of a check:</p> <pre><code>checks:\n  &lt;fully_qualified_table_name&gt;:\n    &lt;dimension&gt;:\n      - &lt;check_expression&gt;:\n          name: &lt;check_name&gt;\n          attributes:\n            description: &lt;human_readable_description&gt;\n            tags: [&lt;tag1&gt;, &lt;tag2&gt;]\n</code></pre> <p>Example:</p> <pre><code>checks:\n  analytics.customers:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_customers\n          attributes:\n            description: \"At least 100 customers expected in production\"\n            tags: [critical, daily]\n</code></pre> <p>The <code>name</code> field is required and should be descriptive. The <code>attributes</code> section is optional but useful for documentation and filtering.</p>"},{"location":"components/checks/checks/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Checks are organized by 8 standard dimensions (based on ODPS v3.1). Each dimension focuses on a different aspect of data quality:</p>"},{"location":"components/checks/checks/#1-completeness","title":"1. Completeness","text":"<p>No missing required data. This is probably the most common dimension you'll use.</p> <pre><code>completeness:\n  - missing_count(customer_id) = 0\n\n  - missing_percent(email) &lt; 5\n\n  - row_count &gt; 1000\n</code></pre>"},{"location":"components/checks/checks/#2-validity","title":"2. Validity","text":"<p>Data conforms to format/syntax. Is that email actually an email? Is that date in the right format?</p> <pre><code>validity:\n  - failed rows:\n      fail query: |\n        SELECT * FROM table\n        WHERE email NOT LIKE '%@%'\n</code></pre>"},{"location":"components/checks/checks/#3-accuracy","title":"3. Accuracy","text":"<p>Data matches reality. Is the average age reasonable? Is revenue in the expected range?</p> <pre><code>accuracy:\n  - anomaly detection for avg(revenue)\n\n  - avg(age) between 18 and 65\n</code></pre>"},{"location":"components/checks/checks/#4-consistency","title":"4. Consistency","text":"<p>Data agrees across sources. Do orders match customers? Are totals consistent?</p> <pre><code>consistency:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders o\n        LEFT JOIN customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n</code></pre>"},{"location":"components/checks/checks/#5-uniqueness","title":"5. Uniqueness","text":"<p>No duplicates. Is that email really unique? Can customers have multiple orders per day?</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0\n\n  - duplicate_count(order_id) = 0\n</code></pre>"},{"location":"components/checks/checks/#6-timeliness","title":"6. Timeliness","text":"<p>Data is current. Is the data fresh? Are updates happening on time?</p> <pre><code>timeliness:\n  - change for row_count &gt;= -30%\n\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM orders\n        WHERE updated_at &lt; CURRENT_DATE - INTERVAL '7 days'\n</code></pre>"},{"location":"components/checks/checks/#7-conformity","title":"7. Conformity","text":"<p>Follows standards. Does the zip code have the right format? Are codes valid?</p> <pre><code>conformity:\n  - failed rows:\n      fail query: |\n        SELECT *\n        FROM addresses\n        WHERE LENGTH(zip_code) != 5\n</code></pre>"},{"location":"components/checks/checks/#8-coverage","title":"8. Coverage","text":"<p>All records are present. Did we get all the data we expected?</p> <pre><code>coverage:\n  - row_count &gt;= 95% of historical_avg(row_count)\n</code></pre>"},{"location":"components/checks/checks/#filtering-checks","title":"Filtering Checks","text":"<p>Sometimes you want to apply checks to a subset of your data. Maybe you only care about completed orders, or US customers. That's where filters come in:</p> <pre><code>checks:\n  analytics.orders:\n    filter: \"status = 'completed' AND order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n\n    completeness:\n      - missing_count(customer_id) = 0:\n          name: completed_orders_have_customers\n</code></pre> <p>Multiple filters:</p> <p>You can define the same table multiple times with different filters:</p> <pre><code>checks:\n  analytics.customers:\n    filter: \"country = 'US'\"\n    completeness:\n      - row_count &gt; 1000\n\n  analytics.customers:\n    filter: \"country = 'EU'\"\n    completeness:\n      - row_count &gt; 500\n</code></pre> <p>This lets you have different expectations for different regions.</p>"},{"location":"components/checks/checks/#check-attributes","title":"Check Attributes","text":"<p>Add metadata to your checks to make them easier to manage and understand:</p> <pre><code>checks:\n  analytics.revenue:\n    completeness:\n      - row_count &gt; 1000:\n          name: sufficient_revenue_data\n          attributes:\n            description: \"Revenue table must have at least 1000 rows for analysis\"\n            tags: [critical, daily, revenue]\n            owner: data-team\n            jira: DATA-1234\n            sla: \"&lt; 1 hour\"\n</code></pre> <p>Standard attributes:</p> <ul> <li> <p><code>description</code> - Human-readable explanation</p> </li> <li> <p><code>severity</code> - <code>error</code> (default) or <code>warning</code> (warnings are less urgent)</p> </li> <li> <p><code>tags</code> - List of tags for filtering/organization (find all \"critical\" checks easily)</p> </li> <li> <p><code>owner</code> - Team or person responsible (who do I call when this fails?)</p> </li> <li> <p>Custom attributes - Any key-value pairs (add whatever metadata you need)</p> </li> </ul>"},{"location":"components/checks/checks/#built-in-check-types","title":"Built-in Check Types","text":"<p>Vulcan provides several built-in check types that cover most common scenarios. Let's walk through them:</p>"},{"location":"components/checks/checks/#missing-data-checks","title":"Missing Data Checks","text":""},{"location":"components/checks/checks/#missing_countcolumn","title":"<code>missing_count(column)</code>","text":"<p>Count of NULL values. Simple and straightforward:</p> <pre><code>completeness:\n  - missing_count(email) = 0:\n      name: no_missing_emails\n\n  - missing_count(phone) &lt;= 100:\n      name: phone_mostly_complete\n</code></pre> <p>The first ensures zero missing emails (strict). The second allows up to 100 missing phone numbers (maybe phones are optional for some customers).</p>"},{"location":"components/checks/checks/#missing_percentcolumn","title":"<code>missing_percent(column)</code>","text":"<p>Percentage of NULL values. Useful when you care about proportions rather than absolute counts:</p> <pre><code>completeness:\n  - missing_percent(email) &lt; 5:\n      name: email_95_percent_complete\n\n  - missing_percent(optional_field) &lt; 50:\n      name: optional_field_half_complete\n</code></pre> <p>This is useful when table sizes vary. 5% missing might be fine for a million-row table but concerning for a hundred-row table.</p>"},{"location":"components/checks/checks/#row-count-checks","title":"Row Count Checks","text":""},{"location":"components/checks/checks/#row_count","title":"<code>row_count</code>","text":"<p>Total rows in table. Use this to ensure you have enough data:</p> <pre><code>completeness:\n  - row_count &gt; 1000:\n      name: sufficient_data\n\n  - row_count between 1000 and 100000:\n      name: expected_row_range\n</code></pre> <p>The second example shows a range check, maybe you know your table should be between 1K and 100K rows, and anything outside that range is suspicious.</p>"},{"location":"components/checks/checks/#row_count-with-filter","title":"<code>row_count</code> with filter","text":"<p>You can also check row counts on filtered data:</p> <pre><code>completeness:\n  - row_count &gt; 500:\n      name: sufficient_active_users\n      filter: \"status = 'active'\"\n</code></pre> <p>This checks that you have at least 500 active users, regardless of how many total users you have.</p>"},{"location":"components/checks/checks/#duplicate-count-checks","title":"Duplicate Count Checks","text":""},{"location":"components/checks/checks/#duplicate_countcolumn","title":"<code>duplicate_count(column)</code>","text":"<p>Count of duplicate values. Perfect for ensuring uniqueness:</p> <pre><code>uniqueness:\n  - duplicate_count(email) = 0:\n      name: unique_emails\n\n  - duplicate_count(customer_id) = 0:\n      name: unique_customer_ids\n</code></pre> <p>If this returns anything greater than zero, you've got duplicates. The check fails and you can investigate.</p>"},{"location":"components/checks/checks/#duplicate_countcolumn1-column2","title":"<code>duplicate_count(column1, column2)</code>","text":"<p>Composite key duplicates. Check combinations of columns:</p> <pre><code>uniqueness:\n  - duplicate_count(customer_id, order_date) = 0:\n      name: unique_customer_date\n      attributes:\n        description: \"Each customer can have at most one order per day\"\n</code></pre> <p>Maybe customers can have multiple orders, but only one per day. This check enforces that business rule.</p>"},{"location":"components/checks/checks/#failed-rows-checks","title":"Failed Rows Checks","text":""},{"location":"components/checks/checks/#sql-based-validation-with-samples","title":"SQL-based validation with samples","text":"<p>This is the most flexible check type, you can write any SQL query you want:</p> <pre><code>validity:\n  - failed rows:\n      name: invalid_revenue\n      fail query: |\n        SELECT customer_id, revenue, order_date\n        FROM analytics.orders\n        WHERE revenue &lt; 0 OR revenue &gt; 10000000\n      samples limit: 20\n      attributes:\n        description: \"Revenue must be between 0 and 10M\"\n</code></pre> <p>How it works:</p> <ul> <li> <p><code>fail query</code> - A SELECT statement that returns invalid rows</p> </li> <li> <p><code>samples limit</code> - How many example rows to capture when the check fails (default: 5)</p> </li> <li> <p>Returns empty = check passes (no invalid rows found)</p> </li> <li> <p>Returns rows = check fails (captures samples so you can see what's wrong)</p> </li> </ul> <p>Complex validation:</p> <p>You can get fancy with joins and CTEs:</p> <pre><code>validity:\n  - failed rows:\n      name: orphaned_orders\n      fail query: |\n        SELECT o.order_id, o.customer_id\n        FROM analytics.orders o\n        LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n        WHERE c.customer_id IS NULL\n      samples limit: 10\n</code></pre> <p>This finds orders that reference customers that don't exist, a classic referential integrity check.</p>"},{"location":"components/checks/checks/#threshold-checks","title":"Threshold Checks","text":""},{"location":"components/checks/checks/#numeric-aggregations","title":"Numeric aggregations","text":"<p>Check aggregated values against thresholds:</p> <pre><code>accuracy:\n  - avg(revenue) between 100 and 10000:\n      name: revenue_in_expected_range\n\n  - sum(amount) &gt; 1000000:\n      name: sufficient_total_revenue\n\n  - max(age) &lt;= 120:\n      name: age_within_human_range\n\n  - min(price) &gt;= 0:\n      name: non_negative_prices\n</code></pre> <p>You can use any aggregation function: <code>avg</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>count</code>, <code>distinct_count</code>, etc.</p>"},{"location":"components/checks/checks/#statistical-checks","title":"Statistical checks","text":"<p>Get fancy with statistical functions:</p> <pre><code>accuracy:\n  - stddev(revenue) &lt; 5000:\n      name: revenue_low_variance\n\n  - percentile(revenue, 95) &lt; 50000:\n      name: revenue_95th_percentile_check\n</code></pre> <p>These detect when your data distribution changes unexpectedly.</p>"},{"location":"components/checks/checks/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"components/checks/checks/#ml-based-anomaly-detection","title":"ML-based anomaly detection","text":"<p>This is where checks get really powerful. Anomaly detection uses historical check results to learn what's normal and flag unusual patterns:</p> <pre><code>accuracy:\n  - anomaly detection for row_count:\n      name: row_count_anomaly\n      attributes:\n        description: \"Detect unusual changes in row count\"\n\n  - anomaly detection for avg(revenue):\n      name: revenue_anomaly\n\n  - anomaly detection for distinct_count(customer_id):\n      name: customer_count_anomaly\n</code></pre> <p>How it works: 1. Collects historical metric values over time (every time the check runs) 2. Builds a statistical model (mean, standard deviation, trends) 3. Compares current value to expected range 4. Flags significant deviations (typically &gt; 3 standard deviations)</p> <p>Requirements:</p> <ul> <li> <p>Needs historical data (runs multiple times to build a baseline)</p> </li> <li> <p>Works best with regular schedules (daily, hourly)</p> </li> <li> <p>More accurate after 30+ data points (the more history, the better)</p> </li> </ul> <p>So if you're setting up anomaly detection, be patient, it needs to run a few times before it's useful. But once it has enough data, it's really good at spotting problems you might not think to check for.</p>"},{"location":"components/checks/checks/#change-over-time-checks","title":"Change Over Time Checks","text":""},{"location":"components/checks/checks/#monitor-changes-compared-to-previous-run","title":"Monitor changes compared to previous run","text":"<p>Track how metrics change between runs:</p> <pre><code>timeliness:\n  - change for row_count &gt;= -50%:\n      name: row_count_drop_alert\n      attributes:\n        description: \"Alert if row count drops more than 50% from last week\"\n\n  - change for avg(revenue) &gt;= -20%:\n      name: revenue_drop_alert\n\n  - change for distinct_count(customer_id) &gt;= 10%:\n      name: customer_growth_check\n</code></pre> <p>Change calculation: </p><pre><code>change = (current_value - previous_value) / previous_value * 100\n</code></pre><p></p> <p>Examples:</p> <ul> <li> <p><code>change &gt;= -30%</code> - Alert if metric drops more than 30% (negative change)</p> </li> <li> <p><code>change &gt;= 10%</code> - Alert if metric grows more than 10% (positive change)</p> </li> <li> <p><code>change between -10% and 10%</code> - Alert if metric changes more than 10% either way</p> </li> </ul> <p>This catches sudden changes that might indicate a problem or an opportunity.</p>"},{"location":"components/checks/checks/#data-profiling","title":"Data Profiling","text":""},{"location":"components/checks/checks/#what-is-profiling","title":"What is Profiling?","text":"<p>Profiles automatically collect statistical metrics about your data over time.</p> <p>Unlike checks (which validate), profiles observe and track data characteristics. They're like a data scientist watching your tables and taking notes:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind FULL,\n  grains (customer_id),\n  profiles (revenue, signup_date, customer_tier, order_count)\n);\n</code></pre> <p>What gets profiled:</p> <p>Table-level metrics:</p> <ul> <li>Row count</li> </ul> <p>Column-level metrics (all columns):</p> <ul> <li> <p>Null count &amp; percentage</p> </li> <li> <p>Distinct count</p> </li> <li> <p>Duplicate count</p> </li> <li> <p>Uniqueness percentage</p> </li> </ul> <p>Numeric columns:</p> <ul> <li> <p>Min, max, avg, sum</p> </li> <li> <p>Standard deviation, variance</p> </li> <li> <p>Histogram buckets</p> </li> </ul> <p>Text columns:</p> <ul> <li> <p>Min, max, avg length</p> </li> <li> <p>Most frequent values</p> </li> </ul> <p>Profiles track how things change over time so you can spot trends and drift.</p>"},{"location":"components/checks/checks/#profile-configuration","title":"Profile Configuration","text":"<p>Enable profiling in your MODEL definition:</p> <pre><code>MODEL (\n  name analytics.revenue_metrics,\n  kind INCREMENTAL_BY_TIME_RANGE (time_column metric_date),\n\n  -- Profile these columns\n  profiles (\n    revenue,\n    order_count,\n    customer_tier,\n    region\n  )\n);\n</code></pre> <p>Just list the columns you want to profile. Vulcan will automatically collect metrics for them every time the model runs.</p>"},{"location":"components/checks/checks/#profile-storage","title":"Profile Storage","text":"<p>Profiles are stored in the <code>_check_profiles</code> table, which you can query like any other table:</p> Column Meaning <code>id</code> Unique identifier for this metric row <code>run_id</code> Identifies which profiling run this metric belongs to <code>table_name</code> Name of the table being profiled <code>column_name</code> Name of the column being profiled (NULL for table-level metrics like row_count) <code>profile_type</code> The type of metric, e.g., row_count, distinct, missing_count, frequent_values, min, max, avg_length, etc. <code>value_number</code> Numeric metric value (for metrics like row_count, distinct, min, max, avg, etc.) <code>value_text</code> Used for text values (rare) <code>value_json</code> JSON-encoded metric (for histograms, frequent values, etc.) <code>value_type</code> Type of value stored (number, json, etc.) <code>profiled_at</code> When the profiling was performed (epoch ms) <code>created_ts</code> When the row was inserted"},{"location":"components/checks/checks/#querying-profiles","title":"Querying Profiles","text":""},{"location":"components/checks/checks/#track-missing-count-over-time","title":"Track missing count over time","text":"<p>See how null percentages change:</p> <pre><code>SELECT\n  to_timestamp(profiled_at/1000)::date AS date,\n  value_number AS missing_count\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\n  AND column_name = 'mrr'\n  AND profile_type = 'missing_count'\nORDER BY profiled_at DESC\nLIMIT 30;  -- Last 30 days\n</code></pre> <p>This shows you a time series of missing values for spotting trends.</p>"},{"location":"components/checks/checks/#monitor-data-drift","title":"Monitor data drift","text":"<p>Compare current values to historical averages:</p> <pre><code>WITH latest_profile AS (\n  -- Pick the most recent profiling timestamp for that table/column\n  SELECT profiled_at\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n  ORDER BY profiled_at DESC\n  LIMIT 1\n),\n\ncurrent AS (\n  -- Get the most recent distinct count and average value from that profiling run\n  SELECT\n    MAX(CASE WHEN profile_type = 'distinct' THEN value_number END)     AS distinct_count,\n    MAX(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_value\n  FROM _check_profiles p\n  JOIN latest_profile l ON p.profiled_at = l.profiled_at\n  WHERE p.table_name = 'warehouse.hello.subscriptions'\n    AND p.column_name = 'mrr'\n),\n\nhistorical AS (\n  -- 30-day historical averages (profiled_at stored as epoch ms \u2192 convert to timestamp)\n  SELECT\n    AVG(CASE WHEN profile_type = 'distinct' THEN value_number END)      AS avg_distinct,\n    AVG(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_mrr\n  FROM _check_profiles\n  WHERE table_name = 'warehouse.hello.subscriptions'\n    AND column_name = 'mrr'\n    AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days'\n)\n\nSELECT\n  c.distinct_count,\n  h.avg_distinct,\n  CASE\n    WHEN h.avg_distinct IS NULL THEN NULL\n    ELSE (c.distinct_count - h.avg_distinct) / NULLIF(h.avg_distinct, 0) * 100\n  END AS distinct_change_pct,\n  c.avg_value,\n  h.avg_mrr,\n  CASE\n    WHEN h.avg_mrr IS NULL THEN NULL\n    ELSE (c.avg_value - h.avg_mrr) / NULLIF(h.avg_mrr, 0) * 100\n  END AS mrr_change_pct\nFROM current c, historical h;\n</code></pre> <p>This query compares current metrics to 30-day historical averages and calculates percentage changes. Perfect for detecting drift!</p>"},{"location":"components/checks/checks/#using-profiles-to-inform-checks","title":"Using Profiles to Inform Checks","text":"<p>Workflow:</p> <ol> <li>Enable profiling on new models (just add <code>profiles (...)</code> to your MODEL)</li> <li>Observe patterns for 30+ days (let profiles collect data)</li> <li>Identify anomalies in profile data (query <code>_check_profiles</code> and look for trends)</li> <li>Create checks based on observed patterns (now you know what's normal)</li> </ol> <p>Example:</p> <pre><code>-- Step 1: Enable profiling\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <pre><code>-- Step 2: Query profiles after 30 days\nSELECT\n    MIN(value_number) AS min_revenue,\n    MAX(value_number) AS max_revenue,\n    AVG(value_number) AS typical_revenue,\n    STDDEV(value_number) AS revenue_stddev\nFROM _check_profiles\nWHERE table_name = 'warehouse.hello.subscriptions'\n  AND column_name = 'mrr'\n  AND profile_type IN ('avg', 'mean', 'average', 'avg_value')\n  AND to_timestamp(profiled_at/1000) &gt;= CURRENT_DATE - INTERVAL '30 days';\n\n-- Results:\n\n-- min_revenue: 45000\n\n-- max_revenue: 75000\n\n-- typical_revenue: 58000\n\n-- revenue_stddev: 6000\n</code></pre> <pre><code># Step 3: Create checks based on observed patterns\nchecks:\n  analytics.orders:\n    accuracy:\n      - avg(revenue) between 40000 and 80000:\n          name: revenue_within_observed_range\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n\n      - anomaly detection for avg(revenue):\n          name: revenue_anomaly_detection\n</code></pre> <p>Now your checks are informed by actual data patterns, not guesses. Much better!</p>"},{"location":"components/checks/checks/#profile-best-practices","title":"Profile Best Practices","text":"<p>DO:</p> <ul> <li> <p>Profile high-value production tables (the ones that matter)</p> </li> <li> <p>Profile columns used in downstream analysis (if it's important, profile it)</p> </li> <li> <p>Use profiles to understand new data sources (what does this data look like?)</p> </li> <li> <p>Query profiles to detect data drift (is something changing?)</p> </li> <li> <p>Use profiles to inform check thresholds (data-driven thresholds are better)</p> </li> </ul> <p>DON'T:</p> <ul> <li> <p>Profile sensitive/PII columns (privacy risk, be careful)</p> </li> <li> <p>Profile every column (performance overhead, pick what matters)</p> </li> <li> <p>Profile temporary/experimental models (waste of resources)</p> </li> <li> <p>Use profiles as a replacement for checks (they serve different purposes)</p> </li> <li> <p>Profile very high-frequency models (storage cost adds up)</p> </li> </ul> <p>When to use profiles:</p> <ul> <li> <p>Building new models (understand the data first)</p> </li> <li> <p>Monitoring production tables (watch for changes)</p> </li> <li> <p>Detecting data drift (is the data changing?)</p> </li> <li> <p>Informing audit/check strategy (what should we check?)</p> </li> <li> <p>Debugging data quality issues (what's normal vs abnormal?)</p> </li> </ul> <p>When to skip profiles:</p> <ul> <li> <p>Temporary models (they won't be around long)</p> </li> <li> <p>Models with sensitive data (privacy concerns)</p> </li> <li> <p>Very high-frequency models (&gt; 100 runs/day, storage costs)</p> </li> <li> <p>Models where you only need pass/fail validation (profiles are overkill)</p> </li> </ul>"},{"location":"components/checks/checks/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"components/checks/checks/#cross-model-validation","title":"Cross-Model Validation","text":"<p>Validate relationships between models. This ensures referential integrity:</p> <pre><code># checks/cross_model.yml\nchecks:\n  analytics.orders:\n    consistency:\n      - failed rows:\n          name: orphaned_orders\n          fail query: |\n            SELECT o.order_id, o.customer_id\n            FROM analytics.orders o\n            LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id\n            WHERE c.customer_id IS NULL\n          samples limit: 10\n          attributes:\n            description: \"All orders must have a valid customer\"\n\n      - failed rows:\n          name: revenue_mismatch\n          fail query: |\n            SELECT\n              o.order_id,\n              o.revenue as order_revenue,\n              r.revenue as revenue_table_revenue\n            FROM analytics.orders o\n            JOIN analytics.revenue r ON o.order_id = r.order_id\n            WHERE ABS(o.revenue - r.revenue) &gt; 0.01\n</code></pre> <p>The first check finds orders without valid customers (orphaned records). The second ensures revenue matches across tables (consistency check).</p>"},{"location":"components/checks/checks/#time-based-validation","title":"Time-Based Validation","text":"<p>Ensure data timeliness. Is your data fresh? Are updates happening on schedule?</p> <pre><code>checks:\n  analytics.orders:\n    timeliness:\n      - failed rows:\n          name: stale_data\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE updated_at &lt; CURRENT_TIMESTAMP - INTERVAL '24 hours'\n              AND status != 'completed'\n          attributes:\n            description: \"Pending orders should update within 24 hours\"\n\n      - failed rows:\n          name: future_dates\n          fail query: |\n            SELECT *\n            FROM analytics.orders\n            WHERE order_date &gt; CURRENT_DATE\n</code></pre> <p>The first check finds stale pending orders (maybe something's stuck). The second catches future dates (data entry errors).</p>"},{"location":"components/checks/checks/#statistical-outlier-detection","title":"Statistical Outlier Detection","text":"<p>Custom outlier detection using SQL. Sometimes you need more control than anomaly detection provides:</p> <pre><code>checks:\n  analytics.revenue:\n    accuracy:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            WITH stats AS (\n              SELECT\n                AVG(revenue) as mean,\n                STDDEV(revenue) as stddev\n              FROM analytics.revenue\n            )\n            SELECT r.*,\n              (r.revenue - s.mean) / s.stddev as z_score\n            FROM analytics.revenue r, stats s\n            WHERE ABS((r.revenue - s.mean) / s.stddev) &gt; 3\n          samples limit: 20\n</code></pre> <p>This finds rows where revenue is more than 3 standard deviations from the mean (classic outlier detection). The z-score tells you how extreme each outlier is.</p>"},{"location":"components/checks/checks/#best-practices","title":"Best Practices","text":""},{"location":"components/checks/checks/#check-organization","title":"Check Organization","text":"<p>Organize your checks in a way that makes sense for your team. Here are two common approaches:</p> <p>By domain:</p> <pre><code>checks/\n\u251c\u2500\u2500 customers/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u251c\u2500\u2500 validity.yml\n\u2502   \u2514\u2500\u2500 consistency.yml\n\u251c\u2500\u2500 orders/\n\u2502   \u251c\u2500\u2500 completeness.yml\n\u2502   \u2514\u2500\u2500 timeliness.yml\n\u2514\u2500\u2500 revenue/\n    \u2514\u2500\u2500 accuracy.yml\n</code></pre> <p>By priority:</p> <pre><code>checks/\n\u251c\u2500\u2500 critical.yml      # Must never fail\n\u251c\u2500\u2500 important.yml     # Should rarely fail\n\u251c\u2500\u2500 monitoring.yml    # Track trends\n\u2514\u2500\u2500 experimental.yml  # Testing new checks\n</code></pre> <p>Pick whatever works for your team. The important thing is consistency, if everyone knows where to find things, life is easier.</p>"},{"location":"components/checks/checks/#naming-conventions","title":"Naming Conventions","text":"<p>Use descriptive names:</p> <pre><code># Bad - what does \"check1\" tell you?\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: check1\n\n# Good - clear and descriptive\nchecks:\n  analytics.customers:\n    completeness:\n      - missing_count(email) = 0:\n          name: no_missing_customer_emails\n          attributes:\n            description: \"All customers must have an email for marketing\"\n</code></pre> <p>Naming pattern:</p> <ul> <li> <p><code>&lt;dimension&gt;_&lt;what&gt;_&lt;constraint&gt;</code> or <code>&lt;what&gt;_&lt;constraint&gt;</code></p> </li> <li> <p>Examples:</p> </li> <li> <p><code>completeness_email_required</code> or <code>no_missing_emails</code></p> </li> <li> <p><code>validity_email_format</code> or <code>valid_email_format</code></p> </li> <li> <p><code>uniqueness_email_no_duplicates</code> or <code>unique_emails</code></p> </li> <li> <p><code>timeliness_order_within_24hrs</code> or <code>orders_update_daily</code></p> </li> </ul> <p>The key is that someone reading the name should understand what it checks without looking at the code.</p>"},{"location":"components/checks/checks/#threshold-selection","title":"Threshold Selection","text":"<p>Start conservative, adjust based on data:</p> <pre><code># Step 1: Start with wide range\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count &gt; 100:\n          name: sufficient_orders_v1\n\n# Step 2: Monitor for 30 days, see actual range: 5000-10000\n\n# Step 3: Tighten based on observed patterns\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 4000 and 12000:\n          name: sufficient_orders_v2\n          attributes:\n            description: \"Based on 30-day historical analysis\"\n</code></pre> <p>Don't set thresholds based on guesses, let the data tell you what's normal. Use profiles to understand your data first, then set checks based on what you learn.</p> <p>Use profiles to inform thresholds:</p> <pre><code>-- Query profiles to understand your data\nSELECT\n  MIN(value_number) as min_observed,\n  MAX(value_number) as max_observed,\n  AVG(value_number) as typical,\n  STDDEV(value_number) as stddev\nFROM check_results\nWHERE check_name = 'row_count'\n  AND executed_at &gt;= CURRENT_DATE - INTERVAL '90 days';\n\n-- Set threshold as: typical \u00b1 3*stddev\n</code></pre> <p>This gives you data-driven thresholds instead of wild guesses. Much better!</p>"},{"location":"components/checks/checks/#integration-strategy","title":"Integration Strategy","text":"<p>Layer validation:</p> <pre><code>-- LAYER 1: Audits (critical - blocks)\n\n-- Stop bad data at the door\nMODEL (\n  name analytics.orders,\n  assertions (\n    not_null(columns := (order_id, customer_id)),\n    unique_values(columns := (order_id))\n  )\n);\n</code></pre> <pre><code># LAYER 2: Checks (monitoring - warns)\n# Watch for problems but don't block\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 5000 and 15000:\n          name: order_count_in_range\n\n    timeliness:\n      - change for row_count &gt;= -30%:\n          name: order_count_stable\n</code></pre> <pre><code>-- LAYER 3: Profiles (observe - tracks)\n\n-- Just watch and learn\nMODEL (\n  name analytics.orders,\n  profiles (order_count, revenue, customer_tier)\n);\n</code></pre> <p>This three-layer approach gives you comprehensive data quality coverage: audits stop problems, checks warn about issues, and profiles help you understand what's normal.</p>"},{"location":"components/checks/checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/checks/checks/#check-failures","title":"Check Failures","text":""},{"location":"components/checks/checks/#investigate-failed-check","title":"Investigate failed check","text":"<p>When a check fails, you'll want to dig into why:</p> <pre><code># Run specific check with verbose output\nvulcan check --select analytics.customers.invalid_emails --verbose\n</code></pre> <p>This gives you more details about what went wrong.</p>"},{"location":"components/checks/checks/#query-failed-samples","title":"Query failed samples","text":"<p>If your check captures samples (like <code>failed rows</code> checks do), you can query them:</p> <pre><code>-- Get samples from last failed run\nSELECT *\nFROM check_samples\nWHERE check_name = 'invalid_emails'\n  AND status = 'failed'\nORDER BY executed_at DESC\nLIMIT 10;\n</code></pre> <p>This shows you actual rows that failed for debugging.</p>"},{"location":"components/checks/checks/#performance-issues","title":"Performance Issues","text":""},{"location":"components/checks/checks/#slow-check-queries","title":"Slow check queries","text":"<p>Problem: Check takes too long to run</p> <p>Solution 1: Add filters</p> <pre><code># Slow - scans entire table\nchecks:\n  analytics.orders:\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n\n# Fast - filters to recent data\nchecks:\n  analytics.orders:\n    filter: \"order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    validity:\n      - failed rows:\n          fail query: |\n            SELECT * FROM analytics.orders\n            WHERE email NOT LIKE '%@%'\n</code></pre> <p>Filtering reduces the amount of data the check needs to scan, which makes it faster.</p> <p>Solution 2: Add indexes</p> <pre><code>-- Add index on frequently checked columns\nCREATE INDEX idx_orders_email ON analytics.orders(email);\nCREATE INDEX idx_orders_order_date ON analytics.orders(order_date);\n</code></pre> <p>Indexes help queries run faster, especially for <code>failed rows</code> checks that filter on specific columns.</p>"},{"location":"components/checks/checks/#false-positives","title":"False Positives","text":""},{"location":"components/checks/checks/#threshold-too-strict","title":"Threshold too strict","text":"<p>Problem: Check fails during normal variance</p> <pre><code># Too strict - exact match is unrealistic\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count = 10000  # Exact match\n\n# Allow variance - more realistic\nchecks:\n  analytics.orders:\n    completeness:\n      - row_count between 9000 and 11000  # \u00b110% variance\n</code></pre> <p>Real data has variance. Don't set thresholds that are too strict, you'll just get false positives.</p>"},{"location":"components/checks/checks/#use-anomaly-detection-instead","title":"Use anomaly detection instead","text":"<p>Sometimes strict thresholds aren't the right approach:</p> <pre><code># Replace strict threshold with ML-based detection\nchecks:\n  analytics.orders:\n    accuracy:\n      - anomaly detection for row_count:\n          name: row_count_anomaly\n</code></pre> <p>Anomaly detection learns what's normal and adapts to variance, which reduces false positives.</p>"},{"location":"components/checks/checks/#summary","title":"Summary","text":"<p>Quality checks provide a comprehensive way to monitor data quality over time without blocking your models. Here's what we covered:</p>"},{"location":"components/checks/checks/#core-concepts","title":"Core Concepts","text":"<p>1. Quality Checks - YAML-configured validation rules</p> <ul> <li> <p>Non-blocking (don't stop models)</p> </li> <li> <p>Track trends over time</p> </li> <li> <p>Integrate with Activity API</p> </li> </ul> <p>2. Check Types - Missing data checks (<code>missing_count</code>, <code>missing_percent</code>)</p> <ul> <li> <p>Row count checks (<code>row_count</code>)</p> </li> <li> <p>Duplicate checks (<code>duplicate_count</code>)</p> </li> <li> <p>Failed rows (SQL-based, flexible)</p> </li> <li> <p>Anomaly detection (ML-based, learns from history)</p> </li> <li> <p>Change monitoring (compare to previous runs)</p> </li> </ul> <p>3. Data Profiling - Automatic statistical metric collection</p> <ul> <li> <p>Stored in <code>_check_profiles</code> table</p> </li> <li> <p>Observe patterns without validation</p> </li> <li> <p>Inform check threshold selection</p> </li> </ul> <p>4. Data Quality Strategy - Audits - Critical, blocking (stop bad data)</p> <ul> <li> <p>Checks - Monitoring, non-blocking (watch for problems)</p> </li> <li> <p>Profiles - Observation, tracking (understand what's normal)</p> </li> </ul> <p>Remember: start simple, use profiles to understand your data, then create checks based on what you learn. And don't forget, checks are there to help you, not stress you out. If a check is giving you too many false positives, adjust the threshold or switch to anomaly detection. The goal is better data quality, not perfect check scores.</p>"},{"location":"components/model/model_kinds/","title":"Kinds","text":""},{"location":"components/model/model_kinds/#kinds","title":"Kinds","text":"<p>Model kinds determine how Vulcan loads and processes your data. Each kind is optimized for different use cases. Some rebuild everything from scratch, others update incrementally, and some create views that compute on-demand.</p>"},{"location":"components/model/model_kinds/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p><code>INCREMENTAL_BY_TIME_RANGE</code> models are perfect for time-series data, things like events, logs, transactions, or any data that arrives over time. Instead of rebuilding everything each run (like FULL models do), these models only process the time intervals that are missing or need updating.</p> <p>If you're processing daily sales data, you don't want to reprocess all of 2023 just to add today's data. With <code>INCREMENTAL_BY_TIME_RANGE</code>, Vulcan only processes the new intervals, which saves time and money.</p> <p>To use this kind, you need to tell Vulcan two things:</p> <ol> <li>Which column has your time data - So Vulcan knows how to partition and filter</li> <li>A WHERE clause - That filters your upstream data by time range using Vulcan's time macros</li> </ol> <p>You specify the time column in your <code>MODEL</code> DDL using the <code>time_column</code> key. Here's a simple example:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p> In addition to specifying a time column in the <code>MODEL</code> DDL, the model's query must contain a <code>WHERE</code> clause that filters the upstream records by time range. Vulcan provides special macros that represent the start and end of the time range being processed: <code>@start_date</code> / <code>@end_date</code> and <code>@start_ds</code> / <code>@end_ds</code>. See Macros for more information.</p> Example SQL sequence when applying this model kind (ex: BigQuery) <p>This example demonstrates incremental by time range models.</p> <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incrementals_demo,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    -- How does this model kind behave?\n\n    --   DELETE by time range, then INSERT\n    time_column transaction_date,\n\n    -- How do I handle late-arriving data?\n\n    --   Handle late-arriving events for the past 2 (2*1) days based on cron\n\n    --   interval. Each time it runs, it will process today, yesterday, and\n\n    --   the day before yesterday.\n    lookback 2,\n  ),\n\n  -- Don't backfill data before this date\n  start '2024-10-25',\n\n  -- What schedule should I run these at?\n\n  --   Daily at Midnight UTC\n  cron '@daily',\n\n  -- Good documentation for the primary key\n  grains (transaction_id),\n\n  -- How do I test this data?\n\n  --   Validate that the `transaction_id` primary key values are both unique\n\n  --   and non-null. Data audit tests only run for the processed intervals,\n\n  --   not for the entire table.\n\n  -- audits (\n\n  --   UNIQUE_VALUES(columns = (transaction_id)),\n\n  --   NOT_NULL(columns = (transaction_id))\n\n  -- )\n);\n\nWITH sales_data AS (\n  SELECT\n    transaction_id,\n    product_id,\n    customer_id,\n    transaction_amount,\n    -- How do I account for UTC vs. PST (California baby) timestamps?\n\n    --   Make sure all time columns are in UTC and convert them to PST in the\n\n    --   presentation layer downstream.\n    transaction_timestamp,\n    payment_method,\n    currency\n  FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data\n  -- How do I make this run fast and only process the necessary intervals?\n\n  --   Use our date macros that will automatically run the necessary intervals.\n\n  --   Because Vulcan manages state, it will know what needs to run each time\n\n  --   you invoke `vulcan run`.\n  WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B\n  -- Include usage data from the 30 days before the interval\n  WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n)\n\nSELECT\n  s.transaction_id,\n  s.product_id,\n  s.customer_id,\n  s.transaction_amount,\n  -- Extract the date from the timestamp to partition by day\n  DATE(s.transaction_timestamp) as transaction_date,\n  -- Convert timestamp to PST using a SQL function in the presentation layer for end users\n  DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,\n  s.payment_method,\n  s.currency,\n  -- Product usage metrics\n  p.last_usage_date,\n  p.usage_count,\n  p.feature_utilization_score,\n  p.user_segment,\n  -- Derived metrics\n  CASE\n    WHEN p.usage_count &gt; 100 AND p.feature_utilization_score &gt; 0.8 THEN 'Power User'\n    WHEN p.usage_count &gt; 50 THEN 'Regular User'\n    WHEN p.usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type,\n  -- Time since last usage\n  DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage\nFROM sales_data s\nLEFT JOIN product_usage p\n  ON s.product_id = p.product_id\n  AND s.customer_id = p.customer_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>50975949</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (\n  `transaction_id` STRING,\n  `product_id` STRING,\n  `customer_id` STRING,\n  `transaction_amount` NUMERIC,\n  `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),\n  `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),\n  `payment_method` STRING,\n  `currency` STRING,\n  `last_usage_date` TIMESTAMP,\n  `usage_count` INT64,\n  `feature_utilization_score` FLOAT64,\n  `user_segment` STRING,\n  `user_type` STRING OPTIONS (description='Derived metrics'),\n  `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')\n  )\n  PARTITION BY `transaction_date`\n</code></pre> <p>Vulcan will validate the SQL before processing data (note the <code>WHERE FALSE LIMIT 0</code> and the placeholder timestamps).</p> <pre><code>WITH `sales_data` AS (\n  SELECT\n    `sales`.`transaction_id` AS `transaction_id`,\n    `sales`.`product_id` AS `product_id`,\n    `sales`.`customer_id` AS `customer_id`,\n    `sales`.`transaction_amount` AS `transaction_amount`,\n    `sales`.`transaction_timestamp` AS `transaction_timestamp`,\n    `sales`.`payment_method` AS `payment_method`,\n    `sales`.`currency` AS `currency`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n  WHERE (\n    `sales`.`transaction_timestamp` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `sales`.`transaction_timestamp` &gt;= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND\n    FALSE\n),\n`product_usage` AS (\n  SELECT\n    `product_usage`.`product_id` AS `product_id`,\n    `product_usage`.`customer_id` AS `customer_id`,\n    `product_usage`.`last_usage_date` AS `last_usage_date`,\n    `product_usage`.`usage_count` AS `usage_count`,\n    `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,\n    `product_usage`.`user_segment` AS `user_segment`\n  FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n  WHERE (\n    `product_usage`.`last_usage_date` &lt;= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND\n    `product_usage`.`last_usage_date` &gt;= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)\n    ) AND\n    FALSE\n)\n\nSELECT\n  `s`.`transaction_id` AS `transaction_id`,\n  `s`.`product_id` AS `product_id`,\n  `s`.`customer_id` AS `customer_id`,\n  CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,\n  DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n  DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n  `s`.`payment_method` AS `payment_method`,\n  `s`.`currency` AS `currency`,\n  `p`.`last_usage_date` AS `last_usage_date`,\n  `p`.`usage_count` AS `usage_count`,\n  `p`.`feature_utilization_score` AS `feature_utilization_score`,\n  `p`.`user_segment` AS `user_segment`,\n  CASE\n    WHEN `p`.`feature_utilization_score` &gt; 0.8 AND `p`.`usage_count` &gt; 100 THEN 'Power User'\n    WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n    WHEN `p`.`usage_count` IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END AS `user_type`,\n  DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\nFROM `sales_data` AS `s`\nLEFT JOIN `product_usage` AS `p`\n  ON `p`.`customer_id` = `s`.`customer_id` AND\n  `p`.`product_id` = `s`.`product_id`\nWHERE FALSE\nLIMIT 0\n</code></pre> <p>Vulcan will merge data into the empty table.</p> <pre><code>MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (\n  WITH `sales_data` AS (\n    SELECT\n      `transaction_id`,\n      `product_id`,\n      `customer_id`,\n      `transaction_amount`,\n      `transaction_timestamp`,\n      `payment_method`,\n      `currency`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`\n    WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  ),\n  `product_usage` AS (\n    SELECT\n      `product_id`,\n      `customer_id`,\n      `last_usage_date`,\n      `usage_count`,\n      `feature_utilization_score`,\n      `user_segment`\n    FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`\n    WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)\n  )\n\n  SELECT\n    `transaction_id`,\n    `product_id`,\n    `customer_id`,\n    `transaction_amount`,\n    `transaction_date`,\n    `transaction_timestamp_pst`,\n    `payment_method`,\n    `currency`,\n    `last_usage_date`,\n    `usage_count`,\n    `feature_utilization_score`,\n    `user_segment`,\n    `user_type`,\n    `days_since_last_usage`\n  FROM (\n    SELECT\n      `s`.`transaction_id` AS `transaction_id`,\n      `s`.`product_id` AS `product_id`,\n      `s`.`customer_id` AS `customer_id`,\n      `s`.`transaction_amount` AS `transaction_amount`,\n      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,\n      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,\n      `s`.`payment_method` AS `payment_method`,\n      `s`.`currency` AS `currency`,\n      `p`.`last_usage_date` AS `last_usage_date`,\n      `p`.`usage_count` AS `usage_count`,\n      `p`.`feature_utilization_score` AS `feature_utilization_score`,\n      `p`.`user_segment` AS `user_segment`,\n      CASE\n        WHEN `p`.`usage_count` &gt; 100 AND `p`.`feature_utilization_score` &gt; 0.8 THEN 'Power User'\n        WHEN `p`.`usage_count` &gt; 50 THEN 'Regular User'\n        WHEN `p`.`usage_count` IS NULL THEN 'New User'\n        ELSE 'Light User'\n      END AS `user_type`,\n      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`\n    FROM `sales_data` AS `s`\n    LEFT JOIN `product_usage` AS `p`\n      ON `s`.`product_id` = `p`.`product_id`\n      AND `s`.`customer_id` = `p`.`customer_id`\n  ) AS `_subquery`\n  WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)\n) AS `__MERGE_SOURCE__`\nON FALSE\nWHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE\nWHEN NOT MATCHED THEN\n  INSERT (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n  VALUES (\n    `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,\n    `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,\n    `days_since_last_usage`\n  )\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer to pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS\nSELECT *\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`\n</code></pre> <p>Important: Timezone Requirements</p> <p>Your <code>time_column</code> should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.</p> <p>Why UTC? It's a data engineering best practice, convert everything to UTC when it enters your system, then convert to local timezones only when data leaves for end users. This prevents timezone-related bugs as data flows between models.</p> <p>Important: The <code>cron_tz</code> flag doesn't change this requirement, it only affects when your model runs, not how time intervals are calculated.</p> <p>If you absolutely must use a different timezone, you can try to work around it using <code>lookback</code>, <code>allow_partials</code>, or cron offsets, but UTC is strongly recommended. Trust us on this one, timezone bugs are no fun!</p> <p>This example implements a complete <code>INCREMENTAL_BY_TIME_RANGE</code> model that specifies the time column name <code>order_date</code> in the <code>MODEL</code> DDL and includes a SQL <code>WHERE</code> clause to filter records by time range:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_time_range,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  grains (order_date, product_id),\n  cron '@daily'\n);\n\nSELECT\n  o.order_date,\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  COUNT(DISTINCT o.order_id) AS order_count,\n  SUM(oi.quantity) AS total_quantity,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p\n  ON oi.product_id = p.product_id\nWHERE\n  o.order_date BETWEEN @start_ds AND @end_ds\nGROUP BY\n  o.order_date, p.product_id, p.name, p.category\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_time_range_py\",\n    columns={\n        \"order_date\": \"date\",\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"total_sales_amount\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_date\", \"product_id\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\", \"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT o.order_date, p.product_id, p.name AS product_name,\n           SUM(oi.quantity * oi.unit_price) AS total_sales_amount\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    JOIN vulcan_demo.products p ON oi.product_id = p.product_id\n    WHERE o.order_date BETWEEN '{start}' AND '{end}'\n    GROUP BY o.order_date, p.product_id, p.name\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/model_kinds/#time-column","title":"Time Column","text":"<p>Vulcan needs to know which column in your model's output represents the timestamp or date for each record. This is your <code>time_column</code>.</p> <p>Remember: UTC Timezone</p> <p>Your <code>time_column</code> should be in UTC timezone. Learn more about why this matters above.</p> <p>The time column is used to determine which records will be overwritten during data restatement and provides a partition key for engines that support partitioning (such as Apache Spark). The name of the time column is specified in the <code>MODEL</code> DDL <code>kind</code> specification:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- This model's time information is stored in the `order_date` column\n  )\n);\n</code></pre> <p>By default, Vulcan assumes your time column is in <code>%Y-%m-%d</code> format (like <code>2025-01-15</code>). If your dates are in a different format, you can specify it:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column (order_date, '%Y-%m-%d')\n  )\n);\n</code></pre> <p>Format String Dialect</p> <p>Use the same SQL dialect for your format string as the one used in your model's query.</p> <p>Safety feature: Vulcan automatically adds a time range filter to your query's output to prevent data leakage. This means even if your <code>WHERE</code> clause has a bug, Vulcan won't accidentally store records outside the target interval.</p> <p>Here's how it works:</p> <ul> <li> <p>Your WHERE clause filters the input data as it's read from upstream tables (makes queries faster)</p> </li> <li> <p>Vulcan's automatic filter filters the output data before it's stored (prevents data leakage)</p> </li> </ul> <p>This is especially important when handling late-arriving data, you don't want to accidentally overwrite unrelated records!</p> <p>Example: sometimes your upstream data uses a different time column than your model. In this case, you filter on the upstream column (<code>shipped_date</code>), but Vulcan still adds a filter on your model's time column (<code>order_date</code>):</p> <pre><code>MODEL (\n  name vulcan_demo.shipment_events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date -- `order_date` is model's time column\n  )\n);\n\nSELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds; -- Filter is based on the user-supplied `shipped_date` column\n</code></pre> <p>At runtime, Vulcan will automatically modify the model's query to look like this:</p> <pre><code>SELECT\n  o.order_date,\n  s.shipped_date,\n  s.carrier\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id\nWHERE\n  s.shipped_date BETWEEN @start_ds AND @end_ds\n  AND o.order_date BETWEEN @start_ds AND @end_ds; -- `order_date` time column filter automatically added by Vulcan\n</code></pre>"},{"location":"components/model/model_kinds/#partitioning","title":"Partitioning","text":"<p>By default, Vulcan automatically adds your <code>time_column</code> to the partition key. This lets database engines do partition pruning (skipping partitions that don't match your query), which makes queries faster.</p> <p>Why this matters: If you're querying data from the last 7 days, the engine can skip scanning all the old partitions. That's a huge performance win!</p> <p>Sometimes you might not want this though, maybe you want to partition exclusively on another column, or you want to partition on <code>month(time_column)</code> but your engine doesn't support expression-based partitioning.</p> <p>To disable automatic time column partitioning, set <code>partition_by_time_column false</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.daily_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    partition_by_time_column false\n  ),\n  partitioned_by (warehouse_id) -- order_date will no longer be automatically added here and the partition key will just be 'warehouse_id'\n);\n</code></pre>"},{"location":"components/model/model_kinds/#idempotency","title":"Idempotency","text":"<p>Make incremental by time range model queries idempotent to prevent unexpected results during data restatement.</p> <p>Make your incremental by time range queries idempotent. This means running the same query multiple times produces the same result, which prevents surprises during data restatement.</p> <p>Watch out: Your upstream models can affect idempotency. If you reference a FULL model (which rebuilds everything each run), your incremental model becomes non-idempotent because that upstream data changes every time. This is usually fine, but it's good to be aware of.</p>"},{"location":"components/model/model_kinds/#materialization-strategy","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_TIME_RANGE</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE by time column partition Databricks INSERT OVERWRITE by time column partition Snowflake DELETE by time range, then INSERT BigQuery DELETE by time range, then INSERT Redshift DELETE by time range, then INSERT Postgres DELETE by time range, then INSERT DuckDB DELETE by time range, then INSERT"},{"location":"components/model/model_kinds/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p><code>INCREMENTAL_BY_UNIQUE_KEY</code> models update data based on a unique key. It works like an upsert operation: if a key exists, update it; if it doesn't, insert it.</p> <p>Here's how it works:</p> <ul> <li> <p>New key? \u2192 Insert the row</p> </li> <li> <p>Existing key? \u2192 Update the row with new data</p> </li> <li> <p>Key missing from new data? \u2192 Leave the existing row alone</p> </li> </ul> <p>Why use this? Perfect for dimension tables, customer records, or any data where you want to keep the latest version of each record without rebuilding everything. It's like updating a contact list, you update existing contacts and add new ones, but you don't delete contacts that aren't in your latest import.</p> <p>This kind is a good fit for datasets that have the following traits:</p> <ul> <li>Each record has a unique key associated with it.</li> <li>There is at most one record associated with each unique key.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their keys match.</li> </ul> <p>A Slowly Changing Dimension (SCD) is one approach that fits this description well. See the SCD Type 2 model kind for SCD Type 2 models.</p> <p>The name of the unique key column must be provided as part of the <code>MODEL</code> DDL, as in this example:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.incremental_by_unique_key,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  MAX(o.order_date) AS last_order_date\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nWHERE\n  o.order_date IS NULL OR o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.incremental_by_unique_key_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"last_order_date\": \"date\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id,\n           SUM(oi.quantity * oi.unit_price) as total_spent,\n           MAX(o.order_date) as last_order_date\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id\n    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY c.customer_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>You can use composite keys (multiple columns) too:</p> <pre><code>MODEL (\n  name vulcan_demo.order_items_agg,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, product_id)\n  )\n);\n</code></pre> <p>You can also filter upstream records by time range using <code>@start_date</code>, <code>@end_date</code>, or other time macros (just like <code>INCREMENTAL_BY_TIME_RANGE</code>). This is useful when you only want to process records from a specific time period.</p> <p>Note: Vulcan's time macros are always in UTC timezone.</p> <pre><code>SELECT\n  c.customer_id,\n  c.name AS customer_name,\n  COUNT(o.order_id) AS total_orders\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nWHERE\n  o.order_date BETWEEN @start_date AND @end_date\nGROUP BY c.customer_id, c.name\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.incremental_by_unique_key_example,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key id\n  ),\n  start '2020-01-01',\n  cron '@daily',\n);\n\nSELECT\n  id,\n  item_id,\n  event_date\nFROM demo.seed_model\nWHERE\n  event_date BETWEEN @start_date AND @end_date\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>1161945221</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>FALSE LIMIT 0</code> in the <code>WHERE</code> statement and the placeholder dates).</p> <pre><code>SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE (`seed_model`.`event_date` &lt;= CAST('1970-01-01' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('1970-01-01' AS DATE)) AND FALSE LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`\nWHERE `seed_model`.`event_date` &lt;= CAST('2024-10-30' AS DATE) AND `seed_model`.`event_date` &gt;= CAST('2020-01-01' AS DATE)) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incremental_by_unique_key_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221`\n</code></pre> <p>Note: Models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are inherently non-idempotent, which should be taken into consideration during data restatement. As a result, partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated.</p>"},{"location":"components/model/model_kinds/#unique-key-expressions","title":"Unique Key Expressions","text":"<p>You're not limited to column names. You can use SQL expressions when you need to create a key from multiple columns or transform values. Example using <code>COALESCE</code>:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_unique,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key COALESCE(\"email\", '')\n  )\n);\n</code></pre>"},{"location":"components/model/model_kinds/#when-matched-expression","title":"When Matched Expression","text":"<p>By default, when a key matches (source and target have the same key), Vulcan updates all columns. But sometimes you want more control, maybe you want to preserve certain values, or only update specific columns.</p> <p>You can customize this behavior with <code>when_matched</code> expressions:</p> <pre><code>MODEL (\n  name vulcan_demo.customers_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched (\n      WHEN MATCHED THEN UPDATE SET target.email = COALESCE(source.email, target.email)\n    )\n  )\n);\n</code></pre> <p>Important: You must use <code>source</code> and <code>target</code> aliases to distinguish between the source (new data) and target (existing table) columns.</p> <p>You can also provide multiple <code>WHEN MATCHED</code> expressions for more complex logic:</p> <pre><code>MODEL (\n  name vulcan_demo.products_update,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key product_id,\n    when_matched (\n      WHEN MATCHED AND source.price IS NULL THEN UPDATE SET target.price = target.price\n      WHEN MATCHED THEN UPDATE SET target.category = COALESCE(source.category, target.category)\n    )\n  )\n);\n</code></pre> <p>Engine Support</p> <p><code>when_matched</code> only works on engines that support the <code>MERGE</code> statement. Supported engines include:</p> <ul> <li> <p>BigQuery</p> </li> <li> <p>Databricks</p> </li> <li> <p>Postgres</p> </li> <li> <p>Redshift (requires <code>enable_merge: true</code> in connection config)</p> </li> <li> <p>Snowflake</p> </li> <li> <p>Spark</p> </li> </ul> <p>Redshift users: You need to enable MERGE support by setting <code>enable_merge: true</code> in your connection config. It's disabled by default.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      enable_merge: true\n</code></pre> <p>Redshift supports only the <code>UPDATE</code> or <code>DELETE</code> actions for the <code>WHEN MATCHED</code> clause and does not allow multiple <code>WHEN MATCHED</code> expressions. For further information, refer to the Redshift documentation.</p>"},{"location":"components/model/model_kinds/#merge-filter-expression","title":"Merge Filter Expression","text":"<p>MERGE operations can be slow on large tables because they typically scan the entire existing table. If you're only updating a small subset of records, this is wasteful.</p> <p>Solution: Use <code>merge_filter</code> to add conditions to the MERGE's <code>ON</code> clause. This limits the scan to only the rows that might match, making things much faster.</p> <p>The <code>merge_filter</code> accepts predicates (single or combined with AND) that get added to the MERGE operation:</p> <pre><code>MODEL (\n  name vulcan_demo.orders_recent,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key order_id,\n    merge_filter source._operation IS NULL AND target.order_date &gt; dateadd(day, -7, current_date)\n  )\n);\n</code></pre> <p>Just like <code>when_matched</code>, use <code>source</code> and <code>target</code> aliases to reference the source and target tables.</p> <p>If your dbt project uses <code>incremental_predicates</code>, Vulcan automatically converts them to <code>merge_filter</code>.</p>"},{"location":"components/model/model_kinds/#materialization-strategy_1","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_UNIQUE_KEY</code> kind are materialized using the following strategies:</p> Engine Strategy Spark not supported Databricks MERGE ON unique key Snowflake MERGE ON unique key BigQuery MERGE ON unique key Redshift MERGE ON unique key Postgres MERGE ON unique key DuckDB DELETE ON matched + INSERT new rows"},{"location":"components/model/model_kinds/#full","title":"FULL","text":"<p><code>FULL</code> models are the simplest kind, they rebuild everything from scratch every time they run. No incremental logic, no time columns, no unique keys. Just run the query and replace the entire table.</p> <p>When to use FULL:</p> <ul> <li> <p>Small datasets where rebuilding is fast and cheap</p> </li> <li> <p>Aggregate tables without a time dimension</p> </li> <li> <p>Tables that change completely each run (like a \"current state\" snapshot)</p> </li> <li> <p>Development and testing (simpler is better when you're iterating)</p> </li> </ul> <p>When NOT to use FULL:</p> <ul> <li> <p>Large datasets (you'll wait forever and pay a lot)</p> </li> <li> <p>Time-series data (use <code>INCREMENTAL_BY_TIME_RANGE</code> instead)</p> </li> <li> <p>Tables that only change partially (use incremental kinds)</p> </li> </ul> <p>The trade-off is simplicity vs. performance. For small tables, FULL is perfect. For large tables, incremental kinds will save you time and money.</p> <p>This example specifies a <code>FULL</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  start '2025-01-01',\n  grains (customer_id)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\nORDER BY total_spent DESC\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT p.product_id, p.name AS product_name, p.category,\n           COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.full_model_example,\n  kind FULL,\n  cron '@daily',\n  grains (item_id),\n);\n\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM demo.incremental_model\nGROUP BY\n  item_id\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>2345651858</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` (`item_id` INT64, `num_orders` INT64)\n</code></pre> <p>Vulcan will validate the model's query before processing data (note the <code>WHERE FALSE</code> and <code>LIMIT 0</code>).</p> <pre><code>SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nWHERE FALSE\nGROUP BY `incremental_model`.`item_id` LIMIT 0\n</code></pre> <p>Vulcan will create a versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` AS\nSELECT CAST(`item_id` AS INT64) AS `item_id`, CAST(`num_orders` AS INT64) AS `num_orders`\nFROM (SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`\nFROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`\nGROUP BY `incremental_model`.`item_id`) AS `_subquery`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`full_model_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858`\n</code></pre>"},{"location":"components/model/model_kinds/#materialization-strategy_2","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>FULL</code> kind are materialized using the following strategies:</p> Engine Strategy Spark INSERT OVERWRITE Databricks INSERT OVERWRITE Snowflake CREATE OR REPLACE TABLE BigQuery CREATE OR REPLACE TABLE Redshift DROP TABLE, CREATE TABLE, INSERT Postgres DROP TABLE, CREATE TABLE, INSERT DuckDB CREATE OR REPLACE TABLE"},{"location":"components/model/model_kinds/#view","title":"VIEW","text":"<p>Unlike the other kinds, <code>VIEW</code> models don't store any data. Instead, they create a virtual table (a view) that runs your query every time someone queries it.</p> <p>How it works: When a downstream model or user queries your VIEW model, the database executes your query on-the-fly. No data is pre-computed or stored.</p> <p>When to use VIEW:</p> <ul> <li> <p>Simple transformations that are fast to compute</p> </li> <li> <p>When you want always-fresh data (no caching)</p> </li> <li> <p>When storage is expensive but compute is cheap</p> </li> <li> <p>For lightweight transformations that don't need materialization</p> </li> </ul> <p>When NOT to use VIEW:</p> <ul> <li> <p>Expensive queries that run frequently (you'll pay the compute cost every time)</p> </li> <li> <p>Complex aggregations or joins (materialize these instead)</p> </li> <li> <p>Python models (VIEW isn't supported for Python, use SQL)</p> </li> </ul> <p>Default Kind</p> <p><code>VIEW</code> is the default model kind if you don't specify one. So if you write a model without a <code>kind</code>, it becomes a VIEW automatically.</p> <p>Performance Consideration</p> <p>Since VIEW queries run every time they're referenced, expensive queries can get costly fast. If your view is referenced by many downstream models, you might be running that expensive query dozens of times. Consider materializing expensive views as FULL or incremental models instead.</p> <p>This example specifies a <code>VIEW</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.view_model,\n  kind VIEW,\n  grains (warehouse_performance_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  r.region_name,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', o.order_date::TEXT) AS warehouse_performance_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.warehouses AS w\nLEFT JOIN vulcan_demo.regions AS r\n  ON w.region_id = r.region_id\nLEFT JOIN vulcan_demo.orders AS o\n  ON w.warehouse_id = o.warehouse_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY w.warehouse_id, w.name, r.region_name, o.order_date\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.example_view,\n  kind VIEW,\n  cron '@daily',\n);\n\nSELECT\n  'hello there' as a_column\n</code></pre> <p>Vulcan will execute this SQL to create a versioned view in the physical layer. Note that the view's version fingerprint, <code>1024042926</code>, is part of the view name.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n(`a_column`) AS SELECT 'hello there' AS `a_column`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned view in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`example_view` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`\n</code></pre>"},{"location":"components/model/model_kinds/#materialized-views","title":"Materialized Views","text":"<p>Want the best of both worlds? You can turn a VIEW into a materialized view by setting <code>materialized: true</code>. Materialized views store the query results (like a table) but automatically refresh when the underlying data changes (like a view).</p> <p>Set it up like this:</p> <pre><code>MODEL (\n  name vulcan_demo.sales_summary,\n  kind VIEW (\n    materialized true\n  )\n);\n</code></pre> <p>Engine Support</p> <p>Materialized views are only supported on:</p> <ul> <li> <p>BigQuery</p> </li> <li> <p>Databricks</p> </li> <li> <p>Snowflake</p> </li> </ul> <p>On other engines, this flag is ignored and you'll get a regular VIEW.</p> <p>Vulcan only recreates the materialized view when your query changes or the view doesn't exist. This provides the performance benefits of materialized views without unnecessary refreshes.</p>"},{"location":"components/model/model_kinds/#embedded","title":"EMBEDDED","text":"<p><code>EMBEDDED</code> models are like reusable SQL snippets. They don't create tables or views, instead, their query gets injected directly into any downstream model that references them, as a subquery.</p> <p>Why use this? If you have common logic that multiple models need (like a CTE that filters active customers), you can define it once in an EMBEDDED model and reuse it everywhere. It's like a macro, but for SQL.</p> <p>Perfect for:</p> <ul> <li> <p>Common CTEs used across multiple models</p> </li> <li> <p>Reusable business logic (like \"active customers\" or \"valid orders\")</p> </li> <li> <p>Avoiding code duplication</p> </li> </ul> <p>Python Models</p> <p>Python models don't support the <code>EMBEDDED</code> kind, use a SQL model instead.</p> <p>This example specifies an <code>EMBEDDED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.unique_customers,\n  kind EMBEDDED\n);\n\nSELECT DISTINCT\n  customer_id,\n  name AS customer_name,\n  email\nFROM vulcan_demo.customers\n</code></pre>"},{"location":"components/model/model_kinds/#seed","title":"SEED","text":"<p>The <code>SEED</code> model kind is used to specify seed models for using static CSV datasets in your Vulcan project.</p> <p>How it works: You point to a CSV file, define the schema, and Vulcan loads it into a table. The data only gets reloaded if you change the model definition or update the CSV file.</p> <p>Use cases:</p> <ul> <li> <p>Reference data (countries, states, categories)</p> </li> <li> <p>Lookup tables</p> </li> <li> <p>Static configuration data</p> </li> <li> <p>Test data</p> </li> </ul> <p>Python Models</p> <p>Python models don't support the <code>SEED</code> kind, use a SQL model instead.</p> <p>When Data Reloads</p> <p>Seed models are loaded once and stay loaded unless you update the model definition or change the CSV file. This keeps things efficient, no point reloading static data every run!</p> <p>This example specifies a <code>SEED</code> model kind:</p> <pre><code>MODEL (\n  name vulcan_demo.seed_model,\n  kind SEED (\n    path '../seeds/seed_data.csv'\n  ),\n  columns (\n    id INT,\n    item_id INT,\n    event_date DATE\n  ),\n  grains (id),\n  assertions (\n    UNIQUE_COMBINATION_OF_COLUMNS(columns := (id, event_date)),\n    NOT_NULL(columns := (id, item_id, event_date))\n  )\n)\n</code></pre> Example SQL sequence when applying this model kind (ex: BigQuery) <p>Create a model with the following definition and run <code>vulcan plan dev</code>:</p> <pre><code>MODEL (\n  name demo.seed_example,\n  kind SEED (\n    path '../../seeds/seed_example.csv'\n  ),\n  columns (\n    id INT64,\n    item_id INT64,\n    event_date DATE\n  ),\n  grains (id, event_date)\n)\n</code></pre> <p>Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, <code>3038173937</code>, is part of the table name.</p> <pre><code>CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` (`id` INT64, `item_id` INT64, `event_date` DATE)\n</code></pre> <p>Vulcan will upload the seed as a temp table in the physical layer.</p> <pre><code>vulcan-public-demo.vulcan__demo.__temp_demo__seed_example__3038173937_9kzbpld7\n</code></pre> <p>Vulcan will create a versioned table in the physical layer from the temp table.</p> <pre><code>CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` AS\nSELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`\nFROM (SELECT `id`, `item_id`, `event_date`\nFROM `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`) AS `_subquery`\n</code></pre> <p>Vulcan will drop the temp table in the physical layer.</p> <pre><code>DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`\n</code></pre> <p>Vulcan will create a suffixed <code>__dev</code> schema based on the name of the plan environment.</p> <pre><code>CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`\n</code></pre> <p>Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.</p> <pre><code>CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`seed_example` AS\nSELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937`\n</code></pre>"},{"location":"components/model/model_kinds/#scd-type-2","title":"SCD Type 2","text":"<p>SCD Type 2 is a model kind that supports slowly changing dimensions (SCDs) in your Vulcan project. SCDs are a common pattern in data warehousing that allow you to track changes to records over time.</p> <p>Vulcan achieves this by adding a <code>valid_from</code> and <code>valid_to</code> column to your model. The <code>valid_from</code> column is the timestamp that the record became valid (inclusive) and the <code>valid_to</code> column is the timestamp that the record became invalid (exclusive). The <code>valid_to</code> column is set to <code>NULL</code> for the latest record.</p> <p>Therefore, you can use these models to not only tell you what the latest value is for a given record but also what the values were anytime in the past. Note that maintaining this history does come at a cost of increased storage and compute and this may not be a good fit for sources that change frequently since the history could get very large.</p> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss, so data restatement is disabled for models of this kind by default.</p> <p>Vulcan supports two ways to detect changes: By Time (recommended) or By Column. Let's look at both:</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-recommended","title":"SCD Type 2 By Time (Recommended)","text":"<p>By Time is the recommended approach. It works with source tables that have an \"Updated At\" timestamp column (like <code>updated_at</code>, <code>modified_at</code>, <code>last_changed</code>).</p> <p>Why it's recommended: The timestamp tells you exactly when a record changed, which makes your SCD Type 2 table more accurate. You get precise <code>valid_from</code> times based on when the source system actually updated the record.</p> <p>If your source table has an <code>updated_at</code> column, use this approach!</p> <p>This example specifies a <code>SCD_TYPE_2_BY_TIME</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_time,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key dt\n  ),\n  grains (dt)\n);\n\nSELECT\n  dd.dt,\n  dd.year,\n  dd.month,\n  dd.day_of_week,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity) AS total_quantity_sold,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  CURRENT_TIMESTAMP AS updated_at\nFROM vulcan_demo.dim_dates AS dd\nLEFT JOIN vulcan_demo.orders AS o\n  ON dd.dt = o.order_date\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY dd.dt, dd.year, dd.month, dd.day_of_week\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_time_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"region_name\": \"string\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n    grains=[\"customer_id\"],\n    depends_on=[\"vulcan_demo.customers\", \"vulcan_demo.regions\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT c.customer_id, c.name as customer_name, c.email, r.region_name\n    FROM vulcan_demo.customers c\n    LEFT JOIN vulcan_demo.regions r ON c.region_id = r.region_id\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p> <p>The <code>updated_at</code> column name can also be changed by adding the following to your model definition: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    updated_at_name my_updated_at -- Name for `updated_at` column\n  )\n);\n\nSELECT\n  id,\n  name,\n  price,\n  my_updated_at\nFROM\n  stg.current_menu_items;\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  my_updated_at TIMESTAMP,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#scd-type-2-by-column","title":"SCD Type 2 By Column","text":"<p>By Column works when your source table doesn't have an \"Updated At\" timestamp. Instead, Vulcan compares the values in specific columns between runs and detects changes.</p> <p>How it works: You specify which columns to watch (or use <code>*</code> to watch all columns). When Vulcan detects a change in any of those columns, it records <code>valid_from</code> as the execution time when the change was detected.</p> <p>Use this when: Your source system doesn't track update timestamps, but you still want to maintain history. The trade-off is that <code>valid_from</code> times are based on when Vulcan detected the change, not when the source system actually changed it.</p> <p>This example specifies a <code>SCD_TYPE_2_BY_COLUMN</code> model kind:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.scd_type2_by_column,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key ARRAY[product_id],\n    columns ARRAY[product_name, category, price]\n  ),\n  grains (product_id)\n);\n\nSELECT\n  p.product_id,\n  p.name AS product_name,\n  p.category,\n  p.price,\n  s.name AS supplier_name,\n  r.region_name\nFROM vulcan_demo.products AS p\nLEFT JOIN vulcan_demo.suppliers AS s\n  ON p.supplier_id = s.supplier_id\nLEFT JOIN vulcan_demo.regions AS r\n  ON s.region_id = r.region_id\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.scd_type2_by_column_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"price\": \"decimal(10,2)\"\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"product_name\", \"category\", \"price\"],\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT product_id, name as product_name, category, price\n    FROM vulcan_demo.products\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  valid_from TIMESTAMP,\n  valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#change-column-names","title":"Change Column Names","text":"<p>Vulcan automatically adds <code>valid_from</code> and <code>valid_to</code> columns to your table. If you want different names (maybe to match your existing schema conventions), you can customize them: </p><pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    valid_from_name my_valid_from, -- Name for `valid_from` column\n    valid_to_name my_valid_to -- Name for `valid_to` column\n  )\n);\n</code></pre><p></p> <p>Vulcan will materialize this table with the following structure: </p><pre><code>TABLE db.menu_items (\n  id INT,\n  name STRING,\n  price DOUBLE,\n  updated_at TIMESTAMP,\n  my_valid_from TIMESTAMP,\n  my_valid_to TIMESTAMP\n);\n</code></pre><p></p>"},{"location":"components/model/model_kinds/#deletes","title":"Deletes","text":"<p>A \"hard delete\" is when a record disappears from your source table entirely. How should SCD Type 2 handle this?</p> <p>Default behavior (<code>invalidate_hard_deletes: false</code>):</p> <ul> <li><code>valid_to</code> column will continue to be set to <code>NULL</code> (therefore still considered \"valid\")</li> <li>If the record is added back, then the <code>valid_to</code> column will be set to the <code>valid_from</code> of the new record.</li> </ul> <p>When a record is added back, the new record will be inserted into the table with <code>valid_from</code> set to:</p> <ul> <li>SCD_TYPE_2_BY_TIME: the largest of either the <code>updated_at</code> timestamp of the new record or the <code>valid_from</code> timestamp of the deleted record in the SCD Type 2 table</li> <li>SCD_TYPE_2_BY_COLUMN: the <code>execution_time</code> when the record was detected again</li> </ul> <p>With <code>invalidate_hard_deletes: true</code>:</p> <ul> <li><code>valid_to</code> is set to the execution time when Vulcan detected the missing record</li> <li>If the record comes back later, <code>valid_to</code> stays unchanged (you'll have a gap in history)</li> </ul> <p>Which should you use?</p> <ul> <li> <p><code>false</code> (default): Missing records are still considered \"valid\" (no gaps in history). Use this if you think missing records might be temporary or if you prefer continuous history.</p> </li> <li> <p><code>true</code>: Deletes are accurately tracked with precise timestamps. Use this if you want to know exactly when records were deleted, even if it creates gaps in history.</p> </li> </ul> <p>With <code>false</code>, missing records are still considered valid. With <code>true</code>, missing records are treated as deleted at that time.</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-time-in-action","title":"Example of SCD Type 2 By Time in Action","text":"<p>Let's walk through a real example. Say you're tracking a restaurant menu, and you start with this source data (with <code>invalidate_hard_deletes: true</code>):</p> ID Name Price Updated At 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 2 Cheeseburger 8.99 2020-01-01 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 <p>The target table, which is currently empty, will be materialized with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your models ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price Updated At 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 3 French Fries 4.99 2020-01-01 00:00:00 4 Chocolate Milkshake 3.99 2020-01-02 00:00:00 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>Target table will be updated with the following data:</p> ID Name Price Updated At Valid From Valid To 1 Chicken Sandwich 10.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 00:00:00 1 Chicken Sandwich 12.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 1 Chicken Sandwich 14.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 2 Cheeseburger 8.99 2020-01-01 00:00:00 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL 3 French Fries 4.99 2020-01-01 00:00:00 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 00:00:00 2020-01-02 00:00:00 2020-01-03 00:00:00 4 Chocolate Milkshake 3.99 2020-01-03 00:00:00 2020-01-03 00:00:00 NULL <p>Notice: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 00:00:00</code>. If you query the table for that time range, you won't see it, which accurately reflects that it wasn't on the menu during that period.</p> <p>This is the most accurate representation based on your source data. If <code>Cheeseburger</code> had been added back with its original <code>updated_at</code> timestamp (<code>2020-01-01</code>), Vulcan would have set the new record's <code>valid_from</code> to <code>2020-01-02 11:00:00</code> (when it was detected again), filling the gap. But since the timestamp didn't change, it's likely the item was removed in error, and the gap accurately represents that.</p>"},{"location":"components/model/model_kinds/#example-of-scd-type-2-by-column-in-action","title":"Example of SCD Type 2 By Column in Action","text":"<p>Now let's see how By Column works. Same restaurant menu example, but this time the source table doesn't have an <code>updated_at</code> column. We'll configure the model to watch <code>Name</code> and <code>Price</code> for changes.</p> <p>Starting data:</p> ID Name Price 1 Chicken Sandwich 10.99 2 Cheeseburger 8.99 3 French Fries 4.99 <p>After the first run, your SCD Type 2 table looks like this:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL <p>Now lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 12.99 3 French Fries 4.99 4 Milkshake 3.99 <p>Summary of Changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $10.99 to $12.99.</li> <li>Cheeseburger was removed from the menu.</li> <li>Milkshakes were added to the menu.</li> </ul> <p>Assuming your models ran at <code>2020-01-02 11:00:00</code>, target table will be updated with the following data:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 NULL <p>For our final pass, lets say that you update the source table with the following data:</p> ID Name Price 1 Chicken Sandwich 14.99 2 Cheeseburger 8.99 3 French Fries 4.99 4 Chocolate Milkshake 3.99 <p>Summary of changes:</p> <ul> <li>The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)</li> <li>Cheeseburger was added back to the menu with original name and price.</li> <li>Milkshake name was updated to be \"Chocolate Milkshake\".</li> </ul> <p>After running at <code>2020-01-03 11:00:00</code>, your final SCD Type 2 table:</p> ID Name Price Valid From Valid To 1 Chicken Sandwich 10.99 1970-01-01 00:00:00 2020-01-02 11:00:00 1 Chicken Sandwich 12.99 2020-01-02 11:00:00 2020-01-03 11:00:00 1 Chicken Sandwich 14.99 2020-01-03 11:00:00 NULL 2 Cheeseburger 8.99 1970-01-01 00:00:00 2020-01-02 11:00:00 2 Cheeseburger 8.99 2020-01-03 11:00:00 NULL 3 French Fries 4.99 1970-01-01 00:00:00 NULL 4 Milkshake 3.99 2020-01-02 11:00:00 2020-01-03 11:00:00 4 Chocolate Milkshake 3.99 2020-01-03 11:00:00 NULL <p>Notice: <code>Cheeseburger</code> was deleted from <code>2020-01-02 11:00:00</code> to <code>2020-01-03 11:00:00</code>. Query the table for that time range, and you won't see it, which accurately reflects that it wasn't on the menu during that period.</p>"},{"location":"components/model/model_kinds/#shared-configuration-options","title":"Shared Configuration Options","text":"Name Description Type unique_key Unique key used for identifying rows between source and target List of strings or string valid_from_name The name of the <code>valid_from</code> column to create in the target table. Default: <code>valid_from</code> string valid_to_name The name of the <code>valid_to</code> column to create in the target table. Default: <code>valid_to</code> string invalidate_hard_deletes If set to <code>true</code>, when a record is missing from the source table it will be marked as invalid. Default: <code>false</code> bool batch_size The maximum number of intervals that can be evaluated in a single backfill task. If this is <code>None</code>, all intervals will be processed as part of a single task. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int <p>BigQuery Data Types</p> <p>On BigQuery, <code>valid_from</code> and <code>valid_to</code> columns default to <code>DATETIME</code>. If you want <code>TIMESTAMP</code> instead, specify it in your model definition:</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    time_data_type TIMESTAMP\n  )\n);\n</code></pre> <p>This might work on other engines too, but it's only been tested on BigQuery.</p>"},{"location":"components/model/model_kinds/#scd-type-2-by-time-configuration-options","title":"SCD Type 2 By Time Configuration Options","text":"Name Description Type updated_at_name The name of the column containing a timestamp to check for new or updated records. Default: <code>updated_at</code> string updated_at_as_valid_from By default, for new rows <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code>. This changes the behavior to set it to the valid of <code>updated_at</code> when the row is inserted. Default: <code>false</code> bool"},{"location":"components/model/model_kinds/#scd-type-2-by-column-configuration-options","title":"SCD Type 2 By Column Configuration Options","text":"Name Description Type columns The name of the columns to check for changes. <code>*</code> to represent that all columns should be checked. List of strings or string execution_time_as_valid_from By default, when the model is first loaded <code>valid_from</code> is set to <code>1970-01-01 00:00:00</code> and future new rows will have <code>execution_time</code> of when the models ran. This changes the behavior to always use <code>execution_time</code>. Default: <code>false</code> bool updated_at_name If sourcing from a table that includes as timestamp to use as valid_from, set this property to that column. See Processing Source Table with Historical Data for more info on this use case. (Default: <code>None</code>) int"},{"location":"components/model/model_kinds/#processing-source-table-with-historical-data","title":"Processing Source Table with Historical Data","text":"<p>Most of the time, you're creating history for a table that doesn't have it. Like the restaurant menu, it just shows what's available now, but you want to track what was available over time. For this use case, leave <code>batch_size</code> as <code>None</code> (the default).</p> <p>But what if your source already has history? Some systems create \"daily snapshot\" tables that contain historical records. If you're sourcing from one of these, set <code>batch_size</code> to <code>1</code> to process each interval sequentially (one day at a time if you're using <code>@daily</code> cron).</p> <p>Why sequential? SCD Type 2 needs to compare each day's snapshot to the previous day to detect changes. Processing them in order ensures the history is captured correctly.</p>"},{"location":"components/model/model_kinds/#example-source-from-daily-snapshot-table","title":"Example - Source from Daily Snapshot Table","text":"<pre><code>MODEL (\n    name db.table,\n    kind SCD_TYPE_2_BY_COLUMN (\n        unique_key id,\n        columns [some_value],\n        updated_at_name ds,\n        batch_size 1\n    ),\n    start '2025-01-01',\n    cron '@daily'\n);\nSELECT\n    id,\n    some_value,\n    ds\nFROM\n    source_table\nWHERE\n    ds between @start_ds and @end_ds\n</code></pre> <p>This processes each day sequentially, checking if <code>some_value</code> changed. When a change is detected, <code>valid_from</code> is set to match the <code>ds</code> column value (except for the very first record, which gets <code>1970-01-01 00:00:00</code>).</p> <p>If the source data was the following:</p> id some_value ds 1 1 2025-01-01 1 2 2025-01-02 1 3 2025-01-03 1 3 2025-01-04 <p>Then the resulting SCD Type 2 table would be:</p> id some_value ds valid_from valid_to 1 1 2025-01-01 1970-01-01 00:00:00 2025-01-02 00:00:00 1 2 2025-01-02 2025-01-02 00:00:00 2025-01-03 00:00:00 1 3 2025-01-03 2025-01-03 00:00:00 NULL"},{"location":"components/model/model_kinds/#querying-scd-type-2-models","title":"Querying SCD Type 2 Models","text":"<p>Even though SCD Type 2 models track history, querying the current version is still simple. Here are some common patterns:</p>"},{"location":"components/model/model_kinds/#querying-the-current-version","title":"Querying the Current Version","text":"<p>Want just the latest version of each record? Filter for <code>valid_to IS NULL</code>:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  valid_to IS NULL;\n</code></pre> <p>You can also create a view that adds an <code>is_current</code> flag to make it even easier for downstream consumers:</p> <pre><code>SELECT\n  *,\n  valid_to IS NULL AS is_current\nFROM\n  menu_items;\n</code></pre>"},{"location":"components/model/model_kinds/#querying-for-a-specific-point-in-time","title":"Querying for a Specific Point in Time","text":"<p>Want to see what the menu looked like on a specific date? Filter by <code>valid_from</code> and <code>valid_to</code>:</p> <pre><code>SELECT\n  *\nFROM\n  menu_items\nWHERE\n  id = 1\n  AND '2020-01-02 01:00:00' &gt;= valid_from\n  AND '2020-01-02 01:00:00' &lt; COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>Here's how you'd use it in a join (to get the menu item price that was valid when an order was placed):</p> <pre><code>SELECT\n  *\nFROM\n  orders\nINNER JOIN\n  menu_items\n  ON orders.menu_item_id = menu_items.id\n  AND orders.created_at &gt;= menu_items.valid_from\n  AND orders.created_at &lt; COALESCE(menu_items.valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));\n</code></pre> <p>You can create a view that handles the <code>COALESCE</code> automatically, making point-in-time queries even easier:</p> <pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP)) AS valid_to\n  valid_to IS NULL AS is_current,\nFROM\n  menu_items;\n</code></pre> <p>Want to make <code>valid_to</code> inclusive so users can use <code>BETWEEN</code>? Adjust it like this: </p><pre><code>SELECT\n  id,\n  name,\n  price,\n  updated_at,\n  valid_from,\n  COALESCE(valid_to, CAST('2200-01-01 00:00:00+00:00' AS TIMESTAMP)) - INTERVAL 1 SECOND AS valid_to\n  valid_to IS NULL AS is_current,\n</code></pre><p></p> <p>Timestamp Precision</p> <p>This example uses second precision, so we subtract 1 second. Adjust the subtraction based on your timestamp precision (milliseconds, microseconds, etc.).</p>"},{"location":"components/model/model_kinds/#querying-for-deleted-records","title":"Querying for Deleted Records","text":"<p>To find records that were deleted, query for IDs that don't have a current version (<code>valid_to IS NULL</code>). Here's how:</p> <pre><code>SELECT\n  id,\n  MAX(CASE WHEN valid_to IS NULL THEN 0 ELSE 1 END) AS is_deleted\nFROM\n  menu_items\nGROUP BY\n  id\n</code></pre>"},{"location":"components/model/model_kinds/#reset-scd-type-2-model-clearing-history","title":"Reset SCD Type 2 Model (Clearing History)","text":"<p>By default, SCD Type 2 models protect your history, once it's gone, you can't recreate it. But sometimes you need to start fresh (maybe you're fixing a bug, or the history got corrupted).</p> <p>Warning: This will delete all historical data. Make sure you really want to do this!</p> <p>To reset history, follow these steps:</p> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n    disable_restatement false\n  )\n);\n</code></pre> <p>Plan/apply this change to production. Then you will want to restate the model.</p> <p>Data Loss Warning</p> <p>This will permanently remove all historical data. In most cases, you cannot recover it. Make absolutely sure this is what you want!</p> <ol> <li>Once complete, remove <code>disable_restatement</code> from your model definition (sets it back to <code>true</code>) to prevent accidental data loss in the future</li> </ol> <pre><code>MODEL (\n  name db.menu_items,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key id,\n  )\n);\n</code></pre> <ol> <li>Plan and apply this change to production</li> </ol>"},{"location":"components/model/model_kinds/#external","title":"EXTERNAL","text":"<p>The EXTERNAL model kind is used to specify external models that store metadata about external tables. External models are special; they are not specified in .sql files like the other model kinds. They are optional but useful for propagating column and type information for external tables queried in your Vulcan project.</p>"},{"location":"components/model/model_kinds/#managed","title":"MANAGED","text":"<p>Warning</p> <p>Managed models are still under development and the API / semantics may change as support for more engines is added</p> <p>Note: Python models do not support the <code>MANAGED</code> model kind - use a SQL model instead.</p> <p>The <code>MANAGED</code> model kind is used to create models where the underlying database engine manages the data lifecycle.</p> <p>These models don't get updated with new intervals or refreshed when <code>vulcan run</code> is called. Responsibility for keeping the data up to date falls on the engine.</p> <p>You can control how the engine creates the managed model by using the <code>physical_properties</code> to pass engine-specific parameters for adapter to use when issuing commands to the underlying database.</p> <p>Due to there being no standard, each vendor has a different implementation with different semantics and different configuration parameters. Therefore, <code>MANAGED</code> models are not as portable between database engines as other Vulcan model types. In addition, due to their black-box nature, Vulcan has limited visibility into the integrity and state of the model.</p> <p>We would recommend using standard Vulcan model types in the first instance. However, if you do need to use Managed models, you still gain other Vulcan benefits like the ability to use them in virtual environments.</p> <p>See Managed Models for more information on which engines are supported and which properties are available.</p>"},{"location":"components/model/model_kinds/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Models of the <code>INCREMENTAL_BY_PARTITION</code> kind are computed incrementally based on partition. A set of columns defines the model's partitioning key, and a partition is the group of rows with the same partitioning key value.</p> <p>Should you use this model kind?</p> <p>Any model kind can use a partitioned table by specifying the <code>partitioned_by</code> key in the <code>MODEL</code> DDL.</p> <p>The \"partition\" in <code>INCREMENTAL_BY_PARTITION</code> is about how the data is loaded when the model runs.</p> <p><code>INCREMENTAL_BY_PARTITION</code> models are inherently non-idempotent, so restatements and other actions can cause data loss. This makes them more complex to manage than other model kinds.</p> <p>In most scenarios, an <code>INCREMENTAL_BY_TIME_RANGE</code> model can meet your needs and will be easier to manage. The <code>INCREMENTAL_BY_PARTITION</code> model kind should only be used when the data must be loaded by partition (usually for performance reasons).</p> <p>This model kind is designed for the scenario where data rows should be loaded and updated as a group based on their shared value for the partitioning key.</p> <p>It may be used with any SQL engine. Vulcan will automatically create partitioned tables on engines that support explicit table partitioning (e.g., BigQuery, Databricks).</p> <p>New rows are loaded based on their partitioning key value:</p> <ul> <li> <p>If a partitioning key in newly loaded data is not present in the model table, the new partitioning key and its data rows are inserted.</p> </li> <li> <p>If a partitioning key in newly loaded data is already present in the model table, all the partitioning key's existing data rows in the model table are replaced with the partitioning key's data rows in the newly loaded data.</p> </li> <li> <p>If a partitioning key is present in the model table but not present in the newly loaded data, the partitioning key's existing data rows are not modified and remain in the model table.</p> </li> </ul> <p>This kind should only be used for datasets that have the following traits:</p> <ul> <li>The dataset's records can be grouped by a partitioning key.</li> <li>Each record has a partitioning key associated with it.</li> <li>It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their partitioning keys match.</li> <li>All existing records associated with a given partitioning key can be removed or overwritten when any new record has the partitioning key value.</li> </ul> <p>The column defining the partitioning key is specified in the model's <code>MODEL</code> DDL <code>partitioned_by</code> key. This example shows the <code>MODEL</code> DDL for an <code>INCREMENTAL_BY_PARTITION</code> model:</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.partition,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by ARRAY[warehouse_id, category],\n  grains (partitioned_analysis_key)\n);\n\nSELECT\n  w.warehouse_id,\n  w.name AS warehouse_name,\n  p.category,\n  o.order_date,\n  CONCAT(w.warehouse_id::TEXT, '_', p.category, '_', o.order_date::TEXT) AS partitioned_analysis_key,\n  COUNT(DISTINCT o.order_id) AS total_transactions,\n  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,\n  COUNT(DISTINCT o.customer_id) AS unique_customers\nFROM vulcan_demo.orders AS o\nJOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nJOIN vulcan_demo.products AS p ON oi.product_id = p.product_id\nJOIN vulcan_demo.warehouses AS w ON o.warehouse_id = w.warehouse_id\nGROUP BY w.warehouse_id, w.name, p.category, o.order_date\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"vulcan_demo.partition_py\",\n    columns={\n        \"warehouse_id\": \"int\",\n        \"order_date\": \"date\",\n        \"daily_revenue\": \"decimal(10,2)\",\n    },\n    partitioned_by=[\"warehouse_id\"],\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_PARTITION,\n    ),\n    grains=[\"warehouse_id\", \"order_date\"],\n    depends_on=[\"vulcan_demo.orders\", \"vulcan_demo.order_items\"],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT o.warehouse_id, o.order_date,\n           SUM(oi.quantity * oi.unit_price) as daily_revenue\n    FROM vulcan_demo.orders o\n    JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id\n    GROUP BY o.warehouse_id, o.order_date\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>You can use multiple columns for composite partition keys:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by (warehouse_id, category)\n);\n</code></pre> <p>Some engines support expression-based partitioning. Here's a BigQuery example that partitions by month:</p> <pre><code>MODEL (\n  name vulcan_demo.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by DATETIME_TRUNC(order_date, MONTH)\n);\n</code></pre> <p>Only Full Restatements Supported</p> <p>Partial data restatements are used to reprocess part of a table's data (usually a limited time range).</p> <p>Partial data restatement is not supported for <code>INCREMENTAL_BY_PARTITION</code> models. If you restate an <code>INCREMENTAL_BY_PARTITION</code> model, its entire table will be recreated from scratch.</p> <p>Restating <code>INCREMENTAL_BY_PARTITION</code> models may lead to data loss and should be performed with care.</p>"},{"location":"components/model/model_kinds/#example","title":"Example","text":"<p>Here's a practical example that shows how to limit which partitions get updated using a CTE. This is a common pattern to avoid full restatements:</p> <pre><code>MODEL (\n  name demo.incremental_by_partition_demo,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by user_segment,\n);\n\n-- This is the source of truth for what partitions need to be updated and will join to the product usage data\n\n-- This could be an INCREMENTAL_BY_TIME_RANGE model that reads in the user_segment values last updated in the past 30 days to reduce scope\n\n-- Use this strategy to reduce full restatements\nWITH partitions_to_update AS (\n  SELECT DISTINCT\n    user_segment\n  FROM demo.incremental_by_time_range_demo  -- upstream table tracking which user segments to update\n  WHERE last_updated_at BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt\n),\n\nproduct_usage AS (\n  SELECT\n    product_id,\n    customer_id,\n    last_usage_date,\n    usage_count,\n    feature_utilization_score,\n    user_segment\n  FROM vulcan-public-demo.tcloud_raw_data.product_usage\n  WHERE user_segment IN (SELECT user_segment FROM partitions_to_update) -- partition filter applied here\n)\n\nSELECT\n  product_id,\n  customer_id,\n  last_usage_date,\n  usage_count,\n  feature_utilization_score,\n  user_segment,\n  CASE\n    WHEN usage_count &gt; 100 AND feature_utilization_score &gt; 0.7 THEN 'Power User'\n    WHEN usage_count &gt; 50 THEN 'Regular User'\n    WHEN usage_count IS NULL THEN 'New User'\n    ELSE 'Light User'\n  END as user_type\nFROM product_usage\n</code></pre> <p>Note: Partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss.</p>"},{"location":"components/model/model_kinds/#materialization-strategy_3","title":"Materialization strategy","text":"<p>Depending on the target engine, models of the <code>INCREMENTAL_BY_PARTITION</code> kind are materialized using the following strategies:</p> Engine Strategy Databricks REPLACE WHERE by partitioning key Spark INSERT OVERWRITE by partitioning key Snowflake DELETE by partitioning key, then INSERT BigQuery DELETE by partitioning key, then INSERT Redshift DELETE by partitioning key, then INSERT Postgres DELETE by partitioning key, then INSERT DuckDB DELETE by partitioning key, then INSERT"},{"location":"components/model/model_kinds/#incremental_unmanaged","title":"INCREMENTAL_UNMANAGED","text":"<p><code>INCREMENTAL_UNMANAGED</code> models are for append-only tables. They're \"unmanaged\" because Vulcan doesn't try to deduplicate or manage the data, it just runs your query and appends whatever it gets to the table.</p> <p>How it works: Every time the model runs, Vulcan executes your query and appends the results to the table. No deduplication, no updates, no deletes, just append, append, append.</p> <p>Should You Use This?</p> <p>Use it for: Data Vault patterns, event logs, audit trails, or any scenario where you need true append-only behavior.</p> <p>Don't use it for: Most other cases. <code>INCREMENTAL_BY_TIME_RANGE</code> or <code>INCREMENTAL_BY_UNIQUE_KEY</code> give you much more control and are usually better choices.</p> <p>When to use:</p> <ul> <li> <p>Data Vault hubs, links, or satellites</p> </li> <li> <p>Event logs where every event should be preserved</p> </li> <li> <p>Audit trails</p> </li> <li> <p>Any pattern that requires true append-only semantics</p> </li> </ul> <p>Here's how you'd set one up:</p> <pre><code>MODEL (\n  name vulcan_demo.incremental_unmanaged,\n  kind INCREMENTAL_UNMANAGED,\n  cron '@daily',\n  start '2025-01-01',\n  grains (shipment_id)\n);\n\n/* Append-only shipment event log */\nSELECT\n  s.shipment_id,\n  s.order_id,\n  s.shipped_date,\n  s.carrier,\n  o.customer_id,\n  c.name AS customer_name,\n  o.order_date,\n  (s.shipped_date - o.order_date::DATE)::INT AS days_to_ship,\n  CURRENT_TIMESTAMP AS shipment_event_timestamp\nFROM vulcan_demo.shipments AS s\nJOIN vulcan_demo.orders AS o ON s.order_id = o.order_id\nJOIN vulcan_demo.customers AS c ON o.customer_id = c.customer_id\nORDER BY s.shipped_date DESC\n</code></pre> <p>Note: Since it's unmanaged, <code>INCREMENTAL_UNMANAGED</code> doesn't support <code>batch_size</code> or <code>batch_concurrency</code> properties. Vulcan just runs your query and appends the results, no batching or concurrency control.</p> <p>Only Full Restatements Supported</p> <p>Similar to <code>INCREMENTAL_BY_PARTITION</code>, attempting to restate an <code>INCREMENTAL_UNMANAGED</code> model will trigger a full restatement. That is, the model will be rebuilt from scratch rather than from a time slice you specify.</p> <p>Be very careful when restating these models!</p>"},{"location":"components/model/overview/","title":"Overview","text":""},{"location":"components/model/overview/#overview","title":"Overview","text":"<p>Models transform raw data into tables and views. Define what you want (the metadata) and how to make it (the SQL query), and Vulcan handles the rest.</p> <p>Models live in <code>.sql</code> and <code>.py</code> files in the <code>models/</code> directory of your project. Vulcan automatically figures out how your models relate to each other by parsing your SQL, so you don't have to manually configure dependencies. Write your SQL, and Vulcan handles the lineage.</p> <p>Every model has two parts:</p> <ul> <li> <p>DDL (Data Definition Language) - The <code>MODEL</code> block that tells Vulcan what this model is (name, schedule, how to materialize it, etc.)</p> </li> <li> <p>DML (Data Manipulation Language) - The <code>SELECT</code> query that does the actual transformation work</p> </li> </ul> <p>The DDL defines the model metadata. The DML contains the transformation logic.</p>"},{"location":"components/model/overview/#model-structure","title":"Model Structure","text":"<p>You can write models in SQL or Python. Both work the same way conceptually; they just look different. Let's see both:</p> SQL ModelPython Model <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics', 'analytics.sales_summary'),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales transactions',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>Breaking it down:</p> <ul> <li> <p>Lines 1-21: The DDL (<code>MODEL</code> block) - tells Vulcan this is a daily sales model with metadata, tags, and column documentation</p> </li> <li> <p>Lines 23-31: The DML (<code>SELECT</code> query) - the actual transformation that aggregates orders by date</p> </li> </ul> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n  \"sales.daily_sales_py\",\n  columns={\n    \"order_date\": \"timestamp\",\n    \"total_orders\": \"int\",\n    \"total_revenue\": \"decimal(18,2)\",\n    \"last_order_id\": \"string\",\n  },\n  kind=dict(name=ModelKindName.FULL),\n  grains=[\"order_date\"],\n  depends_on=[\"raw.raw_orders\"],\n  cron='@daily',\n  tags=[\"silver\", \"sales\", \"aggregation\"],\n  terms=[\"sales.daily_metrics\", \"analytics.sales_summary\"],\n  description=\"Daily sales summary with order counts and revenue\",\n  column_descriptions={\n    \"order_date\": \"Date of the sales transactions\",\n    \"total_orders\": \"Total number of orders for the day\",\n    \"total_revenue\": \"Total revenue for the day\",\n    \"last_order_id\": \"Last order ID processed for the day\",\n  },\n  column_tags={\n    \"order_date\": [\"dimension\", \"grain\", \"date\"],\n    \"total_orders\": [\"measure\", \"count\"],\n    \"total_revenue\": [\"measure\", \"financial\"],\n    \"last_order_id\": [\"dimension\", \"identifier\"],\n  },\n)\ndef execute(\n  context: ExecutionContext,\n  start: datetime,\n  end: datetime,\n  execution_time: datetime,\n  **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n  query = \"\"\"\n  SELECT\n    CAST(order_date AS TIMESTAMP) AS order_date,\n    COUNT(order_id)::INTEGER AS total_orders,\n    SUM(total_amount)::NUMERIC(18,2) AS total_revenue,\n    MAX(order_id)::VARCHAR AS last_order_id\n  FROM raw.raw_orders\n  GROUP BY order_date\n  ORDER BY order_date\n  \"\"\"\n\n  return context.fetchdf(query)\n</code></pre> <p>Breaking it down:</p> <ul> <li> <p>Lines 7-34: The DDL (<code>@model</code> decorator) - same metadata as SQL with tags, terms, and column documentation</p> </li> <li> <p>Lines 35-54: The DML (function body) - runs the SQL and returns a DataFrame</p> </li> </ul> <p>Both formats do the same thing. Choose the one you prefer.</p>"},{"location":"components/model/overview/#ddl-the-model-block","title":"DDL: The MODEL Block","text":"<p>The <code>MODEL</code> block is where you tell Vulcan about your model. It's the first thing in your file (after any comments) and uses a simple, declarative syntax.</p>"},{"location":"components/model/overview/#basic-syntax","title":"Basic Syntax","text":"<p>Here's what a <code>MODEL</code> block looks like:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales'),\n  terms ('sales.daily_metrics'),\n  description 'Daily sales summary'\n);\n</code></pre> <p>This tells Vulcan:</p> <ul> <li> <p><code>name</code> - What to call this model (schema.table format)</p> </li> <li> <p><code>kind</code> - How to materialize it (FULL rebuilds everything, INCREMENTAL only updates changes, etc.)</p> </li> <li> <p><code>cron</code> - When to run it (<code>@daily</code> means every day)</p> </li> <li> <p><code>grains</code> - What makes each row unique (uses tuple syntax with parentheses)</p> </li> <li> <p><code>tags</code> - Labels for categorization (uses tuple syntax)</p> </li> <li> <p><code>terms</code> - Business glossary terms using dot notation</p> </li> <li> <p><code>description</code> - Human-readable description of the model</p> </li> </ul>"},{"location":"components/model/overview/#common-properties","title":"Common Properties","text":"<p>Here are the properties you'll use most often:</p> Property What It Does Example <code>name</code> Fully qualified model name (schema.table) <code>sales.daily_sales</code> <code>kind</code> Materialization strategy <code>FULL</code>, <code>INCREMENTAL</code>, <code>VIEW</code> <code>cron</code> When to run (scheduling) <code>'@daily'</code>, <code>'0 0 * * *'</code> <code>grains</code> Column(s) that make rows unique <code>(order_date)</code> or <code>(customer_id, order_date)</code> <code>owner</code> Who owns this model (for governance) <code>analytics_team</code> <code>description</code> Human-readable description <code>'Daily sales aggregates'</code> <code>tags</code> Labels for organizing models <code>('gold', 'analytics', 'customer')</code> <code>terms</code> Business glossary terms <code>('customer.rfm_analysis')</code> <p>More DDL Properties</p> <p>There are more properties available beyond these common ones, including <code>column_descriptions</code>, <code>column_tags</code>, and <code>column_terms</code> for column-level metadata. Check out the Model Properties reference for the complete list of all available model properties and their configurations.</p>"},{"location":"components/model/overview/#dml-the-select-query","title":"DML: The SELECT Query","text":"<p>The <code>SELECT</code> query is where the magic happens. This is your transformation logic, the SQL that actually does the work.</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>This query:</p> <ul> <li> <p>Reads from <code>raw.raw_orders</code></p> </li> <li> <p>Groups by <code>order_date</code></p> </li> <li> <p>Counts orders, sums revenue, finds the latest order ID</p> </li> <li> <p>Returns the results ordered by date</p> </li> </ul> <p>Pretty standard SQL! Vulcan will automatically figure out that this model depends on <code>raw.raw_orders</code> and build the dependency graph for you.</p>"},{"location":"components/model/overview/#conventions","title":"Conventions","text":"<p>Vulcan tries to be smart and infer as much as possible from your SQL. This means you don't have to write a bunch of YAML config files, just write SQL and Vulcan figures it out. But to do this, your SQL needs to follow some conventions.</p>"},{"location":"components/model/overview/#sql-model-conventions","title":"SQL Model Conventions","text":""},{"location":"components/model/overview/#unique-column-names","title":"Unique Column Names","text":"<p>Your final <code>SELECT</code> needs unique column names. No duplicates allowed!</p> <pre><code>-- Good: Each column has a unique name\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>If you have duplicate column names, Vulcan won't know which one you mean, and that causes problems.</p>"},{"location":"components/model/overview/#explicit-types","title":"Explicit Types","text":"<p>Cast your types explicitly. This prevents surprises and ensures your schema is consistent:</p> <pre><code>-- Explicit casting ensures consistent schema\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,  -- explicit timestamp\n  COUNT(order_id)::INTEGER AS total_orders,                 -- explicit integer\n  SUM(total_amount)::FLOAT AS total_revenue,                -- explicit float\n  MAX(order_id)::VARCHAR AS last_order_id                   -- explicit varchar\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan uses PostgreSQL-style casting (<code>x::int</code>), but don't worry, it automatically converts this to whatever your execution engine needs. So you write <code>::INTEGER</code> and Vulcan handles the rest.</p> <p>Why this matters: Without explicit types, you might get <code>FLOAT</code> when you expected <code>INTEGER</code>, or <code>VARCHAR</code> when you wanted <code>TIMESTAMP</code>. Explicit casting prevents these surprises.</p>"},{"location":"components/model/overview/#inferrable-names","title":"Inferrable Names","text":"<p>Your columns need names that Vulcan can figure out. If Vulcan can't infer a name, you need to add an alias:</p> <pre><code>SELECT\n  1,                              -- not inferrable (what do you call this?)\n  total_amount + 1,               -- not inferrable (needs an alias)\n  SUM(total_amount),              -- not inferrable (needs an alias)\n  order_date,                     -- inferrable as order_date\n  order_date::TIMESTAMP,          -- inferrable as order_date\n  total_amount + 1 AS adjusted,   -- explicitly named\n  SUM(total_amount) AS revenue    -- explicitly named\n</code></pre> <p>If you forget an alias, Vulcan's formatter will add one automatically when it renders your SQL. But it's better to be explicit, you'll know what the column is called!</p>"},{"location":"components/model/overview/#column-metadata","title":"Column Metadata","text":"<p>Document and categorize your columns using column-level metadata properties. There are several options:</p> <p>Column Descriptions (Recommended)</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  description 'Aggregated daily sales metrics',\n  column_descriptions (\n    order_date = 'The date of the sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts for this date',\n    last_order_id = 'The most recent order ID for this date'\n  )\n);\n</code></pre> <p>This keeps all your documentation in one place, in the MODEL block.</p> <p>Column Tags and Terms</p> <p>Beyond descriptions, you can also add tags and business glossary terms to columns:</p> <pre><code>MODEL (\n  name gold_v1.rfm_customer_segmentation,\n  kind FULL,\n  cron '@daily',\n  grains (customer_id),\n  description 'RFM customer segmentation model',\n  column_tags (\n    customer_id = ('primary_key', 'identifier', 'grain'),\n    customer_name = ('dimension', 'label', 'pii'),\n    email = ('dimension', 'pii', 'contact'),\n    rfm_score = ('measure', 'score', 'composite')\n  ),\n  column_terms (\n    customer_id = ('customer.customer_id', 'identity.customer_id'),\n    rfm_score = ('analytics.rfm_score', 'segmentation.rfm_composite')\n  )\n);\n</code></pre> <ul> <li><code>column_tags</code> - Categorize columns by role (<code>dimension</code>, <code>measure</code>), sensitivity (<code>pii</code>), or purpose</li> <li><code>column_terms</code> - Link columns to business glossary terms for semantic understanding</li> </ul> <p>See Model Properties for detailed documentation on all column-level metadata options.</p> <p>Priority</p> <p>If you use <code>column_descriptions</code> in the DDL, Vulcan will use those and ignore any inline comments in your query. DDL descriptions take priority, so if you define descriptions in both places, the DDL version wins.</p> <p>Option 2: Inline Comments</p> <p>If you don't specify <code>column_descriptions</code> in the DDL, Vulcan will automatically pick up comments from your query:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date)\n);\n\nSELECT\n  order_date::TIMESTAMP AS order_date,           -- The date of sales transactions\n  COUNT(order_id)::INTEGER AS total_orders,      -- Number of orders placed\n  SUM(total_amount)::FLOAT AS total_revenue,     -- Total revenue for the day\n  MAX(order_id)::VARCHAR AS last_order_id        -- Most recent order ID\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan registers these comments as column descriptions in your database.</p> <p>Table comments: If you put a comment before the <code>MODEL</code> block, Vulcan will use it as the table description. But if you also specify <code>description</code> in the MODEL block, that takes priority.</p>"},{"location":"components/model/overview/#python-model-conventions","title":"Python Model Conventions","text":"<p>Python models work a bit differently because Python doesn't have the same type inference capabilities as SQL.</p>"},{"location":"components/model/overview/#explicit-column-definitions","title":"Explicit Column Definitions","text":"<p>You must define your columns explicitly in the <code>@model</code> decorator:</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(name=ModelKindName.FULL),\n)\n</code></pre> <p>Vulcan can't infer types from Python code the way it can from SQL, so you need to tell it explicitly.</p>"},{"location":"components/model/overview/#explicit-dependencies","title":"Explicit Dependencies","text":"<p>Unlike SQL models (where Vulcan figures out dependencies automatically), Python models need you to list them:</p> <pre><code>@model(\n    \"sales.daily_sales_py\",\n    columns={...},\n    depends_on=[\"raw.raw_orders\"],  # Must explicitly list upstream models\n)\n</code></pre> <p>This is because Vulcan can't parse your Python code to find <code>FROM</code> clauses and joins. You need to tell it what this model depends on.</p>"},{"location":"components/model/overview/#column-metadata_1","title":"Column Metadata","text":"<p>Python models can't use inline comments for column descriptions. Instead, specify them in the decorator using <code>column_descriptions</code>, <code>column_tags</code>, and <code>column_terms</code>:</p> <pre><code>@model(\n    \"gold_v1.rfm_customer_segmentation\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"rfm_score\": \"int\",\n    },\n    column_descriptions={\n        \"customer_id\": \"Unique identifier for each customer\",\n        \"customer_name\": \"Customer full name\",\n        \"rfm_score\": \"Combined RFM score (111-555)\",\n    },\n    column_tags={\n        \"customer_id\": [\"primary_key\", \"identifier\", \"grain\"],\n        \"customer_name\": [\"dimension\", \"label\", \"pii\"],\n        \"rfm_score\": [\"measure\", \"score\", \"composite\"],\n    },\n    column_terms={\n        \"customer_id\": [\"customer.customer_id\", \"identity.customer_id\"],\n        \"rfm_score\": [\"analytics.rfm_score\", \"segmentation.rfm_composite\"],\n    },\n)\n</code></pre> <p>Column name validation</p> <p>Vulcan will error if you put a column name in <code>column_descriptions</code>, <code>column_tags</code>, or <code>column_terms</code> that doesn't exist in <code>columns</code>. This prevents typos and keeps things consistent, if you describe a column, it better exist!</p>"},{"location":"components/model/overview/#return-type","title":"Return Type","text":"<p>Your <code>execute</code> function must return a pandas DataFrame, and the columns must match what you defined in <code>columns</code>:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:  # Must return a DataFrame\n    query = \"SELECT ...\"\n    return context.fetchdf(query)\n</code></pre> <p>The DataFrame columns need to match your <code>columns</code> definition exactly, same names, compatible types.</p> <p>Learn more</p> <p>See Python Models for detailed information, advanced patterns, and more examples.</p>"},{"location":"components/model/overview/#comment-registration","title":"Comment Registration","text":"<p>Vulcan registers comments (descriptions) in your database so they show up in your BI tools and data catalogs.</p>"},{"location":"components/model/overview/#how-comments-get-registered","title":"How Comments Get Registered","text":"<p>Model-level comments:</p> <ul> <li> <p>If you put a comment before the <code>MODEL</code> block, Vulcan uses it as the table comment</p> </li> <li> <p>If you also specify <code>description</code> in the MODEL block, that takes priority</p> </li> </ul> <p>Column-level comments:</p> <ul> <li> <p>Use <code>column_descriptions</code> in the DDL (recommended)</p> </li> <li> <p>Or use inline comments in your SELECT query (if <code>column_descriptions</code> isn't specified)</p> </li> </ul>"},{"location":"components/model/overview/#what-gets-registered","title":"What Gets Registered","text":"<p>Not everything gets comments registered:</p> <ul> <li> <p>Physical tables - Comments are registered (tables in the <code>vulcan__[project schema]</code> schema)</p> </li> <li> <p>Production views - Comments are registered</p> </li> <li> <p>Temporary tables - No comments (they're temporary)</p> </li> <li> <p>Non-production views - No comments (keeps things clean)</p> </li> </ul> <p>Note: Some engines automatically pass comments from physical tables to views that select from them. So even if Vulcan didn't explicitly register a comment on a view, it might still show up if the engine does this automatically.</p>"},{"location":"components/model/overview/#engine-support","title":"Engine Support","text":"<p>Different databases support comments differently. Some can register comments in the <code>CREATE</code> statement (one command), others need separate commands for each comment.</p> <p>Here's what each engine supports:</p> Engine <code>TABLE</code> comments <code>VIEW</code> comments Postgres Y Y Snowflake Y Y Spark Y Y <p>If your engine doesn't support comments, Vulcan will skip registration (no errors, it just won't register them).</p>"},{"location":"components/model/overview/#macros","title":"Macros","text":"<p>Macros are like variables for your SQL. They let you parameterize queries and avoid repetition. Vulcan provides several built-in macros (like <code>@start_ds</code> and <code>@end_ds</code> for incremental models), and you can define your own.</p> <p>Macros use the <code>@</code> prefix. For example, <code>@this_model</code> refers to the current model being processed, and <code>@start_ds</code> is the start date for incremental processing.</p> <p>See the macros documentation for details.</p>"},{"location":"components/model/properties/","title":"Properties","text":""},{"location":"components/model/properties/#properties","title":"Properties","text":"<p>The <code>MODEL</code> DDL statement has properties you can use to control how your model behaves. Configure scheduling, storage, validation, and more.</p> <p>This page is a complete reference for all available properties. It covers what each one does, when to use it, and shows examples.</p>"},{"location":"components/model/properties/#quick-reference","title":"Quick Reference","text":"Property Description Type Required <code>name</code> Fully qualified model name (<code>schema.model</code> or <code>catalog.schema.model</code>) <code>str</code> N* <code>project</code> Project name for multi-repo deployments <code>str</code> N <code>kind</code> Model kind (VIEW, FULL, INCREMENTAL, etc.) <code>str</code> | <code>dict</code> N <code>cron</code> Schedule expression for model refresh <code>str</code> N <code>cron_tz</code> Timezone for the cron schedule <code>str</code> N <code>interval_unit</code> Temporal granularity of data intervals <code>str</code> N <code>start</code> Earliest date/time to process <code>str</code> | <code>int</code> N <code>end</code> Latest date/time to process <code>str</code> | <code>int</code> N <code>grain</code> Column(s) defining row uniqueness <code>str</code> | <code>tuple</code> N <code>grains</code> Multiple unique key definitions <code>tuple</code> N <code>owner</code> Model owner for governance <code>str</code> N <code>description</code> Model description (registered as table comment) <code>str</code> N <code>tags</code> Labels for organizing and categorizing models <code>tuple[str]</code> N <code>terms</code> Business glossary terms for semantic linking <code>tuple[str]</code> N <code>column_descriptions</code> Column-level comments <code>dict</code> N <code>column_tags</code> Column-level tags for categorization <code>dict</code> N <code>column_terms</code> Column-level business glossary terms <code>dict</code> N <code>columns</code> Explicit column names and types <code>array</code> N <code>dialect</code> SQL dialect of the model <code>str</code> N <code>assertions</code> Audits to run after model evaluation <code>array</code> N <code>profiles</code> Columns to track statistical metrics over time <code>array</code> N <code>depends_on</code> Explicit model dependencies <code>array[str]</code> N <code>references</code> Non-unique join relationship columns <code>array</code> N <code>partitioned_by</code> Partition key column(s) <code>str</code> | <code>array</code> N <code>clustered_by</code> Clustering column(s) <code>str</code> N <code>table_format</code> Table format (iceberg, hive, delta) <code>str</code> N <code>storage_format</code> Storage format (parquet, orc) <code>str</code> N <code>physical_properties</code> Engine-specific table/view properties <code>dict</code> N <code>virtual_properties</code> Engine-specific view layer properties <code>dict</code> N <code>session_properties</code> Engine session properties <code>dict</code> N <code>stamp</code> Arbitrary version string <code>str</code> N <code>enabled</code> Whether model is enabled <code>bool</code> N <code>allow_partials</code> Allow partial data intervals <code>bool</code> N <code>gateway</code> Specific gateway for execution <code>str</code> N <code>optimize_query</code> Enable query optimization <code>bool</code> N <code>formatting</code> Enable model formatting <code>bool</code> N <code>ignored_rules</code> Linter rules to ignore <code>str</code> | <code>array</code> N <p>Note: Required unless name inference is enabled.</p>"},{"location":"components/model/properties/#general-properties","title":"General Properties","text":""},{"location":"components/model/properties/#name","title":"name","text":"<p>Your model's name is how it's identified in the data warehouse. It needs at least a schema (<code>schema.model</code>), and you can optionally include a catalog (<code>catalog.schema.model</code>).</p> <p>Format: <code>schema.model</code> or <code>catalog.schema.model</code></p> <p>This becomes the production table/view name that other models and users will reference.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,        -- schema.model format\n);\n\n-- Or with catalog\nMODEL (\n  name catalog.sales.daily_sales -- catalog.schema.model format\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",  # schema.model format\n)\ndef execute(context, **kwargs):\n    ...\n\n# Or with catalog\n@model(\n    \"catalog.sales.daily_sales\",  # catalog.schema.model format\n)\n</code></pre> <p>Environment Prefixing</p> <p>In non-production environments, Vulcan automatically prefixes your model names. So <code>sales.daily_sales</code> becomes <code>sales__dev.daily_sales</code> in the dev environment. This keeps your dev and prod data separate without you having to think about it.</p>"},{"location":"components/model/properties/#project","title":"project","text":"<p>If you're running multiple Vulcan projects in the same repository (multi-repo setup), use <code>project</code> to specify which project this model belongs to. This helps Vulcan organize and isolate models from different projects.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  project 'analytics_project',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    project=\"analytics_project\",\n)\n</code></pre>"},{"location":"components/model/properties/#kind","title":"kind","text":"<p>The <code>kind</code> property determines how your model is computed and stored. Do you want to rebuild everything each run? Update incrementally? Create a view? This is where you decide.</p> <p>For all the details on each kind and when to use them, check out the Model Kinds documentation.</p> SQLPython <pre><code>-- VIEW (default for SQL)\nMODEL (\n  name sales.daily_sales,\n  kind VIEW,\n);\n\n-- FULL\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n);\n\n-- Incremental with properties\nMODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n  ),\n);\n\n-- SEED\nMODEL (\n  name raw.holidays,\n  kind SEED (\n    path 'seeds/holidays.csv',\n  ),\n);\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n# FULL (default for Python)\n@model(\n    \"sales.daily_sales\",\n    kind=dict(name=ModelKindName.FULL),\n)\n\n# Incremental\n@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n    ),\n)\n\n# SCD Type 2\n@model(\n    \"dim.customers\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#cron","title":"cron","text":"<p>Controls when your model runs. You can use standard cron expressions or Vulcan's shortcuts for common schedules.</p> <p>Why this matters: Without a schedule, your model only runs when you manually trigger it. Set a cron, and Vulcan will automatically process new data on schedule.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',          -- Daily at midnight UTC\n);\n\nMODEL (\n  name sales.hourly_metrics,\n  cron '@hourly',         -- Every hour\n);\n\nMODEL (\n  name sales.custom_schedule,\n  cron '0 6 * * *',       -- Custom: every day at 6 AM UTC\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n)\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"@hourly\",\n)\n\n@model(\n    \"sales.custom_schedule\",\n    cron=\"0 6 * * *\",  # Every day at 6 AM UTC\n)\n</code></pre> <p>Cron shortcuts: Vulcan provides convenient shortcuts:</p> <ul> <li> <p><code>@hourly</code> - Every hour</p> </li> <li> <p><code>@daily</code> - Every day at midnight UTC</p> </li> <li> <p><code>@weekly</code> - Once per week</p> </li> <li> <p><code>@monthly</code> - Once per month</p> </li> </ul> <p>These are much easier than writing <code>0 * * * *</code>!</p>"},{"location":"components/model/properties/#cron_tz","title":"cron_tz","text":"<p>Sets the timezone for your cron schedule. This only affects when the model runs, not how time intervals are calculated (those are always UTC).</p> <p>Example: If you set <code>cron '@daily'</code> and <code>cron_tz 'America/Los_Angeles'</code>, your model runs at midnight Pacific time, but the time intervals it processes are still in UTC.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  cron '@daily',\n  cron_tz 'America/Los_Angeles',  -- Runs at midnight Pacific time\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    cron=\"@daily\",\n    cron_tz=\"America/Los_Angeles\",\n)\n</code></pre>"},{"location":"components/model/properties/#interval_unit","title":"interval_unit","text":"<p>Controls the granularity of time intervals for incremental models. By default, Vulcan figures this out from your <code>cron</code> expression, but you can override it if needed.</p> <p>Supported values: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>half_hour</code>, <code>quarter_hour</code>, <code>five_minute</code></p> <p>When to override: If your cron runs daily but you want to process hourly intervals, set <code>interval_unit 'hour'</code>. This is useful when you want finer-grained control over incremental processing.</p> SQLPython <pre><code>MODEL (\n  name sales.hourly_metrics,\n  cron '30 7 * * *',      -- Run daily at 7:30 AM\n  interval_unit 'hour',   -- Process hourly intervals (not daily)\n  );\n</code></pre> <pre><code>from vulcan import IntervalUnit\n\n@model(\n    \"sales.hourly_metrics\",\n    cron=\"30 7 * * *\",\n    interval_unit=IntervalUnit.HOUR,\n)\n</code></pre>"},{"location":"components/model/properties/#start","title":"start","text":"<p>Sets the earliest date/time your model should process. This is useful for limiting backfills or defining when your model's data begins.</p> <p>You can use:</p> <ul> <li> <p>Absolute dates: <code>'2024-01-01'</code></p> </li> <li> <p>Relative expressions: <code>'1 year ago'</code></p> </li> <li> <p>Epoch milliseconds: <code>1704067200000</code></p> </li> </ul> SQLPython <pre><code>-- Absolute date\nMODEL (\n  name sales.daily_sales,\n  start '2024-01-01',\n);\n\n-- Relative expression\nMODEL (\n  name sales.recent_sales,\n  start '1 year ago',\n);\n\n-- Epoch milliseconds\nMODEL (\n  name sales.events,\n  start 1704067200000,\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    start=\"2024-01-01\",\n)\n\n@model(\n    \"sales.recent_sales\",\n    start=\"1 year ago\",\n)\n</code></pre>"},{"location":"components/model/properties/#end","title":"end","text":"<p>Sets the latest date/time your model should process. Uses the same format as <code>start</code>. This is handy for historical models or limiting processing to a specific time range.</p> SQLPython <pre><code>MODEL (\n  name sales.historical_sales,\n  start '2020-01-01',\n  end '2023-12-31',\n  );\n</code></pre> <pre><code>@model(\n    \"sales.historical_sales\",\n    start=\"2020-01-01\",\n    end=\"2023-12-31\",\n)\n</code></pre>"},{"location":"components/model/properties/#grain-grains","title":"grain / grains","text":"<p>In Vulcan, this acts as the primary key. It tells Vulcan what identifies a single row in your table, and defines the column(s) that make each row unique.</p> <p>Why this matters: Tools like <code>table_diff</code> use grains to compare tables. It also helps Vulcan understand your data structure for better optimization and validation.</p> <p>You can specify a single grain or multiple grains using the tuple syntax with parentheses.</p> SQLPython <pre><code>-- Single column grain\nMODEL (\n  name sales.daily_sales,\n  grains (order_date),\n);\n\n-- Composite grain\nMODEL (\n  name sales.customer_daily,\n  grains (customer_id, order_date),\n);\n\n-- Multiple grains\nMODEL (\n  name sales.orders,\n  grains (\n    order_id,\n    (customer_id, order_date)\n  ),\n);\n</code></pre> <pre><code># Single grain\n@model(\n    \"sales.daily_sales\",\n    grains=[\"order_date\"],\n)\n\n# Composite grain\n@model(\n    \"sales.customer_daily\",\n    grains=[(\"customer_id\", \"order_date\")],\n)\n\n# Multiple grains\n@model(\n    \"sales.orders\",\n    grains=[\n        \"order_id\",\n        (\"customer_id\", \"order_date\"),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#owner","title":"owner","text":"<p>Sets the owner of the model, usually a team name or individual. This is used for governance, notifications, and knowing who to contact when something breaks.</p> <p>Example: <code>owner 'analytics_team'</code> or <code>owner 'data_engineers'</code></p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  owner 'analytics_team',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    owner=\"analytics_team\",\n)\n</code></pre>"},{"location":"components/model/properties/#description","title":"description","text":"<p>A human-readable description of what your model does. Vulcan automatically registers this as a table comment in your SQL engine (if it supports comments), so it shows up in your BI tools and data catalogs.</p> <p>Pro tip: Write descriptions that explain the business purpose, not just the technical details. Future you (and your teammates) will thank you!</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  description 'Aggregated daily sales metrics including total orders and revenue',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    description=\"Aggregated daily sales metrics including total orders and revenue\",\n)\n</code></pre>"},{"location":"components/model/properties/#column_descriptions","title":"column_descriptions","text":"<p>Document your columns! This property lets you add descriptions for each column, which get registered as column comments in your database.</p> <p>Why document columns? When someone queries your table in a BI tool, they'll see what each column means. It's like inline documentation that travels with your data.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  column_descriptions (\n    order_date = 'The date of sales transactions',\n    total_orders = 'Count of orders placed on this date',\n    total_revenue = 'Sum of all order amounts',\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n    },\n    column_descriptions={\n        \"order_date\": \"The date of sales transactions\",\n        \"total_orders\": \"Count of orders placed on this date\",\n        \"total_revenue\": \"Sum of all order amounts\",\n    },\n)\n</code></pre> <p>Priority</p> <p>If <code>column_descriptions</code> is present, inline column comments will not be registered.</p>"},{"location":"components/model/properties/#column_tags","title":"column_tags","text":"<p>Assign tags to individual columns for categorization, governance, and discovery. Column tags help classify columns by their role, sensitivity, or purpose.</p> <p>Common column tag categories:</p> <ul> <li>Role tags: <code>primary_key</code>, <code>identifier</code>, <code>grain</code>, <code>dimension</code>, <code>measure</code></li> <li>Sensitivity tags: <code>pii</code>, <code>confidential</code>, <code>contact</code></li> <li>Domain tags: <code>financial</code>, <code>metric</code>, <code>score</code>, <code>label</code></li> </ul> SQLPython <pre><code>MODEL (\n  name gold_v1.rfm_customer_segmentation,\n  column_tags (\n    customer_id = (\n      'primary_key',\n      'identifier',\n      'grain'\n    ),\n    customer_name = ('dimension', 'label', 'pii'),\n    email = ('dimension', 'pii', 'contact'),\n    region_name = ('dimension', 'label'),\n    monetary_value = (\n      'measure',\n      'financial',\n      'rfm_component'\n    ),\n    rfm_score = (\n      'measure',\n      'score',\n      'composite'\n    ),\n    rfm_segment = (\n      'dimension',\n      'classification',\n      'label'\n    )\n  )\n);\n</code></pre> <pre><code>@model(\n    \"gold_v1.rfm_customer_segmentation\",\n    column_tags={\n        \"customer_id\": [\"primary_key\", \"identifier\", \"grain\"],\n        \"customer_name\": [\"dimension\", \"label\", \"pii\"],\n        \"email\": [\"dimension\", \"pii\", \"contact\"],\n        \"region_name\": [\"dimension\", \"label\"],\n        \"monetary_value\": [\"measure\", \"financial\", \"rfm_component\"],\n        \"rfm_score\": [\"measure\", \"score\", \"composite\"],\n        \"rfm_segment\": [\"dimension\", \"classification\", \"label\"],\n    },\n)\n</code></pre> <p>PII Tracking</p> <p>Use the <code>pii</code> tag on columns containing personally identifiable information. This helps with data governance, compliance audits, and access control policies.</p>"},{"location":"components/model/properties/#column_terms","title":"column_terms","text":"<p>Link individual columns to business glossary terms. Column terms connect technical column names to business vocabulary, enabling better discovery and semantic understanding.</p> <p>Format: Use dot notation for hierarchical terms like <code>domain.concept</code> (e.g., <code>customer.customer_id</code>, <code>analytics.rfm_score</code>).</p> SQLPython <pre><code>MODEL (\n  name gold_v1.rfm_customer_segmentation,\n  column_terms (\n    customer_id = (\n      'customer.customer_id',\n      'identity.customer_id'\n    ),\n    rfm_score = (\n      'analytics.rfm_score',\n      'segmentation.rfm_composite'\n    ),\n    rfm_segment = (\n      'customer.segment',\n      'analytics.customer_classification'\n    ),\n    monetary_value = (\n      'customer.ltv',\n      'finance.customer_lifetime_value'\n    )\n  )\n);\n</code></pre> <pre><code>@model(\n    \"gold_v1.rfm_customer_segmentation\",\n    column_terms={\n        \"customer_id\": [\"customer.customer_id\", \"identity.customer_id\"],\n        \"rfm_score\": [\"analytics.rfm_score\", \"segmentation.rfm_composite\"],\n        \"rfm_segment\": [\"customer.segment\", \"analytics.customer_classification\"],\n        \"monetary_value\": [\"customer.ltv\", \"finance.customer_lifetime_value\"],\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#columns","title":"columns","text":"<p>Explicitly defines your model's column names and data types. When you use this, Vulcan won't try to infer types from your query, it'll use exactly what you specify.</p> <p>When to use:</p> <ul> <li> <p>Python models (required, Vulcan can't infer types from Python code)</p> </li> <li> <p>Seed models (you need to define the CSV schema)</p> </li> <li> <p>When you want strict type control</p> </li> </ul> SQLPython <pre><code>MODEL (\n  name sales.national_holidays,\n  kind SEED (path 'holidays.csv'),\n  columns (\n    holiday_name VARCHAR,\n    holiday_date DATE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n)\ndef execute(context, **kwargs) -&gt; pd.DataFrame:\n    ...\n</code></pre> <p>Python Models</p> <p>This is required for Python models since Vulcan can't infer column types from Python code. You must explicitly define your schema.</p>"},{"location":"components/model/properties/#dialect","title":"dialect","text":"<p>Specifies the SQL dialect your model uses. Defaults to whatever you set in <code>model_defaults</code>.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  dialect 'snowflake',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    dialect=\"snowflake\",\n)\n</code></pre>"},{"location":"components/model/properties/#tags","title":"tags","text":"<p>Labels for organizing, filtering, and categorizing models. Tags help you group related models and can be used for filtering in CLI commands and organizing your project.</p> <p>Common tag categories:</p> <ul> <li>Layer tags: <code>gold</code>, <code>silver</code>, <code>bronze</code> for data lake layers</li> <li>Domain tags: <code>analytics</code>, <code>customer</code>, <code>sales</code>, <code>finance</code></li> <li>Purpose tags: <code>reporting</code>, <code>segmentation</code>, <code>aggregation</code></li> <li>Sensitivity tags: <code>pii</code>, <code>confidential</code>, <code>public</code></li> </ul> SQLPython <pre><code>MODEL (\n  name gold_v1.rfm_customer_segmentation,\n  tags (\n    'gold',\n    'analytics',\n    'customer',\n    'rfm',\n    'segmentation'\n  )\n);\n</code></pre> <pre><code>@model(\n    \"gold_v1.rfm_customer_segmentation\",\n    tags=[\"gold\", \"analytics\", \"customer\", \"rfm\", \"segmentation\"],\n)\n</code></pre>"},{"location":"components/model/properties/#terms","title":"terms","text":"<p>Business glossary terms that link your model to semantic definitions. Terms provide a bridge between technical model names and business vocabulary, making it easier to discover and understand models.</p> <p>Format: Use dot notation for hierarchical terms like <code>domain.concept</code> (e.g., <code>customer.rfm_analysis</code>, <code>analytics.customer_segmentation</code>).</p> SQLPython <pre><code>MODEL (\n  name gold_v1.rfm_customer_segmentation,\n  terms (\n    'customer.rfm_analysis',\n    'analytics.customer_segmentation'\n  )\n);\n</code></pre> <pre><code>@model(\n    \"gold_v1.rfm_customer_segmentation\",\n    terms=[\"customer.rfm_analysis\", \"analytics.customer_segmentation\"],\n)\n</code></pre>"},{"location":"components/model/properties/#assertions","title":"assertions","text":"<p>Attach assertions directly to your model. These validations run after each model evaluation and will block the models if they fail.</p> <p>Why use assertions? They're your safety net, they catch bad data before it flows downstream. If revenue can't be negative, assert it. If customer IDs must be unique, assert it. Fail fast, fix fast.</p> <p>Think of assertions as \"this data must be true\" validations that run automatically.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    not_null(columns := (order_date, customer_id)),\n    unique_values(columns := (order_id)),\n    accepted_range(column := price, min_v := 0, max_v := 1000),\n    forall(criteria := (price &gt; 0, quantity &gt;= 1))\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    assertions=[\n        (\"not_null\", {\"columns\": [\"order_date\", \"customer_id\"]}),\n        (\"unique_values\", {\"columns\": [\"order_id\"]}),\n        (\"accepted_range\", {\"column\": \"price\", \"min_v\": 0, \"max_v\": 1000}),\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#profiles","title":"profiles","text":"<p>Enable automatic data profiling for specific columns. Profiles track statistical metrics over time (like null percentages, distinct counts, distributions) without blocking your models.</p> <p>How it works: Vulcan collects metrics each run and stores them in the <code>_check_profiles</code> table. You can query this to see how your data changes over time, detect data drift, understand patterns, and decide which checks or audits to add.</p> <p>Use cases:</p> <ul> <li> <p>Track null percentages over time</p> </li> <li> <p>Monitor distinct value counts</p> </li> <li> <p>Detect data drift</p> </li> <li> <p>Understand column distributions</p> </li> <li> <p>Inform which checks/audits to create</p> </li> </ul> <p>Think of profiles as your data observability layer, they watch and learn, but don't block.</p> SQLPython <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL,\n  grains (customer_id),\n  profiles (customer_id, customer_name, email, total_orders, total_spent, avg_order_value)\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"string\",\n        \"email\": \"string\",\n        \"total_orders\": \"int\",\n        \"total_spent\": \"decimal(10,2)\",\n        \"avg_order_value\": \"decimal(10,2)\",\n    },\n    kind=\"full\",\n    grains=[\"customer_id\"],\n    profiles=[\"customer_id\", \"customer_name\", \"email\", \"total_orders\", \"total_spent\", \"avg_order_value\"],\n)\ndef execute(context, **kwargs):\n    ...\n</code></pre>"},{"location":"components/model/properties/#depends_on","title":"depends_on","text":"<p>Explicitly declare model dependencies. Vulcan automatically infers dependencies from SQL queries, but sometimes you need to add extra ones.</p> <p>When to use:</p> <ul> <li> <p>Python models (required, Vulcan can't parse Python to find dependencies)</p> </li> <li> <p>Hidden dependencies (like a macro that references another model)</p> </li> <li> <p>External dependencies that aren't in your SQL</p> </li> </ul> <p>Note: Dependencies you declare here are added to the ones Vulcan infers, they don't replace them.</p> SQLPython <pre><code>MODEL (\n  name sales.summary,\n  depends_on ['sales.daily_sales', 'sales.products'],\n);\n</code></pre> <pre><code>@model(\n    \"sales.summary\",\n    depends_on=[\"sales.daily_sales\", \"sales.products\"],\n)\n</code></pre> <p>Python Models</p> <p>Python models require <code>depends_on</code> since Vulcan can't automatically infer dependencies from Python code. You need to tell it explicitly what your model depends on.</p>"},{"location":"components/model/properties/#references","title":"references","text":"<p>Declare non-unique join relationships to other models. These help Vulcan understand how models relate to each other for better lineage and optimization.</p> <p>Example: If your <code>orders</code> table has a <code>customer_id</code> that joins to <code>customers.customer_id</code>, you'd add <code>customer_id</code> to references. This tells Vulcan about the relationship even though <code>customer_id</code> isn't unique in the orders table.</p> SQLPython <pre><code>MODEL (\n  name sales.orders,\n  references (\n    customer_id,\n    guest_id AS account_id,  -- Alias for joining to account_id grain\n  ),\n);\n</code></pre> <pre><code>@model(\n    \"sales.orders\",\n    references=[\n        \"customer_id\",\n        (\"guest_id\", \"account_id\"),  # Alias\n    ],\n)\n</code></pre>"},{"location":"components/model/properties/#storage-properties","title":"Storage Properties","text":"<p>These properties control how your data is physically stored in the database. They're engine-specific, so check your engine's documentation for what's supported.</p>"},{"location":"components/model/properties/#partitioned_by","title":"partitioned_by","text":"<p>Defines the partition key for your table. Partitioning splits your table into chunks based on column values, which makes queries faster (the engine can skip irrelevant partitions).</p> <p>Supported engines: Spark, BigQuery, Databricks, and others that support table partitioning.</p> <p>Why partition? If you're querying data from the last 7 days and your table is partitioned by date, the engine only scans 7 partitions instead of scanning the entire table. That's a huge performance win!</p> SQLPython <pre><code>-- Single column partition\nMODEL (\n  name sales.events,\n  partitioned_by event_date,\n);\n\n-- Partition with transformation (BigQuery)\nMODEL (\n  name sales.events,\n  partitioned_by TIMESTAMP_TRUNC(event_ts, DAY),\n);\n\n-- Multi-column partition\nMODEL (\n  name sales.events,\n  partitioned_by (year, month, day),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n)\n\n# Multi-column\n@model(\n    \"sales.events\",\n    partitioned_by=[\"year\", \"month\", \"day\"],\n)\n</code></pre>"},{"location":"components/model/properties/#clustered_by","title":"clustered_by","text":"<p>Sets clustering columns for engines that support it (like BigQuery). Clustering organizes data within partitions based on column values, which makes range queries and filters faster.</p> <p>How it works: Data is physically stored sorted by the clustering columns. When you filter on those columns, the engine can skip reading irrelevant data blocks.</p> <p>Example: If you cluster by <code>customer_id</code>, queries filtering by customer will be faster because related data is stored together.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  partitioned_by event_date,\n  clustered_by (customer_id, product_id),\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    partitioned_by=[\"event_date\"],\n    clustered_by=[\"customer_id\", \"product_id\"],\n)\n</code></pre>"},{"location":"components/model/properties/#table_format","title":"table_format","text":"<p>Specifies the table format for engines that support multiple formats. Different formats have different features and performance characteristics.</p> <p>Supported formats: <code>iceberg</code>, <code>hive</code>, <code>delta</code></p> <p>When to use: If your engine supports multiple formats, choose based on your needs:</p> <ul> <li> <p>Iceberg: Great for time travel and schema evolution</p> </li> <li> <p>Delta: Good for ACID transactions and time travel</p> </li> <li> <p>Hive: Traditional format, widely supported</p> </li> </ul> SQLPython <pre><code>MODEL (\n  name sales.events,\n  table_format 'iceberg',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    table_format=\"iceberg\",\n)\n</code></pre>"},{"location":"components/model/properties/#storage_format","title":"storage_format","text":"<p>Sets the physical file format for your table's data files. This affects compression, query performance, and storage costs.</p> <p>Common formats: <code>parquet</code>, <code>orc</code></p> <p>Parquet is usually the best choice, it's columnar (great for analytics), has good compression, and is widely supported. ORC is another option, especially if you're using Hive.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  storage_format 'parquet',\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    storage_format=\"parquet\",\n)\n</code></pre>"},{"location":"components/model/properties/#engine-properties","title":"Engine Properties","text":"<p>These properties let you pass engine-specific settings to Vulcan. Each engine has different capabilities, so these properties vary by engine.</p>"},{"location":"components/model/properties/#physical_properties","title":"physical_properties","text":"<p>Pass engine-specific properties directly to the physical table/view creation. This is where you set things like retention policies, labels, or other engine-specific features.</p> <p>Use cases:</p> <ul> <li> <p>Set table retention (BigQuery: <code>partition_expiration_days</code>)</p> </li> <li> <p>Add labels or tags (BigQuery, Snowflake)</p> </li> <li> <p>Configure table type (Snowflake: <code>TRANSIENT</code> tables)</p> </li> <li> <p>Any other engine-specific table settings</p> </li> </ul> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  physical_properties (\n    partition_expiration_days = 7,\n    require_partition_filter = true,\n    creatable_type = TRANSIENT,  -- Creates TRANSIENT TABLE\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    physical_properties={\n        \"partition_expiration_days\": 7,\n        \"require_partition_filter\": True,\n        \"creatable_type\": \"TRANSIENT\",\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#virtual_properties","title":"virtual_properties","text":"<p>Pass engine-specific properties to the virtual layer view. This is useful for things like view-level security, labels, or other view-specific settings.</p> <p>Use cases:</p> <ul> <li> <p>Create secure views (Snowflake: <code>SECURE</code> views)</p> </li> <li> <p>Add labels to views</p> </li> <li> <p>Set view-level permissions</p> </li> <li> <p>Configure view-specific engine features</p> </li> </ul> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  virtual_properties (\n    creatable_type = SECURE,  -- Creates SECURE VIEW\n    labels = [('team', 'analytics')]\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    virtual_properties={\n        \"creatable_type\": \"SECURE\",\n        \"labels\": [(\"team\", \"analytics\")],\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#session_properties","title":"session_properties","text":"<p>Set session-level properties that apply when Vulcan executes your model. These affect how queries run but don't change the table structure.</p> <p>Use cases:</p> <ul> <li> <p>Set query timeouts</p> </li> <li> <p>Configure parallelism</p> </li> <li> <p>Adjust memory limits</p> </li> <li> <p>Set engine-specific session variables</p> </li> </ul> <p>Example: If you have a large query that needs more time, set <code>query_timeout: 3600</code> to give it an hour instead of the default timeout.</p> SQLPython <pre><code>MODEL (\n  name sales.large_query,\n  session_properties (\n    query_timeout = 3600,\n    max_parallel_workers = 8,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.large_query\",\n    session_properties={\n        \"query_timeout\": 3600,\n        \"max_parallel_workers\": 8,\n    },\n)\n</code></pre>"},{"location":"components/model/properties/#gateway","title":"gateway","text":"<p>Specifies which gateway to use for executing this model. Useful when you have multiple database connections and want to route specific models to specific databases.</p> <p>When to use: Multi-warehouse setups, isolated environments, or when you need to run certain models on a different database than the default.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  gateway 'warehouse_gateway',\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    gateway=\"warehouse_gateway\",\n)\n</code></pre>"},{"location":"components/model/properties/#behavior-properties","title":"Behavior Properties","text":"<p>These properties control how Vulcan behaves when processing your model.</p>"},{"location":"components/model/properties/#stamp","title":"stamp","text":"<p>Force a new model version without changing the definition. This is like a version tag, useful for tracking deployments or forcing a refresh.</p> <p>When to use: When you want to create a new version for tracking purposes, or when you need to force downstream models to rebuild even though this model's definition hasn't changed.</p> SQLPython <pre><code>MODEL (\n  name sales.daily_sales,\n  stamp 'v2.1.0',  -- Force new version\n);\n</code></pre> <pre><code>@model(\n    \"sales.daily_sales\",\n    stamp=\"v2.1.0\",\n)\n</code></pre>"},{"location":"components/model/properties/#enabled","title":"enabled","text":"<p>Control whether the model is active. Set to <code>false</code> to disable a model without deleting it.</p> <p>When to use:</p> <ul> <li> <p>Temporarily disable a model while debugging</p> </li> <li> <p>Deprecate a model but keep it for reference</p> </li> <li> <p>Skip models during development</p> </li> </ul> <p>Default: <code>true</code> (models are enabled by default)</p> SQLPython <pre><code>MODEL (\n  name sales.deprecated_model,\n  enabled false,  -- Model will be ignored\n);\n</code></pre> <pre><code>@model(\n    \"sales.deprecated_model\",\n    enabled=False,\n)\n</code></pre>"},{"location":"components/model/properties/#allow_partials","title":"allow_partials","text":"<p>Allow processing of incomplete data intervals. By default, Vulcan waits for complete intervals before processing (keeps data quality high). Set this to <code>true</code> if you need to process partial intervals.</p> <p>When to use:</p> <ul> <li> <p>Real-time or near-real-time models</p> </li> <li> <p>When you need data ASAP, even if it's incomplete</p> </li> <li> <p>Streaming data scenarios</p> </li> </ul> <p>Trade-off: You lose the ability to distinguish between \"missing data\" (models issue) and \"partial interval\" (expected). Use with caution!</p> <p>Default: <code>false</code> (wait for complete intervals)</p>"},{"location":"components/model/properties/#optimize_query","title":"optimize_query","text":"<p>Enable or disable query optimization. Vulcan optimizes queries by default (rewrites them for better performance), but sometimes you want to disable this.</p> <p>When to disable:</p> <ul> <li> <p>The optimizer is breaking your query</p> </li> <li> <p>You have engine-specific optimizations you want to preserve</p> </li> <li> <p>Debugging query issues</p> </li> </ul> <p>Default: <code>true</code> (optimize queries)</p> SQLPython <pre><code>MODEL (\n  name sales.complex_query,\n  optimize_query false,  -- Disable optimization\n);\n</code></pre> <pre><code>@model(\n    \"sales.complex_query\",\n    optimize_query=False,\n)\n</code></pre>"},{"location":"components/model/properties/#formatting","title":"formatting","text":"<p>Control whether Vulcan formats this model when you run <code>vulcan format</code>. Set to <code>false</code> if you want to preserve custom formatting.</p> <p>When to disable:</p> <ul> <li> <p>Legacy models with specific formatting requirements</p> </li> <li> <p>Models where formatting breaks something</p> </li> <li> <p>When you prefer manual formatting control</p> </li> </ul> <p>Default: <code>true</code> (format models automatically)</p> SQLPython <pre><code>MODEL (\n  name sales.legacy_model,\n  formatting false,  -- Skip formatting\n);\n</code></pre> <pre><code>@model(\n    \"sales.legacy_model\",\n    formatting=False,\n)\n</code></pre>"},{"location":"components/model/properties/#ignored_rules","title":"ignored_rules","text":"<p>Tell Vulcan's linter to ignore specific rules for this model. Useful when you have a legitimate reason to break a rule, or when a rule doesn't apply to your use case.</p> <p>You can ignore specific rules (<code>['rule_name', 'another_rule']</code>) or all rules (<code>'ALL'</code>).</p> <p>Use sparingly: If you're ignoring lots of rules, maybe the rules need updating, or maybe the model needs refactoring.</p> SQLPython <pre><code>-- Ignore specific rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules ['rule_name', 'another_rule'],\n);\n\n-- Ignore all rules\nMODEL (\n  name sales.legacy_model,\n  ignored_rules 'ALL',\n);\n</code></pre> <pre><code># Ignore specific rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=[\"rule_name\", \"another_rule\"],\n)\n\n# Ignore all rules\n@model(\n    \"sales.legacy_model\",\n    ignored_rules=\"ALL\",\n)\n</code></pre>"},{"location":"components/model/properties/#incremental-model-properties","title":"Incremental Model Properties","text":"<p>These properties are specified inside the <code>kind</code> definition for incremental models. They control how incremental models behave, things like handling schema changes, restatements, and batch processing.</p> <p>For the full picture on incremental models, check out the Model Kinds documentation.</p>"},{"location":"components/model/properties/#common-incremental-properties","title":"Common Incremental Properties","text":"<p>These properties work with all incremental model kinds. They're your toolkit for controlling incremental behavior:</p> Property Description Type Default <code>forward_only</code> All changes should be forward-only <code>bool</code> <code>false</code> <code>on_destructive_change</code> Behavior for destructive schema changes <code>str</code> <code>error</code> <code>on_additive_change</code> Behavior for additive schema changes <code>str</code> <code>allow</code> <code>disable_restatement</code> Disable data restatement <code>bool</code> <code>false</code> <code>auto_restatement_cron</code> Cron expression for automatic restatement <code>str</code> - <p>Values for <code>on_destructive_change</code> / <code>on_additive_change</code>:</p> <ul> <li> <p><code>allow</code> - Let the change happen (default for additive)</p> </li> <li> <p><code>warn</code> - Allow but warn about it</p> </li> <li> <p><code>error</code> - Block the change (default for destructive)</p> </li> <li> <p><code>ignore</code> - Pretend it didn't happen</p> </li> </ul> <p>Why this matters: Schema changes can break downstream models. These settings let you control how strict Vulcan should be when your schema evolves.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    forward_only true,\n    on_destructive_change 'warn',\n    on_additive_change 'allow',\n    disable_restatement false,\n  )\n);\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        forward_only=True,\n        on_destructive_change=\"warn\",\n        on_additive_change=\"allow\",\n        disable_restatement=False,\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#incremental_by_time_range","title":"INCREMENTAL_BY_TIME_RANGE","text":"<p>Properties for models that update incrementally based on a time column. These control how time-based incremental processing works.</p> <p>For the full guide on <code>INCREMENTAL_BY_TIME_RANGE</code> models, see the Model Kinds documentation.</p> Property Description Type Required <code>time_column</code> Column containing each row's timestamp (should be UTC) <code>str</code> Y <code>format</code> Format of the time column's data <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>batch_concurrency</code> Maximum concurrent batches <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N <code>auto_restatement_intervals</code> Number of last intervals to auto-restate <code>int</code> N SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column event_ts,\n    time_column (event_ts, '%Y-%m-%d'),  -- With format\n    batch_size 12,\n    batch_concurrency 4,\n    lookback 7,\n    auto_restatement_cron '@weekly',\n    auto_restatement_intervals 7,\n  )\n);\n\nSELECT\n  event_ts::TIMESTAMP AS event_ts,\n  event_type::VARCHAR AS event_type,\n  user_id::INTEGER AS user_id\nFROM raw.events\nWHERE event_ts BETWEEN @start_ts AND @end_ts;\n</code></pre> <pre><code>from vulcan import ModelKindName\n\n@model(\n    \"sales.events\",\n    columns={\n        \"event_ts\": \"timestamp\",\n        \"event_type\": \"varchar\",\n        \"user_id\": \"int\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"event_ts\",\n        batch_size=12,\n        batch_concurrency=4,\n        lookback=7,\n    ),\n    depends_on=[\"raw.events\"],\n)\ndef execute(context, start, end, **kwargs) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT event_ts, event_type, user_id\n    FROM raw.events\n    WHERE event_ts BETWEEN '{start}' AND '{end}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre> <p>Important: UTC Timezone</p> <p>Your <code>time_column</code> should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.</p>"},{"location":"components/model/properties/#incremental_by_unique_key","title":"INCREMENTAL_BY_UNIQUE_KEY","text":"<p>Properties for models that update based on unique keys (upsert operations). These control MERGE behavior and key handling.</p> <p>For details on <code>INCREMENTAL_BY_UNIQUE_KEY</code> models, see the Model Kinds documentation.</p> Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>str</code> | <code>array</code> Y <code>when_matched</code> SQL logic to update columns on match (MERGE engines only) <code>str</code> N <code>merge_filter</code> Predicates for ON clause of MERGE operation <code>str</code> N <code>batch_size</code> Maximum intervals per backfill task <code>int</code> N <code>lookback</code> Prior intervals to include for late-arriving data <code>int</code> N SQLPython <pre><code>-- Single unique key\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n  )\n);\n\n-- Composite unique key\nMODEL (\n  name sales.order_items,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key (order_id, item_id),\n  )\n);\n\n-- With MERGE options\nMODEL (\n  name sales.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id,\n    when_matched WHEN MATCHED THEN UPDATE SET \n      name = source.name,\n      updated_at = source.updated_at,\n    auto_restatement_cron '@weekly',\n  )\n);\n</code></pre> <pre><code># Single unique key\n@model(\n    \"sales.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=\"customer_id\",\n    ),\n    depends_on=[\"raw.customers\"],\n)\n\n# Composite unique key\n@model(\n    \"sales.order_items\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"order_id\", \"item_id\"],\n    ),\n)\n</code></pre> <p>Batch Concurrency</p> <p><code>batch_concurrency</code> isn't supported for this kind because MERGE operations can't safely run in parallel. Vulcan processes these models sequentially to avoid conflicts.</p>"},{"location":"components/model/properties/#incremental_by_partition","title":"INCREMENTAL_BY_PARTITION","text":"<p>Properties for models that update by partition. This kind uses the <code>partitioned_by</code> property (from the General Properties section) as its partition key.</p> <p>Note: There are no additional kind-specific properties, just use <code>partitioned_by</code> to define your partition columns.</p> <p>For details on <code>INCREMENTAL_BY_PARTITION</code> models, see the Model Kinds documentation.</p> SQLPython <pre><code>MODEL (\n  name sales.events,\n  kind INCREMENTAL_BY_PARTITION,\n  partitioned_by event_date,\n);\n\nSELECT\n  event_date::DATE AS event_date,\n  event_type::VARCHAR AS event_type,\n  COUNT(*)::INTEGER AS event_count\nFROM raw.events\nGROUP BY event_date, event_type;\n</code></pre> <pre><code>@model(\n    \"sales.events\",\n    columns={\n        \"event_date\": \"date\",\n        \"event_type\": \"varchar\",\n        \"event_count\": \"int\",\n    },\n    kind=dict(name=ModelKindName.INCREMENTAL_BY_PARTITION),\n    partitioned_by=[\"event_date\"],\n    depends_on=[\"raw.events\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2","title":"SCD_TYPE_2","text":"<p>Properties for Slowly Changing Dimension Type 2 models, which track historical changes to your data.</p> <p>For the complete guide on SCD Type 2 models, see the Model Kinds documentation.</p>"},{"location":"components/model/properties/#common-scd-type-2-properties","title":"Common SCD Type 2 Properties","text":"Property Description Type Required <code>unique_key</code> Column(s) containing each row's unique key <code>array</code> Y <code>valid_from_name</code> Column for valid from date <code>str</code> N (default: <code>valid_from</code>) <code>valid_to_name</code> Column for valid to date <code>str</code> N (default: <code>valid_to</code>) <code>invalidate_hard_deletes</code> Mark missing records as invalid <code>bool</code> N (default: <code>true</code>)"},{"location":"components/model/properties/#scd_type_2_by_time","title":"SCD_TYPE_2_BY_TIME","text":"<p>Properties for SCD Type 2 models that detect changes using an <code>updated_at</code> timestamp column. This is the recommended approach when your source table has update timestamps.</p> Property Description Type Required <code>updated_at_name</code> Column containing updated at date <code>str</code> N (default: <code>updated_at</code>) <code>updated_at_as_valid_from</code> Use <code>updated_at</code> value as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>MODEL (\n  name dim.customers,\n  kind SCD_TYPE_2_BY_TIME (\n    unique_key customer_id,\n    updated_at_name last_modified,\n    valid_from_name effective_from,\n    valid_to_name effective_to,\n    invalidate_hard_deletes true,\n    updated_at_as_valid_from true,\n  )\n);\n\nSELECT\n  customer_id::INTEGER AS customer_id,\n  name::VARCHAR AS name,\n  email::VARCHAR AS email,\n  last_modified::TIMESTAMP AS last_modified\nFROM raw.customers;\n</code></pre> <pre><code>@model(\n    \"dim.customers\",\n    columns={\n        \"customer_id\": \"int\",\n        \"name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"last_modified\": \"timestamp\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_TIME,\n        unique_key=[\"customer_id\"],\n        updated_at_name=\"last_modified\",\n        valid_from_name=\"effective_from\",\n        valid_to_name=\"effective_to\",\n        invalidate_hard_deletes=True,\n    ),\n    depends_on=[\"raw.customers\"],\n)\n</code></pre>"},{"location":"components/model/properties/#scd_type_2_by_column","title":"SCD_TYPE_2_BY_COLUMN","text":"<p>Properties for SCD Type 2 models that detect changes by comparing column values. Use this when your source table doesn't have an <code>updated_at</code> column.</p> Property Description Type Required <code>columns</code> Columns to check for changes (<code>*</code> for all) <code>str</code> | <code>array</code> Y <code>execution_time_as_valid_from</code> Use execution time as <code>valid_from</code> for new rows <code>bool</code> N (default: <code>false</code>) SQLPython <pre><code>-- Track specific columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns (name, price, category),\n    execution_time_as_valid_from true,\n  )\n);\n\n-- Track all columns\nMODEL (\n  name dim.products,\n  kind SCD_TYPE_2_BY_COLUMN (\n    unique_key product_id,\n    columns '*',\n  )\n);\n</code></pre> <pre><code># Track specific columns\n@model(\n    \"dim.products\",\n    columns={\n        \"product_id\": \"int\",\n        \"name\": \"varchar\",\n        \"price\": \"decimal(10,2)\",\n        \"category\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=[\"name\", \"price\", \"category\"],\n        execution_time_as_valid_from=True,\n    ),\n    depends_on=[\"raw.products\"],\n)\n\n# Track all columns\n@model(\n    \"dim.products\",\n    kind=dict(\n        name=ModelKindName.SCD_TYPE_2_BY_COLUMN,\n        unique_key=[\"product_id\"],\n        columns=\"*\",\n    ),\n)\n</code></pre>"},{"location":"components/model/properties/#model-naming","title":"Model Naming","text":"<p>By default, you need to specify the <code>name</code> property in every model. But if you organize your models in a directory structure that matches your schema names, you can enable automatic name inference.</p> <p>How it works: With <code>infer_names: true</code>, a model at <code>models/sales/daily_sales.sql</code> automatically gets the name <code>sales.daily_sales</code>. The directory structure becomes your schema, and the filename becomes your model name.</p> <p>Enable it in your config:</p> <pre><code>model_defaults:\n  dialect: snowflake\n\n# Enable name inference\ninfer_names: true\n</code></pre> <p>When to use: If your project structure matches your schema structure, this saves you from typing <code>name</code> in every model. Pretty convenient!</p> <p>Learn more in the configuration guide.</p>"},{"location":"components/model/statements/","title":"Statements","text":""},{"location":"components/model/statements/#statements","title":"Statements","text":"<p>Statements let you run SQL commands at specific points during model execution. You can run code before your query, after it completes, or when views are created.</p> <p>Why use statements? They're perfect for: - Setting session parameters (timeouts, memory limits)</p> <ul> <li> <p>Loading UDFs or creating temporary tables</p> </li> <li> <p>Creating indexes or clustering</p> </li> <li> <p>Running data quality checks</p> </li> <li> <p>Logging anomalies or errors</p> </li> <li> <p>Granting permissions on views</p> </li> </ul> <p>You can define statements at the model level (for specific needs) or at the project level via <code>model_defaults</code> (for consistent behavior across all models).</p> <p>Statement types:</p> <ul> <li> <p>Pre-statements: Run before the main model query executes</p> </li> <li> <p>Post-statements: Run after the main model query completes</p> </li> <li> <p>On-virtual-update statements: Run when views are created or updated in the virtual layer</p> </li> </ul> <p>Concurrency Considerations</p> <p>Pre-statements should generally only prepare the main query. Avoid creating or altering physical tables in pre-statements, if multiple models run concurrently, you could get race conditions or unpredictable behavior. Stick to session settings, UDFs, and temporary objects.</p>"},{"location":"components/model/statements/#model-defaults","title":"Model Defaults","text":"<p>You can define statements at the project level using <code>model_defaults</code> in your configuration. Use this for setting up common behavior across all models, like session timeouts or default permissions.</p> <p>How it works: Default statements run first, then model-specific statements. So if you set a default timeout in <code>model_defaults</code> and a model-specific timeout in a model, the model-specific one runs after (and can override the default).</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  pre_statements:\n    - \"SET query_timeout = 300000\"\n  post_statements:\n    - \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\"\n  on_virtual_update:\n    - \"GRANT SELECT ON @this_model TO ROLE analyst_role\"\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n  model_defaults=ModelDefaultsConfig(\n    dialect=\"snowflake\",\n    pre_statements=[\n      \"SET query_timeout = 300000\",\n    ],\n    post_statements=[\n      \"@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)\",\n    ],\n    on_virtual_update=[\n      \"GRANT SELECT ON @this_model TO ROLE analyst_role\",\n    ],\n  ),\n)\n</code></pre>"},{"location":"components/model/statements/#pre-statements","title":"Pre-statements","text":"<p>Pre-statements run before your main model query executes. They're perfect for setting up the environment your query needs.</p> <p>Common use cases:</p> <ul> <li> <p>Loading JARs or UDFs that your query uses</p> </li> <li> <p>Creating temporary tables or caching data</p> </li> <li> <p>Setting session parameters (timeouts, memory, etc.)</p> </li> <li> <p>Initializing variables or settings</p> </li> </ul> <p>Pre-statements run in the setup phase before your main query runs.</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily'\n);\n\n/* Pre-statement: Create table for anomaly tracking */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* Pre-statement: Set session variables using Jinja */\nJINJA_STATEMENT_BEGIN;\n{% if start_date is none or end_date is none %}\n  SET start_date = DATE '{{ start }}';\n  SET end_date = CURRENT_DATE;\n{% endif %}\nJINJA_END;\n\n/* Main model query */\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    pre_statements=[\n        \"SET query_timeout = 300000\",\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR\n        )\"\"\",\n        exp.Cache(this=exp.table_(\"orders_cache\"), expression=exp.select(\"*\").from_(\"demo.raw_data.orders\")),\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#post-statements","title":"Post-statements","text":"<p>Post-statements run after your model query completes. They're great for cleanup, optimization, or validation tasks.</p> <p>Important: When you use post-statements in SQL models, your main query must end with a semicolon. This tells Vulcan where the query ends and the statements begin.</p> <p>Common use cases:</p> <ul> <li> <p>Creating indexes or clustering (for query performance)</p> </li> <li> <p>Running data quality checks or validations</p> </li> <li> <p>Logging anomalies or errors to tracking tables</p> </li> <li> <p>Conditional table alterations (like setting retention policies)</p> </li> </ul> <p>Think of it as: The \"cleanup and optimization\" phase after your data is loaded.</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  )\n);\n\nSELECT\n  order_id,\n  order_date,\n  customer_id,\n  quantity,\n  unit_price,\n  total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* Post-statement: Conditional retention policy (only on table creation) */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Post-statement: Add clustering for query performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Post-statement: Capture data anomalies - negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'NEGATIVE_QUANTITY',\n  CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Post-statement: Capture data anomalies - total mismatch */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calculated=', ROUND(unit_price * quantity, 2),\n    '; actual=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS((unit_price * quantity) - total_amount) &gt; 0.01;\n</code></pre> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n)\ndef execute(context: ExecutionContext, start, end, **kwargs):\n    query = f\"\"\"\n    SELECT order_id, order_date, customer_id, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#on-virtual-update-statements","title":"On-virtual-update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.</p> <p>Common use cases:</p> <ul> <li> <p>Granting permissions on views (so users can query them)</p> </li> <li> <p>Setting up access controls or row-level security</p> </li> <li> <p>Applying column masking policies</p> </li> <li> <p>Any view-level configuration</p> </li> </ul> <p>Think of it as: The \"access control\" phase, setting up who can see what.</p> <p>Note: These statements run at the virtual layer, so table names (including <code>@this_model</code>) resolve to view names, not physical table names.</p> SQLPython <p>Use <code>ON_VIRTUAL_UPDATE_BEGIN</code> and <code>ON_VIRTUAL_UPDATE_END</code> to define these statements:</p> <pre><code>MODEL (\n  name analytics.customers,\n  kind INCREMENTAL_BY_UNIQUE_KEY (\n    unique_key customer_id\n  )\n);\n\nSELECT\n  customer_id,\n  full_name,\n  email,\n  customer_segment\nFROM demo.raw_data.customers;\n\n/* Post-statement: Apply masking policy */\nJINJA_STATEMENT_BEGIN;\nALTER TABLE {{ this_model }} MODIFY COLUMN email SET MASKING POLICY demo.security.mask_email_policy;\nJINJA_END;\n\n/* On-virtual-update: Grant permissions when view is created/updated */\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Use the <code>on_virtual_update</code> argument in the <code>@model</code> decorator:</p> <pre><code>from vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"analytics.customers_py\",\n    columns={\n        \"customer_id\": \"varchar\",\n        \"full_name\": \"varchar\",\n        \"email\": \"varchar\",\n        \"customer_segment\": \"varchar\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,\n        unique_key=[\"customer_id\"],\n    ),\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 7)\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(context: ExecutionContext, **kwargs):\n    query = \"\"\"\n    SELECT customer_id, CONCAT(first_name, ' ', last_name) AS full_name,\n           email, customer_segment\n    FROM demo.raw_data.customers\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#complete-example","title":"Complete example","text":"<p>Here's a complete example showing all statement types:</p> SQLPython <pre><code>MODEL (\n  name analytics.orders,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2020-01-01',\n  cron '@daily',\n  grains (order_id),\n  description 'Orders fact table with incremental loading'\n);\n\n/* ============ PRE-STATEMENTS ============ */\n\n/* Create anomaly tracking table */\nCREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n  anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n  order_id VARCHAR,\n  anomaly_type VARCHAR,\n  details VARCHAR,\n  captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n/* ============ MAIN QUERY ============ */\n\nSELECT\n  order_id::VARCHAR AS order_id,\n  order_date::DATE AS order_date,\n  customer_id::VARCHAR AS customer_id,\n  product_id::VARCHAR AS product_id,\n  quantity::INT AS quantity,\n  unit_price::FLOAT AS unit_price,\n  discount::FLOAT AS discount,\n  tax::FLOAT AS tax,\n  shipping_cost::FLOAT AS shipping_cost,\n  total_amount::FLOAT AS total_amount\nFROM demo.raw_data.orders\nWHERE\n  order_date BETWEEN @start_date AND @end_date;\n\n/* ============ POST-STATEMENTS ============ */\n\n/* Conditional: Set retention only on table creation */\n@IF(\n  @runtime_stage IN ('creating'),\n  ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30\n);\n\n/* Add clustering for performance */\nALTER TABLE @this_model CLUSTER BY (order_date, customer_id);\n\n/* Data quality: Log negative quantities */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\nFROM @this_model\nWHERE quantity &lt; 0;\n\n/* Data quality: Log total mismatches */\nINSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\nSELECT\n  order_id,\n  'TOTAL_MISMATCH',\n  CONCAT(\n    'calc=', ROUND(unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0), 2),\n    '; total=', ROUND(total_amount, 2)\n  )\nFROM @this_model\nWHERE ABS(\n  (unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0))\n  - total_amount\n) &gt; 0.01;\n\n/* ============ ON-VIRTUAL-UPDATE ============ */\n\nON_VIRTUAL_UPDATE_BEGIN;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\nfrom sqlglot import exp\n\n@model(\n    \"analytics.orders_py\",\n    columns={\n        \"order_id\": \"varchar\",\n        \"order_date\": \"date\",\n        \"customer_id\": \"varchar\",\n        \"product_id\": \"varchar\",\n        \"quantity\": \"int\",\n        \"unit_price\": \"float\",\n        \"discount\": \"float\",\n        \"tax\": \"float\",\n        \"shipping_cost\": \"float\",\n        \"total_amount\": \"float\",\n    },\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n    grains=[\"order_id\"],\n    depends_on=[\"demo.raw_data.orders\"],\n    description=\"Orders fact table with incremental loading\",\n    pre_statements=[\n        \"\"\"CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (\n            anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,\n            order_id VARCHAR,\n            anomaly_type VARCHAR,\n            details VARCHAR,\n            captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\"\"\",\n    ],\n    post_statements=[\n        \"@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)\",\n        \"ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)\",\n        \"\"\"INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)\n           SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)\n           FROM @this_model WHERE quantity &lt; 0\"\"\",\n    ],\n    on_virtual_update=[\n        \"GRANT SELECT ON @this_model TO ROLE view_only_role\",\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    query = f\"\"\"\n    SELECT\n        order_id, order_date, customer_id, product_id,\n        quantity, unit_price, discount, tax, shipping_cost, total_amount\n    FROM demo.raw_data.orders\n    WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'\n    \"\"\"\n    return context.fetchdf(query)\n</code></pre>"},{"location":"components/model/statements/#useful-macros-and-variables","title":"Useful macros and variables","text":"Macro/Variable Description <code>@this_model</code> References the current model's table/view <code>@runtime_stage</code> Current execution stage: <code>'creating'</code>, <code>'evaluating'</code>, or <code>'testing'</code> <code>@IF(condition, statement)</code> Conditionally execute a statement <code>@start_date</code>, <code>@end_date</code> Time range macros for incremental models <code>{{ this_model }}</code> Jinja equivalent of <code>@this_model</code> <p>For more information on macros, see the Macro Variables documentation.</p>"},{"location":"components/model/types/external_models/","title":"External","text":""},{"location":"components/model/types/external_models/#external","title":"External","text":"<p>Sometimes your models need to query tables that exist outside your Vulcan project, maybe a third-party data source, a table managed by another system, or a read-only database. These are \"external\" tables.</p> <p>Vulcan doesn't manage external tables (you can't create or update them), but it can use metadata about them to make your life easier. By defining external models, you give Vulcan information about column names and types, which enables better column-level lineage and query optimization.</p> <p>Even though Vulcan can't manage them, knowing their schema helps with: - Column-level lineage (see how data flows through external tables)</p> <ul> <li> <p>Query optimization (Vulcan can make better decisions)</p> </li> <li> <p>Documentation (your data catalog knows what's in those tables)</p> </li> </ul> <p>Vulcan stores this metadata as <code>EXTERNAL</code> models.</p>"},{"location":"components/model/types/external_models/#how-external-models-work","title":"How External Models Work","text":"<p><code>EXTERNAL</code> models are metadata-only, they just describe a table's schema (column names and types). There's no query for Vulcan to run, and Vulcan doesn't manage the data.</p> <p>Important limitations:</p> <ul> <li> <p>Vulcan doesn't know what data is in the table (or if it even exists)</p> </li> <li> <p>If someone alters the external table, Vulcan won't detect it</p> </li> <li> <p>If all data is deleted, Vulcan won't know</p> </li> <li> <p>Vulcan never modifies external tables</p> </li> </ul> <p>The querying model's <code>kind</code>, <code>cron</code>, and previously loaded time intervals determine when Vulcan will query the <code>EXTERNAL</code> model.</p> <p>When external tables get queried: Only when a Vulcan model references them. The querying model's <code>kind</code>, <code>cron</code>, and time intervals determine when the external table is actually queried. Vulcan doesn't proactively query external tables, it only queries them as part of executing your models.</p>"},{"location":"components/model/types/external_models/#creating-external-models","title":"Creating External Models","text":"<p>External models are defined in YAML files. You have two options:</p> <ol> <li>Let Vulcan generate it (easiest) - Use the <code>create_external_models</code> CLI command</li> <li>Write it yourself - Hand-craft the YAML if you need more control</li> </ol> <p>The main file is <code>external_models.yaml</code> (or <code>schema.yaml</code>) in your project root. You can also add more files in the <code>external_models/</code> directory.</p> <p>Let's say you have a model that queries external tables. Here's an example:</p> <pre><code>MODEL (\n  name vulcan_demo.full_model,\n  kind FULL\n);\n\nSELECT\n  c.customer_id,\n  c.name AS customer_name,\n  c.email,\n  COUNT(DISTINCT o.order_id) AS total_orders,\n  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent\nFROM vulcan_demo.customers AS c\nLEFT JOIN vulcan_demo.orders AS o\n  ON c.customer_id = o.customer_id\nLEFT JOIN vulcan_demo.order_items AS oi\n  ON o.order_id = oi.order_id\nGROUP BY c.customer_id, c.name, c.email\n</code></pre> <p>The following sections show you how to create external models for these tables. You can define all external models in <code>external_models.yaml</code>, or split them across multiple files in the <code>external_models/</code> directory (useful for organization or when Vulcan regenerates the main file).</p>"},{"location":"components/model/types/external_models/#using-cli","title":"Using CLI","text":"<p>Instead of creating the <code>external_models.yaml</code> file manually, Vulcan can generate it for you with the create_external_models CLI command.</p> <p>The command identifies all external tables referenced in your Vulcan project, fetches their column information from the SQL engine's metadata, and then stores the information in the <code>external_models.yaml</code> file.</p> <p>If Vulcan does not have access to an external table's metadata, the table will be omitted from the file and Vulcan will issue a warning.</p> <p><code>create_external_models</code> solely queries SQL engine metadata and does not query external tables themselves.</p>"},{"location":"components/model/types/external_models/#gateway-specific-external-models","title":"Gateway-specific external models","text":"<p>In some use-cases such as isolated systems with multiple gateways, there are external models that only exist on a certain gateway.</p> <p>Gateway names are case-insensitive in external model configurations. You can specify the gateway name using any case (e.g., <code>gateway: dev</code>, <code>gateway: DEV</code>, <code>gateway: Dev</code>) and Vulcan will handle the matching correctly.</p> <p>Consider the following model that queries an external table with a dynamic database based on the current gateway:</p> <pre><code>vulcan create_external_models\n</code></pre> <p>What it does:</p> <ul> <li> <p>Scans your project for references to external tables</p> </li> <li> <p>Fetches column information from your SQL engine's metadata</p> </li> <li> <p>Writes everything to <code>external_models.yaml</code></p> </li> </ul> <p>Important: This command only queries metadata (table schemas), not the actual data. It's fast and safe.</p> <p>If Vulcan can't access a table's metadata: That table gets skipped and Vulcan warns you. You'll need to define it manually (see the \"Writing YAML by hand\" section below).</p>"},{"location":"components/model/types/external_models/#gateway-specific-external-models_1","title":"Gateway-Specific External Models","text":"<p>If you're using isolated systems with multiple gateways, you might have external tables that only exist on specific gateways.</p> <p>Example: Your model uses a gateway variable to select different databases:</p> <pre><code>MODEL (\n  name vulcan_demo.customer_summary,\n  kind FULL\n);\n\nSELECT * FROM @{gateway}_db.customers;\n</code></pre> <p>When you run with <code>--gateway dev</code>, it queries <code>dev_db.customers</code>. When you run with <code>--gateway prod</code>, it queries <code>prod_db.customers</code>. These are different tables with potentially different schemas!</p> <p>Solution: Run <code>create_external_models</code> with the <code>--gateway</code> flag:</p> <pre><code>vulcan --gateway dev create_external_models\n</code></pre> <p>This sets <code>gateway: dev</code> on the external model, so it only loads when that gateway is active. Do this for each gateway that has different external tables.</p> <p>Case-Insensitive Gateway Names</p> <p>Gateway names are case-insensitive in external model configs. <code>gateway: dev</code>, <code>gateway: DEV</code>, and <code>gateway: Dev</code> all work the same.</p>"},{"location":"components/model/types/external_models/#writing-yaml-by-hand","title":"Writing YAML by Hand","text":"<p>Sometimes you need to define external models manually, maybe Vulcan can't access the metadata, or you want more control. Here's the structure:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  description: \"Customer dimension table from external system\"\n  gateway: dev  # Optional: only load for this gateway\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n</code></pre> <p>What you need:</p> <ul> <li> <p><code>name</code>: Fully qualified table name (with quotes if needed for case sensitivity)</p> </li> <li> <p><code>columns</code>: Dictionary of column names to data types</p> </li> </ul> <p>Optional fields:</p> <ul> <li> <p><code>description</code>: Human-readable description</p> </li> <li> <p><code>gateway</code>: Gateway name (for gateway-specific tables)</p> </li> </ul> <p>Pro tip: Use triple-quoted names if your table names have special characters or need case sensitivity. The exact format depends on your SQL engine.</p>"},{"location":"components/model/types/external_models/#using-the-external_models-directory","title":"Using the <code>external_models</code> Directory","text":"<p>Here's a common problem: You run <code>vulcan create_external_models</code> and it generates <code>external_models.yaml</code>. But some tables need manual definitions (maybe Vulcan can't access their metadata). If you add them to <code>external_models.yaml</code> and run the command again, your manual changes get overwritten!</p> <p>Solution: Put manual definitions in the <code>external_models/</code> directory:</p> <pre><code>external_models.yaml              # Auto-generated by Vulcan\nexternal_models/manual_tables.yaml # Your manual definitions\nexternal_models/legacy_tables.yaml # More manual definitions\n</code></pre> <p>How it works:</p> <ul> <li> <p>Vulcan loads <code>external_models.yaml</code> first (or <code>schema.yaml</code>)</p> </li> <li> <p>Then it loads all <code>.yaml</code> files from <code>external_models/</code></p> </li> <li> <p>Everything gets merged together</p> </li> </ul> <p>Best practice: Use <code>create_external_models</code> to manage the main file, and put any tables that need manual definitions in the <code>external_models/</code> directory. That way you can regenerate the main file without losing your manual work!</p>"},{"location":"components/model/types/external_models/#external-assertions","title":"External Assertions","text":"<p>You can define assertions on external models! This is super useful for validating upstream data quality before your internal models run.</p> <p>Why this matters: If your external data source has quality issues, you want to catch them early, before they flow into your models and cause bigger problems downstream.</p> <p>Here's how you'd add assertions to an external model:</p> <pre><code>- name: '\"warehouse\".\"vulcan_demo\".\"customers\"'\n  description: Table containing customer information\n  assertions:\n    - name: not_null\n      columns: \"[customer_id, email]\"\n    - name: unique_values\n      columns: \"[customer_id]\"\n  columns:\n    customer_id: INT\n    region_id: INT\n    name: TEXT\n    email: TEXT\n- name: '\"warehouse\".\"vulcan_demo\".\"orders\"'\n  description: Table containing order transactions\n  assertions:\n    - name: not_null\n      columns: \"[order_id, customer_id, order_date]\"\n    - name: accepted_range\n      column: order_id\n      min_v: \"1\"\n  columns:\n    order_id: INT\n    customer_id: INT\n    order_date: TIMESTAMP\n    warehouse_id: INT\n</code></pre>"},{"location":"components/model/types/managed_models/","title":"Managed","text":""},{"location":"components/model/types/managed_models/#managed","title":"Managed","text":"<p>Most Vulcan models manage their own data, you run <code>vulcan run</code>, and Vulcan updates the tables. Managed models are different: the database engine handles data updates automatically in the background.</p> <p>How it works: You define a query, and the engine monitors upstream tables. When source data changes, the engine automatically refreshes your managed table. No manual <code>REFRESH</code> commands needed, it just happens.</p> <p>Use this for scenarios where you need always-fresh data without managing refresh schedules yourself. The engine handles the complexity of incremental updates, change detection, and refresh timing.</p> <p>Best use case: Managed models are typically built on External Models rather than other Vulcan models. Since Vulcan already keeps its models up to date, the main benefit comes when you're reading from external tables that aren't tracked by Vulcan. The engine keeps your managed table in sync with those external sources automatically.</p> <p>Python Models Not Supported</p> <p>Python models don't support the <code>MANAGED</code> model kind. You'll need to use a SQL model instead.</p>"},{"location":"components/model/types/managed_models/#difference-from-materialized-views","title":"Difference from Materialized Views","text":"<p>What's the difference between a managed model and a materialized view?</p> <p>Vulcan already supports materialized views, but they have limitations:</p> <ul> <li> <p>Some engines only allow materialized views from a single base table</p> </li> <li> <p>Materialized views aren't automatically refreshed, you need to run <code>REFRESH MATERIALIZED VIEW</code> manually</p> </li> <li> <p>You're responsible for scheduling refreshes</p> </li> </ul> <p>Managed models are different:</p> <ul> <li> <p>Automatic updates - The engine refreshes data when source tables change</p> </li> <li> <p>Smart refresh - The engine understands your query and can do incremental or full refreshes as needed</p> </li> <li> <p>No manual commands - Everything happens in the background</p> </li> </ul> <p>In some engines, there's no difference (they're the same thing). In others, managed models give you more automation and flexibility.</p>"},{"location":"components/model/types/managed_models/#lifecycle-in-vulcan","title":"Lifecycle in Vulcan","text":"<p>Managed models follow the same lifecycle as other Vulcan models:</p> <ul> <li> <p>Virtual environments create pointers to model snapshots</p> </li> <li> <p>Model changes create new snapshots</p> </li> <li> <p>Upstream changes trigger new snapshots</p> </li> <li> <p>You can deploy and rollback like any other model</p> </li> <li> <p>Snapshots get cleaned up when TTL expires</p> </li> </ul> <p>Cost consideration: Managed models usually cost more than regular tables. For example, Snowflake charges extra for Dynamic Tables. To save money, Vulcan uses regular tables for dev previews (in forward-only plans) and only creates managed tables when deploying to production.</p> <p>Dev vs Prod Differences</p> <p>Since dev uses regular tables and prod uses managed tables, it's possible to write a query that works in dev but fails in prod. This happens if you use features available to regular tables but not managed tables.</p> <p>We think the cost savings are worth it, but if this causes issues, let us know!</p>"},{"location":"components/model/types/managed_models/#supported-engines","title":"Supported Engines","text":"<p>Currently, Vulcan supports managed models on:</p> Engine Implementation Snowflake Dynamic Tables <p>To create a managed model, use the <code>MANAGED</code> model kind.</p>"},{"location":"components/model/types/managed_models/#snowflake","title":"Snowflake","text":"<p>On Snowflake, managed models are implemented as Dynamic Tables. Dynamic Tables automatically refresh when their source data changes, which is exactly what managed models need.</p> <p>Here's how you'd create one:</p> <pre><code>MODEL (\n  name db.events,\n  kind MANAGED,\n  physical_properties (\n    warehouse = datalake,\n    target_lag = '2 minutes',\n    data_retention_time_in_days = 2\n  )\n);\n\nSELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>results in:</p> <pre><code>CREATE OR REPLACE DYNAMIC TABLE db.events\n  WAREHOUSE = \"datalake\",\n  TARGET_LAG = '2 minutes'\n  DATA_RETENTION_TIME_IN_DAYS = 2\nAS SELECT\n  event_date::DATE as event_date,\n  event_payload::TEXT as payload\nFROM raw_events\n</code></pre> <p>No Intervals</p> <p>Vulcan doesn't create intervals or run this model on a schedule. You don't need <code>WHERE</code> clauses with date filters like you would for incremental models. Snowflake handles all the refreshing automatically, you just define the query and let Snowflake do its thing.</p>"},{"location":"components/model/types/managed_models/#table-properties","title":"Table Properties","text":"<p>Dynamic Tables have properties that control refresh frequency, initial data population, retention, and more. You can find the complete list in the Snowflake documentation.</p> <p>In Vulcan, you set these properties using <code>physical_properties</code> in your model definition. Here are the key ones:</p> Snowflake Property Required Notes target_lag Y warehouse N In Snowflake, this is a required property. However, if not specified, then Vulcan will use the result of <code>select current_warehouse()</code>. refresh_mode N initialize N data_retention_time_in_days N max_data_extension_time_in_days N <p>The following Dynamic Table properties can be set directly on the model:</p> Snowflake Property Required Notes cluster by N <code>clustered_by</code> is a standard model property, so set <code>clustered_by</code> on the model to add a <code>CLUSTER BY</code> clause to the Dynamic Table"},{"location":"components/model/types/python_models/","title":"Python","text":""},{"location":"components/model/types/python_models/#python","title":"Python","text":"<p>Sometimes you need Python instead of SQL. Use Python models for machine learning, calling external APIs, or implementing complex business logic that's difficult to express in SQL.</p> <p>Vulcan supports Python models. As long as your function returns a Pandas, Spark, Bigframe, or Snowpark DataFrame, you can use Python.</p> <p>When to use Python models:</p> <ul> <li> <p>Building machine learning workflows</p> </li> <li> <p>Integrating with external APIs</p> </li> <li> <p>Complex transformations that are easier in Python</p> </li> <li> <p>Data processing that benefits from Python libraries</p> </li> </ul> <p>Unsupported Model Kinds</p> <p>Python models don't support these model kinds. If you need one of these, use a SQL model instead:</p> <pre><code>- `VIEW` - Views need to be SQL\n\n- `SEED` - Seed models load CSV files (SQL only)\n\n- `MANAGED` - Managed models require SQL\n\n- `EMBEDDED` - Embedded models inject SQL subqueries\n</code></pre>"},{"location":"components/model/types/python_models/#definition","title":"Definition","text":"<p>Create a Python model by adding a <code>.py</code> file to your <code>models/</code> directory and defining an <code>execute</code> function.</p> <p>Here's what a basic Python model looks like:</p> <pre><code>import typing as t\nimport pandas as pd\nfrom datetime import datetime\nfrom vulcan import ExecutionContext, model\nfrom vulcan import ModelKindName\n\n@model(\n    \"sales.daily_sales_py\",\n    columns={\n        \"order_date\": \"timestamp\",\n        \"total_orders\": \"int\",\n        \"total_revenue\": \"decimal(18,2)\",\n        \"last_order_id\": \"string\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"order_date\"],\n    depends_on=[\"raw.raw_orders\"],\n    cron='@daily',\n    tags=[\"silver\", \"sales\", \"aggregation\"],\n    terms=[\"sales.daily_metrics\", \"analytics.sales_summary\"],\n    description=\"Daily sales aggregated by order_date.\",\n    column_descriptions={\n        \"order_date\": \"Date of the sales transactions\",\n        \"total_orders\": \"Total number of orders for the day\",\n        \"total_revenue\": \"Total revenue for the day\",\n        \"last_order_id\": \"Last order ID processed for the day\",\n    },\n    column_tags={\n        \"order_date\": [\"dimension\", \"grain\", \"date\"],\n        \"total_orders\": [\"measure\", \"count\"],\n        \"total_revenue\": [\"measure\", \"financial\"],\n        \"last_order_id\": [\"dimension\", \"identifier\"],\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    \"\"\"FULL model - rebuilds entire daily_sales table each run\"\"\"\n\n    query = \"\"\"\n    SELECT\n      CAST(order_date AS TIMESTAMP) AS order_date,\n      COUNT(order_id) AS total_orders,\n      SUM(total_amount) AS total_revenue,\n      MAX(order_id) AS last_order_id\n    FROM raw.raw_orders\n    GROUP BY order_date\n    ORDER BY order_date\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>How it works:</p> <p>The <code>@model</code> decorator captures your model's metadata (just like the <code>MODEL</code> DDL in SQL models). You specify column names and types in the <code>columns</code> argument, this is required because Vulcan needs to create the table before your function runs.</p> <p>Function signature: Your <code>execute</code> function receives:</p> <ul> <li> <p><code>context: ExecutionContext</code> - For running queries and getting time intervals</p> </li> <li> <p><code>start</code>, <code>end</code> - Time range for incremental models</p> </li> <li> <p><code>execution_time</code> - When the model is running</p> </li> <li> <p><code>**kwargs</code> - Any other runtime variables</p> </li> </ul> <p>Return types: You can return Pandas, PySpark, Bigframe, or Snowpark DataFrames. If your output is large, you can also use Python generators to return data in chunks for memory management.</p>"},{"location":"components/model/types/python_models/#model-specification","title":"<code>@model</code> Specification","text":"<p>The <code>@model</code> decorator accepts the same properties as SQL models, just use Python syntax instead of SQL DDL. <code>name</code>, <code>kind</code>, <code>cron</code>, <code>grains</code>, etc. They all work the same way.</p> <p>Python model <code>kind</code>s are specified with a Python dictionary containing the kind's name and arguments. All model kind arguments are listed in the models configuration reference page.</p> <pre><code>from vulcan import ModelKindName\n\n@model(\n    \"sales.daily_sales\",\n    kind=dict(\n        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,\n        time_column=\"order_date\",\n    ),\n)\n</code></pre> <p>All model kind properties are documented in the model configuration reference.</p> <p>Supported <code>kind</code> dictionary <code>name</code> values are:</p> <ul> <li> <p><code>ModelKindName.VIEW</code></p> </li> <li> <p><code>ModelKindName.FULL</code></p> </li> <li> <p><code>ModelKindName.SEED</code></p> </li> <li> <p><code>ModelKindName.INCREMENTAL_BY_TIME_RANGE</code></p> </li> <li> <p><code>ModelKindName.INCREMENTAL_BY_UNIQUE_KEY</code></p> </li> <li> <p><code>ModelKindName.INCREMENTAL_BY_PARTITION</code></p> </li> <li> <p><code>ModelKindName.SCD_TYPE_2_BY_TIME</code></p> </li> <li> <p><code>ModelKindName.SCD_TYPE_2_BY_COLUMN</code></p> </li> <li> <p><code>ModelKindName.EMBEDDED</code></p> </li> <li> <p><code>ModelKindName.CUSTOM</code></p> </li> <li> <p><code>ModelKindName.MANAGED</code></p> </li> <li> <p><code>ModelKindName.EXTERNAL</code></p> </li> </ul>"},{"location":"components/model/types/python_models/#execution-context","title":"Execution Context","text":"<p>Python models can do anything you want, but it is strongly recommended for all models to be idempotent. Python models can fetch data from upstream models or even data outside of Vulcan.</p> <p>Fetching data: Use <code>context.fetchdf()</code> to run SQL queries and get DataFrames:</p> <pre><code>df = context.fetchdf(\"SELECT * FROM vulcan_demo.products\")\n</code></pre> <p>Resolving table names: Use <code>context.resolve_table()</code> to get the correct table name for the current environment (handles dev/prod prefixes automatically):</p> <pre><code>table = context.resolve_table(\"vulcan_demo.products\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>Best practice: Make your models idempotent, running them multiple times should produce the same result. This makes debugging and restatements much easier.</p> <pre><code>df = context.fetchdf(\"SELECT * FROM vulcan_demo.products\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-prepost-statements","title":"Optional Pre/Post-Statements","text":"<p>You can run SQL commands before and after your Python model executes. This is useful for setting session parameters, creating indexes, or running data quality checks.</p> <p>Pre-statements: Run before your <code>execute</code> function Post-statements: Run after your <code>execute</code> function completes</p> <p>You can pass SQL strings, SQLGlot expressions, or macro calls as lists to <code>pre_statements</code> and <code>post_statements</code>.</p> <p>Concurrency</p> <p>Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings, UDFs, and temporary objects in pre-statements.</p> <p>Project-level defaults: You can also define pre/post-statements in <code>model_defaults</code> for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_statements\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    pre_statements=[\n        \"SET GLOBAL parameter = 'value';\",\n        exp.Cache(this=exp.table_(\"x\"), expression=exp.select(\"1\")),\n    ],\n    post_statements=[\"@CREATE_INDEX(@this_model, id)\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>The previous example's <code>post_statements</code> called user-defined Vulcan macro <code>@CREATE_INDEX(@this_model, id)</code>.</p> <p>We could define the <code>CREATE_INDEX</code> macro in the project's <code>macros</code> directory like this. The macro creates a table index on a single column, conditional on the runtime stage being <code>creating</code> (table creation time).</p> <pre><code>@macro()\ndef create_index(\n    evaluator: MacroEvaluator,\n    model_name: str,\n    column: str,\n):\n    if evaluator.runtime_stage == \"creating\":\n        return f\"CREATE INDEX idx ON {model_name}({column});\"\n    return None\n</code></pre> <p>Alternative approach: Instead of using the <code>@model</code> decorator's <code>pre_statements</code> and <code>post_statements</code>, you can execute SQL directly in your function using <code>context.engine_adapter.execute()</code>.</p> <p>Important: If you want post-statements to run after your function completes, you need to use <code>yield</code> instead of <code>return</code>. Post-statements specified after a <code>yield</code> will execute after the function finishes.</p> <p>This example function includes both pre- and post-statements:</p> <pre><code>def execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    # pre-statement\n    context.engine_adapter.execute(\"SET GLOBAL parameter = 'value';\")\n\n    # post-statement requires using `yield` instead of `return`\n    yield pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n\n    # post-statement\n    context.engine_adapter.execute(\"CREATE INDEX idx ON vulcan_demo.model_with_statements (id);\")\n</code></pre>"},{"location":"components/model/types/python_models/#optional-on-virtual-update-statements","title":"Optional On-Virtual-Update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.</p> <p>Common use case: Granting permissions on views so users can query them.</p> <p>You can set <code>on_virtual_update</code> in the <code>@model</code> decorator to a list of SQL strings, SQLGlot expressions, or macro calls.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project (including Python models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_grants\",\n    kind=\"full\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    on_virtual_update=[\"GRANT SELECT ON VIEW @this_model TO ROLE dev_role\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre> <p>Virtual Layer Resolution</p> <p>These statements run at the virtual layer, so table names resolve to view names, not physical table names. For example, in a <code>dev</code> environment, <code>vulcan_demo.model_with_grants</code> and <code>@this_model</code> resolve to <code>vulcan_demo__dev.model_with_grants</code> (the view), not the physical table.</p>"},{"location":"components/model/types/python_models/#dependencies","title":"Dependencies","text":"<p>In order to fetch data from an upstream model, you first get the table name using <code>context</code>'s <code>resolve_table</code> method. This returns the appropriate table name for the current runtime environment:</p> <pre><code>table = context.resolve_table(\"vulcan_demo.products\")\ndf = context.fetchdf(f\"SELECT * FROM {table}\")\n</code></pre> <p>The <code>resolve_table</code> method will automatically add the referenced model to the Python model's dependencies.</p> <p>The only other way to set dependencies of models in Python models is to define them explicitly in the <code>@model</code> decorator using the keyword <code>depends_on</code>. The dependencies defined in the model decorator take precedence over any dynamic references inside the function.</p> <pre><code>@model(\n    \"vulcan_demo.full_model_py\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"string\",\n        \"category\": \"string\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n    kind=dict(\n        name=ModelKindName.FULL,\n    ),\n    grains=[\"product_id\"],\n    depends_on=[\"vulcan_demo.products\", \"vulcan_demo.order_items\", \"vulcan_demo.orders\"],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # Dependencies are explicitly declared above\n    query = \"\"\"\n    SELECT \n        p.product_id,\n        p.name AS product_name,\n        p.category,\n        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales\n    FROM vulcan_demo.products p\n    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id\n    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id\n    GROUP BY p.product_id, p.name, p.category\n    ORDER BY total_sales DESC\n    \"\"\"\n\n    return context.fetchdf(query)\n</code></pre> <p>You can use global variables or blueprint variables in <code>resolve_table</code> calls. Here's how:</p> <pre><code>@model(\n    \"@schema_name.test_model2\",\n    kind=\"FULL\",\n    columns={\"id\": \"INT\"},\n)\ndef execute(context, **kwargs):\n    table = context.resolve_table(f\"{context.var('schema_name')}.test_model1\")\n    select_query = exp.select(\"*\").from_(table)\n    return context.fetchdf(select_query)\n</code></pre>"},{"location":"components/model/types/python_models/#returning-empty-dataframes","title":"Returning Empty DataFrames","text":"<p>Python models can't return empty DataFrames directly. If your model might return empty data, use <code>yield</code> instead of <code>return</code>:</p> <p>Why? This allows Vulcan to handle the empty case properly. If you <code>return</code> an empty DataFrame, Vulcan will error. If you <code>yield</code> an empty generator or conditionally yield, it works fine.</p> <pre><code>@model(\n    \"vulcan_demo.empty_df_model\"\n)\ndef execute(\n    context: ExecutionContext,\n) -&gt; pd.DataFrame:\n\n    [...code creating df...]\n\n    if df.empty:\n        yield from ()\n    else:\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#user-defined-variables","title":"User-defined variables","text":"<p>User-defined global variables can be accessed from within the Python model with the <code>context.var</code> method.</p> <p>For example, this model access the user-defined variables <code>var</code> and <code>var_with_default</code>. It specifies a default value of <code>default_value</code> if <code>variable_with_default</code> resolves to a missing value.</p> <pre><code>@model(\n    \"vulcan_demo.model_with_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    var_value = context.var(\"var\")\n    var_with_default_value = context.var(\"var_with_default\", \"default_value\")\n    ...\n</code></pre> <p>Alternatively, you can access global variables via <code>execute</code> function arguments, where the name of the argument corresponds to the name of a variable key.</p> <p>For example, this model specifies <code>my_var</code> as an argument to the <code>execute</code> method. The model code can reference the <code>my_var</code> object directly:</p> <pre><code>@model(\n    \"vulcan_demo.model_with_arg_vars\",\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    my_var: Optional[str] = None,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    my_var_plus1 = my_var + 1\n    ...\n</code></pre> <p>Make sure the argument has a default value if it's possible for the variable to be missing.</p> <p>Note that arguments must be specified explicitly - variables cannot be accessed using <code>kwargs</code>.</p>"},{"location":"components/model/types/python_models/#python-model-blueprinting","title":"Python Model Blueprinting","text":"<p>Python models can serve as templates for creating multiple models. This is called \"blueprinting\", you define one model template, and Vulcan creates multiple models from it.</p> <p>How it works: You parameterize the model name with a variable (using <code>@{variable}</code> syntax) and provide a list of mappings in <code>blueprints</code>. Vulcan creates one model for each mapping.</p> <p>Use case: When you have similar models that differ only by a few parameters (like different schemas, regions, or customers).</p> <p>Here's an example that creates two models:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"@{customer}.some_table\",\n    kind=\"FULL\",\n    blueprints=[\n        {\"customer\": \"customer1\", \"field_a\": \"x\", \"field_b\": \"y\"},\n        {\"customer\": \"customer2\", \"field_a\": \"z\", \"field_b\": \"w\"},\n    ],\n    columns={\n        \"field_a\": \"text\",\n        \"field_b\": \"text\",\n        \"customer\": \"text\",\n    },\n)\ndef entrypoint(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        {\n            \"field_a\": [context.blueprint_var(\"field_a\")],\n            \"field_b\": [context.blueprint_var(\"field_b\")],\n            \"customer\": [context.blueprint_var(\"customer\")],\n        }\n    )\n</code></pre> <p>Important: Notice the <code>@{customer}</code> syntax in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal). Learn more about this syntax here.</p> <p>Dynamic blueprints: You can generate blueprints dynamically using macros. This is handy when your blueprint list comes from external sources (like CSV files or API calls):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@gen_blueprints()\",  # Macro generates the list\n    ...\n)\n</code></pre> <p>For example, the definition of the <code>gen_blueprints</code> may look like this:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((customer := customer1, field_a := x, field_b := y),\"\n        \" (customer := customer2, field_a := z, field_b := w))\"\n    )\n</code></pre> <p>It's also possible to use the <code>@EACH</code> macro, combined with a global list variable (<code>@values</code>):</p> <pre><code>@model(\n    \"@{customer}.some_table\",\n    blueprints=\"@EACH(@values, x -&gt; (customer := schema_@x))\",\n    ...\n)\n...\n</code></pre>"},{"location":"components/model/types/python_models/#using-macros-in-model-properties","title":"Using Macros in Model Properties","text":"<p>Python models support macro variables in model properties, but there's a gotcha when macros appear inside strings.</p> <p>The issue: Cron expressions often use <code>@</code> (like <code>@daily</code>, <code>@hourly</code>), which conflicts with Vulcan's macro syntax.</p> <p>The solution: Wrap the entire expression in quotes and prefix with <code>@</code>:</p> <pre><code># Correct: Wrap the cron expression containing a macro variable\n@model(\n    \"vulcan_demo.scheduled_model\",\n    cron=\"@'*/@{mins} * * * *'\",  # Note the @'...' syntax\n    ...\n)\n\n# This also works with blueprint variables\n@model(\n    \"@{customer}.scheduled_model\",\n    cron=\"@'0 @{hour} * * *'\",\n    blueprints=[\n        {\"customer\": \"customer_1\", \"hour\": 2}, # Runs at 2 AM\n        {\"customer\": \"customer_2\", \"hour\": 8}, # Runs at 8 AM\n    ],\n    ...\n)\n</code></pre> <p>This is necessary because cron expressions often use <code>@</code> for aliases (like <code>@daily</code>, <code>@hourly</code>), which can conflict with Vulcan's macro syntax.</p>"},{"location":"components/model/types/python_models/#examples","title":"Examples","text":"<p>Here are some practical examples showing different ways to use Python models.</p>"},{"location":"components/model/types/python_models/#basic","title":"Basic","text":"<p>A simple Python model that returns a static Pandas DataFrame. All the metadata properties work the same as SQL models, just use Python syntax.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom sqlglot.expressions import to_column\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.basic_model\",\n    owner=\"data_team\",\n    cron=\"@daily\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n    },\n    column_descriptions={\n        \"id\": \"Unique ID\",\n        \"name\": \"Name corresponding to the ID\",\n    },\n    audits=[\n        (\"not_null\", {\"columns\": [to_column(\"id\")]}),\n    ],\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n\n    return pd.DataFrame([\n        {\"id\": 1, \"name\": \"name\"}\n    ])\n</code></pre>"},{"location":"components/model/types/python_models/#sql-query-and-pandas","title":"SQL Query and Pandas","text":"<p>A more realistic example: query upstream models, do some pandas processing, and return the result. This shows how you'd typically use Python models in practice:</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.sql_pandas_model\",\n    columns={\n        \"product_id\": \"int\",\n        \"product_name\": \"text\",\n        \"total_sales\": \"decimal(10,2)\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's name and register it as a dependency\n    products_table = context.resolve_table(\"vulcan_demo.products\")\n    order_items_table = context.resolve_table(\"vulcan_demo.order_items\")\n\n    # fetch data from the model as a pandas DataFrame\n    df = context.fetchdf(f\"\"\"\n        SELECT \n            p.product_id,\n            p.name AS product_name,\n            SUM(oi.quantity * oi.unit_price) as total_sales\n        FROM {products_table} p\n        LEFT JOIN {order_items_table} oi ON p.product_id = oi.product_id\n        GROUP BY p.product_id, p.name\n    \"\"\")\n\n    # do some pandas stuff\n    df['total_sales'] = df['total_sales'].fillna(0)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#pyspark","title":"PySpark","text":"<p>If you're using Spark, use the PySpark DataFrame API instead of Pandas. PySpark DataFrames compute in a distributed fashion (across your Spark cluster), which is much faster for large datasets.</p> <p>Why PySpark over Pandas: Pandas loads everything into memory on a single machine. PySpark distributes the work across your cluster, so you can handle much larger datasets.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom pyspark.sql import DataFrame, functions\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.pyspark_model\",\n    columns={\n        \"customer_id\": \"int\",\n        \"customer_name\": \"text\",\n        \"region\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # get the upstream model's name and register it as a dependency\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    # use the spark DataFrame api to add the region column\n    df = context.spark.table(table).withColumn(\"region\", functions.lit(\"North\"))\n\n    # returns the pyspark DataFrame directly, so no data is computed locally\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#snowpark","title":"Snowpark","text":"<p>If you're using Snowflake, use the Snowpark DataFrame API. Like PySpark, Snowpark DataFrames compute on Snowflake's servers (not locally), which is much more efficient.</p> <p>Why Snowpark over Pandas: All computation happens in Snowflake, so you're not moving data to your local machine. Faster, cheaper, and can handle huge datasets.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom snowflake.snowpark.dataframe import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n@model(\n    \"vulcan_demo.snowpark_model\",\n    columns={\n        \"id\": \"int\",\n        \"name\": \"text\",\n        \"country\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # returns the snowpark DataFrame directly, so no data is computed locally\n    df = context.snowpark.create_dataframe([[1, \"a\", \"usa\"], [2, \"b\", \"cad\"]], schema=[\"id\", \"name\", \"country\"])\n    df = df.filter(df.id &gt; 1)\n    return df\n</code></pre>"},{"location":"components/model/types/python_models/#bigframe","title":"Bigframe","text":"<p>If you're using BigQuery, use the Bigframe DataFrame API. Bigframe looks like Pandas but runs everything in BigQuery.</p> <p>Why Bigframe over Pandas: All computation happens in BigQuery, so you get BigQuery's scale and performance. Plus, you can use BigQuery remote functions (like in the example below) for custom Python logic.</p> <pre><code>import typing as t\nfrom datetime import datetime\n\nfrom bigframes.pandas import DataFrame\n\nfrom vulcan import ExecutionContext, model\n\n\ndef get_bucket(num: int):\n    if not num:\n        return \"NA\"\n    boundary = 10\n    return \"at_or_above_10\" if num &gt;= boundary else \"below_10\"\n\n\n@model(\n    \"vulcan_demo.bigframe_model\",\n    columns={\n        \"title\": \"text\",\n        \"views\": \"int\",\n        \"bucket\": \"text\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; DataFrame:\n    # Create a remote function to be used in the Bigframe DataFrame\n    remote_get_bucket = context.bigframe.remote_function([int], str)(get_bucket)\n\n    # Returns the Bigframe DataFrame handle, no data is computed locally\n    df = context.bigframe.read_gbq(\"bigquery-samples.wikipedia_pageviews.200809h\")\n\n    df = (\n        # This runs entirely on the BigQuery engine lazily\n        df[df.title.str.contains(r\"[Gg]oogle\")]\n        .groupby([\"title\"], as_index=False)[\"views\"]\n        .sum(numeric_only=True)\n        .sort_values(\"views\", ascending=False)\n    )\n\n    return df.assign(bucket=df[\"views\"].apply(remote_get_bucket))\n</code></pre>"},{"location":"components/model/types/python_models/#batching","title":"Batching","text":"<p>If your Python model outputs a huge DataFrame and you can't use Spark/Bigframe/Snowpark, you can batch the output using Python generators.</p> <p>The problem: With Pandas, everything loads into memory. If your output is too large, you'll run out of memory.</p> <p>The solution: Use <code>yield</code> to return DataFrames in chunks. Vulcan processes them one at a time, so you never have more than one chunk in memory at once.</p> <p>Here's how you'd do it:</p> <pre><code>@model(\n    \"vulcan_demo.batching_model\",\n    columns={\n        \"customer_id\": \"int\",\n    },\n)\ndef execute(\n    context: ExecutionContext,\n    start: datetime,\n    end: datetime,\n    execution_time: datetime,\n    **kwargs: t.Any,\n) -&gt; pd.DataFrame:\n    # get the upstream model's table name\n    table = context.resolve_table(\"vulcan_demo.customers\")\n\n    for i in range(3):\n        # run 3 queries to get chunks of data and not run out of memory\n        df = context.fetchdf(f\"SELECT customer_id from {table} WHERE customer_id = {i}\")\n        yield df\n</code></pre>"},{"location":"components/model/types/python_models/#serialization","title":"Serialization","text":"<p>Vulcan executes Python models locally (wherever Vulcan is running) using a custom serialization framework. This means your Python code runs on your machine or CI/CD environment, not in the database.</p> <p>Why this matters: You have full access to Python libraries, can make API calls, do ML processing, etc. The database just receives the final DataFrame.</p>"},{"location":"components/model/types/sql_models/","title":"SQL","text":""},{"location":"components/model/types/sql_models/#sql","title":"SQL","text":"<p>SQL models are the most common type of model you'll write. You can define them using SQL directly, or use Python to generate SQL dynamically.</p> <p>SQL models are simple, powerful, and work with any SQL database. Most of your data transformations will be SQL models.</p>"},{"location":"components/model/types/sql_models/#sql-based-definition","title":"SQL-Based Definition","text":"<p>SQL-based models are the most common type. They use regular SQL with additional Vulcan features.</p> <p>Structure: A SQL model file has these parts (in order):</p> <ol> <li>The <code>MODEL</code> DDL (metadata and configuration)</li> <li>Optional pre-statements (setup SQL)</li> <li>A single query (your transformation logic)</li> <li>Optional post-statements (cleanup/optimization SQL)</li> <li>Optional on-virtual-update statements (view permissions, etc.)</li> </ol> <p>Creating a SQL model: Add a <code>.sql</code> file to your <code>models/</code> directory (or a subdirectory). The filename doesn't matter to Vulcan, but it's conventional to name it after your model. For example, <code>sales.daily_sales</code> \u2192 <code>daily_sales.sql</code>.</p>"},{"location":"components/model/types/sql_models/#example","title":"Example","text":"<p>Here's a simple SQL model to get you started:</p> <pre><code>-- This is the MODEL DDL, where you specify model metadata and configuration information.\nMODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics', 'analytics.sales_summary'),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales transactions',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  )\n);\n\n/*\n  This is the single query that defines the model's logic.\n  Although it is not required, it is considered best practice to explicitly\n  specify the type for each one of the model's columns through casting.\n*/\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"components/model/types/sql_models/#model-ddl","title":"<code>MODEL</code> DDL","text":"<p>The <code>MODEL</code> DDL is where you define your model's metadata, name, kind, schedule, owner, and more. It must be the first statement in your SQL file.</p> <p>The <code>MODEL</code> DDL tells Vulcan everything it needs to know about your model. For a complete list of all available properties, see the Model Properties documentation.</p>"},{"location":"components/model/types/sql_models/#optional-prepost-statements","title":"Optional Pre/Post-Statements","text":"<p>Pre-statements run before your query, post-statements run after. Use them for setup, cleanup, and optimization tasks.</p> <p>Common use cases:</p> <ul> <li> <p>Pre-statements: Set session parameters, load UDFs, cache tables</p> </li> <li> <p>Post-statements: Create indexes, run data quality checks, set retention policies</p> </li> </ul> <p>Important: Pre/post-statements must end with semicolons. If you have post-statements, your main query must also end with a semicolon (so Vulcan knows where the query ends).</p> <p>Concurrency</p> <p>Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings and temporary objects.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\n-- Pre-statement: Cache a table for use in the query\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\n-- The model query (must end with semi-colon when post-statements are present)\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue\nFROM raw.raw_orders\nGROUP BY order_date;\n\n-- Post-statement: Clean up the cached table\nUNCACHE TABLE countries;\n</code></pre> <p>Project-level defaults: You can define pre/post-statements in <code>model_defaults</code> for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the model configuration reference.</p> <p>Statements Run Twice</p> <p>Pre/post-statements are evaluated twice: when a model's table is created and when its query logic is evaluated. Executing statements more than once can have unintended side-effects, so you can conditionally execute them based on Vulcan's runtime stage.</p> <pre><code>**Solution:** Use conditional execution with `@IF` and `@runtime_stage` to control when statements run. For example, only run a post-statement when the query is actually being evaluated:\n</code></pre> <p>We can condition the post-statement to only run after the model query is evaluated using the <code>@IF</code> macro operator and <code>@runtime_stage</code> macro variable like this:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nCACHE TABLE countries AS SELECT * FROM raw.countries;\n\nSELECT\n  order_date::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\n@IF(\n  @runtime_stage = 'evaluating',\n  UNCACHE TABLE countries\n);\n</code></pre> <p>Important: The SQL command inside <code>@IF()</code> doesn't end with a semicolon. The semicolon goes after the <code>@IF()</code> macro's closing parenthesis.</p>"},{"location":"components/model/types/sql_models/#optional-on-virtual-update-statements","title":"Optional On-Virtual-Update Statements","text":"<p>On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view is set up.</p> <p>Common use case: Granting permissions on views so users can query them.</p> <p>Project-level defaults: You can also define on-virtual-update statements at the project level using <code>model_defaults</code> in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the model configuration reference.</p> <p>Syntax: Wrap these statements in <code>ON_VIRTUAL_UPDATE_BEGIN;</code> ... <code>ON_VIRTUAL_UPDATE_END;</code> blocks:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL\n);\n\nSELECT\n  order_date::TIMESTAMP,\n  COUNT(order_id)::INTEGER AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date;\n\nON_VIRTUAL_UPDATE_BEGIN;\nGRANT SELECT ON VIEW @this_model TO ROLE role_name;\nJINJA_STATEMENT_BEGIN;\nGRANT SELECT ON VIEW {{ this_model }} TO ROLE admin;\nJINJA_END;\nON_VIRTUAL_UPDATE_END;\n</code></pre> <p>Jinja support: You can use Jinja expressions in these statements. Just wrap them in <code>JINJA_STATEMENT_BEGIN;</code> ... <code>JINJA_END;</code> blocks (as shown in the example above).</p> <p>Virtual Layer Resolution</p> <p>These statements run at the virtual layer, so table names resolve to view names, not physical table names. In a <code>dev</code> environment, <code>sales.daily_sales</code> and <code>@this_model</code> resolve to <code>sales__dev.daily_sales</code> (the view), not the physical table.</p>"},{"location":"components/model/types/sql_models/#the-model-query","title":"The Model Query","text":"<p>Your model must contain a standalone query. This can be:</p> <ul> <li> <p>A single <code>SELECT</code> statement</p> </li> <li> <p>Multiple <code>SELECT</code> statements combined with <code>UNION</code>, <code>INTERSECT</code>, or <code>EXCEPT</code></p> </li> </ul> <p>The result of this query becomes your model's table or view data. Pretty straightforward!</p>"},{"location":"components/model/types/sql_models/#sql-model-blueprinting","title":"SQL Model Blueprinting","text":"<p>SQL models can serve as templates for creating multiple models. This is called \"blueprinting\", define one template, get multiple models.</p> <p>How it works: Parameterize your model name with a variable (using <code>@{variable}</code> syntax) and provide a list of mappings in <code>blueprints</code>. Vulcan creates one model for each mapping.</p> <p>Use case: When you have similar models that differ only by parameters (like different regions, schemas, or customers).</p> <p>Here's an example that creates four models from one template:</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints (\n    (region := 'north'),\n    (region := 'south'),\n    (region := 'east'),\n    (region := 'west')\n  ),\n  grains region_id\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre> <p>Vulcan creates these four models from that template:</p> <pre><code>-- This uses the first variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__north,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('north')\n\n-- This uses the second variable mapping\nMODEL (\n  name vulcan_demo.fct_daily_sales__south,\n  kind VIEW\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\nWHERE\n  LOWER(region_name) = LOWER('south')\n</code></pre> <p>Important syntax: Notice <code>@{region}</code> in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal).</p> <p>You can see the different behavior in the WHERE clause. <code>@region</code> (without braces) is resolved to the string literal <code>'north'</code> (with single quotes) because the blueprint value is quoted. Learn more about the curly brace syntax here.</p> <p>Dynamic blueprints: You can generate blueprints using macros. This is handy when your blueprint list comes from external sources (CSV files, APIs, etc.):</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  blueprints @gen_blueprints(),  -- Macro generates the list\n  ...\n);\n</code></pre> <p>Here's how you might define the macro:</p> <pre><code>from vulcan import macro\n\n@macro()\ndef gen_blueprints(evaluator):\n    return (\n        \"((region := 'north'),\"\n        \" (region := 'south'),\"\n        \" (region := 'east'),\"\n        \" (region := 'west'))\"\n    )\n</code></pre> <p>You can also use the <code>@EACH</code> macro with a global list variable:</p> <pre><code>MODEL (\n  name vulcan_demo.fct_daily_sales__@{region},\n  kind VIEW,\n  blueprints @EACH(@values, x -&gt; (region := @x)),\n);\n\nSELECT\n  *\nFROM vulcan_demo.fct_daily_sales\n@WHERE(TRUE)\n  LOWER(region_name) = LOWER(@region)\n</code></pre>"},{"location":"components/model/types/sql_models/#python-based-definition","title":"Python-Based Definition","text":"<p>You can also define SQL models using Python! This is useful when:</p> <ul> <li> <p>Your query is too complex for clean SQL</p> </li> <li> <p>You need heavy dynamic logic (would require lots of macros)</p> </li> <li> <p>You want to generate SQL programmatically</p> </li> </ul> <p>How it works: You write Python code that generates SQL, and Vulcan executes it. You still get SQL models (they run SQL queries), but you write them in Python.</p> <p>For the complete guide on Python-based SQL models, including the <code>@model</code> decorator, execution context, and examples, see the Python Models page.</p>"},{"location":"components/model/types/sql_models/#automatic-dependencies","title":"Automatic Dependencies","text":"<p>One of Vulcan's superpowers: it parses your SQL and automatically figures out dependencies. No need to manually specify what your model depends on, Vulcan just knows!</p> <p>How it works: Vulcan analyzes your <code>FROM</code> and <code>JOIN</code> clauses and builds a dependency graph. When you run <code>vulcan plan</code>, it ensures upstream models run first.</p> <p>Example: This query automatically depends on <code>raw.raw_orders</code>:</p> <pre><code>SELECT order_date, COUNT(order_id) AS total_orders\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre> <p>Vulcan will make sure <code>raw.raw_orders</code> runs before this model. Pretty neat!</p> <p>External dependencies: If you reference tables that aren't Vulcan models, Vulcan can handle them too, either implicitly (through execution order) or via signals.</p> <p>Manual dependencies: Sometimes you need to add extra dependencies manually (maybe a hidden dependency or a macro that references another model). Use the <code>depends_on</code> property in your <code>MODEL</code> DDL for that.</p>"},{"location":"components/model/types/sql_models/#conventions","title":"Conventions","text":"<p>Vulcan follows some conventions to keep things consistent and reliable. Here are the key ones:</p>"},{"location":"components/model/types/sql_models/#explicit-type-casting","title":"Explicit Type Casting","text":"<p>Vulcan encourages explicit type casting for all columns. This helps Vulcan understand your data types and prevents incorrect inference.</p> <p>Format: Use <code>column_name::data_type</code> syntax (works in any SQL dialect):</p> <pre><code>SELECT\n  order_date::DATE AS order_date,\n  total_orders::INTEGER AS total_orders,\n  revenue::DECIMAL(10,2) AS revenue\n</code></pre> <p>Why this matters: Explicit types make your models more predictable and help Vulcan optimize queries better.</p>"},{"location":"components/model/types/sql_models/#explicit-selects","title":"Explicit SELECTs","text":"<p>Avoid <code>SELECT *</code> when possible. It's convenient, but dangerous, if an upstream source adds or removes columns, your model's output changes unexpectedly.</p> <p>Best practice: List every column you need explicitly. If you're querying external sources, use <code>create_external_models</code> to capture their schema, or define them as external models.</p> <p>Why avoid <code>SELECT *</code> on external sources: It prevents Vulcan from optimizing queries and determining column-level lineage. Define external models instead!</p>"},{"location":"components/model/types/sql_models/#encoding","title":"Encoding","text":"<p>SQL model files must be UTF-8 encoded. Using other encodings can cause parsing errors or unexpected behavior.</p>"},{"location":"components/model/types/sql_models/#transpilation","title":"Transpilation","text":"<p>Vulcan uses SQLGlot to parse and transpile SQL. This gives you some superpowers:</p> <p>Write in any dialect, run on any engine: Write PostgreSQL-style SQL, and Vulcan converts it to BigQuery. Or write Snowflake SQL and run it on Spark. Pretty cool!</p> <p>Use advanced syntax: You can use features from one dialect even if your engine doesn't support them. For example, <code>x::int</code> (PostgreSQL syntax) works even on engines that only support <code>CAST(x AS INT)</code>. SQLGlot handles the conversion.</p> <p>Formatting flexibility: Trailing commas, extra whitespace, minor formatting differences, SQLGlot normalizes them all. Write SQL however you like, and Vulcan makes it consistent.</p>"},{"location":"components/model/types/sql_models/#macros","title":"Macros","text":"<p>Standard SQL is powerful, but real-world data pipelines need dynamic components. Date filters that change each run, conditional logic, reusable query patterns, macros give you all of this.</p> <p>Macro variables: Vulcan provides automatic date/time variables for incremental models. Use <code>@start_date</code>, <code>@end_date</code>, <code>@start_ds</code>, <code>@end_ds</code> and Vulcan fills them in with the current time range. No more hardcoding dates!</p> <p>Custom macros: For complex logic or reusable patterns, Vulcan supports a powerful macro syntax and Jinja templates. Write macros once, use them everywhere.</p> <p>Why macros matter: They make your SQL more maintainable. Instead of copy-pasting complex logic, define it once as a macro and reuse it. Your queries stay clean and readable.</p> <p>Learn more about macros in the Macros documentation.</p>"},{"location":"components/semantics/business_metrics/","title":"Business Metrics","text":""},{"location":"components/semantics/business_metrics/#business-metrics","title":"Business Metrics","text":"<p>Business metrics are where your semantic layer really shines. They combine measures (the calculations) with dimensions (the attributes) and time (the when) to create complete analytical definitions that are ready for time-series analysis.</p> <p>Semantic models provide the building blocks (measures, dimensions, joins), and business metrics combine those blocks into something you can analyze over time. They're pre-configured for dashboards, reports, and APIs, no SQL required.</p>"},{"location":"components/semantics/business_metrics/#what-are-business-metrics","title":"What are business metrics?","text":"<p>Business metrics are complete analytical definitions that:</p> <ul> <li> <p>Combine measures with time: Let you analyze trends at different time granularities (daily, weekly, monthly, etc.)</p> </li> <li> <p>Include dimensions: Enable slicing and dicing by business attributes (customer tier, region, product category, etc.)</p> </li> <li> <p>Ready for analysis: Pre-configured so they can power dashboards, reports, and APIs directly</p> </li> <li> <p>Examples: <code>monthly_revenue_by_tier</code>, <code>daily_active_users</code>, <code>customer_acquisition_trend</code></p> </li> </ul> <p>They're the bridge between your technical data models and the business questions people actually want to answer.</p>"},{"location":"components/semantics/business_metrics/#basic-structure","title":"Basic structure","text":"<p>A business metric brings together three things:</p> <ul> <li> <p>Measure: The calculation you want to perform (like <code>orders.total_revenue</code>)</p> </li> <li> <p>Time: The time dimension for your analysis (like <code>orders.order_date</code>)</p> </li> <li> <p>Dimensions: Optional attributes for grouping and filtering (like <code>customers.customer_tier</code>)</p> </li> </ul> <p>Here's the simplest possible metric:</p> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue      # Which measure to calculate\n    time: orders.order_date            # Time dimension for analysis\n    description: \"Monthly revenue trends\"\n</code></pre> <p>That's it! This metric is now ready to be queried at any time granularity you want.</p>"},{"location":"components/semantics/business_metrics/#simple-metric","title":"Simple metric","text":"<p>Let's start with the basics, a metric that just has a measure and time:</p> <pre><code>metrics:\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n</code></pre> <p>Even though it's called <code>daily_revenue</code>, you're not locked into daily granularity. You can query this same metric at different time intervals (day, week, month, quarter, year) without redefining it. The metric definition stays the same; you just change the granularity when you query it.</p>"},{"location":"components/semantics/business_metrics/#metric-with-dimensions","title":"Metric with dimensions","text":"<p>Add dimensions to enable slicing and grouping:</p> <pre><code>metrics:\n  revenue_by_tier:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier      # Group by tier\n\n      - customers.country            # And country\n    description: \"Revenue trends by customer tier and country\"\n</code></pre> <p>Now you can answer questions like:</p> <ul> <li> <p>What's our revenue by tier over time?</p> </li> <li> <p>How does revenue vary by country?</p> </li> <li> <p>What's the revenue breakdown by tier and country together?</p> </li> </ul> <p>The dimensions give you flexibility to analyze the metric from different angles.</p>"},{"location":"components/semantics/business_metrics/#cross-model-metrics","title":"Cross-model metrics","text":"<p>You're not limited to one model. Combine measures and dimensions from multiple models:</p> <pre><code>metrics:\n  product_revenue_by_customer_segment:\n    measure: orders.total_revenue      # From orders\n    time: orders.order_date            # From orders\n    dimensions:\n      - products.category              # From products\n\n      - products.brand\n\n      - customers.customer_tier        # From customers\n\n      - customers.region\n    description: \"Product revenue segmented by customer demographics\"\n</code></pre> <p>This metric pulls the measure from <code>orders</code>, time from <code>orders</code>, product dimensions from <code>products</code>, and customer dimensions from <code>customers</code>. As long as you've defined the proper joins between these semantic models, Vulcan will handle the cross-model logic for you.</p> <p>Important: Make sure your semantic models have the right joins defined, or cross-model metrics won't work.</p>"},{"location":"components/semantics/business_metrics/#reference-format","title":"Reference format","text":"<p>Always use dot notation with semantic model aliases when referencing measures, dimensions, and time:</p> <pre><code># Good: Use aliases\nmeasure: orders.total_revenue     # alias.measure_name\ntime: orders.order_date           # alias.column_name\ndimensions:\n  - customers.customer_tier       # alias.column_name\n\n# Bad: Don't use physical names\nmeasure: analytics.fact_orders.revenue\ntime: order_date  # Missing alias\n</code></pre> <p>The dot notation (<code>orders.total_revenue</code>) tells Vulcan which semantic model to look in and what to reference. Physical table names won't work here, you need the semantic aliases.</p>"},{"location":"components/semantics/business_metrics/#time-granularity","title":"Time granularity","text":"<p>Define metrics once, then query them at any time granularity:</p> <pre><code>metrics:\n  revenue_trends:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Revenue at any time granularity\"\n</code></pre> <p>The same metric can be queried with different granularities:</p> <ul> <li> <p>Daily: <code>granularity=day</code></p> </li> <li> <p>Weekly: <code>granularity=week</code></p> </li> <li> <p>Monthly: <code>granularity=month</code></p> </li> <li> <p>Quarterly: <code>granularity=quarter</code></p> </li> <li> <p>Yearly: <code>granularity=year</code></p> </li> </ul> <p>You don't need separate metric definitions for each granularity, just change the query parameter.</p>"},{"location":"components/semantics/business_metrics/#complete-example","title":"Complete example","text":"<p>Here's a more complete example showing different types of metrics:</p> <pre><code>metrics:\n  # Simple revenue metric\n  daily_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    description: \"Daily revenue trends\"\n    tags: [revenue, financial, kpi]\n\n  # Customer acquisition\n  customer_acquisition_trend:\n    measure: customers.new_signups\n    time: customers.signup_date\n    dimensions:\n      - customers.signup_channel\n\n      - customers.customer_tier\n\n      - customers.country\n    description: \"Customer acquisition by channel, tier, and geography\"\n    tags: [acquisition, growth, customer]\n\n  # Cross-model metric\n  product_performance:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - products.category\n\n      - products.brand\n\n      - customers.customer_tier\n    description: \"Product revenue by category, brand, and customer segment\"\n    tags: [revenue, products, segmentation]\n</code></pre> <p>Notice how each metric has a clear purpose, good description, and relevant tags. The tags help organize and discover metrics later.</p>"},{"location":"components/semantics/business_metrics/#benefits","title":"Benefits","text":""},{"location":"components/semantics/business_metrics/#time-series-analysis","title":"Time-series analysis","text":"<p>Metrics are built for analyzing trends over time:</p> <ul> <li> <p>Flexible granularity: Query the same metric at different time intervals without redefinition</p> </li> <li> <p>Consistent definitions: Same calculation logic applies across all time periods</p> </li> <li> <p>Trend analysis: Built-in support for comparing periods (month-over-month, year-over-year, etc.)</p> </li> </ul>"},{"location":"components/semantics/business_metrics/#self-service-analytics","title":"Self-service analytics","text":"<p>Business users can query metrics without writing SQL:</p> <ul> <li> <p>Simple API: Query metrics by name with a time range and dimensions</p> </li> <li> <p>Consistent results: Same metric definition is used everywhere, so everyone gets the same answer</p> </li> <li> <p>No SQL required: Complex joins and aggregations are abstracted away</p> </li> </ul>"},{"location":"components/semantics/business_metrics/#single-source-of-truth","title":"Single source of truth","text":"<p>Centralized metric definitions mean:</p> <ul> <li> <p>Define once: Create metric definitions in YAML files</p> </li> <li> <p>Use everywhere: Same metrics power dashboards, reports, and APIs</p> </li> <li> <p>Version controlled: Metric definitions live alongside your code, so changes are tracked</p> </li> </ul>"},{"location":"components/semantics/business_metrics/#best-practices","title":"Best practices","text":""},{"location":"components/semantics/business_metrics/#descriptive-names","title":"Descriptive names","text":"<p>Make your metric names self-explanatory:</p> <pre><code># Good: Self-explanatory\nmetrics:\n  monthly_revenue_by_tier: ...\n  daily_active_users: ...\n\n# Bad: Vague\nmetrics:\n  metric_1: ...\n  rev: ...\n</code></pre> <p>Good names make it obvious what the metric measures and how it's broken down.</p>"},{"location":"components/semantics/business_metrics/#include-essential-dimensions","title":"Include essential dimensions","text":"<p>Think about what business questions people will want to answer, and include those dimensions:</p> <pre><code># Good: Key business dimensions\nmetrics:\n  revenue_analysis:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n\n      - customers.region\n\n      - products.category\n\n# Too few: Limited analysis\nmetrics:\n  revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    # Missing dimensions - can't slice and dice!\n</code></pre> <p>Dimensions are what make metrics useful. Without them, you can only see the overall trend, not the breakdowns that drive business decisions.</p>"},{"location":"components/semantics/business_metrics/#document-business-context","title":"Document business context","text":"<p>Add descriptions and metadata to help people understand what the metric means:</p> <pre><code>metrics:\n  net_revenue_retention:\n    measure: subscriptions.nrr\n    time: subscriptions.cohort_month\n    description: \"Net Revenue Retention: expansion minus churn\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation: \"(Starting MRR + Expansion - Churn) / Starting MRR\"\n      benchmark: \"&gt;110% is good for SaaS\"\n</code></pre> <p>The <code>meta</code> section is perfect for business context, calculation details, benchmarks, and ownership information. This helps people understand not just what the metric is, but what it means and how to interpret it.</p>"},{"location":"components/semantics/business_metrics/#integration-with-semantic-models","title":"Integration with semantic models","text":"<p>Metrics build on top of semantic models:</p> <ol> <li>Semantic models define measures, dimensions, and joins</li> <li>Metrics combine these components with time for analysis</li> <li>APIs expose metrics for querying and visualization</li> </ol> <p>The semantic layer provides the foundation (the building blocks), and metrics add the time-series analytical capabilities (the finished product).</p>"},{"location":"components/semantics/business_metrics/#next-steps","title":"Next steps","text":"<ul> <li> <p>Learn about Semantic Models that provide the foundation for metrics</p> </li> <li> <p>See the Semantics Overview for the complete picture</p> </li> <li> <p>Explore metric definitions in your project's <code>semantics/</code> directory</p> </li> </ul>"},{"location":"components/semantics/models/","title":"Semantic Models","text":""},{"location":"components/semantics/models/#semantic-models","title":"Semantic Models","text":"<p>Semantic models are your bridge between technical data structures and business understanding. They take your physical Vulcan models (the tables and columns in your database) and map them to business concepts that make sense to analysts, product managers, and other non-technical users.</p> <p>Semantic models are a translation layer. Your database might have tables named <code>dim_customers</code> or <code>fact_orders</code> (technical naming), but your semantic layer can expose them as <code>customers</code> and <code>orders</code> (business-friendly naming). Semantic models define what you can do with the data: dimensions for grouping, measures for calculations, segments for filtering, and joins for combining models.</p>"},{"location":"components/semantics/models/#what-are-semantic-models","title":"What are semantic models?","text":"<p>Semantic models bridge the gap between technical table structures and business understanding:</p> <ul> <li> <p>Reference physical models: Each semantic model references a Vulcan model defined in your <code>models/</code> directory</p> </li> <li> <p>Provide business aliases: Hide technical naming (like <code>dim_customers</code> or <code>fact_orders</code>) behind consumer-friendly names</p> </li> <li> <p>Expose analytical capabilities: Define dimensions, measures, segments, and joins for each model</p> </li> </ul> <p>They're the foundation of your semantic layer, everything else (business metrics, semantic queries) builds on top of semantic models.</p>"},{"location":"components/semantics/models/#basic-structure","title":"Basic structure","text":"<p>A semantic model maps a physical Vulcan model to a semantic representation. Here's the basic structure:</p> <pre><code>models:\n  analytics.customers:  # Physical model name (dictionary key)\n    alias: customers     # Business-friendly semantic alias\n    description: \"Customer master data\"\n    dimensions: {...}    # Optional: control which columns are exposed\n    measures: {...}      # Optional: aggregated calculations\n    segments: {...}      # Optional: reusable filter conditions\n    joins: {...}         # Optional: relationships to other models\n</code></pre> <p>The physical model name (<code>analytics.customers</code>) is the key, and everything else defines how it appears in the semantic layer.</p>"},{"location":"components/semantics/models/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes you use for grouping and filtering. They answer \"by what?\" questions, like \"revenue by customer tier\" or \"orders by country.\"</p> <p>The good news: All columns from your Vulcan model automatically become dimensions. You don't have to define them manually unless you want to control which ones are exposed or add enhancements.</p> <p>Here's how you can control dimensions:</p> <pre><code># All columns from analytics.customers automatically become dimensions:\n# - customers.customer_id\n# - customers.customer_tier\n# - customers.signup_date\n# - customers.country\n\n# You can control which columns are exposed:\ndimensions:\n  excludes:\n    - password_hash       # Hide sensitive data\n\n    - internal_notes\n\n  # Enhance dimensions with additional capabilities:\n  enhancements:\n    - name: start_date\n      granularities:\n        - name: monthly\n          interval: \"1 month\"\n          description: \"Monthly subscription cohorts\"\n        - name: quarterly\n          interval: \"3 months\"\n          description: \"Quarterly cohorts\"\n</code></pre> <p>Use <code>excludes</code> to hide sensitive or internal columns. Use <code>enhancements</code> to add time granularities for cohort analysis, useful for subscription or signup dates.</p>"},{"location":"components/semantics/models/#measures","title":"Measures","text":"<p>Measures are aggregated calculations that answer \"how much?\" or \"how many?\" questions. They're what you use to calculate totals, averages, counts, and other aggregations.</p> <p>You define measures using SQL expressions with aggregations like <code>SUM()</code>, <code>COUNT()</code>, <code>AVG()</code>, etc.:</p> <pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{customers.amount}\"\n    description: \"Total revenue from all orders\"\n    format: currency\n\n  avg_order_value:\n    type: number\n    expression: \"SUM({customers.total_revenue}) / NULLIF(COUNT(*), 0)\"\n    format: currency\n    description: \"Average order value\"\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"\n    filters:\n      - \"{customers.status} = 'active'\"\n    description: \"Number of active customers\"\n</code></pre> <p>Notice the curly braces around column references like <code>{customers.amount}</code>? That's the semantic reference syntax. We'll talk more about that in the best practices section.</p> <p>Measures can have filters (like <code>active_customers</code> above), which let you calculate metrics on subsets of data. They can also have formatting hints (like <code>currency</code>) to help visualization tools display them correctly.</p>"},{"location":"components/semantics/models/#segments","title":"Segments","text":"<p>Segments are reusable filter conditions that answer \"which ones?\" questions. They define meaningful subsets of your data that you can use across multiple queries and metrics.</p> <p>Segments are saved filters. Instead of writing <code>WHERE status = 'active'</code> every time, you define an <code>active_customers</code> segment once and reuse it:</p> <pre><code>segments:\n  active_customers:\n    expression: \"{customers.status} = 'active'\"\n    description: \"Customers with active subscriptions\"\n\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"\n    description: \"Customers who spent over $10K\"\n\n  recent_signups:\n    expression: \"{customers.signup_date} &gt;= CURRENT_DATE - INTERVAL '30 days'\"\n    description: \"Customers who signed up in last 30 days\"\n</code></pre> <p>Segments make your semantic layer more consistent, everyone uses the same definition of \"active customers\" or \"high value,\" so there's no confusion about what those terms mean.</p>"},{"location":"components/semantics/models/#joins","title":"Joins","text":"<p>Joins define relationships between semantic models. They're what enable cross-model analysis, like combining order data with customer data or product data.</p> <p>You define the relationship type (<code>one_to_one</code>, <code>one_to_many</code>, <code>many_to_one</code>) and the join expression:</p> <pre><code>joins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"\n    description: \"Order's customer\"\n\n  products:\n    type: many_to_one\n    expression: \"{orders.product_id} = {products.product_id}\"\n    description: \"Ordered product\"\n</code></pre> <p>The relationship type helps Vulcan understand the cardinality, which is important for aggregations and preventing double-counting. The expression is the actual SQL join condition, using semantic references with curly braces.</p>"},{"location":"components/semantics/models/#cross-model-analysis","title":"Cross-model analysis","text":"<p>Once you've defined joins, you can reference columns and measures from other models in your current model's definitions. This is where semantic models really shine, you can build complex analytical definitions that span multiple models.</p>"},{"location":"components/semantics/models/#referencing-joined-model-fields","title":"Referencing joined model fields","text":"<p>You can use columns from joined models in measure expressions and filters:</p> <pre><code>measures:\n  enterprise_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    filters:\n      - \"{customers.customer_tier} = 'Enterprise'\"\n    description: \"Revenue from Enterprise customers\"\n</code></pre> <p>Even though <code>enterprise_revenue</code> is defined on the <code>orders</code> model, it filters by <code>customers.customer_tier</code> from the joined <code>customers</code> model. Vulcan handles the join logic automatically.</p>"},{"location":"components/semantics/models/#complete-example","title":"Complete example","text":"<p>Here's a complete semantic model definition that brings it all together:</p> <pre><code>models:\n  analytics.customers:\n    alias: customers\n\n    dimensions:\n      excludes:\n        - password_hash\n\n        - internal_notes\n      enhancements:\n        - name: signup_date\n          granularities:\n            - name: monthly\n              interval: \"1 month\"\n              description: \"Monthly signup cohorts\"\n            - name: quarterly\n              interval: \"3 months\"\n              description: \"Quarterly signup cohorts\"\n\n    measures:\n      total_customers:\n        type: count\n        expression: \"{customers.customer_id}\"\n        description: \"Total number of customers\"\n\n      active_customers:\n        type: count_distinct\n        expression: \"{customers.customer_id}\"\n        filters:\n          - \"{customers.status} = 'active'\"\n        description: \"Number of active customers\"\n\n    segments:\n      active:\n        expression: \"{customers.status} = 'active'\"\n        description: \"Active customers\"\n\n      high_value:\n        expression: \"{customers.total_spent} &gt; 10000\"\n        description: \"High-value customers\"\n\n    joins:\n      orders:\n        type: one_to_many\n        expression: \"{customers.customer_id} = {orders.customer_id}\"\n        description: \"Customer's orders\"\n</code></pre> <p>This semantic model: - Exposes customer dimensions (with some exclusions and enhancements)</p> <ul> <li> <p>Defines customer measures (total and active counts)</p> </li> <li> <p>Creates reusable segments (active and high-value customers)</p> </li> <li> <p>Joins to orders for cross-model analysis</p> </li> </ul>"},{"location":"components/semantics/models/#best-practices","title":"Best practices","text":""},{"location":"components/semantics/models/#use-business-friendly-aliases","title":"Use business-friendly aliases","text":"<p>Your aliases should make sense to business users, not just developers:</p> <pre><code># Good: Consumer-friendly\nalias: customers\nalias: orders\nalias: subscriptions\n\n# Bad: Technical naming\nalias: dim_customers\nalias: fact_orders\n</code></pre> <p>The whole point of semantic models is to hide technical complexity. Don't bring it back with technical naming!</p>"},{"location":"components/semantics/models/#design-models-with-semantics-in-mind","title":"Design models with semantics in mind","text":"<p>When you're building your Vulcan models, think about how they'll be used semantically:</p> <pre><code>-- Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name\n  signup_date,        -- Good time dimension\n  total_spent         -- Good for segments\nFROM raw.customers;\n</code></pre> <p>Clean, descriptive column names make semantic models easier to build and use. Avoid abbreviations and technical jargon.</p>"},{"location":"components/semantics/models/#document-business-logic","title":"Document business logic","text":"<p>Add descriptions and metadata to help people understand what measures and segments mean:</p> <pre><code>measures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"\n    description: \"Total revenue from all completed orders\"\n    meta:\n      business_owner: \"Finance Team\"\n      calculation_method: \"Sum of order amounts excluding refunds\"\n</code></pre> <p>The <code>meta</code> section is perfect for business context, ownership, calculation details, and other information that helps people understand and trust the metric.</p>"},{"location":"components/semantics/models/#use-curly-braces-for-references","title":"Use curly braces for references","text":"<p>When referencing any column or measure anywhere in your semantic model definitions, always use curly braces <code>{}</code>:</p> <pre><code># Good: Use curly braces for all references\nmeasures:\n  total_revenue:\n    type: sum\n    expression: \"{orders.amount}\"  # Column reference with curly braces\n\n  active_customers:\n    type: count_distinct\n    expression: \"{customers.customer_id}\"  # Column reference with curly braces\n    filters:\n      - \"{customers.status} = 'active'\"  # Column reference in filter\n\nsegments:\n  high_value:\n    expression: \"{customers.total_spent} &gt; 10000\"  # Column reference with curly braces\n\njoins:\n  customers:\n    type: many_to_one\n    expression: \"{orders.customer_id} = {customers.customer_id}\"  # Both references use curly braces\n</code></pre> <p>Why use curly braces? - Clear distinction between semantic references and SQL functions</p> <ul> <li> <p>Consistent syntax across all semantic model definitions</p> </li> <li> <p>Prevents ambiguity in complex expressions</p> </li> <li> <p>Required for cross-model references (e.g., <code>{customers.customer_tier}</code>)</p> </li> </ul> <p>It's a best practice that makes your semantic models more maintainable and less error-prone.</p>"},{"location":"components/semantics/models/#validation","title":"Validation","text":"<p>Vulcan automatically validates semantic model definitions when you create a plan. It checks:</p> <ul> <li> <p>All column references in measures exist</p> </li> <li> <p>All column references in segments exist</p> </li> <li> <p>Join expressions reference valid columns</p> </li> <li> <p>Cross-model references have valid join paths</p> </li> <li> <p>Semantic aliases are properly defined</p> </li> </ul> <p>If something's wrong, you'll know about it before you try to use the semantic layer. This catches errors early and keeps your semantic models reliable.</p>"},{"location":"components/semantics/models/#next-steps","title":"Next steps","text":"<ul> <li> <p>Learn about Business Metrics that combine measures with time and dimensions</p> </li> <li> <p>Explore semantic model examples in your project's <code>semantics/</code> directory</p> </li> <li> <p>See the Semantics Overview for the complete picture</p> </li> </ul>"},{"location":"components/semantics/overview/","title":"Overview","text":""},{"location":"components/semantics/overview/#overview","title":"Overview","text":"<p>The semantic layer translates technical data models into business-friendly interfaces. It takes your SQL tables and makes them understandable to people who don't write SQL, turning <code>analytics.daily_revenue_metrics</code> into \"Monthly Revenue by Customer Tier\" that anyone can query.</p> <p>Your models are the engine (they do the work), and the semantic layer is the dashboard (it makes things usable). It adds business context, consistent definitions, and a friendly interface so people can use your data.</p>"},{"location":"components/semantics/overview/#what-is-the-semantic-layer","title":"What is the Semantic Layer?","text":"<p>The semantic layer bridges the gap between \"here's a table with columns\" and \"here's what this means for the business.\" It provides a consistent, business-friendly interface to your data that enables self-service analytics while keeping a single source of truth for your business logic.</p> <p>Without a semantic layer, every time someone wants to analyze revenue, they have to remember which table has it, what the column is called, how to join it with other tables, and how to calculate it correctly. With a semantic layer, they ask for \"revenue\" and it works.</p>"},{"location":"components/semantics/overview/#key-benefits","title":"Key Benefits","text":"<p>The semantic layer helps everyone in your organization work with data more effectively:</p> <p>For Developers:</p> <ul> <li> <p>Define metrics once, use everywhere - Write the calculation once, use it in dashboards, APIs, and reports</p> </li> <li> <p>Version-controlled business logic - Your metric definitions live in code, so changes are tracked and reviewable</p> </li> <li> <p>Consistent calculations - No more \"which revenue calculation should I use?\", there's one definition</p> </li> </ul> <p>For Business Users:</p> <ul> <li> <p>Self-service analytics - Query data without writing SQL (or even knowing what SQL is)</p> </li> <li> <p>Consistent metric definitions - Everyone uses the same definition of \"revenue\" or \"active users\"</p> </li> <li> <p>Trusted, validated data - Metrics are defined by the data team, so you know they're correct</p> </li> <li> <p>Works everywhere - Use the same metrics in Tableau, Power BI, Python, or APIs</p> </li> </ul> <p>For Organizations:</p> <ul> <li> <p>Single source of truth - One place where \"revenue\" is defined, not scattered across 20 different dashboards</p> </li> <li> <p>Faster time to insights - Business users can answer questions themselves instead of waiting for the data team</p> </li> <li> <p>Reduced data team bottleneck - Less \"can you build me a dashboard?\" requests</p> </li> <li> <p>Better data governance - Centralized definitions make it easier to audit and maintain data quality</p> </li> </ul>"},{"location":"components/semantics/overview/#core-components","title":"Core Components","text":"<p>The semantic layer has two main pieces that work together. Think of them as building blocks, you start with semantic models, then build metrics on top.</p>"},{"location":"components/semantics/overview/#semantic-models","title":"Semantic Models","text":"<p>Semantic models are like wrappers around your Vulcan models. They take your technical tables and expose them in a business-friendly way. For detailed information, check out the Semantic Models documentation.</p> <p>Here's what semantic models do:</p> <ul> <li> <p>Map physical models - Reference your Vulcan models from the <code>models/</code> directory</p> </li> <li> <p>Expose dimensions - All model columns automatically become dimensions (things you can filter and group by)</p> </li> <li> <p>Define measures - Aggregated calculations like <code>SUM(amount)</code> or <code>COUNT(*)</code></p> </li> <li> <p>Create segments - Reusable filter conditions (like \"high-value customers\" or \"active users\")</p> </li> <li> <p>Establish joins - Relationships between models so you can analyze across tables</p> </li> </ul> <p>Here's a simple example:</p> <pre><code>models:\n  analytics.customers:\n    alias: customers\n    measures:\n      total_customers:\n        type: count\n        expression: \"COUNT(*)\"\n</code></pre> <p>This takes your <code>analytics.customers</code> model and exposes a <code>total_customers</code> measure that anyone can use. Business users can query \"total customers\" without knowing which table it comes from or how to write the SQL.</p>"},{"location":"components/semantics/overview/#business-metrics","title":"Business Metrics","text":"<p>Business metrics combine measures with dimensions and time to create complete analytical definitions. They're like pre-built queries that are ready to use. Learn more in the Business Metrics guide.</p> <p>Here's what makes metrics powerful:</p> <ul> <li> <p>Time-series analysis - Metrics include time dimensions so you can see trends over time</p> </li> <li> <p>Flexible granularity - Query the same metric at different time intervals (day, week, month, etc.)</p> </li> <li> <p>Multi-dimensional - Slice and dice by business attributes (customer tier, region, product category, etc.)</p> </li> <li> <p>Ready for dashboards - Pre-configured for visualization tools</p> </li> </ul> <p>Here's what a metric looks like:</p> <pre><code>metrics:\n  monthly_revenue:\n    measure: orders.total_revenue\n    time: orders.order_date\n    dimensions:\n      - customers.customer_tier\n</code></pre> <p>This creates a <code>monthly_revenue</code> metric that:</p> <ul> <li> <p>Uses the <code>total_revenue</code> measure from the orders model</p> </li> <li> <p>Groups by <code>order_date</code> (time dimension)</p> </li> <li> <p>Can be sliced by <code>customer_tier</code> (business dimension)</p> </li> </ul> <p>Anyone can query \"monthly revenue by customer tier\" without writing SQL. They reference the metric name, and Vulcan handles the complexity.</p>"},{"location":"components/semantics/overview/#how-it-works","title":"How It Works","text":"<p>Setting up your semantic layer is straightforward. Here's the workflow:</p> <ol> <li>Define semantic models - Create YAML files that reference your Vulcan models</li> <li>Add measures and dimensions - Define what can be calculated and filtered</li> <li>Create joins - Connect models so you can analyze across tables</li> <li>Define metrics - Combine measures with time and dimensions for analysis</li> <li>Validate - Vulcan automatically validates your semantic definitions when you create a plan</li> <li>Query - Use the semantic layer via APIs or export to BI tools</li> </ol> <p>The validation step is important, Vulcan checks that your measures reference real columns, joins are valid, and metrics make sense. It'll catch errors before you try to use them, which saves you from debugging issues later.</p>"},{"location":"components/semantics/overview/#file-organization","title":"File Organization","text":"<p>Semantic layer definitions are YAML files in the <code>semantics/</code> directory. You can organize them however makes sense for your team:</p> <pre><code>project/\n\u251c\u2500\u2500 models/           # Vulcan data models (.sql files)\n\u2502   \u251c\u2500\u2500 customers.sql\n\u2502   \u251c\u2500\u2500 orders.sql\n\u2502   \u2514\u2500\u2500 events.sql\n\u2502\n\u251c\u2500\u2500 semantics/        # Semantic layer definitions (YAML)\n\u2502   \u251c\u2500\u2500 customers.yml\n\u2502   \u251c\u2500\u2500 orders.yml\n\u2502   \u2514\u2500\u2500 metrics.yml\n\u2502\n\u2514\u2500\u2500 config.yaml\n</code></pre> <p>File naming: The filename doesn't matter, Vulcan automatically merges all YAML files in the <code>semantics/</code> directory. Organize by domain (like <code>customers.yml</code>, <code>orders.yml</code>) or by model (like <code>revenue_metrics.yml</code>), whatever makes sense for your team.</p>"},{"location":"components/semantics/overview/#integration-with-models","title":"Integration with Models","text":"<p>Here's the key insight: Model columns automatically become dimensions. The semantic layer adds measures, segments, joins, and metrics on top of your existing models. It builds on what you already have, it doesn't replace anything.</p> <p>When you're designing Vulcan models, keep the semantic layer in mind:</p> <pre><code>-- Good: Clean column names, business-friendly\nMODEL (name analytics.customers);\nSELECT\n  customer_id,\n  customer_tier,      -- Good dimension name (can filter/group by this)\n  signup_date,        -- Good time dimension (can analyze trends)\n  total_spent         -- Good for segments (can create \"high-value\" segment)\nFROM raw.customers;\n</code></pre> <p>This model will automatically expose:</p> <ul> <li> <p><code>customer_tier</code> as a dimension (filter by tier, group by tier)</p> </li> <li> <p><code>signup_date</code> as a time dimension (analyze trends over time)</p> </li> <li> <p><code>total_spent</code> as a dimension (create segments like \"high-value customers\")</p> </li> </ul> <p>Then you can add measures and metrics on top. The semantic layer builds on your models, it doesn't replace them. Your models stay exactly as they are; the semantic layer just makes them more accessible.</p>"},{"location":"components/semantics/overview/#next-steps","title":"Next Steps","text":"<p>Next steps:</p> <ul> <li> <p>Semantic Models - Map physical models to business concepts</p> </li> <li> <p>Business Metrics - Create time-series analytical definitions</p> </li> <li> <p>Transpiling Semantic Queries - See how semantic queries get converted to SQL</p> </li> <li> <p>Check your project - Look at the <code>semantics/</code> directory in your Vulcan project for examples</p> </li> </ul> <p>The semantic layer makes your data accessible to everyone, not just SQL experts. Start with semantic models, add measures, then build metrics.</p>"},{"location":"components/tests/tests/","title":"Tests","text":""},{"location":"components/tests/tests/#tests","title":"Tests","text":"<p>Tests are your safety net for data transformations. Just like software engineers write unit tests to catch bugs before they ship, you can write tests to verify that your models transform data correctly, catching problems before they reach production and cause headaches.</p> <p>Tests are executable documentation. They show exactly how your model should behave with specific inputs, and they fail if something changes unexpectedly. Unlike audits (which check data quality at runtime), tests verify the logic of your models against predefined inputs and expected outputs.</p>"},{"location":"components/tests/tests/#why-testing-matters","title":"Why Testing Matters","text":"<p>Data models are tricky beasts. Small errors can snowball into significant business impacts. A small change in one model can cascade into big problems downstream. Here's why testing is worth your time:</p> <ul> <li> <p>Catch breaking changes - Refactor with confidence knowing tests will flag unintended behavior changes</p> </li> <li> <p>Document expected behavior - Tests serve as executable specifications (better than comments that get outdated!)</p> </li> <li> <p>Faster debugging - When something breaks, tests pinpoint exactly which transformation failed</p> </li> <li> <p>Data quality assurance - Verify that aggregations, joins, and calculations produce correct results</p> </li> <li> <p>Confidence in changes - Make updates knowing you'll catch regressions before they hit production</p> </li> </ul> <p>Tests run either on demand (like in CI/CD pipelines) or automatically when you create a new plan.</p>"},{"location":"components/tests/tests/#creating-tests","title":"Creating Tests","text":"<p>Tests live in YAML files in the <code>tests/</code> folder of your project. The filename must start with <code>test</code> and end with <code>.yaml</code> or <code>.yml</code>. You can put multiple tests in one file (organize them however makes sense).</p> <p>At minimum, a test needs three things:</p> <ul> <li> <p>model - Which model you're testing</p> </li> <li> <p>inputs - Mock data for upstream dependencies (what goes in)</p> </li> <li> <p>outputs - Expected results from the model's query (what should come out)</p> </li> </ul> <p>Let's start with a simple example.</p>"},{"location":"components/tests/tests/#your-first-test","title":"Your First Test","text":"<p>Here's a model that aggregates orders by date:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  description 'Daily sales summary with order counts and revenue'\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP) AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>Now let's write a test to verify it works correctly:</p> <pre><code>test_daily_sales_aggregation:\n  model: sales.daily_sales\n  description: &gt;\n    Test that daily_sales correctly aggregates orders by date.\n\n  inputs:\n    raw.raw_orders:\n      rows:\n        - order_id: O001\n          order_date: '2024-03-15'\n          customer_id: C001\n          product_id: P001\n          total_amount: 50.00\n        - order_id: O002\n          order_date: '2024-03-15'\n          customer_id: C002\n          product_id: P002\n          total_amount: 75.00\n        - order_id: O003\n          order_date: '2024-03-16'\n          customer_id: C001\n          product_id: P003\n          total_amount: 100.00\n\n  outputs:\n    query:\n      rows:\n        - order_date: \"2024-03-15\"\n          total_orders: 2\n          total_revenue: 125.00\n          last_order_id: \"O002\"\n        - order_date: \"2024-03-16\"\n          total_orders: 1\n          total_revenue: 100.00\n          last_order_id: \"O003\"\n</code></pre> <p>This test gives the model three orders (two on March 15, one on March 16) and checks that:</p> <ul> <li> <p>Orders are correctly grouped by date</p> </li> <li> <p><code>total_orders</code> counts distinct orders per day (should be 2 for March 15, 1 for March 16)</p> </li> <li> <p><code>total_revenue</code> sums the amounts correctly (50 + 75 = 125 for March 15)</p> </li> <li> <p><code>last_order_id</code> returns the maximum order ID per day (O002 for March 15, O003 for March 16)</p> </li> </ul> <p>If any of these expectations don't match, the test fails and tells you what went wrong.</p>"},{"location":"components/tests/tests/#testing-models-with-multiple-dependencies","title":"Testing Models with Multiple Dependencies","text":"<p>Real-world models often join multiple tables. Here's how you'd test a more complex model that joins customers, orders, and order items:</p> <pre><code>test_full_model_basic:\n  model: vulcan_demo.full_model\n  description: |\n    Validates aggregates and averages:\n    - DISTINCT order counting\n\n    - SUM(quantity * unit_price)\n\n    - avg_order_value = total_spent / total_orders, or NULL when total_orders = 0\n\n  inputs:\n    vulcan_demo.customers:\n      - customer_id: 1\n        name: Alice\n        email: alice@example.com\n      - customer_id: 2\n        name: Bob\n        email: bob@example.com\n      - customer_id: 3\n        name: Charlie\n        email: charlie@example.com\n\n    vulcan_demo.orders:\n      # Alice has 2 orders\n      - order_id: 1001\n        customer_id: 1\n      - order_id: 1002\n        customer_id: 1\n      # Bob has 1 order\n      - order_id: 2001\n        customer_id: 2\n      # Charlie has 0 orders (no rows)\n\n    vulcan_demo.order_items:\n      # Order 1001: 2*50 + 1*25 = 125\n      - order_id: 1001\n        product_id: 501\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 502\n        quantity: 1\n        unit_price: 25\n      # Order 1002: 1*200 = 200 \u2192 Alice total = 325\n      - order_id: 1002\n        product_id: 503\n        quantity: 1\n        unit_price: 200\n      # Order 2001: 2*5 = 10 \u2192 Bob total = 10\n      - order_id: 2001\n        product_id: 504\n        quantity: 2\n        unit_price: 5\n\n  outputs:\n    query:\n      rows:\n        - customer_id: 1\n          customer_name: Alice\n          email: alice@example.com\n          total_orders: 2\n          total_spent: 325\n          avg_order_value: 162.5\n        - customer_id: 2\n          customer_name: Bob\n          email: bob@example.com\n          total_orders: 1\n          total_spent: 10\n          avg_order_value: 10.0\n        - customer_id: 3\n          customer_name: Charlie\n          email: charlie@example.com\n          total_orders: 0\n          total_spent: 0\n          avg_order_value: null  # Division by zero handled\n</code></pre> <p>Notice how we're providing mock data for all three upstream tables. The test verifies that the model correctly:</p> <ul> <li> <p>Joins customers with orders and order items</p> </li> <li> <p>Counts distinct orders per customer</p> </li> <li> <p>Calculates total spent (quantity \u00d7 unit_price summed across all items)</p> </li> <li> <p>Handles division by zero (Charlie has no orders, so avg_order_value should be NULL)</p> </li> </ul> <p>The comments in the YAML help explain the test data, which makes it easier to understand what's being tested.</p>"},{"location":"components/tests/tests/#testing-incremental-models","title":"Testing Incremental Models","text":"<p>Incremental models are a bit special because they filter data by time range. You'll need to set <code>start</code> and <code>end</code> dates using the <code>vars</code> attribute:</p> <pre><code>test_incremental_by_time_range_basic:\n  model: vulcan_demo.incremental_by_time_range\n  description: |\n    Validates per-(order_date, product_id) aggregates over a fixed two-day window.\n    Checks DISTINCT order counts, quantity and revenue sums, and AVG(unit_price).\n  vars:\n    start: '2025-01-01'\n    end: '2025-01-02'\n\n  inputs:\n    vulcan_demo.products:\n      - product_id: 10\n        name: Widget\n        category: Electronics\n      - product_id: 20\n        name: Gizmo\n        category: Home\n\n    vulcan_demo.orders:\n      - order_id: 1001\n        customer_id: 9001\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1002\n        customer_id: 9002\n        warehouse_id: 1\n        order_date: '2025-01-01'\n      - order_id: 1003\n        customer_id: 9003\n        warehouse_id: 2\n        order_date: '2025-01-02'\n\n    vulcan_demo.order_items:\n      # 2025-01-01\n      - order_id: 1001\n        product_id: 10\n        quantity: 2\n        unit_price: 50\n      - order_id: 1001\n        product_id: 20\n        quantity: 1\n        unit_price: 200\n      - order_id: 1002\n        product_id: 10\n        quantity: 1\n        unit_price: 60\n      # 2025-01-02\n      - order_id: 1003\n        product_id: 10\n        quantity: 5\n        unit_price: 40\n\n  outputs:\n    query:\n      rows:\n        - order_date: '2025-01-01'\n          product_id: 20\n          product_name: Gizmo\n          category: Home\n          order_count: 1\n          total_quantity: 1\n          total_sales_amount: 200\n          avg_unit_price: 200\n        - order_date: '2025-01-01'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 2\n          total_quantity: 3\n          total_sales_amount: 160\n          avg_unit_price: 55\n        - order_date: '2025-01-02'\n          product_id: 10\n          product_name: Widget\n          category: Electronics\n          order_count: 1\n          total_quantity: 5\n          total_sales_amount: 200\n          avg_unit_price: 40\n</code></pre> <p>The <code>vars</code> section tells Vulcan what time range to use when running the model. This is important because incremental models filter by <code>@start_ds</code> and <code>@end_ds</code> macros, and you need to control those in your test.</p>"},{"location":"components/tests/tests/#testing-ctes","title":"Testing CTEs","text":"<p>You can also test individual CTEs (Common Table Expressions) within your model. This is useful for debugging complex queries step by step.</p> <p>Say you have a model with a CTE:</p> <pre><code>WITH filtered_orders_cte AS (\n  SELECT id, item_id\n  FROM vulcan_demo.incremental_model\n  WHERE item_id = 1\n)\nSELECT\n  item_id,\n  COUNT(DISTINCT id) AS num_orders\nFROM filtered_orders_cte\nGROUP BY item_id\n</code></pre> <p>You can test both the CTE and the final query:</p> <pre><code>test_model_with_cte:\n  model: vulcan_demo.full_model\n  inputs:\n    vulcan_demo.incremental_model:\n      rows:\n        - id: 1\n          item_id: 1\n        - id: 2\n          item_id: 1\n        - id: 3\n          item_id: 2\n  outputs:\n    ctes:\n      filtered_orders_cte:\n        rows:\n          - id: 1\n            item_id: 1\n          - id: 2\n            item_id: 1\n    query:\n      rows:\n        - item_id: 1\n          num_orders: 2\n</code></pre> <p>This verifies that:</p> <ol> <li>The CTE correctly filters to <code>item_id = 1</code> (should return rows with id 1 and 2)</li> <li>The final query correctly counts distinct orders (should be 2)</li> </ol> <p>Testing CTEs separately makes it easier to pinpoint where things go wrong in complex queries.</p>"},{"location":"components/tests/tests/#supported-data-formats","title":"Supported Data Formats","text":"<p>Vulcan gives you flexibility in how you define test data. Pick whatever format works best for your situation:</p>"},{"location":"components/tests/tests/#yaml-dictionaries-default","title":"YAML Dictionaries (Default)","text":"<p>The most common format, just list your rows as YAML dictionaries:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    rows:\n      - order_id: 1001\n        customer_id: 1\n        order_date: '2025-01-01'\n</code></pre> <p>This works well for small datasets and when you want everything in one place.</p>"},{"location":"components/tests/tests/#csv-format","title":"CSV Format","text":"<p>If you have lots of data, CSV might be easier to read and write:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    rows: |\n      order_id,customer_id,order_date\n      1001,1,2025-01-01\n      1002,2,2025-01-01\n</code></pre> <p>You can also customize CSV parsing with <code>csv_settings</code> if you need different separators or other options.</p>"},{"location":"components/tests/tests/#sql-queries","title":"SQL Queries","text":"<p>Sometimes you want more control over how data is generated. Use a SQL query:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    query: |\n      SELECT 1001 AS order_id, 1 AS customer_id, '2025-01-01' AS order_date\n      UNION ALL\n      SELECT 1002 AS order_id, 2 AS customer_id, '2025-01-01' AS order_date\n</code></pre> <p>This is useful when you need to generate test data programmatically or when the data structure is complex.</p>"},{"location":"components/tests/tests/#external-files","title":"External Files","text":"<p>For large test datasets, store them in separate files:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    format: csv\n    path: fixtures/orders_test_data.csv\n</code></pre> <p>This keeps your test files clean and makes it easy to reuse test data across multiple tests.</p>"},{"location":"components/tests/tests/#omitting-columns","title":"Omitting Columns","text":"<p>For wide tables, you don't need to specify every column. You can omit columns (they'll be treated as <code>NULL</code>) or use partial matching to only test the columns you care about:</p> <pre><code>outputs:\n  query:\n    partial: true  # Only test specified columns\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre> <p>This is useful when you have a table with 50 columns but only care about testing a few of them.</p> <p>Apply partial matching globally:</p> <pre><code>outputs:\n  partial: true\n  query:\n    rows:\n      - customer_id: 1\n        total_spent: 325\n</code></pre> <p>This applies partial matching to all outputs in the test, which is convenient when you're only testing a subset of columns.</p>"},{"location":"components/tests/tests/#freezing-time","title":"Freezing Time","text":"<p>If your model uses <code>CURRENT_TIMESTAMP</code> or similar functions, you'll want to freeze time in your tests to make them deterministic. Otherwise, your tests will fail every time you run them because the timestamp changes!</p> <pre><code>test_with_timestamp:\n  model: vulcan_demo.audit_log\n  outputs:\n    query:\n      - event: \"login\"\n        created_at: \"2023-01-01 12:05:03\"\n  vars:\n    execution_time: \"2023-01-01 12:05:03\"\n</code></pre> <p>Setting <code>execution_time</code> in <code>vars</code> makes <code>CURRENT_TIMESTAMP</code> and <code>CURRENT_DATE</code> return fixed values, so your tests are predictable and repeatable.</p>"},{"location":"components/tests/tests/#running-tests","title":"Running Tests","text":""},{"location":"components/tests/tests/#command-line","title":"Command Line","text":"<p>Run tests from the command line:</p> <pre><code># Run all tests\nvulcan test\n\n# Run specific test file\nvulcan test tests/test_daily_sales.yaml\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_aggregation\n\n# Run tests matching a pattern\nvulcan test tests/test_*\n</code></pre> <p>The <code>::</code> syntax lets you run a specific test from a file when debugging a single failing test.</p>"},{"location":"components/tests/tests/#example-output","title":"Example Output","text":"<p>When tests pass, you'll see something like:</p> <pre><code>$ vulcan test\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.024s\n\nOK\n</code></pre> <p>The dots (<code>.</code>) indicate passing tests. Simple and clean!</p> <p>When tests fail:</p> <pre><code>$ vulcan test\nF\n======================================================================\nFAIL: test_daily_sales_aggregation (tests/test_daily_sales.yaml)\n----------------------------------------------------------------------\nAssertionError: Data mismatch (exp: expected, act: actual)\n\n  total_orders\n         exp  act\n0        3.0  2.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.012s\n\nFAILED (failures=1)\n</code></pre> <p>The output shows you exactly what didn't match. In this case, <code>total_orders</code> was expected to be 3.0 but was actually 2.0. This tells you exactly what to investigate.</p>"},{"location":"components/tests/tests/#automatic-test-generation","title":"Automatic Test Generation","text":"<p>Writing tests can be tedious, especially when you're just getting started. Vulcan can help by generating tests automatically:</p> <pre><code>vulcan create_test vulcan_demo.daily_sales \\\n  --query raw.raw_orders \"SELECT * FROM raw.raw_orders WHERE order_date BETWEEN '2025-01-01' AND '2025-01-02' LIMIT 10\" \n</code></pre> <p>This creates a test file with actual data from your warehouse, which makes it easy to bootstrap your test suite. You can then tweak the generated test to match your needs.</p> <p>Pro tip: Start with generated tests, then refine them to test edge cases and specific scenarios. It's much faster than writing everything from scratch!</p>"},{"location":"components/tests/tests/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/tests/tests/#preserving-fixtures","title":"Preserving Fixtures","text":"<p>When a test fails, you might want to inspect the actual data that was created. Use <code>--preserve-fixtures</code> to keep test fixtures around:</p> <pre><code>vulcan test --preserve-fixtures\n</code></pre> <p>Fixtures are created as views in a schema named <code>vulcan_test_&lt;random_ID&gt;</code>. You can query these views directly to see what data was actually produced for debugging.</p>"},{"location":"components/tests/tests/#type-mismatches","title":"Type Mismatches","text":"<p>Sometimes Vulcan can't figure out the correct types for your test data. If you're seeing type errors, specify them explicitly:</p> <pre><code>inputs:\n  vulcan_demo.orders:\n    columns:\n      order_id: INT\n      order_date: DATE\n      total_amount: DECIMAL(10,2)\n    rows:\n      - order_id: 1001\n        order_date: '2025-01-01'\n        total_amount: 99.99\n</code></pre> <p>The <code>columns</code> section tells Vulcan exactly what types to use, which helps avoid type inference issues. You can also explicitly cast columns in your model's query to help Vulcan infer types more accurately.</p>"},{"location":"components/tests/tests/#test-not-finding-model","title":"Test Not Finding Model","text":"<p>Problem: Test says it can't find the model.</p> <p>Solution: Make sure the model name in your test matches exactly what's in your <code>models/</code> folder. Model names are case-sensitive and must include the schema (like <code>sales.daily_sales</code>, not just <code>daily_sales</code>).</p>"},{"location":"components/tests/tests/#output-order-matters","title":"Output Order Matters","text":"<p>Problem: Test fails even though the data looks correct.</p> <p>Solution: The columns in your expected output must appear in the same order as they're selected in the model's query. Check the <code>SELECT</code> statement order and make sure your test rows match.</p>"},{"location":"components/tests/tests/#partial-matching-not-working","title":"Partial Matching Not Working","text":"<p>Problem: Partial matching isn't ignoring extra columns.</p> <p>Solution: Make sure you set <code>partial: true</code> at the right level. It needs to be under <code>outputs.query</code> (or <code>outputs.ctes.&lt;cte_name&gt;</code>) for CTE-specific partial matching, or under <code>outputs</code> for global partial matching.</p>"},{"location":"components/tests/tests/#test-structure-reference","title":"Test Structure Reference","text":"<p>Here's a complete reference of all the fields you can use in a test. Most tests only need <code>model</code>, <code>inputs</code>, and <code>outputs</code>, but it's good to know what else is available.</p>"},{"location":"components/tests/tests/#test_name","title":"<code>&lt;test_name&gt;</code>","text":"<p>The unique name of your test. Use descriptive names that explain what you're testing, like <code>test_daily_sales_aggregation</code> or <code>test_customer_revenue_calculation</code>.</p>"},{"location":"components/tests/tests/#test_namemodel","title":"<code>&lt;test_name&gt;.model</code>","text":"<p>The fully qualified name of the model being tested (like <code>sales.daily_sales</code>). This model must exist in your project's <code>models/</code> folder.</p>"},{"location":"components/tests/tests/#test_namedescription","title":"<code>&lt;test_name&gt;.description</code>","text":"<p>An optional description that explains what the test validates. This is helpful for your teammates (and future you) to understand what the test is checking.</p>"},{"location":"components/tests/tests/#test_nameschema","title":"<code>&lt;test_name&gt;.schema</code>","text":"<p>The name of the schema that will contain the test fixtures (the views created for this test). If not specified, Vulcan creates a temporary schema.</p>"},{"location":"components/tests/tests/#test_namegateway","title":"<code>&lt;test_name&gt;.gateway</code>","text":"<p>The gateway whose <code>test_connection</code> will be used to run this test. If not specified, the default gateway is used. Useful when you need to test against a specific database or engine.</p>"},{"location":"components/tests/tests/#test_nameinputs","title":"<code>&lt;test_name&gt;.inputs</code>","text":"<p>Mock data for upstream models that your target model depends on. If your model has no dependencies, you can omit this.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_model","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;</code>","text":"<p>A model that your target model depends on. Provide mock data for each upstream model.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelrows","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.rows</code>","text":"<p>The rows of test data, defined as an array of dictionaries:</p> <pre><code>    &lt;upstream_model&gt;:\n      rows:\n        - &lt;column_name&gt;: &lt;column_value&gt;\n        ...\n</code></pre> <p>Shortcut: If <code>rows</code> is the only key, you can omit it:</p> <pre><code>    &lt;upstream_model&gt;:\n      - &lt;column_name&gt;: &lt;column_value&gt;\n      ...\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelformat","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.format</code>","text":"<p>The format of the input data. Options: <code>yaml</code> (default) or <code>csv</code>.</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcsv_settings","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.csv_settings</code>","text":"<p>When using CSV format, customize how the CSV is parsed:</p> <pre><code>    &lt;upstream_model&gt;:\n      format: csv\n      csv_settings: \n        sep: \"#\"\n        skip_blank_lines: true\n      rows: |\n        &lt;column1_name&gt;#&lt;column2_name&gt;\n        &lt;row1_value&gt;#&lt;row1_value&gt;\n</code></pre> <p>See pandas read_csv documentation for all supported settings.</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelpath","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.path</code>","text":"<p>Load data from an external file:</p> <pre><code>    &lt;upstream_model&gt;:\n      path: filepath/test_data.yaml\n</code></pre>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelcolumns","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.columns</code>","text":"<p>Explicitly specify column types to help Vulcan interpret your data correctly:</p> <pre><code>    &lt;upstream_model&gt;:\n      columns:\n        &lt;column_name&gt;: &lt;column_type&gt;\n        ...\n</code></pre> <p>This is especially useful when Vulcan can't infer types correctly (like with dates or decimals).</p>"},{"location":"components/tests/tests/#test_nameinputsupstream_modelquery","title":"<code>&lt;test_name&gt;.inputs.&lt;upstream_model&gt;.query</code>","text":"<p>Generate input data using a SQL query:</p> <pre><code>    &lt;upstream_model&gt;:\n      query: &lt;sql_query&gt;\n</code></pre> <p>Note: You can't use <code>query</code> together with <code>rows</code>, pick one or the other.</p>"},{"location":"components/tests/tests/#test_nameoutputs","title":"<code>&lt;test_name&gt;.outputs</code>","text":"<p>The expected outputs from your model. This is what you're asserting should be true.</p> <p>Important: Column order matters! The columns in your expected rows must match the order they appear in the model's <code>SELECT</code> statement.</p>"},{"location":"components/tests/tests/#test_nameoutputspartial","title":"<code>&lt;test_name&gt;.outputs.partial</code>","text":"<p>When <code>true</code>, only test the columns you specify. Extra columns in the output are ignored. Useful for wide tables where you only care about a few columns.</p>"},{"location":"components/tests/tests/#test_nameoutputsquery","title":"<code>&lt;test_name&gt;.outputs.query</code>","text":"<p>The expected output of the model's final query. This is optional if you're testing CTEs instead.</p>"},{"location":"components/tests/tests/#test_nameoutputsquerypartial","title":"<code>&lt;test_name&gt;.outputs.query.partial</code>","text":"<p>Same as <code>outputs.partial</code>, but applies only to the query output (not CTEs).</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryrows","title":"<code>&lt;test_name&gt;.outputs.query.rows</code>","text":"<p>The expected rows from the model's query. Same format as input rows.</p>"},{"location":"components/tests/tests/#test_nameoutputsqueryquery","title":"<code>&lt;test_name&gt;.outputs.query.query</code>","text":"<p>Generate expected output using a SQL query. Useful when the expected output is complex or when you want to compute it dynamically.</p>"},{"location":"components/tests/tests/#test_nameoutputsctes","title":"<code>&lt;test_name&gt;.outputs.ctes</code>","text":"<p>Test individual CTEs within your model. This is optional if you're testing the final query output.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_name","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;</code>","text":"<p>The expected output of a specific CTE. Use this to test intermediate steps in complex queries.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namepartial","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.partial</code>","text":"<p>Partial matching for a specific CTE.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namerows","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.rows</code>","text":"<p>Expected rows for a specific CTE.</p>"},{"location":"components/tests/tests/#test_nameoutputsctescte_namequery","title":"<code>&lt;test_name&gt;.outputs.ctes.&lt;cte_name&gt;.query</code>","text":"<p>Generate expected CTE output using a SQL query.</p>"},{"location":"components/tests/tests/#test_namevars","title":"<code>&lt;test_name&gt;.vars</code>","text":"<p>Set values for macro variables used in your model:</p> <pre><code>  vars:\n    start: 2022-01-01\n    end: 2022-01-01\n    execution_time: 2022-01-01\n    &lt;macro_variable_name&gt;: &lt;macro_variable_value&gt;\n</code></pre> <p>Special variables:</p> <ul> <li> <p><code>start</code> - Overrides <code>@start_ds</code> for incremental models</p> </li> <li> <p><code>end</code> - Overrides <code>@end_ds</code> for incremental models  </p> </li> <li> <p><code>execution_time</code> - Overrides <code>@execution_ds</code> and makes <code>CURRENT_TIMESTAMP</code>/<code>CURRENT_DATE</code> return fixed values</p> </li> </ul> <p>These are useful for testing incremental models and making time-dependent tests deterministic.</p>"},{"location":"configurations/overview/","title":"Overview","text":""},{"location":"configurations/overview/#overview","title":"Overview","text":"<p>Your Vulcan project needs a configuration file. It tells Vulcan how to connect to your data warehouse, where to store state, and what defaults to use for your models. Without it, Vulcan doesn't know where your data lives or how to run your transformations.</p>"},{"location":"configurations/overview/#configuration-file","title":"Configuration File","text":"<p>Create a configuration file in your project root. Choose one:</p> <ul> <li> <p><code>config.yaml</code>: YAML format. Use this for most projects. Simple and readable.</p> </li> <li> <p><code>config.py</code>: Python format. Use this if you need dynamic configuration or want to generate settings programmatically.</p> </li> </ul>"},{"location":"configurations/overview/#example-configuration","title":"Example Configuration","text":"<p>Here's what a typical configuration file looks like:</p> <pre><code># Project identity\nname: orders-analytics\ndisplay_name: Orders Analytics Platform\ntenant: engineering\ndescription: Orders Analytics is a centralized data product delivering clean, trusted insights across the full order lifecycle.\n\n# Classification\ntags:\n  - e-commerce\n  - retail\n  - sales_analytics\n  - customer_analytics\n  - postgres\n\nterms:\n  - glossary.data_product\n  - glossary.analytics_platform\n  - glossary.sales_operations\n\n# Metadata\nmetadata:\n  domain: sales_operations\n  use_cases:\n    - Daily and weekly sales reporting\n    - Customer segmentation and RFM analysis\n    - Sales funnel conversion tracking\n    - Product performance analytics\n  limitations:\n    - Demo dataset with synthetic data (100 customers, 1000 orders)\n    - Historical data from November 2025 onwards\n\n# Gateway Connection\ngateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: \"{{ env_var('DB_PASSWORD') }}\"\n    state_connection:\n      type: postgres\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: \"{{ env_var('STATE_DB_PASSWORD') }}\"\n\ndefault_gateway: default\n\n# Model Defaults (required)\nmodel_defaults:\n  dialect: postgres\n  start: 2024-01-01\n  cron: '@daily'\n\n# Linting Rules\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre>"},{"location":"configurations/overview/#configuration-structure","title":"Configuration Structure","text":"<pre><code>graph TB\n    Config[config.yaml]\n    Config --&gt; Project[Project Settings]\n    Config --&gt; Metadata[Metadata]\n    Config --&gt; Gateways[Gateways]\n    Config --&gt; ModelDefaults[Model Defaults]\n    Config --&gt; Options[Optional Features]\n    Project --&gt; Name[name, display_name, tenant]\n    Project --&gt; Desc[description]\n    Project --&gt; Tags[tags, terms]\n    Metadata --&gt; Domain[domain]\n    Metadata --&gt; UseCases[use_cases]\n    Metadata --&gt; Limitations[limitations]\n    Gateways --&gt; Connection[connection]\n    Gateways --&gt; StateConn[state_connection]\n    Gateways --&gt; TestConn[test_connection]\n    Options --&gt; Linter[linter]\n    Options --&gt; Notifications[notifications]\n    Options --&gt; Variables[variables]</code></pre>"},{"location":"configurations/overview/#configuration-sections","title":"Configuration Sections","text":""},{"location":"configurations/overview/#project-settings","title":"Project Settings","text":"<p>Metadata fields that identify your project. They don't affect how Vulcan runs, but they're useful for organization and discovery.</p> Option Description Type Required <code>name</code> Project identifier (used internally) string Yes <code>tenant</code> Tenant or organization name string Yes <code>description</code> Project description string No <code>display_name</code> Human-readable project name for UI/docs string No <code>tags</code> Labels for categorization and filtering array of string No <code>terms</code> Business glossary terms using dot notation (e.g., <code>glossary.data_product</code>) array of string No <pre><code># Project identity\nname: orders-analytics\ndisplay_name: Orders Analytics Platform\ntenant: engineering\ndescription: Orders Analytics delivers insights across the full order lifecycle.\n\n# Classification\ntags:\n  - e-commerce\n  - retail\n  - sales_analytics\n\nterms:\n  - glossary.data_product\n  - glossary.analytics_platform\n  - glossary.sales_operations\n</code></pre>"},{"location":"configurations/overview/#metadata","title":"Metadata","text":"<p>Metadata fields provide additional context about your project's purpose and scope. Use these to document what your project does, where it applies, and any known constraints.</p> Option Description Type Required <code>domain</code> Business domain or data area (e.g., sales_operations, marketing, finance) string No <code>use_cases</code> List of primary use cases or business problems this project addresses array of string No <code>limitations</code> List of known constraints, caveats, or edge cases to be aware of array of string No <pre><code># Metadata\nmetadata:\n  domain: sales_operations\n  use_cases:\n    - Daily and weekly sales reporting\n    - Customer segmentation and RFM analysis\n    - Sales funnel conversion tracking\n  limitations:\n    - Demo dataset with synthetic data (100 customers, 1000 orders)\n    - Historical data from November 2025 onwards\n</code></pre>"},{"location":"configurations/overview/#gateways","title":"Gateways","text":"<p>Gateways define how Vulcan connects to your data warehouse and state backend. Define multiple gateways for different environments: dev, staging, prod. Each gateway has its own connection settings.</p> Component Description Default <code>connection</code> Primary data warehouse connection Required <code>state_connection</code> Where Vulcan stores internal state Uses <code>connection</code> <code>test_connection</code> Connection for running tests Uses <code>connection</code> <code>scheduler</code> Scheduler configuration <code>builtin</code> <code>state_schema</code> Schema name for state tables <code>vulcan</code>"},{"location":"configurations/overview/#model-defaults","title":"Model Defaults","text":"<p>The <code>model_defaults</code> section is required. At minimum, specify <code>dialect</code> to tell Vulcan what SQL dialect your models use. Other defaults are optional but apply to all models automatically, so you don't repeat the same settings in every model file.</p> <pre><code>model_defaults:\n  dialect: postgres     # Required\n  owner: data-team\n  start: 2024-01-01\n  cron: '@daily'\n</code></pre> <p>See Model Defaults for all available options.</p>"},{"location":"configurations/overview/#variables","title":"Variables","text":"<p>Store sensitive information like passwords and API keys without hardcoding them. Use environment variables, <code>.env</code> files, or configuration overrides. Variables also let you override configuration values dynamically.</p> <p>See Variables for details.</p>"},{"location":"configurations/overview/#execution-hooks","title":"Execution Hooks","text":"<p>Run SQL statements automatically at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands. Use <code>before_all</code> for setup tasks like creating temporary tables or granting permissions. Use <code>after_all</code> for cleanup or post-processing.</p> <p>See Execution Hooks for detailed examples and use cases.</p>"},{"location":"configurations/overview/#linter","title":"Linter","text":"<p>Automatic code quality checks that run when you create a plan or run the lint command. Catches common mistakes and enforces coding standards. Use built-in rules or create custom ones.</p> <p>See Linter for rules and custom linter configuration.</p>"},{"location":"configurations/overview/#notifications","title":"Notifications","text":"<p>Set up alerts via Slack or email. Get notified when plans start or finish, when runs complete, or when audits fail.</p> <p>See Notifications for Slack webhooks, API, and email setup.</p>"},{"location":"configurations/overview/#supported-engines","title":"Supported Engines","text":"<p>Vulcan works with these data warehouses:</p> <ul> <li> <p>PostgreSQL</p> </li> <li> <p>Snowflake</p> </li> </ul>"},{"location":"configurations/overview/#configuration-reference","title":"Configuration Reference","text":"Topic Description Configuration Reference Complete list of all configuration parameters Variables Environment variables and <code>.env</code> files Model Defaults Default settings for all models Execution Hooks <code>before_all</code> and <code>after_all</code> statements Linter Code quality rules and custom linters Notifications Slack and email notification setup"},{"location":"configurations/overview/#best-practices","title":"Best Practices","text":"<p>Use environment variables for sensitive data like passwords and API keys. Keeps secrets out of your config files and makes it easier to manage different environments.</p> <p>Set meaningful defaults in <code>model_defaults</code> to reduce boilerplate. If most of your models use the same dialect, start date, or cron schedule, set it once here instead of repeating it everywhere.</p> <p>Enable linting to catch common errors early in development. Fix issues before they make it to production.</p> <p>Separate state connection from your data warehouse for better isolation. Prevents state operations from interfering with your data processing.</p> <p>Use multiple gateways for different environments: dev, staging, prod. Test changes safely before deploying to production. Use different database configurations for each environment.</p>"},{"location":"configurations/ci_cd/","title":"CI/CD","text":""},{"location":"configurations/ci_cd/#cicd","title":"CI/CD","text":"<p>We're working on comprehensive CI/CD documentation to help you integrate Vulcan into your deployment pipelines. Check back soon for guides on automated testing, deployment workflows, and best practices for continuous integration and delivery with Vulcan.</p>"},{"location":"configurations/engines/bigquery/bigquery/","title":"BigQuery","text":""},{"location":"configurations/engines/bigquery/bigquery/#bigquery","title":"BigQuery","text":"<p>Google BigQuery is a serverless, highly scalable data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It's ideal for large-scale analytics, machine learning workloads, and real-time data analysis. Vulcan integrates seamlessly with BigQuery to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/bigquery/bigquery/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>bigquery</code></p>"},{"location":"configurations/engines/bigquery/bigquery/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Google Cloud Platform (GCP) project with BigQuery enabled</li> <li>A service account with appropriate permissions (or OAuth credentials)</li> <li>A service account key file (JSON format) for authentication</li> </ol>"},{"location":"configurations/engines/bigquery/bigquery/#permissions","title":"Permissions","text":"<p>Vulcan requires the following BigQuery permissions:</p> <ul> <li><code>bigquery.datasets.create</code> - to create datasets (schemas)</li> <li><code>bigquery.tables.create</code> - to create tables and views</li> <li><code>bigquery.tables.getData</code> - to read data from tables</li> <li><code>bigquery.tables.updateData</code> - to insert, update, and delete data</li> <li><code>bigquery.jobs.create</code> - to run queries</li> </ul> <p>The <code>BigQuery Data Editor</code> and <code>BigQuery Job User</code> roles provide these permissions.</p>"},{"location":"configurations/engines/bigquery/bigquery/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a BigQuery gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>bigquery</code> string Y <code>method</code> Authentication method (<code>service-account</code>, <code>oauth</code>, <code>oauth-secrets</code>, <code>application-default</code>) string Y <code>project</code> The GCP project ID where BigQuery resources are located string Y <code>keyfile</code> Path to the service account JSON key file (required for <code>service-account</code> method) string N"},{"location":"configurations/engines/bigquery/bigquery/#service-account-key-file","title":"Service Account Key File","text":"<p>The <code>keyfile</code> is a JSON key file downloaded from the Google Cloud Console:</p> <ol> <li>Go to IAM &amp; Admin \u2192 Service Accounts</li> <li>Select your service account (or create a new one)</li> <li>Go to the Keys tab</li> <li>Click Add Key \u2192 Create new key</li> <li>Select JSON and click Create</li> </ol> <p>The downloaded file will have the following structure:</p> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"&lt;your-project-id&gt;\",\n  \"private_key_id\": \"&lt;key-id&gt;\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n&lt;private-key-content&gt;\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"&lt;service-account-name&gt;@&lt;project-id&gt;.iam.gserviceaccount.com\",\n  \"client_id\": \"&lt;client-id&gt;\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/&lt;service-account-name&gt;\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre>"},{"location":"configurations/engines/bigquery/bigquery/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with BigQuery:</p> Image Description <code>tmdcio/vulcan-bigquery:0.228.1.8</code> Main Vulcan API service for BigQuery <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-bigquery:0.228.1.8\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/bigquery/bigquery/#materialization-strategy","title":"Materialization Strategy","text":"<p>BigQuery uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> DELETE by time range, then INSERT Vulcan will first delete existing records within the target time range, then insert the new data. This ensures data consistency and prevents duplicates when reprocessing time intervals. <code>INCREMENTAL_BY_UNIQUE_KEY</code> MERGE ON unique key Vulcan uses BigQuery's MERGE statement to update existing records based on the unique key or insert new ones if they don't exist. <code>INCREMENTAL_BY_PARTITION</code> DELETE by partitioning key, then INSERT Vulcan will delete existing records matching the partitioning key, then insert the new data. This ensures partition-level consistency when reprocessing data. <code>FULL</code> CREATE OR REPLACE TABLE Vulcan uses BigQuery's <code>CREATE OR REPLACE TABLE</code> statement to completely rebuild the table from scratch each time. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>The <code>BigQuery Data Editor</code> and <code>BigQuery Job User</code> roles together provide the minimum permissions required for Vulcan to operate.</p> <p>Warning</p> <p>Never commit your keyfile to version control. Add it to <code>.gitignore</code> and store it securely.</p>"},{"location":"configurations/engines/databricks/databricks/","title":"Databricks","text":""},{"location":"configurations/engines/databricks/databricks/#databricks","title":"Databricks","text":"<p>Databricks is a unified analytics platform built on Apache Spark that provides collaborative notebooks, automated cluster management, and a powerful SQL engine. It's ideal for large-scale data engineering, machine learning, and collaborative analytics. Vulcan integrates with Databricks to manage your data transformations using Unity Catalog and Delta Lake.</p>"},{"location":"configurations/engines/databricks/databricks/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>databricks</code></p>"},{"location":"configurations/engines/databricks/databricks/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Databricks workspace with SQL warehouse or cluster access</li> <li>A personal access token or service principal credentials</li> <li>The HTTP path to your SQL warehouse or cluster</li> </ol>"},{"location":"configurations/engines/databricks/databricks/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Databricks permissions:</p> <ul> <li><code>USE CATALOG</code> on the target catalog</li> <li><code>USE SCHEMA</code> and <code>CREATE SCHEMA</code> on the target schemas</li> <li><code>CREATE TABLE</code> and <code>CREATE VIEW</code> on schemas</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> </ul>"},{"location":"configurations/engines/databricks/databricks/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Databricks gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>databricks</code> string Y <code>server_hostname</code> The Databricks workspace hostname (e.g., <code>adb-xxxxx.azuredatabricks.net</code>) string Y <code>http_path</code> The HTTP path to the SQL warehouse or cluster string Y <code>access_token</code> Personal access token or service principal token for authentication string Y <code>catalog</code> The Unity Catalog name to use as the default catalog string Y"},{"location":"configurations/engines/databricks/databricks/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Databricks:</p> Image Description <code>tmdcio/vulcan-databricks:0.228.1.6</code> Main Vulcan API service for Databricks <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-databricks:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/databricks/databricks/#materialization-strategy","title":"Materialization Strategy","text":"<p>Databricks uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> INSERT OVERWRITE by time column partition Vulcan will overwrite the entire partition that corresponds to the time column, rather than deleting and inserting individual records. This approach is more efficient for partitioned data and leverages Databricks' native partitioning capabilities with Delta Lake. <code>INCREMENTAL_BY_UNIQUE_KEY</code> MERGE ON unique key Vulcan uses Databricks' MERGE statement (with Delta Lake) to update existing records based on the unique key or insert new ones if they don't exist. This provides ACID transactions and efficient upserts. <code>INCREMENTAL_BY_PARTITION</code> REPLACE WHERE by partitioning key Vulcan uses Databricks' <code>REPLACE WHERE</code> clause to efficiently replace data within specific partitions based on the partitioning key, leveraging Delta Lake's capabilities. <code>FULL</code> INSERT OVERWRITE Vulcan uses Databricks' <code>INSERT OVERWRITE</code> statement to completely replace the table contents each time, working seamlessly with Delta Lake. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>The <code>http_path</code> can be found in your Databricks workspace under SQL Warehouses \u2192 [Your Warehouse] \u2192 Connection Details.</p> <p>Warning</p> <p>Never commit your access token to version control. Use environment variables: <code>access_token: {{ env_var('DATABRICKS_TOKEN') }}</code></p>"},{"location":"configurations/engines/fabric/fabric/","title":"Microsoft Fabric","text":""},{"location":"configurations/engines/fabric/fabric/#microsoft-fabric","title":"Microsoft Fabric","text":"<p>Microsoft Fabric is an all-in-one analytics solution that brings together data engineering, data science, real-time analytics, and business intelligence. Built on a lakehouse architecture, it provides a unified experience across your data estate. Vulcan integrates with Fabric to manage your data transformations using SQL endpoints.</p>"},{"location":"configurations/engines/fabric/fabric/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>fabric</code></p>"},{"location":"configurations/engines/fabric/fabric/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Microsoft Fabric workspace with a SQL endpoint</li> <li>Azure Active Directory service principal or user credentials</li> <li>ODBC Driver 18 for SQL Server installed on your system</li> </ol>"},{"location":"configurations/engines/fabric/fabric/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Microsoft Fabric permissions:</p> <ul> <li><code>CREATE SCHEMA</code> on the target database</li> <li><code>CREATE TABLE</code> and <code>CREATE VIEW</code> on schemas</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li>Workspace Contributor or Admin role for full access</li> </ul>"},{"location":"configurations/engines/fabric/fabric/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Fabric gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>fabric</code> string Y <code>host</code> The Fabric SQL endpoint hostname string Y <code>workspace_id</code> The Microsoft Fabric workspace ID (GUID) string Y <code>tenant_id</code> The Azure Active Directory tenant ID string Y <code>port</code> The port number for the SQL endpoint (typically <code>1433</code>) int Y <code>user</code> The service principal client ID or username string Y <code>password</code> The service principal client secret or password string Y <code>database</code> The name of the database (lakehouse or warehouse) to connect to string Y <code>driver_name</code> The ODBC driver name (default: <code>ODBC Driver 18 for SQL Server</code>) string N <code>odbc_properties</code> Additional ODBC connection properties as key-value pairs dict N"},{"location":"configurations/engines/fabric/fabric/#obtaining-credentials","title":"Obtaining Credentials","text":""},{"location":"configurations/engines/fabric/fabric/#tenant-id","title":"Tenant ID","text":"<ol> <li>Go to Azure Portal \u2192 Azure Active Directory</li> <li>Click Overview</li> <li>Copy the Tenant ID (GUID format)</li> </ol>"},{"location":"configurations/engines/fabric/fabric/#workspace-id","title":"Workspace ID","text":"<ol> <li>Open your workspace in the Microsoft Fabric Portal</li> <li>Look at the URL: <code>https://app.fabric.microsoft.com/groups/&lt;workspace-id&gt;/...</code></li> <li>Copy the workspace-id (GUID format)</li> </ol>"},{"location":"configurations/engines/fabric/fabric/#service-principal-client-id-secret","title":"Service Principal (Client ID &amp; Secret)","text":"<ol> <li>Go to Azure Portal \u2192 Azure Active Directory \u2192 App registrations</li> <li>Select your app (or click New registration to create one)</li> <li>Copy the Application (client) ID \u2014 use this as <code>user</code></li> <li>Go to Certificates &amp; secrets \u2192 New client secret</li> <li>Copy the Secret value \u2014 use this as <code>password</code></li> </ol>"},{"location":"configurations/engines/fabric/fabric/#host-sql-endpoint","title":"Host (SQL Endpoint)","text":"<ol> <li>Open your workspace in the Microsoft Fabric Portal</li> <li>Go to Workspace settings</li> <li>Find the SQL connection string or SQL endpoint</li> <li>Copy the hostname (e.g., <code>your-workspace.datawarehouse.fabric.microsoft.com</code>)</li> </ol>"},{"location":"configurations/engines/fabric/fabric/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Microsoft Fabric:</p> Image Description <code>tmdcio/vulcan-fabric:0.228.1.6</code> Main Vulcan API service for Fabric <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-fabric:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/fabric/fabric/#materialization-strategy","title":"Materialization Strategy","text":"<p>Materialization strategies for Microsoft Fabric depend on the model kind and engine capabilities. For detailed information about how different model kinds are materialized, see the model kinds documentation.</p> <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>Ensure the ODBC Driver 18 for SQL Server is installed on your system. You can download it from the Microsoft website.</p> <p>Warning</p> <p>Never commit your client secret to version control. Use environment variables to store sensitive credentials.</p>"},{"location":"configurations/engines/mssql/mssql/","title":"Microsoft SQL Server","text":""},{"location":"configurations/engines/mssql/mssql/#microsoft-sql-server","title":"Microsoft SQL Server","text":"<p>Microsoft SQL Server is a relational database management system known for its robust performance, enterprise-grade security, and comprehensive tooling. It's widely used for transactional workloads, data warehousing, and business intelligence. Vulcan integrates with SQL Server to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/mssql/mssql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mssql</code></p>"},{"location":"configurations/engines/mssql/mssql/#prerequisites","title":"Prerequisites","text":"<ol> <li>A SQL Server instance (on-premises, Azure SQL, or SQL Server in a container)</li> <li>A database user with appropriate permissions</li> <li>Network connectivity to the SQL Server instance</li> </ol>"},{"location":"configurations/engines/mssql/mssql/#permissions","title":"Permissions","text":"<p>Vulcan requires the following SQL Server permissions:</p> <ul> <li><code>CREATE SCHEMA</code> on the target database</li> <li><code>CREATE TABLE</code> and <code>CREATE VIEW</code> on schemas</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li><code>ALTER</code> on schemas for schema modifications</li> </ul>"},{"location":"configurations/engines/mssql/mssql/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a SQL Server gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>mssql</code> string Y <code>host</code> The hostname or IP address of the SQL Server instance string Y <code>port</code> The port number of the SQL Server instance (default: <code>1433</code>) int Y <code>user</code> The username for SQL Server authentication string Y <code>password</code> The password for SQL Server authentication string Y <code>database</code> The name of the database to connect to string Y <code>concurrent_tasks</code> Maximum number of concurrent tasks (default: <code>4</code>) int N <code>trust_server_certificate</code> Whether to trust the server certificate without validation (default: <code>false</code>) bool N"},{"location":"configurations/engines/mssql/mssql/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with SQL Server:</p> Image Description <code>tmdcio/vulcan-mssql:0.228.1.6</code> Main Vulcan API service for SQL Server <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-mssql:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/mssql/mssql/#materialization-strategy","title":"Materialization Strategy","text":"<p>Materialization strategies for Microsoft SQL Server depend on the model kind and engine capabilities. For detailed information about how different model kinds are materialized, see the model kinds documentation.</p> <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>The <code>dialect</code> for SQL Server models should be set to <code>tsql</code> (Transact-SQL), not <code>mssql</code>.</p> <p>Warning</p> <p>Only set <code>trust_server_certificate: true</code> in development environments. In production, configure proper SSL certificates.</p>"},{"location":"configurations/engines/mysql/mysql/","title":"MySQL","text":""},{"location":"configurations/engines/mysql/mysql/#mysql","title":"MySQL","text":"<p>MySQL is one of the world's most popular open-source relational databases, known for its reliability, ease of use, and strong community support. It's widely used for web applications, content management systems, and data warehousing. Vulcan integrates with MySQL to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/mysql/mysql/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>mysql</code></p>"},{"location":"configurations/engines/mysql/mysql/#prerequisites","title":"Prerequisites","text":"<ol> <li>A MySQL server instance (version 5.7 or higher recommended)</li> <li>A database user with appropriate permissions</li> <li>Network connectivity to the MySQL server</li> </ol>"},{"location":"configurations/engines/mysql/mysql/#permissions","title":"Permissions","text":"<p>Vulcan requires the following MySQL permissions:</p> <ul> <li><code>CREATE</code> on the target database for creating schemas and tables</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li><code>ALTER</code> for schema modifications</li> <li><code>DROP</code> for table cleanup during development</li> </ul>"},{"location":"configurations/engines/mysql/mysql/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a MySQL gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>mysql</code> string Y <code>host</code> The hostname or IP address of the MySQL server string Y <code>port</code> The port number of the MySQL server (default: <code>3306</code>) int Y <code>user</code> The username for MySQL authentication string Y <code>password</code> The password for MySQL authentication string Y <code>database</code> The name of the database to connect to string Y <code>charset</code> The character set for the connection (default: <code>utf8mb4</code>) string N <code>ssl</code> SSL configuration options for secure connections dict N"},{"location":"configurations/engines/mysql/mysql/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with MySQL:</p> Image Description <code>tmdcio/vulcan-mysql:0.228.1.6</code> Main Vulcan API service for MySQL <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-mysql:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/mysql/mysql/#materialization-strategy","title":"Materialization Strategy","text":"<p>Materialization strategies for MySQL depend on the model kind and engine capabilities. For detailed information about how different model kinds are materialized, see the model kinds documentation.</p> <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>MySQL 5.7 or higher is recommended. Use the <code>ssl</code> option for secure connections in production environments.</p> <p>Warning</p> <p>Always use environment variables for passwords: <code>password: {{ env_var('MYSQL_PASSWORD') }}</code></p>"},{"location":"configurations/engines/postgres/postgres/","title":"Postgres","text":""},{"location":"configurations/engines/postgres/postgres/#postgres","title":"Postgres","text":"<p>PostgreSQL is a powerful, open-source relational database that works great with Vulcan. It's perfect for smaller projects, development environments, or when you want full control over your database infrastructure.</p>"},{"location":"configurations/engines/postgres/postgres/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>postgres</code></p>"},{"location":"configurations/engines/postgres/postgres/#prerequisites","title":"Prerequisites","text":"<ol> <li>A PostgreSQL server instance (version 12 or higher recommended)</li> <li>A database user with appropriate permissions</li> <li>Network connectivity to the PostgreSQL server</li> </ol>"},{"location":"configurations/engines/postgres/postgres/#permissions","title":"Permissions","text":"<p>Vulcan requires the following PostgreSQL permissions:</p> <ul> <li><code>CREATE</code> on the target database for creating schemas</li> <li><code>CREATE</code> on schemas for creating tables and views</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li><code>USAGE</code> on schemas</li> </ul>"},{"location":"configurations/engines/postgres/postgres/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a PostgreSQL gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>postgres</code> string Y <code>host</code> The hostname of the Postgres server string Y <code>user</code> The username to use for authentication with the Postgres server string Y <code>password</code> The password to use for authentication with the Postgres server string Y <code>port</code> The port number of the Postgres server int Y <code>database</code> The name of the database instance to connect to string Y <code>keepalives_idle</code> The number of seconds between each keepalive packet sent to the server. int N <code>connect_timeout</code> The number of seconds to wait for the connection to the server. (Default: <code>10</code>) int N <code>role</code> The role to use for authentication with the Postgres server string N <code>sslmode</code> The security of the connection to the Postgres server string N <code>application_name</code> The name of the application to use for the connection string N"},{"location":"configurations/engines/postgres/postgres/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with PostgreSQL:</p> Image Description <code>tmdcio/vulcan-postgres:0.228.1.8</code> Main Vulcan API service for PostgreSQL <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-postgres:0.228.1.8\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/postgres/postgres/#materialization-strategy","title":"Materialization Strategy","text":"<p>PostgreSQL uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> DELETE by time range, then INSERT Vulcan will first delete existing records within the target time range, then insert the new data. This ensures data consistency and prevents duplicates when reprocessing time intervals. <code>INCREMENTAL_BY_UNIQUE_KEY</code> MERGE ON unique key Vulcan uses PostgreSQL's MERGE (UPSERT) functionality to update existing records based on the unique key or insert new ones if they don't exist. <code>INCREMENTAL_BY_PARTITION</code> DELETE by partitioning key, then INSERT Vulcan will delete existing records matching the partitioning key, then insert the new data. This ensures partition-level consistency when reprocessing data. <code>FULL</code> DROP TABLE, CREATE TABLE, INSERT Vulcan drops the existing table, creates a new one, and inserts all data. This completely rebuilds the table from scratch each time. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>Use <code>sslmode: require</code> for secure connections in production environments.</p> <p>Warning</p> <p>Always use environment variables for passwords: <code>password: {{ env_var('POSTGRES_PASSWORD') }}</code></p>"},{"location":"configurations/engines/redshift/redshift/","title":"Amazon Redshift","text":""},{"location":"configurations/engines/redshift/redshift/#amazon-redshift","title":"Amazon Redshift","text":"<p>Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. Built for high-performance analytics and business intelligence workloads, it uses columnar storage and massively parallel processing (MPP) to deliver fast query performance. Vulcan integrates seamlessly with Redshift to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/redshift/redshift/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>redshift</code></p>"},{"location":"configurations/engines/redshift/redshift/#prerequisites","title":"Prerequisites","text":"<ol> <li>An Amazon Redshift cluster or Redshift Serverless endpoint</li> <li>A database user with appropriate permissions</li> <li>Network connectivity to the Redshift cluster (VPC configuration may be required)</li> </ol>"},{"location":"configurations/engines/redshift/redshift/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Redshift permissions:</p> <ul> <li><code>CREATE</code> on the target database for creating schemas</li> <li><code>CREATE</code> on schemas for creating tables and views</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li><code>USAGE</code> on schemas</li> </ul>"},{"location":"configurations/engines/redshift/redshift/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Redshift gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>redshift</code> string Y <code>host</code> The Redshift cluster endpoint hostname string Y <code>port</code> The port number of the Redshift cluster (default: <code>5439</code>) int Y <code>user</code> The username for Redshift authentication string Y <code>password</code> The password for Redshift authentication string Y <code>database</code> The name of the database to connect to string Y <code>sslmode</code> SSL mode for the connection (<code>require</code>, <code>verify-ca</code>, <code>verify-full</code>) string N <code>timeout</code> Connection timeout in seconds int N"},{"location":"configurations/engines/redshift/redshift/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Redshift:</p> Image Description <code>tmdcio/vulcan-redshift:0.228.1.6</code> Main Vulcan API service for Redshift <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-redshift:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/redshift/redshift/#materialization-strategy","title":"Materialization Strategy","text":"<p>Redshift uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> DELETE by time range, then INSERT Vulcan will first delete existing records within the target time range, then insert the new data. This ensures data consistency and prevents duplicates when reprocessing time intervals. <code>INCREMENTAL_BY_UNIQUE_KEY</code> MERGE ON unique key Vulcan uses Redshift's MERGE statement to update existing records based on the unique key or insert new ones if they don't exist. <code>INCREMENTAL_BY_PARTITION</code> DELETE by partitioning key, then INSERT Vulcan will delete existing records matching the partitioning key, then insert the new data. This ensures partition-level consistency when reprocessing data. <code>FULL</code> DROP TABLE, CREATE TABLE, INSERT Vulcan drops the existing table, creates a new one, and inserts all data. This completely rebuilds the table from scratch each time. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>Use <code>sslmode: require</code> or higher for secure connections in production environments.</p> <p>Warning</p> <p>Always use environment variables for passwords: <code>password: {{ env_var('REDSHIFT_PASSWORD') }}</code></p>"},{"location":"configurations/engines/snowflake/snowflake/","title":"Snowflake","text":""},{"location":"configurations/engines/snowflake/snowflake/#snowflake","title":"Snowflake","text":"<p>Snowflake is a cloud-based data warehouse that provides scalable storage and compute resources. It's ideal for enterprise workloads, large-scale analytics, and data sharing across organizations. Vulcan integrates seamlessly with Snowflake to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/snowflake/snowflake/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>snowflake</code></p>"},{"location":"configurations/engines/snowflake/snowflake/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Snowflake account with valid credentials</li> <li>A warehouse available for running computations</li> </ol>"},{"location":"configurations/engines/snowflake/snowflake/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Snowflake permissions:</p> <ul> <li><code>USAGE</code> on a warehouse to execute computations</li> <li><code>CREATE SCHEMA</code> on the target database</li> <li><code>CREATE TABLE</code> and <code>CREATE VIEW</code> on schemas</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>TRUNCATE</code> on tables</li> </ul>"},{"location":"configurations/engines/snowflake/snowflake/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Snowflake gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>snowflake</code> string Y <code>account</code> The Snowflake account identifier (e.g., <code>org-name-account-name</code>) string Y <code>user</code> The username to use for authentication with the Snowflake server string Y <code>password</code> The password to use for authentication with the Snowflake server string Y <code>warehouse</code> The name of the Snowflake warehouse to use for running computations string Y <code>database</code> The name of the Snowflake database instance to connect to string Y <code>role</code> The role to use for authentication with the Snowflake server string N <code>authenticator</code> The Snowflake authenticator method (e.g., <code>externalbrowser</code>, <code>oauth</code>) string N <code>token</code> The Snowflake OAuth 2.0 access token for authentication string N <code>private_key_path</code> The path to the private key file to use for authentication string N <code>private_key_passphrase</code> The passphrase to decrypt the private key (if encrypted) string N"},{"location":"configurations/engines/snowflake/snowflake/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Snowflake:</p> Image Description <code>tmdcio/vulcan-snowflake:0.228.1.8</code> Main Vulcan API service for Snowflake <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-snowflake:0.228.1.8\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/snowflake/snowflake/#materialization-strategy","title":"Materialization Strategy","text":"<p>Snowflake uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> DELETE by time range, then INSERT Vulcan will first delete existing records within the target time range, then insert the new data. This ensures data consistency and prevents duplicates when reprocessing time intervals. <code>INCREMENTAL_BY_UNIQUE_KEY</code> MERGE ON unique key Vulcan uses Snowflake's MERGE statement to update existing records based on the unique key or insert new ones if they don't exist. <code>INCREMENTAL_BY_PARTITION</code> DELETE by partitioning key, then INSERT Vulcan will delete existing records matching the partitioning key, then insert the new data. This ensures partition-level consistency when reprocessing data. <code>FULL</code> CREATE OR REPLACE TABLE Vulcan uses Snowflake's <code>CREATE OR REPLACE TABLE</code> statement to completely rebuild the table from scratch each time. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>The <code>account</code> identifier format is <code>&lt;org-name&gt;-&lt;account-name&gt;</code> (e.g., <code>myorg-myaccount</code>). Find it in your Snowflake URL.</p> <p>Warning</p> <p>Always use environment variables for sensitive credentials: <code>password: {{ env_var('SNOWFLAKE_PASSWORD') }}</code></p>"},{"location":"configurations/engines/spark/spark/","title":"Spark","text":""},{"location":"configurations/engines/spark/spark/#spark","title":"Spark","text":"<p>Apache Spark is a unified analytics engine for large-scale data processing. Vulcan integrates with Spark to manage your data transformations with version control and safe deployments.</p>"},{"location":"configurations/engines/spark/spark/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>spark</code></p>"},{"location":"configurations/engines/spark/spark/#prerequisites","title":"Prerequisites","text":"<ol> <li>A running Spark cluster (standalone, YARN, or Kubernetes)</li> <li>Spark 3.x or higher (3.4+ recommended for catalog support)</li> <li>Network connectivity to the Spark master node</li> </ol>"},{"location":"configurations/engines/spark/spark/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Spark permissions:</p> <ul> <li>Access to create and manage tables in the configured catalog</li> <li>Read/write access to the configured storage (S3, HDFS, etc.)</li> <li>Permission to submit Spark applications</li> </ul>"},{"location":"configurations/engines/spark/spark/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Spark gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>spark</code> string Y <code>config_dir</code> Value to set for <code>SPARK_CONFIG_DIR</code> string N <code>catalog</code> The catalog to use when issuing commands string N <code>config</code> Key/value pairs to set for the Spark Configuration dict N"},{"location":"configurations/engines/spark/spark/#catalog-support","title":"Catalog Support","text":"<p>Vulcan's Spark integration is designed for single catalog usage. All models must be defined with a single catalog.</p> <p>If <code>catalog</code> is not set, the behavior depends on the Spark version:</p> Spark Version Default Catalog Behavior &gt;= 3.4 Default catalog determined at runtime &lt; 3.4 Default catalog is <code>spark_catalog</code>"},{"location":"configurations/engines/spark/spark/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Spark:</p> Image Description <code>tmdcio/vulcan-spark:0.228.1.6</code> Main Vulcan API service for Spark <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-spark:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/spark/spark/#materialization-strategy","title":"Materialization Strategy","text":"<p>Spark uses the following materialization strategies depending on the model kind:</p> Model Kind Strategy Description <code>INCREMENTAL_BY_TIME_RANGE</code> INSERT OVERWRITE by time column partition Vulcan will overwrite the entire partition that corresponds to the time column, rather than deleting and inserting individual records. This approach is more efficient for partitioned data and leverages Spark's native partitioning capabilities. <code>INCREMENTAL_BY_UNIQUE_KEY</code> Not supported Spark does not support <code>INCREMENTAL_BY_UNIQUE_KEY</code> models. Consider using <code>INCREMENTAL_BY_TIME_RANGE</code> or <code>INCREMENTAL_BY_PARTITION</code> instead. <code>INCREMENTAL_BY_PARTITION</code> INSERT OVERWRITE by partitioning key Vulcan will overwrite the entire partition based on the partitioning key. This leverages Spark's native partitioning for efficient data management. <code>FULL</code> INSERT OVERWRITE Vulcan uses Spark's <code>INSERT OVERWRITE</code> statement to completely replace the table contents each time. <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>Spark may not be used for the Vulcan state connection. Use a transactional database like PostgreSQL for the <code>state_connection</code>.</p> <p>Warning</p> <p>Always use environment variables for sensitive credentials in your Spark configuration (S3 keys, database passwords, etc.).</p>"},{"location":"configurations/engines/trino/trino/","title":"Trino","text":""},{"location":"configurations/engines/trino/trino/#trino","title":"Trino","text":"<p>Trino (formerly PrestoSQL) is a distributed SQL query engine designed for fast, interactive analytics across large datasets. It excels at querying data from multiple sources including data lakes, databases, and object storage. Vulcan integrates with Trino to manage your data transformations using catalogs like Iceberg, Hive, and Delta Lake.</p>"},{"location":"configurations/engines/trino/trino/#localbuilt-in-scheduler","title":"Local/Built-in Scheduler","text":"<p>Engine Adapter Type: <code>trino</code></p>"},{"location":"configurations/engines/trino/trino/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Trino cluster with coordinator and worker nodes</li> <li>A catalog configured (e.g., Iceberg, Hive, or Delta Lake)</li> <li>Network connectivity to the Trino coordinator</li> </ol>"},{"location":"configurations/engines/trino/trino/#permissions","title":"Permissions","text":"<p>Vulcan requires the following Trino permissions (depending on your security configuration):</p> <ul> <li><code>CREATE SCHEMA</code> on the target catalog</li> <li><code>CREATE TABLE</code> and <code>CREATE VIEW</code> on schemas</li> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on tables</li> <li><code>DROP TABLE</code> for table cleanup during development</li> </ul>"},{"location":"configurations/engines/trino/trino/#connection-options","title":"Connection Options","text":"<p>Here are all the connection parameters you can use when setting up a Trino gateway:</p> Option Description Type Required <code>type</code> Engine type name - must be <code>trino</code> string Y <code>host</code> The hostname of the Trino coordinator string Y <code>port</code> The port number of the Trino coordinator (default: <code>8080</code>) int Y <code>user</code> The username for Trino authentication string Y <code>catalog</code> The default catalog to use for queries string Y <code>http_scheme</code> The HTTP scheme (<code>http</code> or <code>https</code>) string N <code>password</code> The password for Trino authentication (if password authentication is enabled) string N <code>roles</code> Role to use for queries (if role-based access control is enabled) dict N"},{"location":"configurations/engines/trino/trino/#docker-images","title":"Docker Images","text":"<p>The following Docker images are available for running Vulcan with Trino:</p> Image Description <code>tmdcio/vulcan-trino:0.228.1.6</code> Main Vulcan API service for Trino <code>tmdcio/vulcan-transpiler:0.228.1.8</code> SQL transpiler service <p>Pull the images:</p> <pre><code>docker pull tmdcio/vulcan-trino:0.228.1.6\ndocker pull tmdcio/vulcan-transpiler:0.228.1.8\n</code></pre>"},{"location":"configurations/engines/trino/trino/#materialization-strategy","title":"Materialization Strategy","text":"<p>Materialization strategies for Trino depend on the model kind and underlying catalog capabilities (Iceberg, Hive, Delta Lake, etc.). For detailed information about how different model kinds are materialized, see the model kinds documentation.</p> <p>Learn more about materialization strategies:</p> <ul> <li>INCREMENTAL_BY_TIME_RANGE</li> <li>INCREMENTAL_BY_UNIQUE_KEY</li> <li>INCREMENTAL_BY_PARTITION</li> <li>FULL</li> </ul> <p>Note</p> <p>Use <code>http_scheme: https</code> for secure connections in production environments.</p> <p>Warning</p> <p>Always use environment variables for passwords: <code>password: {{ env_var('TRINO_PASSWORD') }}</code></p>"},{"location":"configurations/options/execution_hooks/","title":"Execution Hooks","text":""},{"location":"configurations/options/execution_hooks/#execution-hooks","title":"Execution Hooks","text":"<p>Run SQL statements or macros automatically at the start and end of <code>vulcan plan</code> and <code>vulcan run</code> commands. Automate setup and cleanup tasks: create temporary tables, grant permissions, log pipeline runs, clean up after execution.</p>"},{"location":"configurations/options/execution_hooks/#overview","title":"Overview","text":"<p>Two hooks run at different times:</p> Hook When it Runs Common Use Cases <code>before_all</code> Before any model is processed Setup tables, initialize logging, validate prerequisites <code>after_all</code> After all models are processed Grant privileges, cleanup, send notifications, update metadata <p>The <code>before_all</code> hook runs once at the beginning, before Vulcan processes any models. Use it for setup tasks. The <code>after_all</code> hook runs once at the end, after all models are processed. Use it for cleanup or post-processing.</p>"},{"location":"configurations/options/execution_hooks/#basic-configuration","title":"Basic Configuration","text":"YAMLPython <pre><code>before_all:\n  - CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\n\n  - INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\n\nafter_all:\n  - \"@grant_select_privileges()\"\n\n  - UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\n</code></pre> <pre><code>from vulcan.core.config import Config\n\nconfig = Config(\n    before_all=[\n        \"CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)\",\n        \"INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)\"\n    ],\n    after_all=[\n        \"@grant_select_privileges()\",\n        \"UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'\"\n    ],\n)\n</code></pre>"},{"location":"configurations/options/execution_hooks/#using-macros-in-hooks","title":"Using Macros in Hooks","text":"<p>Hooks execute Vulcan macros using the <code>@macro_name()</code> syntax. Macros have access to runtime context, so hooks can be dynamic. They see what views were created, what schemas are used, and what environment you're running in. Write hooks that adapt to your pipeline state.</p>"},{"location":"configurations/options/execution_hooks/#available-context-variables","title":"Available Context Variables","text":"<p>Macros invoked in hooks have access to:</p> Property Type Description <code>evaluator.views</code> <code>list[str]</code> All view names created in the virtual layer <code>evaluator.schemas</code> <code>list[str]</code> All schema names used by models <code>evaluator.this_env</code> <code>str</code> Current environment name (e.g., <code>prod</code>, <code>dev</code>) <code>evaluator.gateway</code> <code>str</code> Current gateway name"},{"location":"configurations/options/execution_hooks/#use-cases","title":"Use Cases","text":""},{"location":"configurations/options/execution_hooks/#1-granting-privileges-on-views","title":"1. Granting Privileges on Views","text":"<p>Creating many views and granting permissions model-by-model gets tedious. Use <code>after_all</code> to grant access to all views at once:</p> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_select_privileges(evaluator):\n    \"\"\"Grant SELECT on all views to the analytics role.\"\"\"\n    if not evaluator.views:\n        return []\n\n    return [\n        f\"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE analytics_role;\"\n        for view_name in evaluator.views\n    ]\n</code></pre> config.yaml<pre><code>after_all:\n  - \"@grant_select_privileges()\"\n</code></pre> <p>Preventing Name Replacement</p> <p>The <code>/* sqlglot.meta replace=false */</code> comment tells Vulcan not to replace the view name with the physical table name during SQL rendering. Without it, Vulcan might swap in the underlying table name, which breaks your GRANT statement.</p>"},{"location":"configurations/options/execution_hooks/#2-environment-specific-execution","title":"2. Environment-Specific Execution","text":"<p>Use different behavior in different environments. Grant certain permissions only in production. Run cleanup tasks only in development. The <code>@IF</code> macro conditionally executes statements based on the current environment:</p> config.yaml<pre><code>after_all:\n  # Only grant schema usage in production\n  - \"@IF(@this_env = 'prod', @grant_schema_usage())\"\n\n  # Only run cleanup in development\n  - \"@IF(@this_env != 'prod', @cleanup_dev_tables())\"\n</code></pre> macros/privileges.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef grant_schema_usage(evaluator):\n    \"\"\"Grant USAGE on all schemas to admin role (production only).\"\"\"\n    if evaluator.this_env != \"prod\" or not evaluator.schemas:\n        return []\n\n    return [\n        f\"GRANT USAGE ON SCHEMA {schema} TO ROLE admin_role;\"\n        for schema in evaluator.schemas\n    ]\n\n@macro()\ndef cleanup_dev_tables(evaluator):\n    \"\"\"Clean up temporary tables in development environments.\"\"\"\n    return [\n        \"DROP TABLE IF EXISTS temp_debug_output;\",\n        \"DROP TABLE IF EXISTS temp_test_data;\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#3-audit-logging","title":"3. Audit Logging","text":"<p>Track when your pipeline runs and how long it takes. Log the start time in <code>before_all</code> and the completion time in <code>after_all</code>:</p> config.yaml<pre><code>before_all:\n  - |\n    CREATE TABLE IF NOT EXISTS pipeline_audit (\n      run_id VARCHAR,\n      environment VARCHAR,\n      started_at TIMESTAMP,\n      completed_at TIMESTAMP,\n      status VARCHAR\n    )\n  - \"@log_pipeline_start()\"\n\nafter_all:\n  - \"@log_pipeline_end()\"\n</code></pre> macros/audit.py<pre><code>from vulcan.core.macros import macro\nimport uuid\n\n@macro()\ndef log_pipeline_start(evaluator):\n    run_id = str(uuid.uuid4())[:8]\n    return [\n        f\"\"\"\n        INSERT INTO pipeline_audit (run_id, environment, started_at, status)\n        VALUES ('{run_id}', '{evaluator.this_env}', CURRENT_TIMESTAMP, 'running')\n        \"\"\"\n    ]\n\n@macro()\ndef log_pipeline_end(evaluator):\n    return [\n        f\"\"\"\n        UPDATE pipeline_audit \n        SET completed_at = CURRENT_TIMESTAMP, status = 'completed'\n        WHERE environment = '{evaluator.this_env}' \n          AND status = 'running'\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#4-schema-and-database-setup","title":"4. Schema and Database Setup","text":"<p>Before models run, ensure all schemas they depend on exist. Instead of creating them manually or remembering the order, let <code>before_all</code> handle it:</p> config.yaml<pre><code>before_all:\n  - CREATE SCHEMA IF NOT EXISTS staging\n\n  - CREATE SCHEMA IF NOT EXISTS analytics\n\n  - CREATE SCHEMA IF NOT EXISTS reporting\n\n  - \"@setup_external_tables()\"\n</code></pre> macros/setup.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef setup_external_tables(evaluator):\n    \"\"\"Create external tables for data ingestion.\"\"\"\n    return [\n        \"\"\"\n        CREATE EXTERNAL TABLE IF NOT EXISTS staging.raw_events (\n            event_id VARCHAR,\n            event_type VARCHAR,\n            event_data VARCHAR,\n            created_at TIMESTAMP\n        )\n        LOCATION 's3://data-lake/events/'\n        FILE_FORMAT = (TYPE = 'PARQUET')\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#5-data-quality-gates","title":"5. Data Quality Gates","text":"<p>Validate source data before processing. Use <code>before_all</code> to run validation checks. If checks fail, the pipeline stops before processing bad data:</p> config.yaml<pre><code>before_all:\n  - \"@validate_source_data()\"\n</code></pre> macros/validation.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef validate_source_data(evaluator):\n    \"\"\"Validate that source data meets quality requirements.\"\"\"\n    return [\n        \"\"\"\n        DO $$\n        DECLARE\n            row_count INTEGER;\n        BEGIN\n            SELECT COUNT(*) INTO row_count FROM raw_data.events WHERE created_at &gt;= CURRENT_DATE;\n            IF row_count = 0 THEN\n                RAISE EXCEPTION 'No data found for today in raw_data.events';\n            END IF;\n        END $$;\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#6-refresh-materialized-views","title":"6. Refresh Materialized Views","text":"<p>If materialized views depend on your Vulcan models, refresh them after models update. Let <code>after_all</code> handle it:</p> config.yaml<pre><code>after_all:\n  - \"@refresh_materialized_views()\"\n</code></pre> macros/refresh.py<pre><code>from vulcan.core.macros import macro\n\n@macro()\ndef refresh_materialized_views(evaluator):\n    \"\"\"Refresh all materialized views that depend on our models.\"\"\"\n    materialized_views = [\n        \"reporting.daily_summary_mv\",\n        \"reporting.weekly_trends_mv\",\n        \"analytics.user_metrics_mv\"\n    ]\n\n    return [\n        f\"REFRESH MATERIALIZED VIEW {mv};\"\n        for mv in materialized_views\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#7-notification-integration","title":"7. Notification Integration","text":"<p>Notify your team when the pipeline finishes. Use <code>after_all</code> to send notifications. This example logs to a table. Extend it to call an external API or send emails:</p> config.yaml<pre><code>after_all:\n  - \"@notify_completion()\"\n</code></pre> macros/notify.py<pre><code>from vulcan.core.macros import macro\nimport os\n\n@macro()\ndef notify_completion(evaluator):\n    \"\"\"Log completion status (integrate with your notification system).\"\"\"\n    # This example logs to a table; you could also call an external API\n    view_count = len(evaluator.views) if evaluator.views else 0\n    schema_count = len(evaluator.schemas) if evaluator.schemas else 0\n\n    return [\n        f\"\"\"\n        INSERT INTO notifications_log (\n            environment, \n            message, \n            view_count, \n            schema_count, \n            created_at\n        )\n        VALUES (\n            '{evaluator.this_env}',\n            'Pipeline completed successfully',\n            {view_count},\n            {schema_count},\n            CURRENT_TIMESTAMP\n        )\n        \"\"\"\n    ]\n</code></pre>"},{"location":"configurations/options/execution_hooks/#execution-order","title":"Execution Order","text":"<pre><code>graph LR\n    A[Start] --&gt; B[before_all]\n    B --&gt; C[Process Models]\n    C --&gt; D[after_all]\n    D --&gt; E[Complete]\n\n    style B fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style E fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px</code></pre>"},{"location":"configurations/options/execution_hooks/#best-practices","title":"Best Practices","text":"<p>Use macros for complex logic. If hook logic gets complicated, move it to a Python macro. Keeps your YAML config clean and makes the logic easier to test and maintain.</p> <p>Make hooks idempotent. Hooks might run multiple times if a plan fails and gets retried. Make sure they're safe to run repeatedly. Use <code>IF NOT EXISTS</code>, <code>ON CONFLICT</code>, or similar patterns.</p> <p>Use environment checks. Not everything should run in every environment. Use <code>@IF(@this_env = 'prod', ...)</code> to gate production-only operations so you don't accidentally run them in development.</p> <p>Handle failures gracefully. Think about what happens if a hook fails. Will it break your entire pipeline? Use transactions where appropriate, and handle failures appropriately.</p> <p>Document your hooks. Add comments explaining why each hook exists and what it does.</p> <p>Test in development first. Always test hooks in a development environment before running them in production. Hooks run automatically, so mistakes can be costly.</p>"},{"location":"configurations/options/execution_hooks/#comparison-with-model-level-hooks","title":"Comparison with Model-Level Hooks","text":"<p>You might be wondering: when should I use execution hooks versus model-level hooks? Here's the difference:</p> Feature <code>before_all</code> / <code>after_all</code> Model <code>pre_statements</code> / <code>post_statements</code> Scope Entire pipeline Single model Runs Once per plan/run Once per model execution Access to All views, schemas, environment Model-specific context Use for Global setup, cleanup, privileges Model-specific operations <p>Use execution hooks (<code>before_all</code>/<code>after_all</code>) for operations that apply to your entire pipeline: setting up audit tables, granting permissions on all views, sending completion notifications.</p> <p>Use model-level hooks (<code>pre_statements</code>/<code>post_statements</code>) for operations specific to individual models: creating temporary tables that only one model needs, running model-specific validations.</p>"},{"location":"configurations/options/linter/","title":"Linter","text":""},{"location":"configurations/options/linter/#linter","title":"Linter","text":"<p>Linting automatically checks your model definitions against your team's best practices and catches common mistakes before they cause problems.</p> <p>When you create a Vulcan plan, each model's code gets checked against the linting rules you've configured. If any rules are violated, Vulcan tells you so you can fix the issues before deploying.</p> <p>Vulcan includes built-in rules that catch common SQL mistakes and enforce good practices. You can also write custom rules that match your team's specific requirements. This maintains code quality and catches issues early, when they're easier to fix.</p>"},{"location":"configurations/options/linter/#rules","title":"Rules","text":"<p>Linting rules are pattern detectors. Each rule looks for a specific pattern (or lack of a pattern) in your model code.</p> <p>Some rules check that a pattern isn't present, like the <code>NoSelectStar</code> rule that prevents <code>SELECT *</code> in your outermost query. Other rules check that a pattern is present, like making sure every model has an <code>owner</code> field specified. Both types keep your code consistent and maintainable.</p> <p>Rules are written in Python. Each rule is a Python class that inherits from Vulcan's <code>Rule</code> base class. You define the logic for detecting the pattern, and Vulcan handles the rest.</p> <p>When you create a custom rule, implement four things:</p> <ol> <li> <p>Name: The class name becomes the rule's name (converted to lowercase with underscores).</p> </li> <li> <p>Description: A docstring that explains what the rule checks and why it matters.</p> </li> <li> <p>Pattern validation logic: The <code>check_model()</code> method that checks your model code. You can access any attribute of the <code>Model</code> object.</p> </li> <li> <p>Rule violation logic: If the pattern isn't valid, return a <code>RuleViolation</code> object with a message that tells the user what's wrong and how to fix it.</p> </li> </ol> <pre><code># Class name used as rule's name\nclass Rule:\n    # Docstring provides rule's description\n    \"\"\"The base class for a rule.\"\"\"\n\n    # Pattern validation logic goes in `check_model()` method\n    @abc.abstractmethod\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        \"\"\"The evaluation function that checks for a violation of this rule.\"\"\"\n\n    # Rule violation object returned by `violation()` method\n    def violation(self, violation_msg: t.Optional[str] = None) -&gt; RuleViolation:\n        \"\"\"Return a RuleViolation instance if this rule is violated\"\"\"\n        return RuleViolation(rule=self, violation_msg=violation_msg or self.summary)\n</code></pre>"},{"location":"configurations/options/linter/#built-in-rules","title":"Built-in Rules","text":"<p>Vulcan includes built-in rules that catch common SQL mistakes and enforce good coding practices. These rules catch real issues seen in production.</p> <p>For example, the <code>NoSelectStar</code> rule prevents using <code>SELECT *</code> in your outermost query. <code>SELECT *</code> makes it unclear what columns your model produces, which can break downstream models and make debugging harder.</p> <p>Here's what the <code>NoSelectStar</code> rule looks like, with annotations showing how it's structured:</p> <pre><code># Rule's name is the class name `NoSelectStar`\nclass NoSelectStar(Rule):\n    # Docstring explaining rule\n    \"\"\"Query should not contain SELECT * on its outer most projections, even if it can be expanded.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # If this model does not contain a SQL query, there is nothing to validate\n        if not isinstance(model, SqlModel):\n            return None\n\n        # Use the query's `is_star` property to detect the `SELECT *` pattern.\n        # If present, call the `violation()` method to return a `RuleViolation` object.\n        return self.violation() if model.query.is_star else None\n</code></pre> <p>Here are all of Vulcan's built-in linting rules:</p> Name Check type Explanation <code>ambiguousorinvalidcolumn</code> Correctness Vulcan found duplicate columns or was unable to determine whether a column is duplicated or not <code>invalidselectstarexpansion</code> Correctness The query's top-level selection may be <code>SELECT *</code>, but only if Vulcan can expand the <code>SELECT *</code> into individual columns <code>noselectstar</code> Stylistic The query's top-level selection may not be <code>SELECT *</code>, even if Vulcan can expand the <code>SELECT *</code> into individual columns <code>nomissingaudits</code> Governance Vulcan did not find any <code>audits</code> in the model's configuration to test data quality."},{"location":"configurations/options/linter/#user-defined-rules","title":"User-Defined Rules","text":"<p>Built-in rules are good, but every team has different standards. Write custom rules that enforce your team's specific best practices.</p> <p>For example, make sure every model has an <code>owner</code> field so you know who's responsible for it:</p> linter/user.py<pre><code>import typing as t\n\nfrom vulcan.core.linter.rule import Rule, RuleViolation\nfrom vulcan.core.model import Model\n\nclass NoMissingOwner(Rule):\n    \"\"\"Model owner should always be specified.\"\"\"\n\n    def check_model(self, model: Model) -&gt; t.Optional[RuleViolation]:\n        # Rule violated if the model's owner field (`model.owner`) is not specified\n        return self.violation() if not model.owner else None\n</code></pre> <p>Put your custom rules in the <code>linter/</code> directory of your project. Vulcan will automatically find and load any classes that inherit from <code>Rule</code> in that directory.</p> <p>Once you've added a rule to your configuration file, Vulcan will run it automatically when: - You create a plan with <code>vulcan plan</code></p> <ul> <li>You run the <code>vulcan lint</code> command</li> </ul> <p>If a model violates a rule, Vulcan will stop and tell you exactly which model(s) have problems. Here's what that looks like, in this example, <code>full_model.sql</code> is missing an owner, so the plan stops:</p> <pre><code>$ vulcan plan\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>You can also run linting on its own for faster iteration during development:</p> <pre><code>$ vulcan lint\n\nLinter errors for .../models/full_model.sql:\n - nomissingowner: Model owner should always be specified.\n\nError: Linter detected errors in the code. Please fix them before proceeding.\n</code></pre> <p>Use <code>vulcan lint --help</code> for more information.</p>"},{"location":"configurations/options/linter/#applying-linting-rules","title":"Applying Linting Rules","text":"<p>Specify which linting rules a project should apply in the project's configuration file.</p> <p>List which rules to run under the <code>linter</code> key. Globally enable or disable linting with the <code>enabled</code> key (defaults to <code>false</code>, so you need to turn it on).</p> <p>Important</p> <p>Don't forget to set <code>enabled: true</code>! If you don't, Vulcan won't run any linting rules, even if you've specified them.</p>"},{"location":"configurations/options/linter/#specific-linting-rules","title":"Specific Linting Rules","text":"<p>Use just a few specific rules. List them in the <code>rules</code> array. Example that enables two built-in rules:</p> YAMLPython <pre><code>linter:\n  enabled: true\n  rules: [\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=[\"ambiguousorinvalidcolumn\", \"invalidselectstarexpansion\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#all-linting-rules","title":"All Linting Rules","text":"<p>Enable all rules. Use <code>\"ALL\"</code> instead of listing them individually. This runs every built-in rule plus any custom rules you've defined:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\"\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        rules=\"all\",\n    )\n)\n</code></pre> <p>Sometimes you want almost everything, but one or two rules don't fit your workflow. Use <code>\"ALL\"</code> and exclude specific rules with <code>ignored_rules</code>:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  rules: \"ALL\" # apply all built-in and user-defined rules and error if violated\n  ignored_rules: [\"noselectstar\"] # but don't run the `noselectstar` rule\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # apply all built-in and user-defined linting rules and error if violated\n        rules=\"all\",\n         # but don't run the `noselectstar` rule\n        ignored_rules=[\"noselectstar\"]\n    )\n)\n</code></pre>"},{"location":"configurations/options/linter/#exclude-a-model-from-linting","title":"Exclude a Model from Linting","text":"<p>Sometimes a model legitimately needs to violate a rule. Maybe it's a legacy model you're migrating, or there's a special case. Exclude specific models from specific rules (or all rules) by adding <code>ignored_rules</code> to the model's <code>MODEL</code> block.</p> <p>Here's an example where we exclude one model from one rule:</p> <pre><code>MODEL(\n  name docs_example.full_model,\n  ignored_rules [\"invalidselectstarexpansion\"] # or \"ALL\" to turn off linting completely\n);\n</code></pre>"},{"location":"configurations/options/linter/#rule-violation-behavior","title":"Rule Violation Behavior","text":"<p>By default, when a rule is violated, Vulcan treats it as an error and stops execution. This ensures you fix issues before they make it to production.</p> <p>Sometimes you want a rule to be a suggestion rather than a hard requirement. Maybe it's a style preference that's nice to have but not critical. Put the rule in <code>warn_rules</code> instead of <code>rules</code>. Violations are still reported, but they won't stop execution:</p> YAMLPython <pre><code>linter:\n  enabled: True\n  # error if `ambiguousorinvalidcolumn` rule violated\n  rules: [\"ambiguousorinvalidcolumn\"]\n  # but only warn if \"invalidselectstarexpansion\" is violated\n  warn_rules: [\"invalidselectstarexpansion\"]\n</code></pre> <pre><code>from vulcan.core.config import Config, LinterConfig\n\nconfig = Config(\n    linter=LinterConfig(\n        enabled=True,\n        # error if `ambiguousorinvalidcolumn` rule violated\n        rules=[\"ambiguousorinvalidcolumn\"],\n        # but only warn if \"invalidselectstarexpansion\" is violated\n        warn_rules=[\"invalidselectstarexpansion\"],\n    )\n)\n</code></pre> <p>Vulcan will raise an error if the same rule is included in more than one of the <code>rules</code>, <code>warn_rules</code>, and <code>ignored_rules</code> keys since they should be mutually exclusive.</p>"},{"location":"configurations/options/model_defaults/","title":"Model Defaults","text":""},{"location":"configurations/options/model_defaults/#model-defaults","title":"Model Defaults","text":"<p>The <code>model_defaults</code> section is required. You must specify a value for the <code>dialect</code> key.</p> <p>All supported <code>model_defaults</code> keys are listed in the models configuration reference.</p>"},{"location":"configurations/options/model_defaults/#basic-configuration","title":"Basic Configuration","text":"<p>Example:</p> YAMLPython <pre><code>model_defaults:\n  dialect: snowflake\n  owner: jen\n  start: 2022-01-01\n</code></pre> <pre><code>from vulcan.core.config import Config, ModelDefaultsConfig\n\nconfig = Config(\n    model_defaults=ModelDefaultsConfig(\n        dialect=\"snowflake\",\n        owner=\"jen\",\n        start=\"2022-01-01\",\n    ),\n)\n</code></pre> <p>The default model kind is <code>VIEW</code> unless you override it with the <code>kind</code> key. See model kinds for more information.</p>"},{"location":"configurations/options/model_defaults/#identifier-resolution","title":"Identifier Resolution","text":"<p>When a SQL engine receives a query like <code>SELECT id FROM \"some_table\"</code>, it needs to understand what database objects the identifiers <code>id</code> and <code>\"some_table\"</code> correspond to. This is identifier resolution.</p> <p>Different SQL dialects resolve identifiers differently. Some identifiers are case-sensitive if quoted. Case-insensitive identifiers are usually lowercased or uppercased before the engine looks up the object.</p> <p>Vulcan analyzes model queries to extract information like column-level lineage. To do this, it normalizes and quotes all identifiers in queries, respecting each dialect's resolution rules.</p> <p>The normalization strategy determines whether case-insensitive identifiers are lowercased or uppercased. You can configure this per dialect. To treat all identifiers as case-sensitive in a BigQuery project:</p> YAML <pre><code>model_defaults:\n  dialect: \"bigquery,normalization_strategy=case_sensitive\"\n</code></pre> <p>This is useful when you need to preserve name casing, since Vulcan won't normalize them.</p> <p>See normalization strategies for all supported options.</p>"},{"location":"configurations/options/model_defaults/#gateway-specific-model-defaults","title":"Gateway-Specific Model Defaults","text":"<p>Define gateway-specific <code>model_defaults</code> in the <code>gateways</code> section. These override the global defaults for that gateway.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n    model_defaults:\n      dialect: \"snowflake,normalization_strategy=case_insensitive\"\n  snowflake:\n    connection:\n      type: snowflake\n\ndefault_gateway: snowflake\n\nmodel_defaults:\n  dialect: snowflake\n  start: 2025-02-05\n</code></pre> <p>This lets you customize model behavior for each gateway without affecting global <code>model_defaults</code>.</p> <p>Some SQL engines treat table and column names as case-sensitive. Others treat them as case-insensitive. If your project uses both types of engines, models need to align with each engine's normalization behavior, which makes maintenance and debugging harder.</p> <p>Gateway-specific <code>model_defaults</code> let you change how Vulcan performs identifier normalization per engine to align their behavior.</p> <p>In the example above, the project's default dialect is <code>snowflake</code> (line 14). The <code>redshift</code> gateway overrides that with <code>\"snowflake,normalization_strategy=case_insensitive\"</code> (line 6).</p> <p>This tells Vulcan that the <code>redshift</code> gateway's models are written in Snowflake SQL dialect (so they need to be transpiled from Snowflake to Redshift), but the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.</p>"},{"location":"configurations/options/notifications/","title":"Notifications","text":""},{"location":"configurations/options/notifications/#notifications","title":"Notifications","text":"<p>Vulcan can send notifications when certain events occur. Configure notifications and specify recipients in your configuration file.</p>"},{"location":"configurations/options/notifications/#notification-targets","title":"Notification Targets","text":"<p>Configure notifications with notification targets. Specify targets in a project's configuration file (<code>config.yml</code> or <code>config.py</code>). You can specify multiple targets for a project.</p> <p>A project can specify both global and user-specific notifications. Each target's notifications are sent for all instances of each event type (for example, notifications for <code>run</code> are sent for all of the project's environments), with exceptions for audit failures and when an override is configured for development.</p> <p>Audit failure notifications can be sent for specific models if five conditions are met:</p> <ol> <li>A model's <code>owner</code> field is populated</li> <li>The model executes one or more audits</li> <li>The owner has a user-specific notification target configured</li> <li>The owner's notification target <code>notify_on</code> key includes audit failure events</li> <li>The audit fails in the <code>prod</code> environment</li> </ol> <p>When those conditions are met, the audit owner will be notified if their audit failed in the <code>prod</code> environment.</p> <p>There are three types of notification targets, corresponding to the two Slack notification methods and email notification. Specify them in either a specific user's <code>notification_targets</code> key or the top-level <code>notification_targets</code> configuration key.</p> <p>This example shows the location of both user-specific and global notification targets:</p> YAMLPython <pre><code># User notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n  - username: User2\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n\n# Global notification targets\nnotification_targets:\n  - notification_target_1\n    ...\n  - notification_target_2\n    ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # User notification targets\n    users=[\n        User(\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n        User(\n            username=\"User2\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        )\n    ],\n\n    # Global notification targets\n    notification_targets=[\n        notification_target_1(...),\n        notification_target_2(...),\n    ],\n    ...\n)\n</code></pre>"},{"location":"configurations/options/notifications/#notifications-during-development","title":"Notifications During Development","text":"<p>Events triggering notifications may execute repeatedly during code development. To prevent excessive notifications, Vulcan can stop all but one user's notification targets.</p> <p>Specify the top-level <code>username</code> configuration key with a value also present in a user-specific notification target's <code>username</code> key to only notify that user. Specify this key in either the project configuration file or a machine-specific configuration file located in <code>~/.vulcan</code>. The latter is useful if a specific machine is always used for development.</p> <p>This example stops all notifications other than those for <code>User1</code>:</p> YAMLPython <pre><code># Top-level `username` key: only notify User1\nusername: User1\n# User1 notification targets\nusers:\n  - username: User1\n    ...\n    notification_targets:\n      - notification_target_1\n        ...\n      - notification_target_2\n        ...\n</code></pre> <pre><code>config = Config(\n    ...,\n    # Top-level `username` key: only notify User1\n    username=\"User1\",\n    users=[\n        User(\n            # User1 notification targets\n            username=\"User1\",\n            notification_targets=[\n                notification_target_1(...),\n                notification_target_2(...),\n            ],\n        ),\n    ]\n)\n</code></pre>"},{"location":"configurations/options/notifications/#vulcan-event-types","title":"Vulcan Event Types","text":"<p>Vulcan notifications are triggered by events. Specify which events should trigger a notification in the notification target's <code>notify_on</code> field.</p> <p>Notifications are supported for <code>plan</code> application start/end/failure, <code>run</code> start/end/failure, and <code>audit</code> failures.</p> <p>For <code>plan</code> and <code>run</code> start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.</p> <p>This table lists each event, its associated <code>notify_on</code> value, and its notification message:</p> Event <code>notify_on</code> Key Value Notification message Plan application start apply_start \"Plan apply started for environment <code>{environment}</code>.\" Plan application end apply_end \"Plan apply finished for environment <code>{environment}</code>.\" Plan application failure apply_failure \"Failed to apply plan.\\n{exception}\" Vulcan run start run_start \"Vulcan run started for environment <code>{environment}</code>.\" Vulcan run end run_end \"Vulcan run finished for environment <code>{environment}</code>.\" Vulcan run failure run_failure \"Failed to run Vulcan.\\n{exception}\" Audit failure audit_failure \"{audit_error}\" <p>Any combination of these events can be specified in a notification target's <code>notify_on</code> field.</p>"},{"location":"configurations/options/notifications/#slack-notifications","title":"Slack Notifications","text":"<p>Vulcan supports two types of Slack notifications. Slack webhooks notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.</p>"},{"location":"configurations/options/notifications/#webhook-configuration","title":"Webhook Configuration","text":"<p>Vulcan uses Slack's \"Incoming Webhooks\" for webhook notifications. When you create an incoming webhook in Slack, you receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.</p> <p>This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable <code>SLACK_WEBHOOK_URL</code> instead of hard-coding the URL:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_webhook\n    notify_on:\n      - apply_start\n\n      - apply_failure\n\n      - run_start\n    url: \"{{ env_var('SLACK_WEBHOOK_URL') }}\"\n</code></pre> <pre><code>notification_targets=[\n    SlackWebhookNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_failure\", \"run_start\"],\n        url=os.getenv(\"SLACK_WEBHOOK_URL\"),\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#api-configuration","title":"API Configuration","text":"<p>To notify users, use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See Slack's official documentation for information on getting an API token.</p> <p>This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable <code>SLACK_API_TOKEN</code> instead of hard-coding the token:</p> YAMLPython <pre><code>notification_targets:\n  - type: slack_api\n    notify_on:\n      - apply_start\n\n      - apply_end\n\n      - audit_failure\n    token: \"{{ env_var('SLACK_API_TOKEN') }}\"\n    channel: \"UXXXXXXXXX\"  # Channel or a user's Slack member ID\n</code></pre> <pre><code>notification_targets=[\n    SlackApiNotificationTarget(\n        notify_on=[\"apply_start\", \"apply_end\", \"audit_failure\"],\n        token=os.getenv(\"SLACK_API_TOKEN\"),\n        channel=\"UXXXXXXXXX\",  # Channel or a user's Slack member ID\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#email-notifications","title":"Email Notifications","text":"<p>Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target can notify multiple recipient email addresses.</p> <p>This example shows an email notification target, where <code>sushi@example.com</code> emails <code>data-team@example.com</code> on Vulcan run failure. The specification uses environment variables <code>SMTP_HOST</code>, <code>SMTP_USER</code>, and <code>SMTP_PASSWORD</code> instead of hard-coding the values:</p> YAMLPython <pre><code>notification_targets:\n  - type: smtp\n    notify_on:\n      - run_failure\n    host: \"{{ env_var('SMTP_HOST') }}\"\n    user: \"{{ env_var('SMTP_USER') }}\"\n    password: \"{{ env_var('SMTP_PASSWORD') }}\"\n    sender: sushi@example.com\n    recipients:\n      - data-team@example.com\n</code></pre> <pre><code>notification_targets=[\n    BasicSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/notifications/#advanced-usage","title":"Advanced Usage","text":""},{"location":"configurations/options/notifications/#overriding-notification-targets","title":"Overriding Notification Targets","text":"<p>In Python configuration files, configure new notification targets to send custom messages.</p> <p>To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (<code>SlackWebhookNotificationTarget</code>, <code>SlackApiNotificationTarget</code>, or <code>BasicSMTPNotificationTarget</code>).</p> <p>Each of those notification target classes is a subclass of <code>BaseNotificationTarget</code>, which contains a <code>notify</code> function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):</p> Function name Contextual information notify_apply_start Environment name: <code>env</code> notify_apply_end Environment name: <code>env</code> notify_apply_failure Exception stack trace: <code>exc</code> notify_run_start Environment name: <code>env</code> notify_run_end Environment name: <code>env</code> notify_run_failure Exception stack trace: <code>exc</code> notify_audit_failure Audit error trace: <code>audit_error</code> <p>This example creates a new notification target class <code>CustomSMTPNotificationTarget</code>.</p> <p>It overrides the default <code>notify_run_failure</code> function to read a log file <code>\"/home/vulcan/vulcan.log\"</code> and append its contents to the exception stack trace <code>exc</code>:</p> Python <pre><code>from vulcan.core.notification_target import BasicSMTPNotificationTarget\n\nclass CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):\n    def notify_run_failure(self, exc: str) -&gt; None:\n        with open(\"/home/vulcan/vulcan.log\", \"r\", encoding=\"utf-8\") as f:\n            msg = f\"{exc}\\n\\nLogs:\\n{f.read()}\"\n        super().notify_run_failure(msg)\n</code></pre> <p>Use this new class by specifying it as a notification target in the configuration file:</p> Python <pre><code>notification_targets=[\n    CustomSMTPNotificationTarget(\n        notify_on=[\"run_failure\"],\n        host=os.getenv(\"SMTP_HOST\"),\n        user=os.getenv(\"SMTP_USER\"),\n        password=os.getenv(\"SMTP_PASSWORD\"),\n        sender=\"notifications@example.com\",\n        recipients=[\n            \"data-team@example.com\",\n        ],\n    )\n]\n</code></pre>"},{"location":"configurations/options/variables/","title":"Variables","text":""},{"location":"configurations/options/variables/#variables","title":"Variables","text":"<p>Store sensitive information like passwords and API keys without hardcoding them in your configuration files. Use environment variables, <code>.env</code> files, or configuration overrides to change settings dynamically.</p>"},{"location":"configurations/options/variables/#environment-variables","title":"Environment Variables","text":"<p>Vulcan accesses environment variables during configuration. Store secrets outside configuration files and change settings based on who's running Vulcan.</p>"},{"location":"configurations/options/variables/#using-env-files","title":"Using .env Files","text":"<p>Vulcan automatically loads environment variables from a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nSNOWFLAKE_PW=my_secret_password\nS3_BUCKET=s3://my-data-bucket/warehouse\nDATABASE_URL=postgresql://user:pass@localhost/db\n\n# Override Vulcan configuration values\nVULCAN__DEFAULT_GATEWAY=production\nVULCAN__MODEL_DEFAULTS__DIALECT=snowflake\n</code></pre> <p>Security</p> <p>Add <code>.env</code> to your <code>.gitignore</code> file to avoid committing sensitive information.</p>"},{"location":"configurations/options/variables/#custom-env-file-location","title":"Custom .env File Location","text":"<p>Specify a custom path using the <code>--dotenv</code> CLI flag:</p> <pre><code>vulcan --dotenv /path/to/custom/.env plan\n</code></pre> <p>Or set the <code>VULCAN_DOTENV_PATH</code> environment variable:</p> <pre><code>export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env\nvulcan plan\n</code></pre> <p>Note</p> <p>The <code>--dotenv</code> flag must be placed before the subcommand (e.g., <code>plan</code>, <code>run</code>).</p>"},{"location":"configurations/options/variables/#accessing-variables-in-configuration","title":"Accessing Variables in Configuration","text":"YAMLPython <p>Use <code>{{ env_var('VARIABLE_NAME') }}</code> syntax:</p> <pre><code>gateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      user: admin\n      password: \"{{ env_var('SNOWFLAKE_PW') }}\"\n      account: my_account\n</code></pre> <p>Use <code>os.environ</code>:</p> <pre><code>import os\nfrom vulcan.core.config import Config, GatewayConfig, SnowflakeConnectionConfig\n\nconfig = Config(\n    gateways={\n        \"my_gateway\": GatewayConfig(\n            connection=SnowflakeConnectionConfig(\n                user=\"admin\",\n                password=os.environ['SNOWFLAKE_PW'],\n                account=\"my_account\",\n            ),\n        ),\n    }\n)\n</code></pre>"},{"location":"configurations/options/variables/#configuration-overrides","title":"Configuration Overrides","text":"<p>Environment variables have the highest precedence. They override configuration file values if they follow the <code>VULCAN__</code> naming convention.</p>"},{"location":"configurations/options/variables/#override-naming-structure","title":"Override Naming Structure","text":"<p>Use double underscores <code>__</code> to navigate the configuration hierarchy:</p> <pre><code>VULCAN__&lt;ROOT_KEY&gt;__&lt;NESTED_KEY&gt;__&lt;FIELD&gt;=value\n</code></pre> <p>Example: Override a gateway connection password:</p> <pre><code># config.yaml\ngateways:\n  my_gateway:\n    connection:\n      type: snowflake\n      password: dummy_pw  # This will be overridden\n</code></pre> <pre><code># Override with environment variable\nexport VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD=\"real_pw\"\n</code></pre>"},{"location":"configurations/options/variables/#dynamic-configuration","title":"Dynamic Configuration","text":""},{"location":"configurations/options/variables/#user-based-target-environment","title":"User-based Target Environment","text":"<p>Use the <code>{{ user() }}</code> function to dynamically set configuration based on the current user:</p> YAMLPython <pre><code># Each user gets their own dev environment\ndefault_target_environment: dev_{{ user() }}\n</code></pre> <pre><code>import getpass\nfrom vulcan.core.config import Config\n\nconfig = Config(\n    default_target_environment=f\"dev_{getpass.getuser()}\",\n)\n</code></pre> <p>This allows running <code>vulcan plan</code> instead of <code>vulcan plan dev_username</code>.</p>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#cookbook","title":"Cookbook","text":"<p>Coming soon...</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Coming soon...</p>"},{"location":"guides/data_quality/","title":"Data Quality","text":""},{"location":"guides/data_quality/#data-quality","title":"Data Quality","text":"<p>This guide explains how to use Audits, Checks, and Tests together to ensure data quality in your Orders360 project. You'll learn when to use each tool and see complex examples where they work together.</p> <p>These three tools work as layers of protection for your data. Each serves a different purpose. Together they provide comprehensive coverage.</p>"},{"location":"guides/data_quality/#the-three-layer-quality-strategy","title":"The Three-Layer Quality Strategy","text":"<pre><code>flowchart TB\n    subgraph \"Layer 1: Audits - Critical Blocking\"\n        AUDIT[Audits&lt;br/&gt;Block invalid data&lt;br/&gt;Run with model]\n        EXAMPLES1[\"\u2022 Primary keys unique&lt;br/&gt;\u2022 Revenue non-negative&lt;br/&gt;\u2022 Foreign keys valid\"]\n    end\n\n    subgraph \"Layer 2: Checks - Monitoring\"\n        CHECK[Checks&lt;br/&gt;Track quality trends&lt;br/&gt;Non-blocking]\n        EXAMPLES2[\"\u2022 Row count anomalies&lt;br/&gt;\u2022 Completeness trends&lt;br/&gt;\u2022 Cross-model validation\"]\n    end\n\n    subgraph \"Layer 3: Tests - Logic Validation\"\n        TEST[Tests&lt;br/&gt;Validate transformations&lt;br/&gt;Unit testing]\n        EXAMPLES3[\"\u2022 SQL logic correct&lt;br/&gt;\u2022 Expected outputs&lt;br/&gt;\u2022 Edge cases\"]\n    end\n\n    AUDIT --&gt; EXAMPLES1\n    CHECK --&gt; EXAMPLES2\n    TEST --&gt; EXAMPLES3\n\n    style AUDIT fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style TEST fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>When to Use Each:</p> <p>Here's a quick guide to help you choose:</p> Tool Purpose Blocks Pipeline? Best For Audits Critical validation Yes (always) Business rules, data integrity Checks Quality monitoring No Trends, anomalies, monitoring Tests Logic validation No SQL correctness, edge cases <p>The key difference: Audits stop everything if they fail. Checks and tests warn you, so you can investigate without blocking production.</p> <p>[Screenshot: Visual comparison of the three quality tools]</p>"},{"location":"guides/data_quality/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/data_quality/#audits-critical-blocking-validation","title":"Audits: Critical Blocking Validation","text":"<p>Use audits when: Data must be correct or the pipeline should stop. These are your \"must never fail\" rules, if an audit fails, something is seriously wrong and you don't want that bad data flowing downstream.</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Primary key validation\n    not_null(columns := (order_date)),\n    unique_values(columns := (order_date)),\n\n    -- Business rules\n    positive_values(column := total_revenue),\n    accepted_range(column := total_orders, min_v := 0, max_v := 10000)\n  )\n);\n</code></pre> <p>Why audits are useful:</p> <ul> <li> <p>Always blocking - if they fail, execution stops immediately. No bad data gets through</p> </li> <li> <p>Run automatically with model execution - you don't have to remember to run them</p> </li> <li> <p>Fast feedback during development - catch issues before they hit production</p> </li> <li> <p>Use for critical business rules like \"revenue must be positive\" or \"primary keys must be unique\"</p> </li> </ul> <p>Think of audits as your bouncer, they check IDs at the door and don't let anyone sketchy in.</p> <p>[Screenshot: Audit failure blocking plan execution]</p>"},{"location":"guides/data_quality/#checks-quality-monitoring","title":"Checks: Quality Monitoring","text":"<p>Use checks when: You want to monitor trends and detect anomalies over time. Unlike audits, checks don't block your pipeline, they just keep an eye on things and warn you if something looks off.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n</code></pre> <p>Why checks are useful:</p> <ul> <li> <p>Non-blocking - warnings, not failures. Your pipeline keeps running even if a check flags something</p> </li> <li> <p>Historical tracking - see trends over time. You can spot patterns like \"revenue always drops on weekends\" or \"row counts are trending down\"</p> </li> <li> <p>Anomaly detection - statistical analysis. Checks can detect when something is statistically unusual, even if it's not technically wrong</p> </li> <li> <p>Use for monitoring and alerting. Set up alerts for when checks fail, so you know to investigate.</p> </li> </ul> <p>Checks monitor everything and alert you if something suspicious happens, but they don't stop execution.</p> <p>[Screenshot: Check results showing trends over time]</p>"},{"location":"guides/data_quality/#tests-logic-validation","title":"Tests: Logic Validation","text":"<p>Use tests when: You need to validate SQL transformations and edge cases. These are your unit tests for SQL, they make sure your logic is correct before you deploy it.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_aggregation\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n</code></pre> <p>Why tests are useful:</p> <ul> <li> <p>Unit testing for SQL logic - test your transformations in isolation</p> </li> <li> <p>Validates expected outputs - make sure you're getting the results you expect</p> </li> <li> <p>Tests edge cases - what happens with empty data? Null values? Boundary conditions?</p> </li> <li> <p>Use for development. Catch bugs before they make it to production.</p> </li> </ul> <p>Tests run in a controlled environment before production.</p> <p>[Screenshot: Test execution showing pass/fail results]</p>"},{"location":"guides/data_quality/#complex-example-orders360-daily-sales","title":"Complex Example: Orders360 Daily Sales","text":"<p>Let's see how all three tools work together for the <code>sales.daily_sales</code> model. This is a real-world example that shows you how to layer these tools for maximum protection.</p>"},{"location":"guides/data_quality/#the-model","title":"The Model","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics'),\n  description 'Daily sales summary with order counts and revenue',\n  assertions (\n    -- Audit: Critical validations\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    unique_values(columns := (order_date)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue),\n    accepted_range(column := total_revenue, min_v := 0, max_v := 1000000)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre>"},{"location":"guides/data_quality/#layer-1-audits-critical-blocking","title":"Layer 1: Audits (Critical Blocking)","text":"<p>Why: These rules must never fail. Invalid data should not flow downstream. If revenue is negative or primary keys aren't unique, that's a critical problem that needs to stop everything immediately.</p> <pre><code>-- audits/revenue_consistency.sql\nAUDIT (name assert_revenue_consistency);\n-- Ensure revenue matches sum of individual orders\nSELECT \n  ds.order_date,\n  ds.total_revenue,\n  SUM(o.total_amount) as calculated_revenue\nFROM @this_model ds\nJOIN raw.raw_orders o ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 0.01;\n</code></pre> <p>Attach to model: </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- ... other audits ...\n    assert_revenue_consistency  -- Custom audit\n  )\n);\n</code></pre><p></p> <p>[Screenshot: Audit failure showing revenue mismatch]</p>"},{"location":"guides/data_quality/#layer-2-checks-monitoring","title":"Layer 2: Checks (Monitoring)","text":"<p>Why: Monitor trends and detect anomalies without blocking the pipeline. You want to know if revenue spikes unexpectedly or if row counts drop, but these might be legitimate business events, so you investigate rather than blocking.</p> <pre><code># checks/daily_sales.yml\nchecks:\n  sales.daily_sales:\n    # Completeness: Ensure data exists\n    completeness:\n      - row_count &gt; 0:\n          name: daily_records_exist\n          attributes:\n            description: \"At least one record per day\"\n\n      - missing_count(order_date) = 0:\n          name: no_missing_dates\n          attributes:\n            description: \"All dates must be present\"\n\n    # Validity: Check data ranges\n    validity:\n      - failed rows:\n          name: revenue_outliers\n          fail query: |\n            SELECT order_date, total_revenue\n            FROM sales.daily_sales\n            WHERE total_revenue &gt; 500000 OR total_revenue &lt; 0\n          samples limit: 10\n          attributes:\n            description: \"Revenue outside expected range\"\n\n    # Accuracy: Anomaly detection\n    accuracy:\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly\n          attributes:\n            description: \"Detect unusual revenue patterns\"\n\n      - anomaly detection for total_orders:\n          name: order_count_anomaly\n          attributes:\n            description: \"Detect unusual order volume\"\n\n    # Consistency: Cross-model validation\n    consistency:\n      - failed rows:\n          name: revenue_mismatch_with_raw\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue as daily_revenue,\n              SUM(o.total_amount) as raw_revenue\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 1.0\n          samples limit: 5\n          attributes:\n            description: \"Daily revenue should match sum of raw orders\"\n\n    # Timeliness: Check data freshness\n    timeliness:\n      - change for row_count &gt;= -20%:\n          name: row_count_drop_alert\n          attributes:\n            description: \"Alert if daily records drop more than 20%\"\n</code></pre> <p>[Screenshot: Check dashboard showing trends and anomalies]</p>"},{"location":"guides/data_quality/#layer-3-tests-logic-validation","title":"Layer 3: Tests (Logic Validation)","text":"<p>Why: Validate SQL logic and edge cases during development. Before you even deploy, you want to make sure your SQL is doing what you think it's doing. Tests catch logic errors early.</p> <pre><code># tests/test_daily_sales.yaml\ntests:\n  - name: test_daily_sales_single_order\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          customer_id: CUST-001\n          product_id: PROD-001\n          total_amount: 100.50\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 1\n        total_revenue: 100.50\n        last_order_id: ORD-001\n\n  - name: test_daily_sales_multiple_orders\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-15\n          total_amount: 50.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 3\n        total_revenue: 350.00\n        last_order_id: ORD-003\n\n  - name: test_daily_sales_empty_day\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders: []\n    outputs: []\n\n  - name: test_daily_sales_date_grouping\n    model: sales.daily_sales\n    inputs:\n      raw.raw_orders:\n        - order_id: ORD-001\n          order_date: 2025-01-15 10:00:00\n          total_amount: 100.00\n        - order_id: ORD-002\n          order_date: 2025-01-15 15:30:00\n          total_amount: 200.00\n        - order_id: ORD-003\n          order_date: 2025-01-16 09:00:00\n          total_amount: 150.00\n    outputs:\n      - order_date: 2025-01-15\n        total_orders: 2\n        total_revenue: 300.00\n      - order_date: 2025-01-16\n        total_orders: 1\n        total_revenue: 150.00\n</code></pre> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/data_quality/#how-they-work-together","title":"How They Work Together","text":"<pre><code>flowchart TB\n    subgraph \"Development Workflow\"\n        DEV[Developer writes model]\n        TEST[Run Tests&lt;br/&gt;Validate logic]\n        PLAN[Run Plan&lt;br/&gt;Apply changes]\n    end\n\n    subgraph \"Execution Flow\"\n        EXEC[Model Executes]\n        AUDIT_RUN[Audits Run&lt;br/&gt;Block if fail]\n        CHECK_RUN[Checks Run&lt;br/&gt;Track trends]\n    end\n\n    subgraph \"Results\"\n        PASS[Pass&lt;br/&gt;Data flows]\n        FAIL[Fail&lt;br/&gt;Pipeline stops]\n        TREND[Trends&lt;br/&gt;Monitor quality]\n    end\n\n    DEV --&gt; TEST\n    TEST --&gt; PLAN\n    PLAN --&gt; EXEC\n    EXEC --&gt; AUDIT_RUN\n    EXEC --&gt; CHECK_RUN\n\n    AUDIT_RUN --&gt;|Pass| PASS\n    AUDIT_RUN --&gt;|Fail| FAIL\n    CHECK_RUN --&gt; TREND\n\n    style DEV fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style TEST fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style AUDIT_RUN fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CHECK_RUN fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PASS fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FAIL fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <p>Execution Order: 1. Tests run during development (validate logic) - catch bugs before deployment 2. Plan applies changes to environment - your changes go live 3. Model executes transformation - data gets processed 4. Audits run immediately (block if fail) - critical validation happens right away 5. Checks run (track trends, don't block) - monitoring happens in the background</p> <p>Tests happen first, then audits catch critical issues, and checks monitor everything. This layered approach provides comprehensive coverage.</p> <p>[Screenshot: Complete workflow showing all three layers]</p>"},{"location":"guides/data_quality/#complex-scenario-revenue-validation","title":"Complex Scenario: Revenue Validation","text":"<p>Here's a complex example where audits and checks work together to validate revenue data. This shows you how to use both tools for the same concern, critical blocking vs. monitoring.</p>"},{"location":"guides/data_quality/#the-problem","title":"The Problem","text":"<p>We need to ensure:</p> <ol> <li>Critical: Revenue is always positive (audit - blocks)</li> <li>Critical: Daily totals match raw order sums (audit - blocks)</li> <li>Monitoring: Revenue trends are normal (check - warns)</li> <li>Monitoring: Detect unusual spikes/drops (check - warns)</li> </ol>"},{"location":"guides/data_quality/#solution-combined-approach","title":"Solution: Combined Approach","text":"<p>Audits (Critical - Blocking): </p><pre><code>MODEL (\n  name sales.daily_sales,\n  assertions (\n    -- Basic validation\n    positive_values(column := total_revenue),\n    not_null(columns := (order_date, total_revenue)),\n\n    -- Complex validation: Revenue consistency\n    assert_revenue_matches_raw_orders\n  )\n);\n\n-- audits/revenue_matches_raw.sql\nAUDIT (name assert_revenue_matches_raw_orders);\nSELECT \n  ds.order_date,\n  ds.total_revenue as daily_total,\n  COALESCE(SUM(o.total_amount), 0) as raw_total,\n  ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) as difference\nFROM @this_model ds\nLEFT JOIN raw.raw_orders o \n  ON DATE(o.order_date) = ds.order_date\nGROUP BY ds.order_date, ds.total_revenue\nHAVING ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) &gt; 0.01;\n</code></pre><p></p> <p>Checks (Monitoring - Non-Blocking): </p><pre><code># checks/revenue_monitoring.yml\nchecks:\n  sales.daily_sales:\n    accuracy:\n      # Anomaly detection for revenue\n      - anomaly detection for total_revenue:\n          name: revenue_anomaly_detection\n          attributes:\n            description: \"Detect statistically unusual revenue\"\n\n      # Trend monitoring\n      - change for total_revenue &gt;= 50%:\n          name: revenue_spike_alert\n          attributes:\n            description: \"Alert if revenue increases &gt;50% day-over-day\"\n\n      - change for total_revenue &lt;= -30%:\n          name: revenue_drop_alert\n          attributes:\n            description: \"Alert if revenue drops &gt;30% day-over-day\"\n\n    consistency:\n      # Cross-model validation (non-blocking)\n      - failed rows:\n          name: revenue_vs_raw_check\n          fail query: |\n            SELECT \n              ds.order_date,\n              ds.total_revenue,\n              SUM(o.total_amount) as raw_sum,\n              ABS(ds.total_revenue - SUM(o.total_amount)) as diff\n            FROM sales.daily_sales ds\n            LEFT JOIN raw.raw_orders o \n              ON DATE(o.order_date) = ds.order_date\n            GROUP BY ds.order_date, ds.total_revenue\n            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) &gt; 10.0\n          samples limit: 5\n          attributes:\n            description: \"Monitor revenue consistency (wider tolerance than audit)\"\n</code></pre><p></p> <p>Why Both?</p> <p>You might wonder why you need both an audit and a check for revenue. Here's the thing:</p> <ul> <li> <p>Audit: Stops pipeline if revenue is wrong (critical). If daily totals don't match raw orders, that's a data integrity issue and everything stops.</p> </li> <li> <p>Check: Warns about trends and anomalies (monitoring) - if revenue spikes 50% day-over-day, that might be legitimate (big sale!) or it might be a problem, but you want to investigate, not block</p> </li> <li> <p>Together: Critical issues blocked, trends monitored - you get both immediate protection and ongoing visibility</p> </li> </ul> <p>The audit has a tight tolerance (0.01) because it's checking for correctness. The check has a wider tolerance (10.0) because it's looking for trends, not exact matches. Pretty clever, right?</p> <p>[Screenshot: Dashboard showing audit blocks vs check warnings]</p>"},{"location":"guides/data_quality/#running-quality-tools","title":"Running Quality Tools","text":""},{"location":"guides/data_quality/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\nvulcan test\n\n# Run specific test\nvulcan test tests/test_daily_sales.yaml::test_daily_sales_single_order\n\n# Run tests matching pattern\nvulcan test tests/test_daily*\n</code></pre> <p>[Screenshot: Test execution output]</p>"},{"location":"guides/data_quality/#run-audits","title":"Run Audits","text":"<pre><code># Run all audits\nvulcan audit\n\n# Audits also run automatically with plan\nvulcan plan dev\n</code></pre> <p>[Screenshot: Audit execution output]</p>"},{"location":"guides/data_quality/#run-checks","title":"Run Checks","text":"<pre><code># Run all checks\nvulcan check\n\n# Run checks for specific model\nvulcan check --select sales.daily_sales\n\n# Checks also run automatically with plan/run\nvulcan plan dev\n</code></pre> <p>[Screenshot: Check execution output with trends]</p>"},{"location":"guides/data_quality/#best-practices","title":"Best Practices","text":""},{"location":"guides/data_quality/#do","title":"DO:","text":"<p>Here are some tips to help you use these tools effectively:</p> <ol> <li>Start with Audits - Add critical blocking validations first. Get your safety net in place before worrying about trends.</li> <li>Add Checks Gradually - Monitor trends, then add anomaly detection. Don't try to check everything at once, build up your monitoring over time.</li> <li>Test During Development - Write tests before deploying. Catch logic errors before they hit production.</li> <li>Use Descriptive Names - Makes debugging easier. Names like <code>revenue_mismatch_with_raw</code> are much better than <code>check_1</code>.</li> <li>Order Audits Efficiently - Fast checks first, slow checks last. If you have multiple audits, put the quick ones first so you fail fast.</li> </ol>"},{"location":"guides/data_quality/#dont","title":"DON'T:","text":"<p>And here's what to avoid:</p> <ol> <li>Don't use Checks for Critical Rules - Use audits instead. If it's critical, it should block. Checks are for monitoring, not blocking.</li> <li>Don't Skip Audit Failures - Fix the root cause. If an audit fails, something is wrong. Don't just disable it, fix the problem.</li> <li>Don't Over-Audit - Focus on critical business rules. Too many audits can slow things down. Only audit what really matters.</li> <li>Don't Ignore Check Trends - They indicate data quality issues. If checks are consistently failing, there's probably a real problem you need to address.</li> </ol>"},{"location":"guides/data_quality/#summary","title":"Summary","text":"<p>Three-Layer Strategy:</p> <ul> <li> <p>Audits = Critical blocking validation (must pass)</p> </li> <li> <p>Checks = Quality monitoring (trends, anomalies)</p> </li> <li> <p>Tests = Logic validation (development)</p> </li> </ul> <p>Use Together:</p> <ul> <li> <p>Audits block invalid data</p> </li> <li> <p>Checks monitor quality trends</p> </li> <li> <p>Tests validate SQL logic</p> </li> </ul> <p>Orders360 Example:</p> <ul> <li> <p>Audits ensure revenue is positive and matches raw data</p> </li> <li> <p>Checks detect anomalies and trends</p> </li> <li> <p>Tests validate aggregation logic</p> </li> </ul>"},{"location":"guides/data_quality/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn about Built-in Audits</p> </li> <li> <p>Explore Check Dimensions</p> </li> <li> <p>Read about Testing</p> </li> </ul>"},{"location":"guides/deployment_guide/","title":"DataOS 2.0 Env Deployment","text":""},{"location":"guides/deployment_guide/#dataos-20-env-deployment","title":"DataOS 2.0 Env Deployment","text":"<p>This guide provides step-by-step instructions for deploying Vulcan data products in a DataOS environment.</p>"},{"location":"guides/deployment_guide/#prerequisites","title":"Prerequisites","text":"<p>Before deploying a Vulcan data product, ensure you have the following resources configured in your DataOS environment:</p>"},{"location":"guides/deployment_guide/#1-dataos-20-cli","title":"1. DataOS 2.0 CLI","text":"<p>Ensure you have the DataOS CLI installed and configured:</p> <pre><code># Verify CLI installation\nds version\n\n# Login to your DataOS instance\nds login\n</code></pre>"},{"location":"guides/deployment_guide/#2-depot-data-source-connection","title":"2. Depot (Data Source Connection)","text":"<p>A depot must be configured to connect to your data warehouse (e.g., Snowflake, BigQuery, Databricks).</p> <p>List available depots: </p><pre><code>ds resource -t depot get -a\n</code></pre><p></p> <p>Note: Ensure the depot has read/write permissions for your data warehouse schema.</p>"},{"location":"guides/deployment_guide/#3-engine-stack","title":"3. Engine Stack","text":"<p>An engine stack defines the execution environment for Vulcan operations (e.g., Snowflake, BigQuery, Spark).</p> <p>List available stacks: </p><pre><code>ds resource -t stack get -a\n</code></pre><p></p> <p>Supported engines: - <code>snowflake</code> - <code>bigquery</code> - <code>databricks</code> - <code>postgres</code> - <code>redshift</code> - <code>trino</code> - <code>mysql</code> - <code>mssql</code></p>"},{"location":"guides/deployment_guide/#4-compute-resource","title":"4. Compute Resource","text":"<p>A compute resource provides the execution environment for running Vulcan workflows.</p> <p>List available compute resources: </p><pre><code>ds resource -t compute get -a\n</code></pre><p></p> <p>Example compute resources: - <code>cyclone-compute</code> (general purpose) - <code>minerva-compute</code> (query engine) - Custom compute clusters</p>"},{"location":"guides/deployment_guide/#5-git-sync-secret","title":"5. Git-Sync Secret","text":"<p>A secret is required to access your private Git repository containing Vulcan models and configurations.</p> <p>Create a git-sync secret: </p><pre><code>ds resource apply -f git-sync-secret.yaml\n</code></pre><p></p> <p>Example secret configuration: </p><pre><code>version: v1\ntype: secret\nname: git-sync\nworkspace: &lt;workspace-name&gt;\nspec:\n  type: key-value-properties\n  acl: r\n  data:\n    username: &lt;your-git-username&gt;\n    password: &lt;your-git-token-or-password&gt;\n</code></pre><p></p> <p>Important: Replace credentials with your actual Git repository access tokens.</p>"},{"location":"guides/deployment_guide/#configuration-files","title":"Configuration Files","text":"<p>Vulcan deployments require two key configuration files:</p>"},{"location":"guides/deployment_guide/#1-configyaml-vulcan-configuration","title":"1. <code>config.yaml</code> - Vulcan Configuration","text":"<p>This file contains Vulcan-specific configurations including model defaults, gateways, notifications, and metadata.</p> <p>Location: <code>&lt;project-root&gt;/config.yaml</code></p> <p>Key sections:</p>"},{"location":"guides/deployment_guide/#basic-metadata","title":"Basic Metadata","text":"<pre><code>name: &lt;data-product-name&gt;\ndisplay_name: &lt;Data Product Title&gt;\ntenant: &lt;tenant-name&gt;\ndescription: &lt;Description .... &gt;\n\ntags:\n  - &lt;tag1&gt;\n  - &lt;tag2&gt;\n\nterms:\n  - glossary.&lt;term1&gt;\n  - glossary.&lt;term2&gt;\n\nmetadata:\n  domain: &lt;business-domain&gt;\n  use_cases:\n    - &lt;use-case-1&gt;\n    - &lt;use-case-2&gt;\n  limitations:\n    - &lt;limitation-1&gt;\n    - &lt;limitation-2&gt;\n</code></pre>"},{"location":"guides/deployment_guide/#model-defaults","title":"Model Defaults","text":"<pre><code>model_defaults:\n  dialect: &lt;engine-dialect&gt;          # Database dialect eg. snowflake, bigquery\n  start: '2025-01-01'        # Start date for time-based models\n  cron: '&lt;cron&gt;'             # Default scheduling cadence @daily\n</code></pre>"},{"location":"guides/deployment_guide/#gateway-configuration","title":"Gateway Configuration","text":"<pre><code>gateways:\n  default:\n    connection:\n      type: depot\n      address: dataos://&lt;depot-name&gt;  # Reference to your depot\n</code></pre>"},{"location":"guides/deployment_guide/#users-and-ownership","title":"Users and Ownership","text":"<pre><code>users:\n  - username: &lt;username1&gt;\n    email: &lt;username1@email.id&gt;\n    type: OWNER\n  - username: &lt;username2&gt;\n    email: &lt;username2@email.id&gt;\n    type: CONTRIBUTOR\n</code></pre>"},{"location":"guides/deployment_guide/#complete-configyaml-example","title":"Complete config.yaml Example","text":"\ud83d\udccb Click to see complete config.yaml example <pre><code>name: user-engagement\ndisplay_name: User Engagement Analytics\ntenant: engineering\ndescription: User Engagement Analytics is a comprehensive data product delivering insights into user engagement patterns.\n\ntags:\n  - snowflake\n  - user_engagement\n  - device_analytics\n\nterms:\n  - glossary.data_product\n  - glossary.analytics_platform\n  - glossary.user_engagement\n\nmetadata:\n  domain: product_analytics\n  use_cases:\n    - User engagement tracking and analysis\n    - Device usage analytics\n    - Session and activity monitoring\n  limitations:\n    - Data available from 2025 onwards\n    - Refreshes daily at midnight UTC\n\nmodel_defaults:\n  dialect: snowflake\n  start: '2025-01-01'\n  cron: '@daily'\n\ngateways:\n  default:\n    connection:\n      type: depot\n      address: dataos://snowflakevulcan2\n\nnotification_targets:\n  - type: console\n    notify_on:\n      - apply_failure\n      - run_failure\n      - check_failure\n\nusers:\n  - username: shreya\n    email: shreya.sikarwar@tmdc.io\n    type: OWNER\n  - username: rohit\n    email: rohit.raj@tmdc.io\n    type: CONTRIBUTOR\n</code></pre>"},{"location":"guides/deployment_guide/#2-domain-resourceyaml-dataos-resource-configuration","title":"2. <code>domain-resource.yaml</code> - DataOS Resource Configuration","text":"<p>This file defines the DataOS-specific resource configuration for deploying Vulcan as a managed service.</p> <p>Location: <code>&lt;project-root&gt;/domain-resource.yaml</code></p> <p>Key sections:</p>"},{"location":"guides/deployment_guide/#resource-metadata","title":"Resource Metadata","text":"<pre><code>version: v1alpha\ntype: vulcan\nname: &lt;data-product-name&gt;\ntags:\n  - &lt;tag1&gt;\n  - &lt;tag2&gt;\n</code></pre>"},{"location":"guides/deployment_guide/#execution-configuration","title":"Execution Configuration","text":"<pre><code>spec:\n  runAsUser: \"&lt;dataos-username&gt;\"     # DataOS user identity\n  compute: &lt;compute-name&gt;            # Compute cluster name eg. cyclone-compute\n  engine: &lt;engine-name&gt;              # Execution engine eg. snowflake, bigquery\n</code></pre>"},{"location":"guides/deployment_guide/#repository-configuration","title":"Repository Configuration","text":"<pre><code>  repo:\n    url: &lt;git-repository-url&gt;                # eg. https://github.com/org/repo\n    syncFlags:\n      - '--ref=&lt;branch-name&gt;'                # Git branch eg. main\n      - '--submodules=off'\n    baseDir: &lt;path-to-project-in-repo&gt;       # Path to project folder\n    secret: &lt;workspace&gt;:&lt;secret&gt;          # Git credentials secret eg. engineering:git-sync-name\n</code></pre>"},{"location":"guides/deployment_guide/#depot-references","title":"Depot References","text":"<pre><code>  depots:\n    - dataos://&lt;depot-name&gt;?purpose=rw      # Read-write depot access\n</code></pre>"},{"location":"guides/deployment_guide/#workflow-configuration","title":"Workflow Configuration","text":"<pre><code>  workflow:\n    type: schedule              # Run on a schedule\n    schedule:\n      crons:\n        - '&lt;cron-expression&gt;'  # eg. '*/45 * * * *' (Every 45 minutes)\n      endOn: '&lt;end-date&gt;'      # eg. '2027-01-01T00:00:00-00:00'\n      timezone: '&lt;timezone&gt;'   # eg. 'US/Pacific'\n      concurrencyPolicy: Forbid\n\n    logLevel: INFO\n\n    resource:                   # Resource allocation\n      request:\n        cpu: \"&lt;cpu-request&gt;\"   # eg. \"200m\"\n        memory: \"&lt;memory-request&gt;\"  # eg. \"512Mi\"\n      limit:\n        cpu: \"&lt;cpu-limit&gt;\"     # eg. \"1000m\"\n        memory: \"&lt;memory-limit&gt;\"    # eg. \"1Gi\"\n</code></pre>"},{"location":"guides/deployment_guide/#vulcan-commands","title":"Vulcan Commands","text":"<pre><code>    migrate:                    # Schema migration\n      command: [vulcan]\n      arguments: [migrate]\n\n    plan:                       # Plan changes\n      command: [vulcan]\n      arguments:\n        - --log-to-stdout\n        - plan\n        - --auto-apply\n\n    run:                        # Execute models\n      command: [vulcan]\n      arguments:\n        - --log-to-stdout\n        - run\n</code></pre>"},{"location":"guides/deployment_guide/#api-configuration","title":"API Configuration","text":"<pre><code>  api:\n    replicas: &lt;replica-count&gt;     # eg. 1\n    logLevel: INFO\n    resource:\n      request:\n        cpu: \"&lt;cpu-request&gt;\"      # eg. \"200m\"\n        memory: \"&lt;memory-request&gt;\"     # eg. \"512Mi\"\n      limit:\n        cpu: \"&lt;cpu-limit&gt;\"        # eg. \"5000m\"\n        memory: \"&lt;memory-limit&gt;\"       # eg. \"4Gi\"\n</code></pre>"},{"location":"guides/deployment_guide/#complete-domain-resourceyaml-example","title":"Complete domain-resource.yaml Example","text":"\ud83d\udccb Click to see complete domain-resource.yaml example <pre><code>version: v1alpha\ntype: vulcan\nname: user-engagement\ntags:\n  - snowflake-analytics\n  - user_engagement\n  - device_analytics\nspec:\n  runAsUser: \"shreyasikarwartmdcio\"\n  compute: cyclone-compute\n  engine: snowflake\n  repo:\n    url: https://bitbucket.org/rubik_/vulcan-examples\n    syncFlags:\n      - '--ref=main'\n      - '--submodules=off'\n    baseDir: vulcan-examples/customer-usecase/usdk\n    secret: engineering:git-sync\n  depots:\n    - dataos://snowflakevulcan2?purpose=rw\n  workflow:\n    type: schedule\n    schedule:\n      crons:\n        - '*/45 * * * *'\n      endOn: '2027-01-01T00:00:00-00:00'\n      timezone: 'US/Pacific'\n      concurrencyPolicy: Forbid\n    logLevel: INFO\n    resource:\n      request:\n        cpu: \"200m\"\n        memory: \"512Mi\"\n      limit:\n        cpu: \"1000m\"\n        memory: \"1Gi\"\n    migrate:\n      command:\n        - vulcan\n      arguments:\n        - migrate\n    plan:\n      command:\n        - vulcan\n      arguments:\n        - --log-to-stdout\n        - plan\n        - --auto-apply\n    run:\n      command:\n        - vulcan\n      arguments:\n        - --log-to-stdout\n        - run\n  api:\n    replicas: 1\n    logLevel: INFO\n    resource:\n      request:\n        cpu: \"200m\"\n        memory: \"512Mi\"\n      limit:\n        cpu: \"5000m\"\n        memory: \"4Gi\"\n</code></pre>"},{"location":"guides/deployment_guide/#deployment-steps","title":"Deployment Steps","text":""},{"location":"guides/deployment_guide/#step-1-prepare-your-repository","title":"Step 1: Prepare Your Repository","text":"<ol> <li> <p>Create your Vulcan project structure: </p><pre><code>your-project/\n\u251c\u2500\u2500 config.yaml              # Vulcan configuration\n\u251c\u2500\u2500 domain-resource.yaml     # DataOS resource definition\n\u251c\u2500\u2500 models/                  # SQL model files\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 marts/\n\u251c\u2500\u2500 seeds/                   # Static data files\n\u251c\u2500\u2500 checks/                  # Data quality checks\n\u251c\u2500\u2500 audits/                  # Audit queries\n\u2514\u2500\u2500 semantics/              # Semantic layer definitions\n</code></pre><p></p> </li> <li> <p>Configure <code>config.yaml</code> with your project settings</p> </li> <li>Configure <code>domain-resource.yaml</code> with DataOS settings</li> <li>Push your code to a Git repository</li> </ol>"},{"location":"guides/deployment_guide/#step-2-create-required-secrets","title":"Step 2: Create Required Secrets","text":"<pre><code># Create git-sync secret (if not exists)\nds resource apply -f git-sync-secret.yaml\n</code></pre>"},{"location":"guides/deployment_guide/#step-3-verify-prerequisites","title":"Step 3: Verify Prerequisites","text":"<pre><code># Verify depot exists\nds resource -t depot get -n &lt;depot-name&gt; -a\n\n# Verify compute exists\nds resource -t compute get -n &lt;compute-name&gt; -a\n\n# Verify stack exists\nds resource -t stack get -a \n</code></pre>"},{"location":"guides/deployment_guide/#step-4-deploy-vulcan-resource","title":"Step 4: Deploy Vulcan Resource","text":"<pre><code># Apply the domain-resource configuration\nds resource apply -f domain-resource.yaml\n</code></pre>"},{"location":"guides/deployment_guide/#step-5-monitor-deployment","title":"Step 5: Monitor Deployment","text":"<pre><code># Get resource status\nds resource -t vulcan -n &lt;data-product-name&gt; get\n\n\n# Check logs\nds resource -t vulcan -n &lt;data-product-name&gt; logs\n</code></pre>"},{"location":"guides/deployment_guide/#verification","title":"Verification","text":""},{"location":"guides/deployment_guide/#verify-models-in-data-warehouse","title":"Verify Models in Data Warehouse","text":"<p>Connect to your data warehouse and verify that tables/views have been created:</p> <pre><code>-- For Snowflake\nSHOW TABLES IN SCHEMA &lt;database&gt;.&lt;schema&gt;;\n\n-- Check specific table\nSELECT * FROM &lt;database&gt;.&lt;schema&gt;.&lt;table-name&gt; LIMIT 10;\n</code></pre>"},{"location":"guides/deployment_guide/#access-vulcan-api","title":"Access Vulcan API","text":"<pre><code># Test API (if exposed)\ncurl --location 'https://&lt;env-fqn&gt;/&lt;tenant&gt;/vulcan/&lt;data-product-name&gt;/livez' \\\n  --header 'Authorization: Bearer &lt;your-token&gt;'\n</code></pre>"},{"location":"guides/incremental_by_time/","title":"Incremental by Time","text":""},{"location":"guides/incremental_by_time/#incremental-by-time","title":"Incremental by Time","text":"<p>This guide explains how incremental by time models work in Vulcan using the Orders360 example project. You'll learn why they're efficient, how they process data, and how to create them.</p> <p>See the models guide for general model information or the model kinds page for all model types.</p>"},{"location":"guides/incremental_by_time/#why-use-incremental-models","title":"Why Use Incremental Models?","text":""},{"location":"guides/incremental_by_time/#the-problem-full-refreshes-are-expensive","title":"The Problem: Full Refreshes Are Expensive","text":"<p>If you have a table with sales data from the last year (365 days), every time you run a <code>FULL</code> model, it processes all 365 days. This is inefficient when you only need to process today's data.</p> <pre><code>flowchart LR\n    subgraph \"FULL Model - Every Run\"\n        FULL[FULL Model Run]\n        PROCESS[Process ALL 365 Days]\n        DAY1[Day 1]\n        DAY2[Day 2]\n        DAY3[Day 3]\n        DOTS[...]\n        DAY365[Day 365]\n    end\n\n    subgraph \"Results\"\n        TIME1[Time: 10 minutes]\n        COST1[Cost: $10]\n        DATA1[All 365 days]\n    end\n\n    FULL --&gt; PROCESS\n    PROCESS --&gt; DAY1\n    PROCESS --&gt; DAY2\n    PROCESS --&gt; DAY3\n    PROCESS --&gt; DOTS\n    PROCESS --&gt; DAY365\n\n    DAY365 --&gt; TIME1\n    DAY365 --&gt; COST1\n    DAY365 --&gt; DATA1\n\n    style FULL fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000\n    style PROCESS fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style TIME1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing FULL model processing all 365 days]</p>"},{"location":"guides/incremental_by_time/#the-solution-only-process-whats-new","title":"The Solution: Only Process What's New","text":"<p>With incremental models, Vulcan only processes new or missing days:</p> <pre><code>flowchart LR\n    subgraph \"INCREMENTAL Model - Every Run\"\n        INCR[INCREMENTAL Model Run]\n        CHECK[Check State Database]\n        SKIP[Skip Days 1-364]\n        PROCESS_NEW[Process Day 365 Only]\n    end\n\n    subgraph \"Results\"\n        TIME2[Time: 30 seconds]\n        COST2[Cost: $0.20]\n        DATA2[Only Day 365]\n    end\n\n    INCR --&gt; CHECK\n    CHECK --&gt; SKIP\n    CHECK --&gt; PROCESS_NEW\n\n    PROCESS_NEW --&gt; TIME2\n    PROCESS_NEW --&gt; COST2\n    PROCESS_NEW --&gt; DATA2\n\n    style INCR fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style PROCESS_NEW fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TIME2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style COST2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style DATA2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual showing incremental model processing only Day 365]</p> <p>Result: 50x faster and 50x cheaper! </p> <p>Incremental models process only what you need, when you need it.</p>"},{"location":"guides/incremental_by_time/#how-incremental-models-work","title":"How Incremental Models Work","text":"<p>Incremental models use time intervals to track what's been processed. Think of it like a calendar where Vulcan checks off each day. It knows what's done and what still needs work.</p> <pre><code>flowchart TB\n    subgraph \"Incremental Processing Flow\"\n        START[vulcan run]\n        CHECK[Check State Database&lt;br/&gt;What's already processed?]\n\n        subgraph \"State Database\"\n            PROCESSED1[Jan 1-7: Processed]\n            PROCESSED2[Jan 8-14: Processed]\n            MISSING[Jan 15-21: Missing]\n        end\n\n        CALC[Calculate Missing Intervals&lt;br/&gt;Jan 15-21 needs processing]\n        SET_MACROS[Set Macros&lt;br/&gt;@start_ds = '2025-01-15'&lt;br/&gt;@end_ds = '2025-01-21']\n        QUERY[Run Query&lt;br/&gt;WHERE order_date BETWEEN @start_ds AND @end_ds]\n        INSERT[Insert Results&lt;br/&gt;Into weekly_sales table]\n        UPDATE[Update State&lt;br/&gt;Mark Jan 15-21 as processed]\n    end\n\n    START --&gt; CHECK\n    CHECK --&gt; PROCESSED1\n    CHECK --&gt; PROCESSED2\n    CHECK --&gt; MISSING\n\n    MISSING --&gt; CALC\n    CALC --&gt; SET_MACROS\n    SET_MACROS --&gt; QUERY\n    QUERY --&gt; INSERT\n    INSERT --&gt; UPDATE\n\n    UPDATE --&gt; PROCESSED1\n    UPDATE --&gt; PROCESSED2\n\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESSED1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style PROCESSED2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style MISSING fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CALC fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style SET_MACROS fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style QUERY fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style INSERT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style UPDATE fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/incremental_by_time/#step-1-vulcan-checks-whats-already-done","title":"Step 1: Vulcan Checks What's Already Done","text":"<p>When you run <code>vulcan run</code>, Vulcan looks at your state database and asks:</p> <ul> <li> <p>\"What dates have I already processed?\" - These are done, skip them</p> </li> <li> <p>\"What dates are missing?\" - These need work, process them</p> </li> </ul> <p>It's like checking your to-do list, you only work on what's not done yet.</p> <pre><code>State Database Check:\nJan 1-7:   Already processed\nJan 8-14:  Already processed  \nJan 15-21: Missing - needs processing\n</code></pre> <p>[Screenshot: Visual diagram showing state database check with processed vs missing intervals]</p>"},{"location":"guides/incremental_by_time/#step-2-vulcan-processes-only-missing-intervals","title":"Step 2: Vulcan Processes Only Missing Intervals","text":"<p>Vulcan then processes only the missing dates. It sets up the macros (<code>@start_ds</code> and <code>@end_ds</code>) and runs your query for just that time range:</p> <pre><code>Processing Jan 15-21:\n@start_ds = '2025-01-15'\n@end_ds   = '2025-01-21'\n\nQuery runs:\nSELECT ... FROM daily_sales\nWHERE order_date BETWEEN '2025-01-15' AND '2025-01-21'\n</code></pre> <p>[Screenshot: Visual showing how @start_ds and @end_ds are used in the query]</p>"},{"location":"guides/incremental_by_time/#step-3-results-are-inserted","title":"Step 3: Results Are Inserted","text":"<p>The processed data is inserted into your table, and Vulcan records that these dates are now complete. Next time you run, it'll know these dates are done and skip them. Efficient!</p> <pre><code>Jan 15-21: Now processed and recorded\n</code></pre> <p>[Screenshot: Visual showing data insertion and state update]</p>"},{"location":"guides/incremental_by_time/#understanding-time-intervals","title":"Understanding Time Intervals","text":"<p>Vulcan divides time into intervals based on your model's schedule. Each interval is a chunk of time that gets processed together, like a day, a week, or an hour, depending on your model's <code>cron</code> schedule.</p>"},{"location":"guides/incremental_by_time/#daily-intervals-example","title":"Daily Intervals Example","text":"<p>For a daily model (<code>cron '@daily'</code>), each day is one interval:</p> <pre><code>gantt\n    title Daily Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Jan 1 Complete     :done, interval1, 2025-01-01, 1d\n    Jan 2 Complete     :done, interval2, 2025-01-02, 1d\n    Jan 3 In Progress :active, interval3, 2025-01-03, 1d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 3, 2025 at 2pm\n\nIntervals:\n- Jan 1: Complete (full day passed)\n\n- Jan 2: Complete (full day passed)\n\n- Jan 3: In progress (day not finished yet)\n</code></pre> <p>[Screenshot: Calendar view showing daily intervals with Jan 1-2 complete, Jan 3 in progress]</p>"},{"location":"guides/incremental_by_time/#weekly-intervals-example","title":"Weekly Intervals Example","text":"<p>For a weekly model (<code>cron '@weekly'</code>), each week is one interval:</p> <pre><code>gantt\n    title Weekly Intervals\n    dateFormat YYYY-MM-DD\n    section Intervals\n    Week 1 Jan 1-7     :done, week1, 2025-01-01, 7d\n    Week 2 Jan 8-14    :done, week2, 2025-01-08, 7d\n    Week 3 Jan 15-21   :active, week3, 2025-01-15, 7d</code></pre> <pre><code>Model Start: Jan 1, 2025\nToday: Jan 15, 2025\n\nIntervals:\n- Week 1 (Jan 1-7):   Complete\n\n- Week 2 (Jan 8-14):  Complete\n\n- Week 3 (Jan 15-21): In progress\n</code></pre> <p>[Screenshot: Calendar view showing weekly intervals]</p>"},{"location":"guides/incremental_by_time/#how-vulcan-tracks-intervals","title":"How Vulcan Tracks Intervals","text":"<p>When you first run <code>vulcan plan</code> on an incremental model, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"First Plan - Jan 15, 2025\"\n        PLAN1[vulcan plan dev]\n        CALC1[Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;3 weeks total]\n        PROCESS1[Process All Intervals&lt;br/&gt;Backfill everything]\n        RECORD1[Record in State DB&lt;br/&gt;Weeks 1-3 processed]\n\n        subgraph \"State Database After Plan\"\n            W1[Week 1: Jan 1-7]\n            W2[Week 2: Jan 8-14]\n            W3[Week 3: Jan 15-21]\n        end\n    end\n\n    PLAN1 --&gt; CALC1\n    CALC1 --&gt; PROCESS1\n    PROCESS1 --&gt; RECORD1\n    RECORD1 --&gt; W1\n    RECORD1 --&gt; W2\n    RECORD1 --&gt; W3\n\n    style PLAN1 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CALC1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PROCESS1 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RECORD1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style W3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates all intervals from the start date to now</li> <li>Processes all missing intervals (backfill)</li> <li>Records what was processed in the state database</li> </ol> <pre><code>First Plan (Jan 15, 2025):\n- Calculates: 3 weeks of intervals\n\n- Processes: All 3 weeks\n\n- Records: \"Weeks 1-3 processed\"\n\nState Database:\nWeek 1 (Jan 1-7)\nWeek 2 (Jan 8-14)\nWeek 3 (Jan 15-21)\n</code></pre> <p>[Screenshot: Visual showing first plan calculating and processing all intervals]</p> <p>When you run <code>vulcan run</code> later, Vulcan:</p> <pre><code>flowchart TB\n    subgraph \"Second Run - Jan 22, 2025\"\n        RUN2[vulcan run]\n        CALC2[Calculate Intervals&lt;br/&gt;From start to now&lt;br/&gt;4 weeks total]\n        CHECK[Check State DB&lt;br/&gt;What's already processed?]\n\n        subgraph \"Current State\"\n            W1_EXIST[Week 1: Jan 1-7]\n            W2_EXIST[Week 2: Jan 8-14]\n            W3_EXIST[Week 3: Jan 15-21]\n            W4_MISS[Week 4: Jan 22-28]\n        end\n\n        PROCESS2[Process Only Week 4&lt;br/&gt;Skip Weeks 1-3]\n        RECORD2[Update State DB&lt;br/&gt;Week 4 now processed]\n\n        subgraph \"Updated State\"\n            W1_NEW[Week 1: Jan 1-7]\n            W2_NEW[Week 2: Jan 8-14]\n            W3_NEW[Week 3: Jan 15-21]\n            W4_NEW[Week 4: Jan 22-28]\n        end\n    end\n\n    RUN2 --&gt; CALC2\n    CALC2 --&gt; CHECK\n    CHECK --&gt; W1_EXIST\n    CHECK --&gt; W2_EXIST\n    CHECK --&gt; W3_EXIST\n    CHECK --&gt; W4_MISS\n\n    W4_MISS --&gt; PROCESS2\n    PROCESS2 --&gt; RECORD2\n\n    RECORD2 --&gt; W1_NEW\n    RECORD2 --&gt; W2_NEW\n    RECORD2 --&gt; W3_NEW\n    RECORD2 --&gt; W4_NEW\n\n    style RUN2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style CALC2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style W4_MISS fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style PROCESS2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RECORD2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Calculates intervals from start to now</li> <li>Compares with what's already processed</li> <li>Processes only new intervals</li> </ol> <pre><code>Second Run (Jan 22, 2025):\n- Calculates: 4 weeks total\n\n- Already processed: Weeks 1-3\n\n- Missing: Week 4 (Jan 22-28)\n\n- Processes: Only Week 4\n\nState Database:\nWeek 1 (Jan 1-7)\nWeek 2 (Jan 8-14)\nWeek 3 (Jan 15-21)\nWeek 4 (Jan 22-28) \u2190 NEW\n</code></pre> <p>[Screenshot: Visual showing second run processing only new Week 4]</p>"},{"location":"guides/incremental_by_time/#creating-an-incremental-model","title":"Creating an Incremental Model","text":"<p>Let's create a weekly sales aggregation model for Orders360.</p>"},{"location":"guides/incremental_by_time/#step-1-create-the-model-file","title":"Step 1: Create the Model File","text":"<pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing new weekly_sales.sql file]</p>"},{"location":"guides/incremental_by_time/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit <code>models/sales/weekly_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,  -- \u23f0 This column contains the date\n    batch_size 1             -- Process 1 week at a time\n  ),\n  start '2025-01-01',       -- Start processing from this date\n  cron '@weekly',            -- Run weekly\n  grain [order_date],        -- One row per week\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- Filter by time range\nGROUP BY DATE_TRUNC('week', order_date)\nORDER BY order_date\n</code></pre> <p>[Screenshot: Code editor showing complete weekly_sales.sql model]</p>"},{"location":"guides/incremental_by_time/#key-components-explained","title":"Key Components Explained","text":""},{"location":"guides/incremental_by_time/#1-time-column-declaration","title":"1. Time Column Declaration","text":"<pre><code>kind INCREMENTAL_BY_TIME_RANGE (\n  time_column order_date  -- Tell Vulcan which column has dates\n)\n</code></pre> <p>What it does: Tells Vulcan which column contains the timestamp/date for each row. This is how Vulcan knows how to filter and group your data by time.</p> <p>[Screenshot: Code highlighting time_column declaration]</p>"},{"location":"guides/incremental_by_time/#2-where-clause-with-macros","title":"2. WHERE Clause with Macros","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>What it does: Filters data to only the time range being processed. Without it, you'd process all your data every time.</p> <ul> <li> <p><code>@start_ds</code> = Start date of the interval (e.g., '2025-01-15')</p> </li> <li> <p><code>@end_ds</code> = End date of the interval (e.g., '2025-01-21')</p> </li> </ul> <p>Vulcan automatically replaces these with the correct dates! You don't have to figure out what dates to process, Vulcan does that for you.</p> <p>[Screenshot: Code highlighting WHERE clause with macros, showing how they're replaced]</p>"},{"location":"guides/incremental_by_time/#3-start-date","title":"3. Start Date","text":"<pre><code>start '2025-01-01'\n</code></pre> <p>What it does: Tells Vulcan when your data begins. Vulcan will backfill from this date when you first create the model, processing all historical data up to today.</p> <p>[Screenshot: Code highlighting start date]</p>"},{"location":"guides/incremental_by_time/#step-3-apply-the-model","title":"Step 3: Apply the Model","text":"<p>Run <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales model to be added]</p> <p>Vulcan will process each week incrementally:</p> <pre><code>[1/3] sales.weekly_sales  [insert 2025-01-01 - 2025-01-07]  1.2s\n[2/3] sales.weekly_sales  [insert 2025-01-08 - 2025-01-14]  1.1s\n[3/3] sales.weekly_sales  [insert 2025-01-15 - 2025-01-21]  1.3s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 3/3 \u2022 0:00:03\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre> <p>[Screenshot: Backfill progress showing each week being processed]</p>"},{"location":"guides/incremental_by_time/#real-example-daily-sales-from-orders360","title":"Real Example: Daily Sales from Orders360","text":"<p>Here's the actual <code>daily_sales</code> model from Orders360 (currently FULL, but could be incremental):</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,  -- Could be INCREMENTAL_BY_TIME_RANGE\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics', 'analytics.sales_summary'),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing complete model]</p> <p>To make this incremental, you would:</p> <ol> <li>Change <code>kind FULL</code> to <code>kind INCREMENTAL_BY_TIME_RANGE</code></li> <li>Add <code>time_column order_date</code></li> <li>Add <code>WHERE order_date BETWEEN @start_ds AND @end_ds</code></li> </ol> <pre><code>MODEL (\n  name sales.daily_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date\n  ),\n  start '2025-01-01',\n  cron '@daily',\n  -- ... rest stays the same\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE order_date BETWEEN @start_ds AND @end_ds  -- ADD THIS\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: Comparison showing FULL vs INCREMENTAL changes]</p>"},{"location":"guides/incremental_by_time/#understanding-the-where-clause","title":"Understanding the WHERE Clause","text":"<p>You might wonder: \"Why do I need a WHERE clause if Vulcan adds one automatically?\" They serve different purposes.</p>"},{"location":"guides/incremental_by_time/#two-where-clauses-two-purposes","title":"Two WHERE Clauses, Two Purposes","text":"<p>Vulcan actually uses two WHERE clauses:</p>"},{"location":"guides/incremental_by_time/#1-your-models-where-clause","title":"1. Your Model's WHERE Clause","text":"<pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data read into the model</p> <ul> <li> <p>Only reads necessary data from upstream tables - this is a performance optimization</p> </li> <li> <p>Saves processing time and resources - why read data you're not going to use?</p> </li> <li> <p>You control this in your SQL - you write this WHERE clause</p> </li> </ul> <p>This is your optimization. Without it, you'd read all the data from upstream tables, even though you only need a small date range.</p> <p>[Screenshot: Visual showing model WHERE clause filtering input data]</p>"},{"location":"guides/incremental_by_time/#2-vulcans-automatic-where-clause","title":"2. Vulcan's Automatic WHERE Clause","text":"<p>Vulcan automatically adds another filter on the output:</p> <pre><code>-- Vulcan adds this automatically:\nWHERE order_date BETWEEN @start_ds AND @end_ds\n</code></pre> <p>Purpose: Filters data output by the model</p> <ul> <li> <p>Prevents data leakage (ensures no rows outside the time range) - this is a safety check</p> </li> <li> <p>Safety mechanism - even if your SQL has a bug, Vulcan makes sure you don't output wrong dates</p> </li> <li> <p>Vulcan controls this automatically - you don't write this one, Vulcan adds it</p> </li> </ul> <p>This is Vulcan's safety net. It makes sure that even if your WHERE clause has a bug, you can't accidentally output data for the wrong time period.</p> <p>[Screenshot: Visual showing Vulcan's automatic WHERE clause filtering output]</p>"},{"location":"guides/incremental_by_time/#why-both-are-needed","title":"Why Both Are Needed","text":"<p>Here's why you need both:</p> <ul> <li> <p>Your WHERE clause: Optimizes performance by reading less data - makes your queries faster</p> </li> <li> <p>Vulcan's WHERE clause: Ensures correctness by preventing data leakage - makes sure your data is right</p> </li> </ul> <p>Always include the WHERE clause in your model SQL! It's not optional, Vulcan needs it to know what time range to process, and it makes your queries way more efficient.</p> <p>[Screenshot: Side-by-side comparison showing both WHERE clauses and their purposes]</p>"},{"location":"guides/incremental_by_time/#running-incremental-models","title":"Running Incremental Models","text":"<p>Vulcan has two commands for processing models:</p>"},{"location":"guides/incremental_by_time/#vulcan-plan-for-model-changes","title":"<code>vulcan plan</code> - For Model Changes","text":"<p>Use when you've changed a model:</p> <pre><code>vulcan plan dev\n</code></pre> <p>What it does:</p> <ul> <li> <p>Detects model changes</p> </li> <li> <p>Shows what will be affected</p> </li> <li> <p>Backfills missing intervals</p> </li> <li> <p>Applies changes to the environment</p> </li> </ul> <p>[Screenshot: Plan command output showing model changes]</p>"},{"location":"guides/incremental_by_time/#vulcan-run-for-scheduled-execution","title":"<code>vulcan run</code> - For Scheduled Execution","text":"<p>Use when no models have changed:</p> <pre><code>vulcan run\n</code></pre> <p>What it does:</p> <ul> <li> <p>Checks each model's <code>cron</code> schedule</p> </li> <li> <p>Processes only models that are due</p> </li> <li> <p>Processes only missing intervals</p> </li> <li> <p>Fast and efficient</p> </li> </ul> <p>[Screenshot: Run command output showing scheduled execution]</p>"},{"location":"guides/incremental_by_time/#how-cron-schedules-work","title":"How Cron Schedules Work","text":"<p>Each model has a <code>cron</code> parameter that determines how often it should run:</p> <pre><code>flowchart LR\n    subgraph \"Cron Schedules\"\n        DAILY[@daily&lt;br/&gt;Every 24 hours]\n        WEEKLY[@weekly&lt;br/&gt;Every 7 days]\n        HOURLY[@hourly&lt;br/&gt;Every 1 hour]\n    end\n\n    subgraph \"Example Models\"\n        M1[sales.daily_sales&lt;br/&gt;cron: @daily]\n        M2[sales.weekly_sales&lt;br/&gt;cron: @weekly]\n    end\n\n    DAILY --&gt; M1\n    WEEKLY --&gt; M2\n\n    style DAILY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style WEEKLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style HOURLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style M1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style M2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>cron '@daily'   -- Run once per day\ncron '@weekly'  -- Run once per week\ncron '@hourly'  -- Run once per hour\n</code></pre> <p>Example from Orders360:</p> <pre><code>-- Daily model\nMODEL (\n  name sales.daily_sales,\n  cron '@daily'  -- Runs every day\n);\n\n-- Weekly model\nMODEL (\n  name sales.weekly_sales,\n  cron '@weekly'  -- Runs once per week\n);\n</code></pre> <p>[Screenshot: Visual showing cron schedules for daily vs weekly models]</p> <p>When you run <code>vulcan run</code>:</p> <pre><code>flowchart TB\n    subgraph \"vulcan run Execution\"\n        RUN[vulcan run&lt;br/&gt;at 2pm on Jan 15]\n\n        subgraph \"Model Evaluation\"\n            CHECK1[Check daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Last run: 24h ago]\n            CHECK2[Check weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Last run: 2 days ago]\n        end\n\n        subgraph \"Decision\"\n            DUE1[Due!&lt;br/&gt;Process daily_sales]\n            SKIP[Not due&lt;br/&gt;Skip weekly_sales]\n        end\n\n        EXEC1[Execute daily_sales&lt;br/&gt;Process missing intervals]\n    end\n\n    RUN --&gt; CHECK1\n    RUN --&gt; CHECK2\n\n    CHECK1 --&gt; DUE1\n    CHECK2 --&gt; SKIP\n\n    DUE1 --&gt; EXEC1\n\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style CHECK2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DUE1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ol> <li>Vulcan checks each model's <code>cron</code></li> <li>Determines if enough time has passed since last run</li> <li>Processes only models that are due</li> </ol> <pre><code>vulcan run at 2pm on Jan 15:\n\ndaily_sales (@daily):   Last run 24h ago \u2192 Due, process!\nweekly_sales (@weekly): Last run 2 days ago \u2192 Not due, skip\n</code></pre> <p>[Screenshot: Visual showing cron evaluation logic]</p>"},{"location":"guides/incremental_by_time/#batch-processing","title":"Batch Processing","text":"<p>For large datasets, you can process intervals in batches using <code>batch_size</code>:</p> <pre><code>flowchart TB\n    subgraph \"Without batch_size Default\"\n        ALL[12 Weeks Missing]\n        SINGLE[\"Single Job&lt;br/&gt;Process all 12 weeks\"]\n        RESULT1[\"All done in 1 job&lt;br/&gt;30 minutes\"]\n    end\n\n    subgraph \"With batch_size = 4\"\n        ALL2[12 Weeks Missing]\n        BATCH1[\"Batch 1&lt;br/&gt;Weeks 1-4\"]\n        BATCH2[\"Batch 2&lt;br/&gt;Weeks 5-8\"]\n        BATCH3[\"Batch 3&lt;br/&gt;Weeks 9-12\"]\n        RESULT2[\"All done in 3 jobs&lt;br/&gt;10 min each\"]\n    end\n\n    ALL --&gt; SINGLE\n    SINGLE --&gt; RESULT1\n\n    ALL2 --&gt; BATCH1\n    ALL2 --&gt; BATCH2\n    ALL2 --&gt; BATCH3\n    BATCH1 --&gt; RESULT2\n    BATCH2 --&gt; RESULT2\n    BATCH3 --&gt; RESULT2\n\n    style ALL fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style SINGLE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style ALL2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style BATCH1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BATCH3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RESULT2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 4  -- Process 4 weeks at a time\n  )\n);\n</code></pre> <p>Without batch_size (default):</p> <ul> <li> <p>Processes all missing intervals in one job</p> </li> <li> <p>Example: 12 weeks = 1 job</p> </li> </ul> <p>With batch_size:</p> <ul> <li> <p>Divides intervals into batches</p> </li> <li> <p>Example: 12 weeks \u00f7 4 = 3 jobs</p> </li> </ul> <p>[Screenshot: Visual comparison showing batch processing vs single job]</p> <p>When to use batches:</p> <ul> <li> <p>Large datasets that might timeout</p> </li> <li> <p>Need better progress tracking</p> </li> <li> <p>Want to parallelize processing</p> </li> </ul> <p>When not to use batches:</p> <ul> <li> <p>Small datasets (&lt; 1GB)</p> </li> <li> <p>Fast queries (&lt; 1 minute)</p> </li> <li> <p>Simple transformations</p> </li> </ul>"},{"location":"guides/incremental_by_time/#forward-only-models","title":"Forward-Only Models","text":"<p>Sometimes you have models so large that rebuilding them is impossible. Forward-only models solve this. Use them for massive tables where a full backfill would take days or cost too much.</p>"},{"location":"guides/incremental_by_time/#what-are-forward-only-models","title":"What Are Forward-Only Models?","text":"<p>Forward-only models never rebuild historical data. Changes are only applied going forward in time. Once historical data is processed, you can't go back and change it. You can only change what happens going forward.</p> <pre><code>flowchart TB\n    subgraph \"Regular Model Change\"\n        REG_CHANGE[\"Breaking Change Detected\"]\n        REG_REBUILD[\"Rebuild Entire Table&lt;br/&gt;All dates: Jan 1 - Dec 31\"]\n        REG_RESULT[\"All data updated\"]\n    end\n\n    subgraph \"Forward-Only Model Change\"\n        FWD_CHANGE[\"Breaking Change Detected&lt;br/&gt;forward_only: true\"]\n        FWD_CHECK[\"Check Existing Data&lt;br/&gt;Jan 1 - Dec 15: Keep as-is\"]\n        FWD_APPLY[\"Apply Change Forward&lt;br/&gt;Dec 16 - Dec 31: New data\"]\n        FWD_RESULT[\"Historical preserved&lt;br/&gt;Future updated\"]\n    end\n\n    REG_CHANGE --&gt; REG_REBUILD\n    REG_REBUILD --&gt; REG_RESULT\n\n    FWD_CHANGE --&gt; FWD_CHECK\n    FWD_CHECK --&gt; FWD_APPLY\n    FWD_APPLY --&gt; FWD_RESULT\n\n    style REG_CHANGE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style REG_REBUILD fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style REG_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_CHANGE fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style FWD_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style FWD_APPLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style FWD_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Regular Model Change: </p><pre><code>Breaking change \u2192 Rebuild entire table (all dates)\n</code></pre><p></p> <p>Forward-Only Model Change: </p><pre><code>Breaking change \u2192 Only apply to new dates going forward\n</code></pre><p></p> <p>[Screenshot: Visual comparison showing regular rebuild vs forward-only]</p>"},{"location":"guides/incremental_by_time/#when-to-use-forward-only","title":"When to Use Forward-Only","text":"<p>Use forward-only when:</p> <ul> <li> <p>Tables are too large to rebuild - if a full backfill would take forever or cost too much</p> </li> <li> <p>Historical data can't be reprocessed - maybe the source data is gone or too expensive to reprocess</p> </li> <li> <p>You only care about future data - if historical data is \"good enough\" and you just need new data to be correct</p> </li> </ul> <p>Don't use forward-only when:</p> <ul> <li> <p>You need to fix historical data - forward-only won't help you fix the past</p> </li> <li> <p>Schema changes affect past data - if your change affects how historical data should look, you need a full rebuild</p> </li> <li> <p>You want full data consistency - forward-only means historical and new data might have different schemas</p> </li> </ul> <p>It's a trade-off: you get speed and cost savings, but you lose the ability to fix historical data. Make sure that's a trade-off you're okay with!</p>"},{"location":"guides/incremental_by_time/#making-a-model-forward-only","title":"Making a Model Forward-Only","text":"<p>Add <code>forward_only true</code> to your model:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    forward_only true  -- All changes are forward-only\n  )\n);\n</code></pre> <p>[Screenshot: Code showing forward_only configuration]</p>"},{"location":"guides/incremental_by_time/#forward-only-plans","title":"Forward-Only Plans","text":"<p>You can also make a specific plan forward-only:</p> <pre><code>vulcan plan dev --forward-only\n</code></pre> <p>This treats all changes in the plan as forward-only, even if models aren't configured that way.</p> <p>[Screenshot: Plan command with --forward-only flag]</p>"},{"location":"guides/incremental_by_time/#schema-changes-in-forward-only-models","title":"Schema Changes in Forward-Only Models","text":"<p>When you change a forward-only model, Vulcan checks for schema changes that could cause problems.</p>"},{"location":"guides/incremental_by_time/#types-of-schema-changes","title":"Types of Schema Changes","text":""},{"location":"guides/incremental_by_time/#destructive-changes","title":"Destructive Changes","text":"<p>Changes that remove or modify existing data:</p> <pre><code>flowchart LR\n    subgraph \"Destructive Changes\"\n        DROP[\"Dropping Column&lt;br/&gt;total_amount removed\"]\n        RENAME[\"Renaming Column&lt;br/&gt;order_id to id\"]\n        TYPE[\"Changing Type&lt;br/&gt;INT to STRING&lt;br/&gt;may cause loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE1[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER1[\"After:&lt;br/&gt;order_id only\"]\n    end\n\n    DROP --&gt; BEFORE1\n    BEFORE1 --&gt; AFTER1\n\n    style DROP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style RENAME fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style TYPE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style BEFORE1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <ul> <li> <p>Dropping a column</p> </li> <li> <p>Renaming a column</p> </li> <li> <p>Changing data type (could cause data loss)</p> </li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (destructive - drops total_amount)\nSELECT order_id FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing destructive change example]</p>"},{"location":"guides/incremental_by_time/#additive-changes","title":"Additive Changes","text":"<p>Changes that add new data without removing existing:</p> <pre><code>flowchart LR\n    subgraph \"Additive Changes\"\n        ADD[\"Adding Column&lt;br/&gt;customer_name added\"]\n        TYPE2[\"Compatible Type Change&lt;br/&gt;INT to STRING&lt;br/&gt;no data loss\"]\n    end\n\n    subgraph \"Example\"\n        BEFORE2[\"Before:&lt;br/&gt;order_id, total_amount\"]\n        AFTER2[\"After:&lt;br/&gt;order_id, total_amount,&lt;br/&gt;customer_name\"]\n    end\n\n    ADD --&gt; BEFORE2\n    BEFORE2 --&gt; AFTER2\n\n    style ADD fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style TYPE2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style BEFORE2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style AFTER2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <ul> <li> <p>Adding a new column</p> </li> <li> <p>Changing data type (compatible, e.g., INT \u2192 STRING)</p> </li> </ul> <p>Example: </p><pre><code>-- Before\nSELECT order_id, total_amount FROM orders\n\n-- After (additive - adds customer_name)\nSELECT order_id, total_amount, customer_name FROM orders\n</code></pre><p></p> <p>[Screenshot: Visual showing additive change example]</p>"},{"location":"guides/incremental_by_time/#controlling-schema-change-behavior","title":"Controlling Schema Change Behavior","text":"<p>You can control how Vulcan handles schema changes:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change error,  -- Block destructive changes\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>Options:</p> <ul> <li> <p><code>error</code> - Stop and raise an error (default for destructive)</p> </li> <li> <p><code>warn</code> - Log a warning but continue</p> </li> <li> <p><code>allow</code> - Silently proceed (default for additive)</p> </li> <li> <p><code>ignore</code> - Skip the check entirely (dangerous!)</p> </li> </ul> <p>[Screenshot: Code showing schema change configuration options]</p>"},{"location":"guides/incremental_by_time/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/incremental_by_time/#strict-schema-control","title":"Strict Schema Control","text":"<p>Prevent any schema changes:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change error,  -- Block destructive\n    on_additive_change error       -- Block even new columns\n  )\n);\n</code></pre> <p>[Screenshot: Strict schema control example]</p>"},{"location":"guides/incremental_by_time/#development-model","title":"Development Model","text":"<p>Allow all changes for rapid iteration:</p> <pre><code>MODEL (\n  name sales.dev_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n        on_destructive_change allow,  -- Allow dropping columns\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Development model example]</p>"},{"location":"guides/incremental_by_time/#production-safety","title":"Production Safety","text":"<p>Allow safe changes, warn about risky ones:</p> <pre><code>MODEL (\n  name sales.production_model,\n    kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n        forward_only true,\n    on_destructive_change warn,   -- Warn but allow\n    on_additive_change allow      -- Allow new columns\n  )\n);\n</code></pre> <p>[Screenshot: Production safety example]</p>"},{"location":"guides/incremental_by_time/#important-notes","title":"Important Notes","text":""},{"location":"guides/incremental_by_time/#time-column-must-be-utc","title":"Time Column Must Be UTC","text":"<p>Always use UTC timezone for your <code>time_column</code>:</p> <pre><code>-- Good: UTC timezone\ntime_column order_date_utc\n\n-- Bad: Local timezone\ntime_column order_date_local\n</code></pre> <p>Why? Ensures correct interval calculations and proper interaction with Vulcan's scheduler. If you use local timezones, you'll run into issues with daylight saving time changes, timezone differences, and interval calculations. UTC is consistent everywhere.</p> <p>[Screenshot: Visual warning about UTC requirement]</p>"},{"location":"guides/incremental_by_time/#always-include-where-clause","title":"Always Include WHERE Clause","text":"<p>Your model SQL must include a WHERE clause with <code>@start_ds</code> and <code>@end_ds</code>:</p> <pre><code>-- Required - This tells Vulcan what time range to process\nWHERE order_date BETWEEN @start_ds AND @end_ds\n\n-- Missing WHERE clause - Don't do this\n\n-- WHERE clause is required\n</code></pre> <p>Without this WHERE clause, Vulcan won't know what time range to process, and your queries will be inefficient (or fail entirely). Always include it!</p> <p>[Screenshot: Code showing required WHERE clause]</p>"},{"location":"guides/incremental_by_time/#set-a-start-date","title":"Set a Start Date","text":"<p>Always specify when your data begins:</p> <pre><code>start '2025-01-01'  -- Start processing from this date\n</code></pre> <p>This tells Vulcan where to start backfilling. If your data goes back to 2020 but you only want to process from 2025, set the start date to 2025-01-01. Vulcan will backfill from this date when you first create the model.</p> <p>[Screenshot: Code showing start date configuration]</p>"},{"location":"guides/incremental_by_time/#choose-appropriate-batch_size","title":"Choose Appropriate batch_size","text":"<ul> <li> <p>Start with <code>batch_size 1</code> for small datasets - process one interval at a time</p> </li> <li> <p>Increase for larger datasets that might timeout - if processing all intervals at once times out, batch them</p> </li> <li> <p>Monitor performance to find the right balance. Too small and you have too many jobs. Too large and jobs timeout.</p> </li> </ul> <p>The default is to process all missing intervals in one job. If that doesn't work for you, use <code>batch_size</code> to break it up.</p> <p>[Screenshot: Visual guide for choosing batch_size]</p>"},{"location":"guides/incremental_by_time/#summary","title":"Summary","text":"<p>Incremental by time models:</p> <ul> <li> <p>Only process new or missing time intervals</p> </li> <li> <p>Faster and cheaper than full refreshes</p> </li> <li> <p>Use for time-based data (orders, events, transactions)</p> </li> <li> <p>Require a time column and WHERE clause</p> </li> <li> <p>Use cron schedules to control execution frequency</p> </li> </ul> <p>Key concepts:</p> <ul> <li> <p>Intervals: Time periods (days, weeks, hours) that Vulcan tracks</p> </li> <li> <p>Backfill: Processing historical intervals when first creating a model</p> </li> <li> <p>Cron: Schedule that determines how often a model runs</p> </li> <li> <p>Forward-only: Models that never rebuild historical data</p> </li> </ul>"},{"location":"guides/incremental_by_time/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Kinds for all model types</li> </ul>"},{"location":"guides/model_selection/","title":"Model Selection","text":""},{"location":"guides/model_selection/#model-selection","title":"Model Selection","text":"<p>This guide explains how to select specific models to include in a Vulcan plan using the Orders360 example project. Use this when you want to test or apply changes to a subset of your models without processing everything.</p> <p>In large projects, model selection saves time. Instead of waiting for all models to process, focus on what you're working on.</p> <p>Note: The selector syntax described below is also used for the Vulcan <code>plan</code> <code>--allow-destructive-model</code> and <code>--allow-additive-model</code> selectors.</p>"},{"location":"guides/model_selection/#background","title":"Background","text":"<p>A Vulcan plan automatically detects changes between your local project and the deployed environment. When applied, it backfills directly modified models and their downstream dependencies.</p> <p>In large projects, a single model change can impact many downstream models, making plans take a long time. Model selection lets you filter which changes to include, so you can test specific models without processing everything.</p> <p>Key Concept:</p> <ul> <li> <p>Directly Modified: Models you changed in your code - these are the ones you actually edited</p> </li> <li> <p>Indirectly Modified: Downstream models affected by your changes - these depend on what you changed, so they need to be reprocessed too</p> </li> </ul> <p>Understanding this distinction helps you understand what model selection is doing. You're filtering which directly modified models to include, and Vulcan automatically figures out the indirect ones.</p> <p>[Screenshot: Visual showing directly vs indirectly modified models]</p>"},{"location":"guides/model_selection/#understanding-model-dependencies","title":"Understanding Model Dependencies","text":"<p>Before we dive into selection, let's understand how models relate to each other in Orders360. This will help you understand why selecting one model might include others.</p> <pre><code>flowchart TD\n    subgraph \"Orders360 Model DAG\"\n        RAW_CUSTOMERS[raw.raw_customers&lt;br/&gt;Seed Model]\n        RAW_ORDERS[raw.raw_orders&lt;br/&gt;Seed Model]\n        RAW_PRODUCTS[raw.raw_products&lt;br/&gt;Seed Model]\n\n        DAILY_SALES[sales.daily_sales&lt;br/&gt;Daily Aggregation]\n        WEEKLY_SALES[sales.weekly_sales&lt;br/&gt;Weekly Aggregation]\n    end\n\n    RAW_CUSTOMERS --&gt; DAILY_SALES\n    RAW_ORDERS --&gt; DAILY_SALES\n    RAW_PRODUCTS --&gt; DAILY_SALES\n\n    DAILY_SALES --&gt; WEEKLY_SALES\n\n    style RAW_CUSTOMERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_ORDERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style RAW_PRODUCTS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY_SALES fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style WEEKLY_SALES fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Dependency Flow:</p> <ul> <li> <p><code>raw.raw_orders</code> \u2192 <code>sales.daily_sales</code> \u2192 <code>sales.weekly_sales</code></p> </li> <li> <p>Changing <code>raw.raw_orders</code> affects <code>daily_sales</code> (indirectly modified) - because daily_sales reads from raw_orders</p> </li> <li> <p>Changing <code>daily_sales</code> affects <code>weekly_sales</code> (indirectly modified) - because weekly_sales reads from daily_sales</p> </li> </ul> <p>This is important! When you select a model, Vulcan automatically includes its downstream dependencies. You can't process <code>weekly_sales</code> without processing <code>daily_sales</code> first, because <code>weekly_sales</code> depends on it.</p> <p>[Screenshot: Orders360 project structure showing model files]</p>"},{"location":"guides/model_selection/#syntax","title":"Syntax","text":"<p>Model selections use the <code>--select-model</code> argument in <code>vulcan plan</code>. You can select models in several ways, by name, pattern, tags, git changes, and more. Let's explore all the options!</p>"},{"location":"guides/model_selection/#basic-selection","title":"Basic Selection","text":"<p>Select a single model by name:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>Select multiple models:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\" --select-model \"raw.raw_orders\"\n</code></pre> <p>[Screenshot: Plan output showing multiple models selected]</p>"},{"location":"guides/model_selection/#wildcard-selection","title":"Wildcard Selection","text":"<p>Use <code>*</code> to match multiple models:</p> <pre><code># Select all models starting with \"raw.\"\nvulcan plan dev --select-model \"raw.*\"\n\n# Select all models ending with \"_sales\"\nvulcan plan dev --select-model \"sales.*_sales\"\n\n# Select all models containing \"daily\"\nvulcan plan dev --select-model \"*daily*\"\n</code></pre> <p>Examples:</p> <ul> <li> <p><code>\"raw.*\"</code> matches <code>raw.raw_customers</code>, <code>raw.raw_orders</code>, <code>raw.raw_products</code> - all models in the raw schema</p> </li> <li> <p><code>\"sales.*_sales\"</code> matches <code>sales.daily_sales</code>, <code>sales.weekly_sales</code> - all models ending with _sales in the sales schema</p> </li> <li> <p><code>\"*.daily_sales\"</code> matches <code>sales.daily_sales</code> - matches daily_sales in any schema</p> </li> </ul> <p>Wildcards let you select a group of related models without listing them all individually.</p> <p>[Screenshot: Plan output showing wildcard selection results]</p>"},{"location":"guides/model_selection/#tag-selection","title":"Tag Selection","text":"<p>Select models by tags using <code>tag:tag_name</code>:</p> <pre><code># Select all models with \"seed\" tag\nvulcan plan dev --select-model \"tag:seed\"\n\n# Select all models with tags starting with \"reporting\"\nvulcan plan dev --select-model \"tag:reporting*\"\n</code></pre> <p>Example: If <code>raw.raw_orders</code> and <code>raw.raw_customers</code> have the <code>seed</code> tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n# Selects: raw.raw_orders, raw.raw_customers\n</code></pre> <p>[Screenshot: Plan output showing tag-based selection]</p>"},{"location":"guides/model_selection/#upstreamdownstream-selection","title":"Upstream/Downstream Selection","text":"<p>Use <code>+</code> to include upstream or downstream models:</p> <ul> <li> <p><code>+model_name</code> = Include upstream models (dependencies)</p> </li> <li> <p><code>model_name+</code> = Include downstream models (dependents)</p> </li> </ul> <pre><code>flowchart LR\n    subgraph \"Model Dependencies\"\n        RAW[raw.raw_orders]\n        DAILY[sales.daily_sales]\n        WEEKLY[sales.weekly_sales]\n    end\n\n    RAW --&gt;|upstream| DAILY\n    DAILY --&gt;|downstream| WEEKLY\n\n    style RAW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style DAILY fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000\n    style WEEKLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Examples:</p> <pre><code># Select daily_sales only\nvulcan plan dev --select-model \"sales.daily_sales\"\n# Result: daily_sales (directly modified)\n\n# Select daily_sales + upstream (raw.raw_orders)\nvulcan plan dev --select-model \"+sales.daily_sales\"\n# Result: raw.raw_orders, daily_sales\n\n# Select daily_sales + downstream (weekly_sales)\nvulcan plan dev --select-model \"sales.daily_sales+\"\n# Result: daily_sales, weekly_sales\n\n# Select daily_sales + both upstream and downstream\nvulcan plan dev --select-model \"+sales.daily_sales+\"\n# Result: raw.raw_orders, daily_sales, weekly_sales\n</code></pre> <p>[Screenshot: Plan outputs showing different selection results]</p>"},{"location":"guides/model_selection/#git-based-selection","title":"Git-Based Selection","text":"<p>Select models changed in a git branch:</p> <pre><code># Select models changed in feature branch\nvulcan plan dev --select-model \"git:feature\"\n\n# Select changed models + downstream\nvulcan plan dev --select-model \"git:feature+\"\n\n# Select changed models + upstream\nvulcan plan dev --select-model \"+git:feature\"\n</code></pre> <p>What it includes:</p> <ul> <li> <p>Untracked files (new models) - models you've created but haven't committed yet</p> </li> <li> <p>Uncommitted changes - models you've modified but haven't committed</p> </li> <li> <p>Committed changes different from target branch - models that differ between your branch and the target (like <code>main</code>)</p> </li> </ul> <p>Use this for feature branches. Select all models you've changed in your feature branch without listing them manually.</p> <p>[Screenshot: Plan output showing git-based selection]</p>"},{"location":"guides/model_selection/#complex-selections","title":"Complex Selections","text":"<p>Combine conditions with logical operators:</p> <ul> <li> <p><code>&amp;</code> (AND): Both conditions must be true</p> </li> <li> <p><code>|</code> (OR): Either condition must be true</p> </li> <li> <p><code>^</code> (NOT): Negates a condition</p> </li> </ul> <pre><code># Models with finance tag that don't have deprecated tag\nvulcan plan dev --select-model \"(tag:finance &amp; ^tag:deprecated)\"\n\n# daily_sales + upstream OR weekly_sales + downstream\nvulcan plan dev --select-model \"(+sales.daily_sales | sales.weekly_sales+)\"\n\n# Changed models that also have finance tag\nvulcan plan dev --select-model \"(tag:finance &amp; git:main)\"\n\n# Models in sales schema without test tag\nvulcan plan dev --select-model \"^(tag:test) &amp; sales.*\"\n</code></pre> <p>[Screenshot: Plan output showing complex selection results]</p>"},{"location":"guides/model_selection/#examples-with-orders360","title":"Examples with Orders360","text":"<p>Let's see how model selection works with the Orders360 project. We'll modify <code>raw.raw_orders</code> and <code>sales.daily_sales</code> to demonstrate different selection scenarios.</p>"},{"location":"guides/model_selection/#example-setup","title":"Example Setup","text":"<p>We've modified two models:</p> <ul> <li> <p><code>raw.raw_orders</code> (directly modified)</p> </li> <li> <p><code>sales.daily_sales</code> (directly modified)</p> </li> </ul> <p>The dependency chain: </p><pre><code>raw.raw_orders \u2192 sales.daily_sales \u2192 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Orders360 project showing modified files]</p>"},{"location":"guides/model_selection/#no-selection-default","title":"No Selection (Default)","text":"<p>Without selection, Vulcan includes all directly modified models and their downstream dependencies:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing all modified models]</p> <p>What Happened:</p> <ul> <li> <p>Both directly modified models are included - you changed both, so both are in the plan</p> </li> <li> <p><code>weekly_sales</code> is indirectly modified (depends on <code>daily_sales</code>) - even though you didn't change weekly_sales, it depends on daily_sales, so it needs to be reprocessed</p> </li> </ul> <p>This is the default behavior, Vulcan includes everything that's affected. Model selection lets you narrow this down.</p>"},{"location":"guides/model_selection/#select-single-model","title":"Select Single Model","text":"<p>Select only <code>sales.daily_sales</code>:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales selected]</p> <p>What Happened:</p> <ul> <li> <p><code>raw.raw_orders</code> is excluded (not selected) - you changed it, but you didn't select it, so it's not in the plan</p> </li> <li> <p><code>daily_sales</code> is included (directly modified) - you selected it, so it's in the plan</p> </li> <li> <p><code>weekly_sales</code> is included (indirectly modified, downstream of <code>daily_sales</code>) - it depends on daily_sales, so Vulcan automatically includes it</p> </li> </ul> <p>Notice how Vulcan automatically includes downstream models. You can't process daily_sales without processing weekly_sales, because weekly_sales depends on it.</p>"},{"location":"guides/model_selection/#select-with-upstream-indicator","title":"Select with Upstream Indicator","text":"<p>Select <code>daily_sales</code> and include its upstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"+sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 raw.raw_orders\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream selection]</p> <p>What Happened:</p> <ul> <li> <p><code>raw.raw_orders</code> is included (upstream of <code>daily_sales</code>)</p> </li> <li> <p><code>daily_sales</code> is included (selected)</p> </li> <li> <p><code>weekly_sales</code> is included (downstream of <code>daily_sales</code>)</p> </li> </ul>"},{"location":"guides/model_selection/#select-with-downstream-indicator","title":"Select with Downstream Indicator","text":"<p>Select <code>daily_sales</code> and include its downstream dependencies:</p> <pre><code>vulcan plan dev --select-model \"sales.daily_sales+\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u251c\u2500\u2500 sales.daily_sales\n\u2502   \u2514\u2500\u2500 sales.weekly_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    (none)\n</code></pre><p></p> <p>[Screenshot: Plan output showing downstream selection]</p> <p>What Happened:</p> <ul> <li> <p><code>daily_sales</code> is included (selected)</p> </li> <li> <p><code>weekly_sales</code> is included (downstream, now directly modified)</p> </li> <li> <p><code>raw.raw_orders</code> is excluded (not selected)</p> </li> </ul>"},{"location":"guides/model_selection/#select-with-wildcard","title":"Select with Wildcard","text":"<p>Select all models matching a pattern:</p> <pre><code>vulcan plan dev --select-model \"sales.*_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing wildcard selection]</p> <p>What Happened:</p> <ul> <li> <p><code>sales.daily_sales</code> matches the pattern (selected)</p> </li> <li> <p><code>sales.weekly_sales</code> matches the pattern but is indirectly modified</p> </li> <li> <p><code>raw.raw_orders</code> doesn't match (excluded)</p> </li> </ul>"},{"location":"guides/model_selection/#select-with-tags","title":"Select with Tags","text":"<p>If models have tags, select by tag:</p> <pre><code>vulcan plan dev --select-model \"tag:seed\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 raw.raw_orders\n\u2514\u2500\u2500 Indirectly Modified:\n    \u251c\u2500\u2500 sales.daily_sales\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing tag-based selection]</p> <p>What Happened:</p> <ul> <li> <p><code>raw.raw_orders</code> has <code>seed</code> tag (selected)</p> </li> <li> <p>Downstream models are indirectly modified</p> </li> </ul>"},{"location":"guides/model_selection/#select-with-git-changes","title":"Select with Git Changes","text":"<p>Select models changed in a git branch:</p> <pre><code>vulcan plan dev --select-model \"git:feature\"\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales  # Changed in feature branch\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n</code></pre><p></p> <p>[Screenshot: Plan output showing git-based selection]</p> <p>What Happened:</p> <ul> <li> <p>Only models changed in <code>feature</code> branch are selected</p> </li> <li> <p>Downstream models are included automatically</p> </li> </ul>"},{"location":"guides/model_selection/#backfill-selection","title":"Backfill Selection","text":"<p>By default, Vulcan backfills all models in a plan. You can limit which models are backfilled using <code>--backfill-model</code>.</p> <p>Important: <code>--backfill-model</code> only works in development environments (not <code>prod</code>).</p>"},{"location":"guides/model_selection/#how-backfill-selection-works","title":"How Backfill Selection Works","text":"<pre><code>flowchart TB\n    subgraph \"Backfill Selection Flow\"\n        PLAN[vulcan plan dev]\n        SELECT[--select-model&lt;br/&gt;Which models in plan?]\n        BACKFILL[--backfill-model&lt;br/&gt;Which models to backfill?]\n\n        subgraph \"Plan Includes\"\n            IN_PLAN[Models in Plan&lt;br/&gt;daily_sales, weekly_sales]\n        end\n\n        subgraph \"Backfill Includes\"\n            BACKFILL_LIST[Models to Backfill&lt;br/&gt;Only daily_sales]\n        end\n\n        RESULT[Result:&lt;br/&gt;Plan shows all models&lt;br/&gt;Only selected models backfilled]\n    end\n\n    PLAN --&gt; SELECT\n    PLAN --&gt; BACKFILL\n    SELECT --&gt; IN_PLAN\n    BACKFILL --&gt; BACKFILL_LIST\n    IN_PLAN --&gt; RESULT\n    BACKFILL_LIST --&gt; RESULT\n\n    style PLAN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style SELECT fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style BACKFILL fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000</code></pre> <p>Key Points:</p> <ul> <li> <p><code>--select-model</code> determines which models appear in the plan - this is about what's in the plan</p> </li> <li> <p><code>--backfill-model</code> determines which models are actually backfilled - this is about what gets processed</p> </li> <li> <p>Upstream models are always backfilled (required for downstream models) - if you backfill weekly_sales, you need daily_sales first</p> </li> </ul> <p>This separation lets you see what would be affected (select-model) but only process what you need (backfill-model). Useful for testing.</p> <p>[Screenshot: Visual diagram explaining backfill selection]</p>"},{"location":"guides/model_selection/#backfill-examples","title":"Backfill Examples","text":""},{"location":"guides/model_selection/#no-backfill-selection-default","title":"No Backfill Selection (Default)","text":"<p>All models in the plan are backfilled:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing all models needing backfill]</p>"},{"location":"guides/model_selection/#backfill-specific-model","title":"Backfill Specific Model","text":"<p>Only backfill <code>daily_sales</code>:</p> <pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u2514\u2500\u2500 sales__dev.daily_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing only daily_sales needs backfill]</p> <p>What Happened:</p> <ul> <li> <p><code>weekly_sales</code> is excluded from backfill - it's in the plan, but it won't be processed</p> </li> <li> <p>Only <code>daily_sales</code> will be processed - just what you selected</p> </li> </ul> <p>Use this in development. See what would be affected, but only process what you're actually testing. Saves time and compute costs.</p>"},{"location":"guides/model_selection/#backfill-with-upstream","title":"Backfill with Upstream","text":"<p>When you backfill a model, its upstream dependencies are automatically included:</p> <pre><code>vulcan plan dev --backfill-model \"sales.weekly_sales\"\n</code></pre> <p>Expected Output: </p><pre><code>Models needing backfill (missing dates):\n\u251c\u2500\u2500 raw__dev.raw_orders: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales__dev.weekly_sales: 2025-01-01 - 2025-01-15\n</code></pre><p></p> <p>[Screenshot: Plan output showing upstream models included in backfill]</p> <p>What Happened:</p> <ul> <li> <p><code>weekly_sales</code> is selected for backfill - you want to process this one</p> </li> <li> <p><code>raw.raw_orders</code> is automatically included (upstream dependency) - weekly_sales depends on daily_sales, which depends on raw_orders, so Vulcan includes it</p> </li> <li> <p><code>daily_sales</code> is excluded (not upstream of <code>weekly_sales</code>) - wait, that doesn't seem right...</p> </li> </ul> <p>Actually, this example might be incorrect. If weekly_sales depends on daily_sales, then daily_sales should be included as an upstream dependency. The key point is: Vulcan automatically includes upstream dependencies when you backfill a model.</p>"},{"location":"guides/model_selection/#visual-selection-guide","title":"Visual Selection Guide","text":"<p>Here's a quick reference for common selection patterns:</p> <pre><code>flowchart LR\n    subgraph \"Selection Patterns\"\n        PAT1[\"sales.daily_sales&lt;br/&gt;Select only this model\"]\n        PAT2[\"+sales.daily_sales&lt;br/&gt;Select + upstream\"]\n        PAT3[\"sales.daily_sales+&lt;br/&gt;Select + downstream\"]\n        PAT4[\"+sales.daily_sales+&lt;br/&gt;Select + both\"]\n        PAT5[\"sales.*_sales&lt;br/&gt;Wildcard match\"]\n        PAT6[\"tag:seed&lt;br/&gt;Tag selection\"]\n    end\n\n    subgraph \"Results\"\n        RES1[daily_sales only]\n        RES2[raw_orders + daily_sales]\n        RES3[daily_sales + weekly_sales]\n        RES4[All connected]\n        RES5[All matching]\n        RES6[All tagged]\n    end\n\n    PAT1 --&gt; RES1\n    PAT2 --&gt; RES2\n    PAT3 --&gt; RES3\n    PAT4 --&gt; RES4\n    PAT5 --&gt; RES5\n    PAT6 --&gt; RES6\n\n    style PAT1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT4 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT5 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style PAT6 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000</code></pre> <p>[Screenshot: Visual cheat sheet for selection patterns]</p>"},{"location":"guides/model_selection/#best-practices","title":"Best Practices","text":"<p>Here are some tips to help you use model selection effectively:</p> <ol> <li> <p>Start Small: Select only the models you're testing    </p><pre><code>vulcan plan dev --select-model \"sales.daily_sales\"\n</code></pre>    Don't process everything if you're only testing one model. Start small, then expand if needed.<p></p> </li> <li> <p>Use Wildcards: When selecting multiple related models    </p><pre><code>vulcan plan dev --select-model \"sales.*\"\n</code></pre>    Wildcards are your friend! They let you select groups of models without listing them all.<p></p> </li> <li> <p>Include Dependencies: Use <code>+</code> when you need upstream/downstream models    </p><pre><code>vulcan plan dev --select-model \"+sales.daily_sales+\"\n</code></pre>    The <code>+</code> syntax includes the full dependency chain. Use it when you need all dependencies.<p></p> </li> <li> <p>Limit Backfill: Use <code>--backfill-model</code> to save time in development    </p><pre><code>vulcan plan dev --backfill-model \"sales.daily_sales\"\n</code></pre>    In dev environments, you often don't need to backfill everything. Use this to save time and money.<p></p> </li> <li> <p>Use Tags: Organize models with tags for easier selection    </p><pre><code>vulcan plan dev --select-model \"tag:reporting\"\n</code></pre>    Tags organize models. If you tag related models, you can select them all at once.<p></p> </li> </ol>"},{"location":"guides/model_selection/#summary","title":"Summary","text":"<p>Model Selection:</p> <ul> <li> <p>Filter which models appear in a plan</p> </li> <li> <p>Use wildcards, tags, and git changes</p> </li> <li> <p>Include upstream/downstream with <code>+</code></p> </li> <li> <p>Combine with logical operators</p> </li> </ul> <p>Backfill Selection:</p> <ul> <li> <p>Limit which models are actually backfilled</p> </li> <li> <p>Upstream models are always included</p> </li> <li> <p>Only works in development environments</p> </li> <li> <p>Saves time when testing specific models</p> </li> </ul>"},{"location":"guides/model_selection/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plans for understanding plan behavior</li> </ul>"},{"location":"guides/models/","title":"Models","text":""},{"location":"guides/models/#models","title":"Models","text":"<p>This guide shows you how to work with models in Vulcan using the Orders360 example project. You'll learn how to add, edit, evaluate, and manage models with practical examples.</p> <p>Models define your data transformations. They contain the SQL or Python code that transforms your data.</p>"},{"location":"guides/models/#prerequisites","title":"Prerequisites","text":"<p>Before adding a model, ensure that you have:</p> <ul> <li> <p>Created your project </p> </li> <li> <p>Applied your first plan</p> </li> <li> <p>Working in a dev environment for testing changes</p> </li> </ul>"},{"location":"guides/models/#understanding-models","title":"Understanding Models","text":"<p>Models in Vulcan consist of two core components:</p> <ol> <li>DDL (Data Definition Language): The <code>MODEL</code> block that defines structure, metadata, and behavior - this is where you configure how the model works</li> <li>DML (Data Manipulation Language): The <code>SELECT</code> query that contains transformation logic - this is where you write your SQL</li> </ol> <p>Think of the MODEL block as the configuration and the SELECT as the actual work. Together, they define what your model does and how it does it.</p>"},{"location":"guides/models/#example-daily-sales-model","title":"Example: Daily Sales Model","text":"<p>Here's a real example from Orders360:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags (\n    'silver',\n    'sales',\n    'aggregation'\n  ),\n  terms (\n    'sales.daily_metrics',\n    'analytics.sales_summary'\n  ),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file in editor showing the complete model definition]</p>"},{"location":"guides/models/#adding-a-model","title":"Adding a Model","text":"<p>To add a new model to your Orders360 project:</p>"},{"location":"guides/models/#step-1-create-model-file","title":"Step 1: Create Model File","text":"<p>Create a new file in your <code>models</code> directory. For example, let's add a weekly sales aggregation:</p> <pre><code>touch models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing models/sales directory structure]</p>"},{"location":"guides/models/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Edit the file and add your model definition:</p> <pre><code>MODEL (\n  name sales.weekly_sales,\n  kind INCREMENTAL_BY_TIME_RANGE (\n    time_column order_date,\n    batch_size 1\n  ),\n  start '2025-01-01',\n  cron '@weekly',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  description 'Weekly aggregated sales metrics'\n);\n\nSELECT\n  DATE_TRUNC('week', order_date) AS order_date,\n  COUNT(DISTINCT order_id) AS total_orders,\n  SUM(total_amount) AS total_revenue,\n  AVG(total_amount) AS avg_order_value\nFROM sales.daily_sales\nWHERE order_date BETWEEN @start_ds AND @end_ds\nGROUP BY DATE_TRUNC('week', order_date)\n</code></pre> <p>[Screenshot: weekly_sales.sql file in editor with model definition]</p>"},{"location":"guides/models/#step-3-check-model-status","title":"Step 3: Check Model Status","text":"<p>Verify your model is detected:</p> <pre><code>vulcan info\n</code></pre> <p>Expected Output: </p><pre><code>Connection: Connected\nModels: 5\n  - raw.raw_customers\n\n  - raw.raw_orders\n\n  - raw.raw_products\n\n  - sales.daily_sales\n\n  - sales.weekly_sales  \u2190 NEW MODEL\n...\n</code></pre><p></p> <p>[Screenshot: <code>vulcan info</code> output showing the new weekly_sales model]</p>"},{"location":"guides/models/#step-4-apply-the-model","title":"Step 4: Apply the Model","text":"<p>Use <code>vulcan plan</code> to apply your new model:</p> <pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Added:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing new weekly_sales model to be added]</p> <p>Type <code>y</code> to apply and backfill the model.</p>"},{"location":"guides/models/#editing-an-existing-model","title":"Editing an Existing Model","text":"<p>To edit an existing model, modify the model file and use Vulcan's tools to preview and apply changes.</p>"},{"location":"guides/models/#step-1-edit-the-model-file","title":"Step 1: Edit the Model File","text":"<p>Let's modify <code>sales.daily_sales</code> to add a new column. Open <code>models/sales/daily_sales.sql</code>:</p> <pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics', 'analytics.sales_summary'),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day',\n    avg_order_value = 'Average order value for the day'  -- NEW COLUMN DESCRIPTION\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    avg_order_value = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,  -- NEW COLUMN\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql file showing the added avg_order_value column]</p>"},{"location":"guides/models/#step-2-evaluate-the-model-optional","title":"Step 2: Evaluate the Model (Optional)","text":"<p>Preview the model output without materializing it:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate command output showing the new avg_order_value column]</p> <p>What Happened?</p> <ul> <li> <p>The <code>evaluate</code> command runs the model query without creating tables. It's a dry run.</p> </li> <li> <p>Shows you the output with the new column. You can see what the data will look like.</p> </li> <li> <p>Useful for testing changes before applying them. Catch issues before they hit production.</p> </li> </ul> <p>Use <code>evaluate</code> to test changes quickly without waiting for full materialization.</p>"},{"location":"guides/models/#step-3-preview-changes-with-plan","title":"Step 3: Preview Changes with Plan","text":"<p>See what will change and how it affects downstream models:</p> <pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Directly Modified:\n    \u2514\u2500\u2500 sales.daily_sales\n\nDirectly Modified: sales.daily_sales (Non-breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -22,6 +22,7 @@\n      SELECT\n        CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n        COUNT(order_id)::INTEGER AS total_orders,\n        SUM(total_amount)::FLOAT AS total_revenue,\n    +   AVG(total_amount)::FLOAT AS avg_order_value,\n        MAX(order_id)::VARCHAR AS last_order_id\n      FROM raw.raw_orders\n\nModels needing backfill (missing dates):\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing non-breaking change with diff highlighting the new column]</p> <p>Understanding the Output:</p> <ul> <li> <p>Non-breaking: Vulcan detected this as non-breaking (adding a column). Adding columns is safe. Existing queries still work.</p> </li> <li> <p>Diff: Shows exactly what changed. The green <code>+</code> indicates an added line. You can see exactly what you modified.</p> </li> <li> <p>No downstream impact: <code>sales.weekly_sales</code> isn't listed because it doesn't use this column yet. Downstream models don't need to be reprocessed.</p> </li> </ul> <p>Non-breaking changes don't cascade. You can add columns without forcing downstream models to reprocess.</p>"},{"location":"guides/models/#step-4-apply-the-changes","title":"Step 4: Apply the Changes","text":"<p>Type <code>y</code> to apply the plan:</p> <pre><code>Apply - Backfill Tables [y/n]: y\n</code></pre> <p>Expected Output: </p><pre><code>[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:05\n\n\u2714 Model batches executed\n\u2714 Plan applied successfully\n</code></pre><p></p> <p>[Screenshot: Plan application showing daily_sales being backfilled]</p>"},{"location":"guides/models/#making-a-breaking-change","title":"Making a Breaking Change","text":"<p>Breaking changes affect downstream models. Let's see how Vulcan handles this.</p>"},{"location":"guides/models/#step-1-add-a-filter-to-daily-sales","title":"Step 1: Add a Filter to Daily Sales","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to add a WHERE clause:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nWHERE total_amount &gt; 10  -- NEW FILTER: Only orders &gt; $10\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql showing the WHERE clause filter]</p>"},{"location":"guides/models/#step-2-create-plan","title":"Step 2: Create Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `prod` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,6 +26,7 @@\n      FROM raw.raw_orders\n    + WHERE total_amount &gt; 10\n      GROUP BY order_date\n\n\u2514\u2500\u2500 Indirectly Modified Children:\n    \u2514\u2500\u2500 sales.weekly_sales (Indirect Breaking)\n\nModels needing backfill (missing dates):\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-01 - 2025-01-15\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-01 - 2025-01-15\n\nApply - Backfill Tables [y/n]:\n</code></pre><p></p> <p>[Screenshot: Plan output showing breaking change with downstream impact on weekly_sales]</p> <p>Understanding Breaking Changes:</p> <ul> <li> <p>Breaking: Adding a WHERE clause filters data, making existing data invalid. Rows that should be filtered out might still be in the table.</p> </li> <li> <p>Indirectly Modified: <code>sales.weekly_sales</code> depends on <code>daily_sales</code>, so it's affected. It needs to be reprocessed with the new filtered data.</p> </li> <li> <p>Cascading backfill: Both models need to be reprocessed. Vulcan handles this automatically, processing upstream first.</p> </li> </ul> <p>Breaking changes are more expensive because they cascade. Make sure you need to make a breaking change before you do it.</p>"},{"location":"guides/models/#evaluating-a-model","title":"Evaluating a Model","text":"<p>The <code>evaluate</code> command tests models without materializing data. Use it for iteration and debugging. It shows what your model will produce without creating tables.</p>"},{"location":"guides/models/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing single day results]</p>"},{"location":"guides/models/#evaluate-multiple-days","title":"Evaluate Multiple Days","text":"<pre><code>vulcan evaluate sales.daily_sales --start=2025-01-10 --end=2025-01-15\n</code></pre> <p>Expected Output: </p><pre><code>order_date          total_orders  total_revenue  avg_order_value  last_order_id\n2025-01-10 00:00:00           38         1120.25           29.48        ORD-00110\n2025-01-11 00:00:00           45         1350.75           30.02        ORD-00111\n2025-01-12 00:00:00           41         1225.50           29.89        ORD-00112\n2025-01-13 00:00:00           39         1180.00           30.26        ORD-00113\n2025-01-14 00:00:00           44         1320.50           30.01        ORD-00114\n2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142\n</code></pre><p></p> <p>[Screenshot: Evaluate output showing multiple days of data]</p>"},{"location":"guides/models/#evaluate-with-filters","title":"Evaluate with Filters","text":"<p>Test your model logic with different conditions:</p> <pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15 --where \"total_amount &gt; 50\"\n</code></pre> <p>[Screenshot: Evaluate command with WHERE clause filter]</p> <p>Use Cases for Evaluate:</p> <ul> <li> <p>Test model logic before applying changes. Make sure your SQL does what you think it does.</p> </li> <li> <p>Debug query issues. See what's actually happening with your data.</p> </li> <li> <p>Verify data transformations. Check that aggregations, joins, etc. are working correctly.</p> </li> <li> <p>Check data quality. Spot issues before they make it to production.</p> </li> <li> <p>Iterate quickly without materialization costs. Test changes fast without waiting for full backfills.</p> </li> </ul>"},{"location":"guides/models/#reverting-a-change","title":"Reverting a Change","text":"<p>Vulcan lets you revert model changes using Virtual Updates. You can revert quickly without reprocessing all your data.</p>"},{"location":"guides/models/#step-1-revert-the-change","title":"Step 1: Revert the Change","text":"<p>Edit <code>models/sales/daily_sales.sql</code> to remove the WHERE clause we added:</p> <pre><code>SELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  AVG(total_amount)::FLOAT AS avg_order_value,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\n-- WHERE total_amount &gt; 10  -- REMOVED FILTER\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql with WHERE clause removed/commented out]</p>"},{"location":"guides/models/#step-2-apply-reverted-plan","title":"Step 2: Apply Reverted Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u251c\u2500\u2500 Directly Modified:\n\u2502   \u2514\u2500\u2500 sales.daily_sales\n\u2514\u2500\u2500 Indirectly Modified:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nDirectly Modified: sales.daily_sales (Breaking)\n\u2514\u2500\u2500 Diff:\n    @@ -26,7 +26,6 @@\n      FROM raw.raw_orders\n    - WHERE total_amount &gt; 10\n      GROUP BY order_date\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan showing reverted change with diff]</p> <p>Virtual Update:</p> <ul> <li> <p>No backfill required. Just updates references. Vulcan changes which physical table the view points to.</p> </li> <li> <p>Fast operation. Completes in seconds. Faster than a full backfill.</p> </li> <li> <p>Previous data remains available. The old data is still there. You're just not using it anymore.</p> </li> </ul> <p>Virtual updates work well for reverting changes. They're fast and don't require reprocessing data.</p>"},{"location":"guides/models/#validating-models","title":"Validating Models","text":"<p>Vulcan provides multiple ways to validate your models. Vulcan checks them automatically.</p>"},{"location":"guides/models/#automatic-validation","title":"Automatic Validation","text":"<p>Vulcan automatically validates models when you run <code>plan</code>:</p> <ol> <li>Unit Tests: Run automatically to validate logic</li> <li>Audits: Execute when data is loaded to tables</li> <li>Assertions: Check data quality constraints</li> </ol> <p>Example Output: </p><pre><code>======================================================================\nSuccessfully Ran 2 tests against postgres\n----------------------------------------------------------------------\n</code></pre><p></p> <p>[Screenshot: Plan output showing tests passed]</p>"},{"location":"guides/models/#manual-validation-options","title":"Manual Validation Options","text":"<ol> <li> <p>Evaluate: Test model output without materialization    </p><pre><code>vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15\n</code></pre><p></p> </li> <li> <p>Unit Tests: Write tests in <code>tests/</code> directory    </p><pre><code>vulcan test\n</code></pre><p></p> </li> <li> <p>Plan Preview: See changes before applying    </p><pre><code>vulcan plan dev\n</code></pre><p></p> </li> </ol> <p>[Screenshot: Test execution showing all tests passing]</p>"},{"location":"guides/models/#deleting-a-model","title":"Deleting a Model","text":"<p>To remove a model from your project:</p>"},{"location":"guides/models/#step-1-delete-model-file","title":"Step 1: Delete Model File","text":"<pre><code>rm models/sales/weekly_sales.sql\n</code></pre> <p>[Screenshot: File explorer showing weekly_sales.sql deleted]</p>"},{"location":"guides/models/#step-2-delete-associated-tests-if-any","title":"Step 2: Delete Associated Tests (if any)","text":"<pre><code>rm tests/test_weekly_sales.yaml\n</code></pre>"},{"location":"guides/models/#step-3-apply-deletion-plan","title":"Step 3: Apply Deletion Plan","text":"<pre><code>vulcan plan dev\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nSuccessfully Ran 1 tests against postgres\n----------------------------------------------------------------------\n\nDifferences from the `dev` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Plan output showing weekly_sales as removed]</p> <p>Type <code>y</code> to apply the deletion.</p> <p>Expected Output: </p><pre><code>Virtually Updating 'dev' \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 0:00:00\n\nThe target environment has been updated successfully\nVirtual Update executed successfully\n</code></pre><p></p> <p>[Screenshot: Virtual update completing successfully]</p>"},{"location":"guides/models/#step-4-apply-to-production","title":"Step 4: Apply to Production","text":"<pre><code>vulcan plan\n</code></pre> <p>Expected Output: </p><pre><code>Differences from the `prod` environment:\n\nModels:\n\u2514\u2500\u2500 Removed Models:\n    \u2514\u2500\u2500 sales.weekly_sales\n\nApply - Virtual Update [y/n]: y\n</code></pre><p></p> <p>[Screenshot: Production plan showing model removal]</p>"},{"location":"guides/models/#model-examples-from-orders360","title":"Model Examples from Orders360","text":""},{"location":"guides/models/#seed-model-raw-orders","title":"Seed Model: Raw Orders","text":"<pre><code>MODEL (\n  name raw.raw_orders,\n  kind SEED (\n    path '../../seeds/raw_orders.csv'\n  ),\n  description 'Seed model loading raw order data from CSV file',\n  columns (\n    order_id VARCHAR,\n    order_date DATE,\n    customer_id VARCHAR,\n    product_id VARCHAR,\n    total_amount FLOAT\n  ),\n  column_descriptions (\n    order_id = 'Unique identifier for each order',\n    order_date = 'Date when the order was placed',\n    customer_id = 'Reference to customer who placed the order',\n    product_id = 'Reference to product that was ordered',\n    total_amount = 'Total order amount in dollars'\n  ),\n  assertions (\n    unique_values(columns := (order_id)),\n    not_null(columns := (order_id, order_date, customer_id, product_id)),\n    positive_values(column := total_amount)\n  ),\n  grain order_id\n);\n</code></pre> <p>[Screenshot: raw_orders.sql seed model file]</p>"},{"location":"guides/models/#transformation-model-daily-sales","title":"Transformation Model: Daily Sales","text":"<pre><code>MODEL (\n  name sales.daily_sales,\n  kind FULL,\n  cron '@daily',\n  grains (order_date),\n  tags ('silver', 'sales', 'aggregation'),\n  terms ('sales.daily_metrics', 'analytics.sales_summary'),\n  description 'Daily sales summary with order counts and revenue',\n  column_descriptions (\n    order_date = 'Date of the sales',\n    total_orders = 'Total number of orders for the day',\n    total_revenue = 'Total revenue for the day',\n    last_order_id = 'Last order ID processed for the day'\n  ),\n  column_tags (\n    order_date = ('dimension', 'grain', 'date'),\n    total_orders = ('measure', 'count'),\n    total_revenue = ('measure', 'financial'),\n    last_order_id = ('dimension', 'identifier')\n  ),\n  assertions (\n    unique_values(columns := (order_date)),\n    not_null(columns := (order_date, total_orders, total_revenue)),\n    positive_values(column := total_orders),\n    positive_values(column := total_revenue)\n  )\n);\n\nSELECT\n  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,\n  COUNT(order_id)::INTEGER AS total_orders,\n  SUM(total_amount)::FLOAT AS total_revenue,\n  MAX(order_id)::VARCHAR AS last_order_id\nFROM raw.raw_orders\nGROUP BY order_date\nORDER BY order_date\n</code></pre> <p>[Screenshot: daily_sales.sql transformation model file]</p>"},{"location":"guides/models/#best-practices","title":"Best Practices","text":"<p>Here are some tips to help you work effectively with models:</p> <ol> <li> <p>Use descriptive names: <code>sales.daily_sales</code> is clearer than <code>sales.ds</code>.</p> </li> <li> <p>Add column descriptions: Document what each column represents. Helps others understand the data.</p> </li> <li> <p>Use assertions: Validate data quality at the model level. Catch issues automatically.</p> </li> <li> <p>Test before applying: Use <code>evaluate</code> to preview changes. Catch bugs before they hit production.</p> </li> <li> <p>Review plans carefully: Check diffs and downstream impacts. Make sure you understand what will change.</p> </li> <li> <p>Use dev environments: Test changes before production. Don't test in prod.</p> </li> </ol>"},{"location":"guides/models/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn about Model Kinds for different model types</p> </li> <li> <p>Explore Model Properties for advanced configuration</p> </li> <li> <p>Read about Plans for applying model changes</p> </li> </ul>"},{"location":"guides/plan_guide/","title":"Plan","text":""},{"location":"guides/plan_guide/#plan","title":"Plan","text":"<p><code>vulcan plan</code> shows you what changed, what will be recomputed, and what will be promoted into an environment's virtual layer before changes take effect.</p> <p>Vulcan plan tracks changes to:</p> <ul> <li>Semantics and metrics (keeps business definitions consistent)</li> <li>Assertions, checks, and profiles (keeps data quality visible and predictable)</li> <li>Model SQL and Python (transformation logic)</li> </ul>"},{"location":"guides/plan_guide/#quick-start","title":"Quick start","text":"<p>Create a plan:</p> <pre><code>vulcan plan [environment name]\n</code></pre> <p>If you omit the environment name, Vulcan plans against <code>prod</code>.</p>"},{"location":"guides/plan_guide/#what-youll-see-in-a-plan","title":"What you'll see in a plan","text":"<p>A plan summary includes:</p> <ul> <li>What changed: added/removed/direct/indirect/metadata updates across models and related definitions</li> <li>Impact classification: breaking vs non-breaking (auto-identified)</li> <li>Backfill window: which models need intervals executed</li> <li>Apply stages: executing model batches, updating virtual layer, running non-blocking quality signals</li> </ul> <p>For a walkthrough with screenshots and CLI snippets, see Plan. For details on environments, snapshots, forward-only plans, and restatements, see Plans.</p>"},{"location":"guides/plan_guide/#how-plan-works","title":"How plan works","text":"<p>Vulcan follows this flow:</p> <ol> <li> <p>Diff scope: Compare local project state to the target environment (models, standalone assertions, semantics, metrics, checks, metadata).</p> </li> <li> <p>Plan type: Determine whether this is a backfill plan or a virtual update (and whether forward-only or restatement options apply).</p> </li> <li> <p>Categorize impact: Auto-identify breaking vs non-breaking and propagate impact to indirectly impacted objects.</p> </li> <li> <p>Compute intervals: Compute the time ranges that must be executed.</p> </li> <li> <p>Apply:    - Create model variants (snapshots)    - Execute model batches (backfill) if needed    - Update the virtual layer (promote)    - Run non-blocking quality signals (checks and profiles)</p> </li> </ol>"},{"location":"guides/plan_guide/#change-types","title":"Change types","text":"<p>Vulcan reports changes using consistent categories. You'll see these for models, and often for semantics, metrics, checks, and assertions.</p> <ul> <li>Added: exists locally, not in the environment (new object will be created/registered).</li> <li>Removed: exists in the environment, removed from local project (will be removed from the environment).</li> <li>Directly modified: you edited the object itself (model SQL/Python, semantics YAML, check YAML, Assertion SQL).</li> <li>Indirectly impacted: You didn't edit the object, but something it depends on changed. Common for downstream models, and for semantics, metrics, checks that reference a changed model or column.</li> <li>Metadata-only: description/tags/ownership/config changes that don\u2019t require historical recomputation.</li> </ul> <p>Example</p> <p>You rename a column in <code>sales.daily_sales</code>.</p> <ul> <li><code>sales.daily_sales</code> is directly modified</li> <li>Downstream models are indirectly impacted</li> <li>A metric or check referencing the renamed column is indirectly impacted. It may fail validation until you update it.</li> </ul>"},{"location":"guides/plan_guide/#change-categories-breaking-vs-non-breaking","title":"Change categories (breaking vs non-breaking)","text":"<p>Vulcan identifies breaking vs non-breaking automatically during plan creation (and may prompt only in ambiguous cases).</p>"},{"location":"guides/plan_guide/#breaking","title":"Breaking","text":"<p>Breaking means the change can alter results in a way that downstream models and consumers must reflect.</p> <ul> <li>For models: Backfill cascades downstream. More expensive, but safest.</li> <li>For semantics, metrics, checks: Definitions may become invalid or meaningfully change for consumers.</li> </ul> <p>Example</p> <p>Adding/modifying a filter changes the set of rows produced:</p> <pre><code>WHERE order_date BETWEEN @start_ds AND @end_ds\n  AND order_amount &gt; 10\n</code></pre>"},{"location":"guides/plan_guide/#non-breaking","title":"Non-breaking","text":"<p>Non-breaking means the change is additive or non-functional in a way that doesn't invalidate downstream dependencies.</p> <ul> <li>For models: Only the directly modified model needs backfill.</li> <li>For semantics, metrics, checks: Additive updates (new metric, new check, updated description) don't break existing consumers.</li> </ul> <p>Example</p> <p>Adding a new column is usually non-breaking:</p> <pre><code>SELECT\n  order_date,\n  SUM(order_amount) AS total_revenue,\n  AVG(order_amount) AS avg_order_value\nFROM raw.raw_orders\nGROUP BY order_date\n</code></pre>"},{"location":"guides/plan_guide/#snapshots-model-variants-and-fingerprinting","title":"Snapshots (model variants) and fingerprinting","text":"<p>Whenever a model definition changes, Vulcan creates a snapshot: a record of that model at a point in time. A snapshot contains everything needed to evaluate and render the model query, including macro definitions and global variables at the time the snapshot was created, and it tracks which time intervals have data.</p> <p>Snapshots have unique fingerprints derived from their models. Vulcan uses fingerprints to decide whether an existing physical table can be reused or whether the model must be backfilled. Vulcan understands SQL, so superficial edits like formatting won't generate a new fingerprint.</p>"},{"location":"guides/plan_guide/#physical-tables-virtual-layer-and-environments","title":"Physical tables, virtual layer, and environments","text":"<p>How these components work together:</p> <ul> <li>Physical layer: Versioned physical tables hold the data for a specific snapshot.</li> <li>Virtual layer: Environment views point to the correct physical tables.</li> <li>Environment: A set of references to model snapshots (and therefore to physical tables).</li> </ul> <p></p>"},{"location":"guides/plan_guide/#backfill-vs-virtual-update","title":"Backfill vs virtual update","text":"<ul> <li>Backfill: Vulcan executes intervals to populate the physical tables for new snapshots.</li> <li>Virtual update: Vulcan only swaps references in the virtual layer (no new execution needed).</li> </ul>"},{"location":"guides/plan_guide/#startend-dates-non-prod-and-limitations","title":"Start/end dates (non-prod) and limitations","text":"<p>In non-prod environments, you can control the backfill window:</p> <pre><code>vulcan plan dev --start \"2024-01-01\" --end \"2024-01-05\"\n</code></pre> <p>Some model kinds are inherently non-idempotent (for example <code>INCREMENTAL_BY_UNIQUE_KEY</code>, <code>INCREMENTAL_BY_PARTITION</code>, and SCD variants). In those cases, Vulcan may compute a preview for a limited range that can\u2019t be reused when deploying to production.</p> <p>If your specified window is smaller than a model\u2019s interval size, the model may be skipped; use <code>--min-intervals</code> to force at least N intervals.</p>"},{"location":"guides/plan_guide/#restatement-plans-restate-model","title":"Restatement plans (<code>--restate-model</code>)","text":"<p>Restatement is how you reprocess existing data for a time range even when the model definition hasn\u2019t changed (for example, upstream data was corrected or you want to re-run a subset of history).</p> <pre><code>vulcan plan --restate-model \"db.model_a\"\n</code></pre> <p>No local changes allowed</p> <p>Restatement plans ignore local file changes. They can only restate model versions already present in the target environment.</p> <p>How it works:</p> <ul> <li>Cascading backfill: selected models and downstream models are reprocessed.</li> <li>Selectors: Select by name, wildcard, or tag. See Model selection.</li> <li>External models: restating an external model triggers downstream backfills (the external model itself is metadata-only).</li> <li>Disable restatement: set <code>disable_restatement: true</code> to prevent restatement for a model.</li> </ul> <p>Prod vs dev behavior differs. Restating <code>prod</code> can clear affected intervals in other environments' state to prevent stale data being promoted later. For details, see Plans.</p>"},{"location":"guides/run_and_scheduling/","title":"Run and Scheduling","text":""},{"location":"guides/run_and_scheduling/#run-and-scheduling","title":"Run and Scheduling","text":"<p>This guide covers Vulcan's run functionality and scheduling strategies. You'll learn how <code>vulcan run</code> processes new data intervals and how to automate it for production.</p> <p>The <code>run</code> command is different from <code>plan</code>, it's for regular scheduled execution, not for applying changes. Once you understand the difference, you'll know when to use each one.</p>"},{"location":"guides/run_and_scheduling/#run-and-scheduler-architecture","title":"Run and Scheduler Architecture","text":"<p>The following diagram illustrates how Vulcan's run system works with cron-based scheduling:</p> <pre><code>graph TB\n    subgraph \"Scheduler Triggers\"\n        CRON[Cron Job / CI/CD&lt;br/&gt;Runs periodically]\n        MANUAL[Manual Execution&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"Run Process\"\n        START[vulcan run&lt;br/&gt;Command starts]\n        CHECK[Check for missing intervals&lt;br/&gt;Compare with state]\n        CRON_CHECK[Check cron schedules&lt;br/&gt;Which models are due?]\n        FILTER[Filter models&lt;br/&gt;Only process due intervals]\n    end\n\n    subgraph \"Model Execution\"\n        M1[sales.daily_sales&lt;br/&gt;cron: @daily&lt;br/&gt;Due: Yes]\n        M2[sales.weekly_sales&lt;br/&gt;cron: @weekly&lt;br/&gt;Due: No]\n        M3[sales.monthly_sales&lt;br/&gt;cron: @monthly&lt;br/&gt;Due: No]\n    end\n\n    subgraph \"State Management\"\n        STATE[State Database&lt;br/&gt;Tracks processed intervals]\n        UPDATE[Update State&lt;br/&gt;Mark intervals as processed]\n    end\n\n    subgraph \"Execution Flow\"\n        EXEC1[Execute daily_sales&lt;br/&gt;Process missing intervals]\n        EXEC2[Skip weekly_sales&lt;br/&gt;Not due yet]\n        EXEC3[Skip monthly_sales&lt;br/&gt;Not due yet]\n    end\n\n    subgraph \"Results\"\n        SUCCESS[Run Complete&lt;br/&gt;Intervals processed]\n        LOG[Log Results&lt;br/&gt;Execution summary]\n    end\n\n    CRON --&gt;|\"Scheduled\"| START\n    MANUAL --&gt;|\"Triggered\"| START\n    START --&gt;|\"to\"| CHECK\n    CHECK --&gt;|\"to\"| CRON_CHECK\n    CRON_CHECK --&gt;|\"to\"| FILTER\n    FILTER --&gt;|\"Due\"| M1\n    FILTER --&gt;|\"Not due\"| M2\n    FILTER --&gt;|\"Not due\"| M3\n\n    M1 --&gt;|\"to\"| EXEC1\n    M2 --&gt;|\"to\"| EXEC2\n    M3 --&gt;|\"to\"| EXEC3\n\n    EXEC1 --&gt;|\"to\"| STATE\n    EXEC2 -.-&gt;|\"Skip\"| STATE\n    EXEC3 -.-&gt;|\"Skip\"| STATE\n\n    STATE --&gt;|\"to\"| UPDATE\n    UPDATE --&gt;|\"to\"| SUCCESS\n    SUCCESS --&gt;|\"to\"| LOG\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style M1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style M2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style M3 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#key-concepts-illustrated","title":"Key Concepts Illustrated","text":"<ol> <li>Scheduler Triggers: Run can be triggered by cron jobs, CI/CD pipelines, or manually</li> <li>Interval Detection: Vulcan checks for missing intervals by comparing current state with model schedules</li> <li>Cron-Based Filtering: Only models whose cron schedules indicate they're due are executed</li> <li>State Tracking: Processed intervals are tracked in the state database</li> <li>Efficient Execution: Models not due are skipped, saving computational resources</li> </ol>"},{"location":"guides/run_and_scheduling/#cron-schedule-flow","title":"Cron Schedule Flow","text":"<p>The following diagram shows how different cron schedules determine model execution:</p> <pre><code>gantt\n    title Model Execution Timeline (Example: Hourly, Daily, Weekly)\n    dateFormat YYYY-MM-DD HH:mm\n    axisFormat %H:%M\n\n    section Hourly Model\n    Run every hour    :active, hourly1, 2025-01-20 00:00, 1h\n    Run every hour    :active, hourly2, 2025-01-20 01:00, 1h\n    Run every hour    :active, hourly3, 2025-01-20 02:00, 1h\n    Run every hour    :active, hourly4, 2025-01-20 03:00, 1h\n\n    section Daily Model\n    Run once daily    :active, daily1, 2025-01-20 00:00, 24h\n\n    section Weekly Model\n    Run once weekly   :active, weekly1, 2025-01-20 00:00, 168h</code></pre> <p>Visual Explanation:  - Hourly models run every hour when <code>vulcan run</code> executes</p> <ul> <li> <p>Daily models run once per day (at the scheduled time)</p> </li> <li> <p>Weekly models run once per week (at the scheduled time)</p> </li> </ul>"},{"location":"guides/run_and_scheduling/#understanding-run-vs-plan","title":"Understanding Run vs Plan","text":"Aspect <code>vulcan plan</code> <code>vulcan run</code> Purpose Apply model changes to environment Execute existing models on schedule When to Use When models are modified/added/removed When no changes, just process new data Change Detection Compares local files vs environment No file comparison needed Backfill Backfills based on changes Processes missing intervals only Cron Schedule Not used (processes all affected dates) Uses model's cron to determine what runs User Interaction Prompts for change categorization Runs automatically Output Shows diffs and change summary Shows execution progress <p>Key Insight: Use <code>plan</code> when you've changed code. Use <code>run</code> for regular scheduled execution. </p> <p>Think of it this way: <code>plan</code> is for deploying changes, <code>run</code> is for processing new data. They serve different purposes!</p>"},{"location":"guides/run_and_scheduling/#how-run-works","title":"How Run Works","text":"<p>The <code>vulcan run</code> command processes missing data intervals for models that haven't changed:</p> <pre><code>flowchart TD\n    START[vulcan run&lt;br/&gt;Command starts] --&gt; CHECK{Check model&lt;br/&gt;definitions}\n\n    CHECK --&gt;|\"Changed\"| ERROR[Error: Use 'vulcan plan'&lt;br/&gt;to apply changes first]\n    CHECK --&gt;|\"No changes\"| STATE[Query state database&lt;br/&gt;Get processed intervals]\n\n    STATE --&gt; CRON[Check cron schedules&lt;br/&gt;Which models are due?]\n\n    CRON --&gt; FILTER{Filter models&lt;br/&gt;by cron schedule}\n\n    FILTER --&gt;|\"Due\"| EXEC1[Execute Model 1&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"Due\"| EXEC2[Execute Model 2&lt;br/&gt;Process missing intervals]\n    FILTER --&gt;|\"Not due\"| SKIP1[Skip Model 3&lt;br/&gt;Not due yet]\n    FILTER --&gt;|\"Not due\"| SKIP2[Skip Model 4&lt;br/&gt;Not due yet]\n\n    EXEC1 --&gt; UPDATE[Update state database&lt;br/&gt;Mark intervals as processed]\n    EXEC2 --&gt; UPDATE\n    SKIP1 -.-&gt;|\"Skip\"| UPDATE\n    SKIP2 -.-&gt;|\"Skip\"| UPDATE\n\n    UPDATE --&gt; SUCCESS[Run complete&lt;br/&gt;Summary output]\n\n    ERROR --&gt; END[Exit]\n    SUCCESS --&gt; END\n\n    style START fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style ERROR fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style CRON fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style EXEC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style SKIP1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style SKIP2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000</code></pre> <p>Process Steps:</p> <ol> <li>No Model Changes: Assumes no model definitions have changed - if they have, you'll get an error telling you to use <code>plan</code> first</li> <li>Cron-Based Execution: Each model's <code>cron</code> parameter determines if it should run - daily models run daily, weekly models run weekly, etc.</li> <li>Missing Intervals: Only processes intervals that haven't been processed yet - efficient!</li> <li>Automatic: No prompts or user interaction required. Works well for automation.</li> </ol> <p>The <code>run</code> command works well for scheduled execution. It's fast, automatic, and only processes what's needed.</p> <p>Interactive Diagrams</p> <p>All diagrams in this guide are interactive! Double-click any diagram to zoom in and explore details. Use drag to pan, arrow keys to navigate, or the zoom controls.</p>"},{"location":"guides/run_and_scheduling/#scenario-1-first-run-processing-new-data","title":"Scenario 1: First Run - Processing New Data","text":"<p>After applying your first plan, use <code>run</code> to process new data as it arrives.</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nModels to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-16 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 1/1 \u2022 0:00:02\n\n[1/1] sales.daily_sales          [insert 2025-01-16 - 2025-01-16]   2.1s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: First run output showing new interval processing]</p> <p>What Happened? - <code>sales.daily_sales</code> has <code>cron: '@daily'</code>, so it runs daily - Vulcan checks if enough time has passed</p> <ul> <li> <p>Yesterday's plan processed up to 2025-01-15 - that's what's already done</p> </li> <li> <p>Today (2025-01-16) is a new interval that needs processing - this is what's missing</p> </li> <li> <p><code>run</code> automatically processes this missing interval - no prompts, just works</p> </li> </ul> <p>This is the beauty of <code>run</code>, it automatically figures out what needs processing and does it. Set it up once, and it keeps running!</p>"},{"location":"guides/run_and_scheduling/#scenario-2-cron-based-execution","title":"Scenario 2: Cron-Based Execution","text":"<p>Different models can have different <code>cron</code> schedules. <code>run</code> respects each model's schedule.</p>"},{"location":"guides/run_and_scheduling/#daily-model-execution","title":"Daily Model Execution","text":"<pre><code>vulcan run\n</code></pre> <p>Expected Output (Day 2): </p><pre><code>Models to execute:\n\u2514\u2500\u2500 sales.daily_sales: 2025-01-17 (1 interval)\n</code></pre><p></p> <p>[Screenshot: Daily run showing only daily model executed]</p>"},{"location":"guides/run_and_scheduling/#weekly-model-execution","title":"Weekly Model Execution","text":"<p>After 7 days, both daily and weekly models run:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>Models to execute:\n\u251c\u2500\u2500 sales.daily_sales: 2025-01-18 - 2025-01-24 (7 intervals)\n\u2514\u2500\u2500 sales.weekly_sales: 2025-01-20 - 2025-01-20 (1 interval)\n\nExecuting model batches \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2/2 \u2022 0:00:08\n\n[1/2] sales.daily_sales          [insert 2025-01-18 - 2025-01-24]   5.2s\n[2/2] sales.weekly_sales         [insert 2025-01-20 - 2025-01-20]   2.8s\n\n\u2714 All model batches executed successfully\n</code></pre><p></p> <p>[Screenshot: Weekly run showing both daily and weekly models]</p> <p>Understanding Cron Schedules:</p> <ul> <li> <p>Daily model (<code>@daily</code>): Processes missing daily intervals - runs every day when <code>run</code> executes</p> </li> <li> <p>Weekly model (<code>@weekly</code>): Only processes when 7 days have elapsed - skips if not enough time has passed</p> </li> <li> <p>Efficient: Each model only processes what's due based on its schedule - no wasted compute</p> </li> </ul> <p>This is why cron schedules are important. They tell Vulcan when each model should run, so you don't process things unnecessarily.</p>"},{"location":"guides/run_and_scheduling/#scenario-3-run-with-no-missing-intervals","title":"Scenario 3: Run with No Missing Intervals","text":"<p>When all intervals are up to date, <code>run</code> skips execution:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nChecking for missing intervals...\n----------------------------------------------------------------------\n\nNo models to execute. All intervals are up to date.\n\n\u2714 Run completed successfully\n</code></pre><p></p> <p>[Screenshot: Run output showing no models to execute]</p> <p>This is normal when running frequently. Nothing to process means everything is up to date. Your automation is working and keeping things current.</p>"},{"location":"guides/run_and_scheduling/#scenario-4-run-after-model-changes-error-case","title":"Scenario 4: Run After Model Changes (Error Case)","text":"<p>If models have changed, Vulcan detects this and requires a plan first:</p> <pre><code>vulcan run\n</code></pre> <p>Expected Output: </p><pre><code>======================================================================\nError: Model definitions have changed. Use 'vulcan plan' to apply changes first.\n\nChanged models:\n\u2514\u2500\u2500 sales.daily_sales\n\nPlease run 'vulcan plan' to apply these changes before using 'vulcan run'.\n</code></pre><p></p> <p>[Screenshot: Error message when trying to run with model changes]</p> <p>Workflow: Always <code>plan</code> first to apply changes, then <code>run</code> for scheduled execution. </p> <p>This is the key workflow: use <code>plan</code> when you've changed code, then use <code>run</code> for regular data processing. Don't mix them up!</p>"},{"location":"guides/run_and_scheduling/#scheduling-for-production","title":"Scheduling for Production","text":"<p>The <code>vulcan run</code> command doesn't run continuously - it executes once and exits. For production, you need to schedule it to run periodically. This is where automation comes in, you'll set up cron jobs, CI/CD pipelines, or Kubernetes CronJobs to trigger <code>run</code> on a schedule.</p>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-architecture","title":"Built-in Scheduler Architecture","text":"<pre><code>graph TB\n    subgraph \"Automation Layer - Triggers\"\n        CRON[Cron Job&lt;br/&gt;Schedule: Every hour&lt;br/&gt;Example: 0 * * * *]\n        CI[CI/CD Pipeline&lt;br/&gt;GitHub Actions / GitLab CI&lt;br/&gt;Scheduled workflows]\n        K8S[Kubernetes CronJob&lt;br/&gt;Container orchestration&lt;br/&gt;K8s native scheduling]\n        MANUAL[Manual Trigger&lt;br/&gt;Developer runs manually&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"Vulcan Run Command\"\n        RUN[vulcan run&lt;br/&gt;Command starts]\n        VALIDATE[Validate Models&lt;br/&gt;Check for changes&lt;br/&gt;Error if modified]\n        QUERY[Query State Database&lt;br/&gt;Get execution history&lt;br/&gt;Read processed intervals]\n    end\n\n    subgraph \"State Database\"\n        STATE[State Storage&lt;br/&gt;PostgreSQL / SQL Engine&lt;br/&gt;Transaction-safe storage]\n\n        subgraph \"State Tables\"\n            INTERVALS[Processed Intervals&lt;br/&gt;model_name, start_ds, end_ds&lt;br/&gt;status: completed]\n            CRON_STATE[Cron Execution State&lt;br/&gt;model_name, last_run_time&lt;br/&gt;next_run_time]\n            MODEL_STATE[Model State&lt;br/&gt;model_name, fingerprint&lt;br/&gt;environment, version]\n        end\n    end\n\n    subgraph \"Cron Evaluation Engine\"\n        CRON_CHECK[Evaluate Cron Schedules&lt;br/&gt;Compare current time&lt;br/&gt;with last execution]\n        CALC[Calculate Missing Intervals&lt;br/&gt;Determine what's due&lt;br/&gt;Based on cron + state]\n        FILTER[Filter Models&lt;br/&gt;Only select due models&lt;br/&gt;Skip not-due models]\n    end\n\n    subgraph \"Model Execution Queue\"\n        QUEUE[Execution Queue&lt;br/&gt;Ordered by dependencies&lt;br/&gt;Upstream first]\n        EXEC1[Execute Hourly Model&lt;br/&gt;@hourly - Due&lt;br/&gt;Process missing intervals]\n        EXEC2[Execute Daily Model&lt;br/&gt;@daily - Due&lt;br/&gt;Process missing intervals]\n        SKIP[Skip Weekly Model&lt;br/&gt;@weekly - Not due&lt;br/&gt;Wait for next week]\n    end\n\n    subgraph \"Update State\"\n        UPDATE[Update State Database&lt;br/&gt;Mark intervals processed&lt;br/&gt;Update cron state]\n        COMMIT[Commit Transaction&lt;br/&gt;Ensure consistency&lt;br/&gt;Rollback on error]\n    end\n\n    subgraph \"Results &amp; Logging\"\n        LOG[Log Execution&lt;br/&gt;Summary output&lt;br/&gt;Success/failure status]\n        NOTIFY[Notifications&lt;br/&gt;Optional: Slack/Email&lt;br/&gt;On success/failure]\n    end\n\n    CRON --&gt;|\"Scheduled trigger\"| RUN\n    CI --&gt;|\"Pipeline trigger\"| RUN\n    K8S --&gt;|\"K8s trigger\"| RUN\n    MANUAL --&gt;|\"Manual trigger\"| RUN\n\n    RUN --&gt;|\"1. Validate\"| VALIDATE\n    VALIDATE --&gt;|\"2. Query state\"| QUERY\n    QUERY --&gt;|\"Read\"| STATE\n\n    STATE --&gt;|\"Intervals\"| INTERVALS\n    STATE --&gt;|\"Cron state\"| CRON_STATE\n    STATE --&gt;|\"Model state\"| MODEL_STATE\n\n    INTERVALS --&gt;|\"Compare\"| CRON_CHECK\n    CRON_STATE --&gt;|\"Check schedule\"| CRON_CHECK\n    MODEL_STATE --&gt;|\"Get models\"| CRON_CHECK\n\n    CRON_CHECK --&gt;|\"Evaluate\"| CALC\n    CALC --&gt;|\"Calculate\"| FILTER\n\n    FILTER --&gt;|\"Due models\"| QUEUE\n    FILTER -.-&gt;|\"Skip\"| SKIP\n\n    QUEUE --&gt;|\"Execute\"| EXEC1\n    QUEUE --&gt;|\"Execute\"| EXEC2\n\n    EXEC1 --&gt;|\"Update\"| UPDATE\n    EXEC2 --&gt;|\"Update\"| UPDATE\n    SKIP -.-&gt;|\"No update\"| UPDATE\n\n    UPDATE --&gt;|\"Commit\"| COMMIT\n    COMMIT --&gt;|\"Success\"| LOG\n    LOG --&gt;|\"Optional\"| NOTIFY\n\n    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style CI fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style K8S fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style MANUAL fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000\n    style VALIDATE fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style STATE fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style INTERVALS fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style MODEL_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style CRON_CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style CALC fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style QUEUE fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000\n    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style EXEC2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style SKIP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style COMMIT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style LOG fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style NOTIFY fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000</code></pre>"},{"location":"guides/run_and_scheduling/#built-in-scheduler-components","title":"Built-in Scheduler Components","text":"<p>The built-in scheduler consists of several key components working together:</p> <ol> <li>Automation Layer: External triggers (cron, CI/CD, Kubernetes) that periodically execute <code>vulcan run</code></li> <li>State Database: Stores execution history, processed intervals, and cron state</li> <li>Cron Evaluation Engine: Determines which models are due based on their schedules</li> <li>Execution Queue: Orders models by dependencies and executes them</li> <li>State Updates: Records what was processed for future runs</li> </ol> <p>Key Features:</p> <ul> <li> <p>Stores state in your SQL engine (or separate state database)</p> </li> <li> <p>Automatically detects missing intervals</p> </li> <li> <p>Respects each model's <code>cron</code> schedule</p> </li> <li> <p>Processes only what's due</p> </li> <li> <p>Transaction-safe state updates</p> </li> <li> <p>Dependency-aware execution order</p> </li> </ul>"},{"location":"guides/run_and_scheduling/#setting-up-automation","title":"Setting Up Automation","text":"<p>Run <code>vulcan run</code> periodically using one of these methods:</p>"},{"location":"guides/run_and_scheduling/#option-1-linuxmac-cron-job","title":"Option 1: Linux/Mac Cron Job","text":"<pre><code># Edit crontab\ncrontab -e\n\n# Run every hour\n0 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n\n# Run every 15 minutes\n*/15 * * * * cd /path/to/project &amp;&amp; vulcan run &gt;&gt; /var/log/vulcan-run.log 2&gt;&amp;1\n</code></pre>"},{"location":"guides/run_and_scheduling/#option-2-cicd-pipeline","title":"Option 2: CI/CD Pipeline","text":"<p>GitHub Actions Example: </p><pre><code>name: Vulcan Run\non:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  workflow_dispatch:\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Run Vulcan\n        run: |\n          docker run --network=vulcan --rm \\\n            -v $PWD:/workspace \\\n            tmdcio/vulcan:latest vulcan run\n</code></pre><p></p> <p>GitLab CI Example: </p><pre><code>vulcan_run:\n  schedule:\n    - cron: '0 * * * *'  # Every hour\n  script:\n    - docker run --network=vulcan --rm \\\n\n        -v $PWD:/workspace \\\n        tmdcio/vulcan:latest vulcan run\n</code></pre><p></p>"},{"location":"guides/run_and_scheduling/#option-3-kubernetes-cronjob","title":"Option 3: Kubernetes CronJob","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: vulcan-run\nspec:\n  schedule: \"0 * * * *\"  # Every hour\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: vulcan\n            image: tmdcio/vulcan:latest\n            command: [\"vulcan\", \"run\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"guides/run_and_scheduling/#determining-run-frequency","title":"Determining Run Frequency","text":"<p>Set your automation frequency based on your most frequent model's <code>cron</code>:</p> <pre><code>graph TD\n    subgraph \"Model Cron Schedules\"\n        H[Hourly Model&lt;br/&gt;cron: @hourly]\n        D[Daily Model&lt;br/&gt;cron: @daily]\n        W[Weekly Model&lt;br/&gt;cron: @weekly]\n    end\n\n    subgraph \"Automation Frequency\"\n        AUTO_H[Run every hour&lt;br/&gt;vulcan run]\n        AUTO_D[Run daily&lt;br/&gt;vulcan run]\n        AUTO_W[Run weekly&lt;br/&gt;vulcan run]\n    end\n\n    subgraph \"Execution Result\"\n        RESULT1[Hourly: Runs every time&lt;br/&gt;Daily: Runs when due&lt;br/&gt;Weekly: Runs when due]\n        RESULT2[Hourly: Skipped&lt;br/&gt;Daily: Runs when due&lt;br/&gt;Weekly: Runs when due]\n        RESULT3[Hourly: Skipped&lt;br/&gt;Daily: Skipped&lt;br/&gt;Weekly: Runs when due]\n    end\n\n    H --&gt;|\"Requires\"| AUTO_H\n    D --&gt;|\"Can use\"| AUTO_H\n    W --&gt;|\"Can use\"| AUTO_H\n\n    AUTO_H --&gt;|\"Hour 1\"| RESULT1\n    AUTO_H --&gt;|\"Hour 2-23\"| RESULT2\n    AUTO_H --&gt;|\"Week 1\"| RESULT3\n\n    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style W fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style AUTO_H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000\n    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style RESULT2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style RESULT3 fill:#ffe082,stroke:#f9a825,stroke-width:2px,color:#000</code></pre> <p>Rule: Schedule <code>vulcan run</code> based on your fastest model's cron.</p> <ul> <li> <p>Hourly models \u2192 Run automation every hour - if you have hourly models, you need to run at least hourly</p> </li> <li> <p>Daily models \u2192 Run automation daily - if your fastest model is daily, you can run daily</p> </li> <li> <p>Weekly models \u2192 Run automation weekly - if your fastest model is weekly, you can run weekly</p> </li> </ul> <p>Example: If your fastest model runs <code>@hourly</code>, schedule <code>vulcan run</code> to execute hourly. Models with slower schedules (daily, weekly) only process when their intervals are due. Vulcan won't process daily models every hour. It waits until they're actually due.</p> <p>The key insight: you can run <code>vulcan run</code> more frequently than your slowest model's schedule. Vulcan will just skip models that aren't due yet.</p>"},{"location":"guides/run_and_scheduling/#advanced-run-options","title":"Advanced Run Options","text":""},{"location":"guides/run_and_scheduling/#run-specific-models","title":"Run Specific Models","text":"<pre><code>vulcan run --select-model \"sales.daily_sales\"\n</code></pre> <p>Processes only the specified model and its upstream dependencies.</p>"},{"location":"guides/run_and_scheduling/#ignore-cron-schedules","title":"Ignore Cron Schedules","text":"<pre><code>vulcan run --ignore-cron\n</code></pre> <p>Processes all missing intervals regardless of cron schedules. Use sparingly - typically for catching up after downtime. </p> <p>This is useful if your automation was down for a while and you need to catch up on missed intervals. But normally, you want Vulcan to respect cron schedules, that's the whole point!</p>"},{"location":"guides/run_and_scheduling/#custom-execution-time","title":"Custom Execution Time","text":"<pre><code>vulcan run --execution-time \"2025-01-20 10:00:00\"\n</code></pre> <p>Simulates running at a specific time. Useful for testing cron schedules.</p>"},{"location":"guides/run_and_scheduling/#run-in-different-environments","title":"Run in Different Environments","text":"<pre><code>vulcan run dev\n</code></pre> <p>Runs models in the <code>dev</code> environment, maintaining separate execution state from production.</p>"},{"location":"guides/run_and_scheduling/#state-database-considerations","title":"State Database Considerations","text":"<p>By default, Vulcan stores scheduler state in your SQL engine. For production:</p> <p>Recommended: Use a separate PostgreSQL database for state storage when: - Your SQL engine is BigQuery (not optimized for frequent transactions)</p> <ul> <li> <p>You observe performance degradation</p> </li> <li> <p>You need better isolation</p> </li> </ul> <p>See Configuration Guide for configuring a separate state database.</p>"},{"location":"guides/run_and_scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>run</code> for scheduled execution - Don't use <code>plan</code> for regular data processing</li> <li>Set up automation - Schedule <code>vulcan run</code> based on your most frequent model's cron</li> <li>Monitor execution - Check logs to ensure intervals are processing correctly</li> <li>Use <code>--ignore-cron</code> sparingly - Only when catching up on missed intervals</li> <li>Separate state database - Consider PostgreSQL for state storage in production</li> <li>Handle errors gracefully - Set up notifications for run failures</li> </ol> <p>Here are some tips to help you use <code>run</code> effectively:</p> <ol> <li>Use <code>run</code> for scheduled execution - Don't use <code>plan</code> for regular data processing. They serve different purposes!</li> <li>Set up automation - Schedule <code>vulcan run</code> based on your most frequent model's cron. Set it and forget it.</li> <li>Monitor execution - Check logs to ensure intervals are processing correctly. Make sure your automation is actually working.</li> <li>Use <code>--ignore-cron</code> sparingly - Only when catching up on missed intervals. Normally, let Vulcan respect cron schedules.</li> <li>Separate state database - Consider PostgreSQL for state storage in production. Some SQL engines aren't optimized for frequent transactions.</li> <li>Handle errors gracefully - Set up notifications for run failures.</li> </ol> <p>Following these practices will help you build reliable, automated data pipelines.</p>"},{"location":"guides/run_and_scheduling/#quick-reference","title":"Quick Reference","text":"Scenario Command When to Use Regular Run <code>vulcan run</code> Scheduled execution (cron jobs, CI/CD) Dev Environment <code>vulcan run dev</code> Running models in dev environment Select Models <code>vulcan run --select-model \"model\"</code> Running specific models only Ignore Cron <code>vulcan run --ignore-cron</code> Catch up on all missing intervals Custom Time <code>vulcan run --execution-time \"...\"</code> Testing/simulating runs"},{"location":"guides/run_and_scheduling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Plan Guide for applying model changes</li> </ul>"},{"location":"guides/transpiling_semantics/","title":"Transpiling Semantics","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantics","title":"Transpiling Semantics","text":"<p>The <code>vulcan transpile</code> command converts semantic queries into executable SQL. Use it to preview, debug, and validate semantic logic before execution.</p> <p>Transpilation converts semantic layer queries (business-friendly) into SQL that your database understands.</p>"},{"location":"guides/transpiling_semantics/#what-is-transpilation","title":"What is Transpilation?","text":"<p>Transpilation transforms semantic layer queries into database-specific SQL. It converts \"business language\" (semantic queries) into \"database language\" (SQL).</p> <ul> <li> <p>Semantic SQL \u2192 Native SQL: Converts semantic SQL queries with <code>MEASURE()</code> functions into standard SQL - takes your business-friendly queries and makes them database-ready</p> </li> <li> <p>REST API Payload \u2192 Native SQL: Converts JSON query payloads into executable SQL statements. Use for API-driven applications.</p> </li> <li> <p>Validation: Catches errors before query execution - find problems before they hit production</p> </li> <li> <p>Debugging: Inspect the generated SQL to understand query behavior - see exactly what your semantic queries are doing under the hood</p> </li> </ul> <p>Semantic queries are easier to write and understand, but databases need SQL. Transpilation bridges that gap.</p>"},{"location":"guides/transpiling_semantics/#basic-structure","title":"Basic Structure","text":""},{"location":"guides/transpiling_semantics/#semantic-sql-query-structure","title":"Semantic SQL Query Structure","text":"<p>Semantic SQL queries follow standard SQL syntax with semantic layer extensions:</p> <pre><code>SELECT \n  alias.dimension_name,           # Dimensions: attributes for grouping and filtering\n  MEASURE(alias.measure_name)  # Measures: aggregated calculations (required wrapper)\nFROM alias                        # Semantic model alias (business-friendly name)\nCROSS JOIN other_alias            # Optional: join multiple models\nWHERE \n  alias.dimension_name = 'value'  # Optional: filter on dimensions\n  AND segment_name = true         # Optional: use segments (only = true supported)\nGROUP BY alias.dimension_name     # Required: all non-aggregated columns\nORDER BY MEASURE(alias.measure_name)    # Optional: sort results\nLIMIT 100                         # Optional: limit result set\nOFFSET 0                          # Optional: pagination offset\n</code></pre> <p>Key Components:</p> <ul> <li> <p><code>alias.dimension_name</code>: Reference dimensions using semantic model alias</p> </li> <li> <p><code>MEASURE(measure_name)</code>: Required wrapper for measures to apply aggregation</p> </li> <li> <p><code>FROM alias</code>: Use semantic model alias, not physical model name</p> </li> <li> <p><code>CROSS JOIN</code>: Join syntax (join conditions automatically inferred)</p> </li> <li> <p><code>segment_name = true</code>: Segments only support <code>= true</code>, not <code>= false</code></p> </li> </ul>"},{"location":"guides/transpiling_semantics/#rest-api-payload-structure","title":"REST API Payload Structure","text":"<p>REST API queries use JSON payloads with semantic query definitions:</p> <pre><code>{\n  \"query\": {\n    \"measures\": [\"alias.measure_name\"],              # Required: array of measure names\n    \"dimensions\": [\"alias.dimension_name\"],         # Optional: array of dimension names\n    \"segments\": [\"segment_name\"],                    # Optional: array of segment names\n    \"timeDimensions\": [{                             # Optional: array of time dimension objects\n      \"dimension\": \"alias.time_dimension\",           # Required: time dimension member\n      \"dateRange\": [\"2024-01-01\", \"2024-12-31\"],    # Optional: date range array or string\n      \"granularity\": \"month\"                         # Optional: hour, day, week, month, quarter, year\n    }],\n    \"filters\": [{                                    # Optional: array of filter objects\n      \"member\": \"alias.dimension_name\",              # Required: fully qualified member name\n      \"operator\": \"equals\",                          # Required: filter operator\n      \"values\": [\"value1\", \"value2\"]                 # Optional: array of filter values\n    }],\n    \"order\": {                                       # Optional: sort order object\n      \"alias.measure_name\": \"desc\",                  # Member name: \"asc\" or \"desc\"\n      \"alias.dimension_name\": \"asc\"\n    },\n    \"limit\": 100,                                    # Optional: maximum rows to return\n    \"offset\": 0,                                     # Optional: rows to skip\n    \"timezone\": \"UTC\",                               # Optional: timezone for date parsing\n    \"renewQuery\": false                              # Optional: bypass cache if true\n  },\n  \"ttl_minutes\": 60                                  # Optional: cache duration in minutes\n}\n</code></pre> <p>Key Components:</p> <ul> <li> <p><code>measures</code>: Array of fully qualified measure names: <code>\"alias.measure_name\"</code></p> </li> <li> <p><code>dimensions</code>: Array of fully qualified dimension names: <code>\"alias.dimension_name\"</code></p> </li> <li> <p><code>segments</code>: Array of segment names (no alias prefix needed)</p> </li> <li> <p><code>timeDimensions</code>: Array of objects with <code>dimension</code>, <code>dateRange</code>, and <code>granularity</code></p> </li> <li> <p><code>filters</code>: Array of filter objects with <code>member</code>, <code>operator</code>, and <code>values</code></p> </li> <li> <p><code>order</code>: Object mapping member names to sort direction (<code>\"asc\"</code> or <code>\"desc\"</code>)</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql-queries","title":"Transpiling Semantic SQL Queries","text":"<p>Convert semantic SQL queries to native SQL:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Output: Generated SQL that can be executed directly against your database.</p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads","title":"Transpiling REST API Payloads","text":"<p>Convert JSON query payloads to native SQL:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Output: Generated SQL from the REST-style query definition.</p>"},{"location":"guides/transpiling_semantics/#command-syntax","title":"Command Syntax","text":""},{"location":"guides/transpiling_semantics/#basic-format","title":"Basic Format","text":"<pre><code>vulcan transpile --format &lt;format&gt; \"&lt;query&gt;\"\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>--format</code> (required): Output format: <code>sql</code> or <code>json</code></p> </li> <li> <p><code>\"&lt;query&gt;\"</code> (required): The semantic query to transpile</p> </li> <li> <p>For SQL format: Semantic SQL query string</p> </li> <li> <p>For JSON format: JSON query payload string</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#advanced-options","title":"Advanced Options","text":"<pre><code>vulcan transpile --format sql \"&lt;query&gt;\" [--disable-post-processing]\n</code></pre> <p>Options:</p> <ul> <li> <p><code>--disable-post-processing</code>: Enable pushdown mode for CTE support and advanced SQL features</p> </li> <li> <p>Default: Post-processing enabled (CTEs not supported)</p> </li> <li> <p>With flag: Pushdown enabled (CTEs supported, no pre-aggregations)</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#transpiling-semantic-sql","title":"Transpiling Semantic SQL","text":""},{"location":"guides/transpiling_semantics/#basic-query","title":"Basic Query","text":"<p>Transpile a simple semantic SQL query:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions and grouping:</p> <pre><code>vulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".plan_type, sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\nGROUP BY \"users\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters","title":"Query with Filters","text":"<p>Transpile queries with WHERE conditions:</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-grouping","title":"Query with Time Grouping","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format sql \"SELECT DATE_TRUNC('month', subscriptions.start_date) as month, MEASURE(total_arr) FROM subscriptions GROUP BY month\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS month,\n       sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-joins","title":"Query with Joins","text":"<p>Transpile queries joining multiple models:</p> <pre><code>vulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users GROUP BY users.industry\"\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"users\".industry, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".user_id = \"users\".user_id\nGROUP BY \"users\".industry\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#transpiling-rest-api-payloads_1","title":"Transpiling REST API Payloads","text":""},{"location":"guides/transpiling_semantics/#minimal-query","title":"Minimal Query","text":"<p>Transpile a basic REST API query:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"users\".user_id) AS total_users\nFROM analytics.users AS \"users\"\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-dimensions_1","title":"Query with Dimensions","text":"<p>Transpile queries with dimensions:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"dimensions\": [\"subscriptions.plan_type\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT \"subscriptions\".plan_type, sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nGROUP BY \"subscriptions\".plan_type\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-time-dimensions","title":"Query with Time Dimensions","text":"<p>Transpile time-based queries:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"orders.total_revenue\"], \"timeDimensions\": [{\"dimension\": \"orders.order_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"orders\".order_date) AS orders_order_date_month,\n       sum(\"orders\".amount) AS total_revenue\nFROM analytics.orders AS \"orders\"\nWHERE \"orders\".order_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"orders\".order_date &lt;= '2024-12-31T23:59:59.999'\nGROUP BY DATE_TRUNC('month', \"orders\".order_date)\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-filters_1","title":"Query with Filters","text":"<p>Transpile queries with filters:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#query-with-segments","title":"Query with Segments","text":"<p>Transpile queries using segments:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\"], \"segments\": [\"subscriptions.active_subscriptions\"]}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT sum(\"subscriptions\".arr) AS total_arr\nFROM analytics.subscriptions AS \"subscriptions\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".end_date IS NULL\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#complex-query","title":"Complex Query","text":"<p>Transpile complex queries with multiple components:</p> <pre><code>vulcan transpile --format json '{\"query\": {\"measures\": [\"subscriptions.total_arr\", \"subscriptions.total_seats\"], \"dimensions\": [\"subscriptions.plan_type\", \"users.industry\"], \"filters\": [{\"member\": \"subscriptions.status\", \"operator\": \"equals\", \"values\": [\"active\"]}], \"timeDimensions\": [{\"dimension\": \"subscriptions.start_date\", \"dateRange\": [\"2024-01-01\", \"2024-12-31\"], \"granularity\": \"month\"}], \"order\": {\"subscriptions.total_arr\": \"desc\"}, \"limit\": 100}}'\n</code></pre> <p>Generated SQL: </p><pre><code>SELECT DATE_TRUNC('month', \"subscriptions\".start_date) AS subscriptions_start_date_month,\n       \"subscriptions\".plan_type,\n       \"users\".industry,\n       sum(\"subscriptions\".arr) AS total_arr,\n       sum(\"subscriptions\".seats) AS total_seats\nFROM analytics.subscriptions AS \"subscriptions\"\nCROSS JOIN analytics.users AS \"users\"\nWHERE \"subscriptions\".status = 'active'\n  AND \"subscriptions\".start_date &gt;= '2024-01-01T00:00:00.000'\n  AND \"subscriptions\".start_date &lt;= '2024-12-31T23:59:59.999'\n  AND \"subscriptions\".user_id = \"users\".user_id\nGROUP BY DATE_TRUNC('month', \"subscriptions\".start_date),\n         \"subscriptions\".plan_type,\n         \"users\".industry\nORDER BY sum(\"subscriptions\".arr) DESC\nLIMIT 100\n</code></pre><p></p>"},{"location":"guides/transpiling_semantics/#use-cases","title":"Use Cases","text":""},{"location":"guides/transpiling_semantics/#query-validation","title":"Query Validation","text":"<p>Validate semantic queries before execution:</p> <pre><code># Check if query syntax is correct\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre> <p>If the query is invalid, you'll get an error message indicating the issue. This is way better than finding out at runtime! Catch errors early, fix them, then execute.</p>"},{"location":"guides/transpiling_semantics/#debugging-query-behavior","title":"Debugging Query Behavior","text":"<p>Inspect generated SQL to understand how semantic queries are translated. When queries return unexpected results, transpile them to see what's actually happening:</p> <pre><code># See how measures are aggregated\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n</code></pre> <p>This shows you exactly how Vulcan is interpreting your semantic query. Sometimes the generated SQL reveals issues you didn't expect!</p>"},{"location":"guides/transpiling_semantics/#performance-analysis","title":"Performance Analysis","text":"<p>Review generated SQL to identify optimization opportunities. The generated SQL shows you exactly what the database will execute, so you can spot performance issues:</p> <pre><code># Check join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre> <p>Look at the generated SQL, are joins efficient? Are filters in the right place? This is your chance to optimize before execution.</p>"},{"location":"guides/transpiling_semantics/#documentation","title":"Documentation","text":"<p>Generate SQL examples for documentation or training. Use transpilation to create SQL examples that show how semantic queries translate:</p> <pre><code># Create SQL reference from semantic queries\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre> <p>Use this for documentation. Show both the semantic query (easy to understand) and the generated SQL (what actually runs).</p>"},{"location":"guides/transpiling_semantics/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"guides/transpiling_semantics/#error-unknown-member-x","title":"Error: \"Unknown member: X\"","text":"<p>Cause: Member doesn't exist in semantic model or is misspelled.</p> <p>Solution:</p> <ul> <li> <p>Verify member exists in your semantic model - check your semantic model definitions</p> </li> <li> <p>Check spelling and casing (case-sensitive) - <code>users.plan_type</code> is different from <code>users.Plan_Type</code></p> </li> <li> <p>Use fully qualified format: <code>alias.member_name</code> - always include the alias prefix</p> </li> </ul> <p>This error usually means you've made a typo or the member doesn't exist yet. Double-check your semantic model!</p>"},{"location":"guides/transpiling_semantics/#error-measure-not-found-x","title":"Error: \"Measure not found: X\"","text":"<p>Cause: Measure referenced without proper qualification or doesn't exist.</p> <p>Solution:</p> <ul> <li> <p>Use <code>MEASURE(measure_name)</code> wrapper for SQL format - measures need the MEASURE() wrapper in SQL</p> </li> <li> <p>Use fully qualified format: <code>alias.measure_name</code> for JSON format - JSON format uses dot notation</p> </li> <li> <p>Verify measure is defined in semantic model - make sure the measure actually exists</p> </li> </ul> <p>The format differs between SQL and JSON, so make sure you're using the right syntax for each!</p>"},{"location":"guides/transpiling_semantics/#error-model-not-found-x","title":"Error: \"Model not found: X\"","text":"<p>Cause: Alias doesn't match any semantic model.</p> <p>Solution:</p> <ul> <li> <p>Check semantic model aliases in your <code>semantics/</code> directory</p> </li> <li> <p>Verify alias spelling and casing</p> </li> <li> <p>Ensure semantic models are properly defined</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#error-invalid-json-format","title":"Error: \"Invalid JSON format\"","text":"<p>Cause: JSON payload is malformed.</p> <p>Solution:</p> <ul> <li> <p>Validate JSON syntax</p> </li> <li> <p>Ensure proper quoting of strings</p> </li> <li> <p>Check array and object structure</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#error-projection-references-non-aggregate-values","title":"Error: \"Projection references non-aggregate values\"","text":"<p>Cause: Non-aggregated columns not in GROUP BY, or measures missing MEASURE() wrapper.</p> <p>Solution:</p> <ul> <li> <p>Add all non-aggregated columns to GROUP BY - if you select a column, it needs to be in GROUP BY (unless it's aggregated)</p> </li> <li> <p>Use MEASURE() wrapper for all measures in SQL format - measures must be wrapped in MEASURE()</p> </li> </ul> <p>This is a SQL rule, you can't mix aggregated and non-aggregated columns without GROUP BY. The error is telling you exactly what's wrong!</p>"},{"location":"guides/transpiling_semantics/#best-practices","title":"Best Practices","text":""},{"location":"guides/transpiling_semantics/#validate-before-execution","title":"Validate Before Execution","text":"<p>Always transpile queries before running them in production. It's like checking your work before turning it in:</p> <pre><code># Good: Validate first\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n# Review output, then execute - make sure the SQL looks right\n\n# Bad: Execute without validation\n# Direct execution without checking generated SQL - don't do this\n</code></pre> <p>Transpilation catches errors early. Use it!</p>"},{"location":"guides/transpiling_semantics/#use-transpilation-for-debugging","title":"Use Transpilation for Debugging","text":"<p>When queries return unexpected results, transpile to inspect generated SQL. The generated SQL often reveals what's actually happening:</p> <pre><code># Debug query behavior\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Compare generated SQL with expected behavior - does it match what you think should happen?\n</code></pre> <p>Sometimes the issue isn't with your semantic query, it's with how it's being translated. Transpilation shows you the translation.</p>"},{"location":"guides/transpiling_semantics/#document-query-patterns","title":"Document Query Patterns","text":"<p>Use transpilation output to document common query patterns:</p> <pre><code># Generate SQL examples for documentation\nvulcan transpile --format sql \"SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#test-both-formats","title":"Test Both Formats","text":"<p>When building applications, test both SQL and JSON formats:</p> <pre><code># Test SQL format\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n\n# Test equivalent JSON format\nvulcan transpile --format json '{\"query\": {\"measures\": [\"users.total_users\"]}}'\n</code></pre>"},{"location":"guides/transpiling_semantics/#choose-appropriate-mode","title":"Choose Appropriate Mode","text":"<p>Select post-processing or pushdown mode based on needs:</p> <ul> <li> <p>Post-processing (default): Use for queries that benefit from pre-aggregations and caching</p> </li> <li> <p>Pushdown (<code>--disable-post-processing</code>): Use when you need CTEs or complex SQL structures</p> </li> </ul>"},{"location":"guides/transpiling_semantics/#integration-with-development-workflow","title":"Integration with Development Workflow","text":""},{"location":"guides/transpiling_semantics/#pre-commit-validation","title":"Pre-commit Validation","text":"<p>Add transpilation checks to your development workflow:</p> <pre><code># Validate semantic queries in CI/CD\nvulcan transpile --format sql \"SELECT MEASURE(total_users) FROM users\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#query-testing","title":"Query Testing","text":"<p>Use transpilation to generate test SQL:</p> <pre><code># Generate SQL for testing\nvulcan transpile --format sql \"SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type\"\n# Use output in test assertions\n</code></pre>"},{"location":"guides/transpiling_semantics/#performance-tuning","title":"Performance Tuning","text":"<p>Analyze generated SQL for optimization:</p> <pre><code># Review join conditions and filter placement\nvulcan transpile --format sql \"SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry\"\n</code></pre>"},{"location":"guides/transpiling_semantics/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn about Semantic Models that define the queryable members</p> </li> <li> <p>Explore Business Metrics for time-series analysis</p> </li> <li> <p>See the Semantics Overview for the complete picture</p> </li> </ul>"},{"location":"guides/get-started/docker/","title":"Get Started","text":""},{"location":"guides/get-started/docker/#get-started","title":"Get Started","text":"<p>This guide shows you how to set up a complete Vulcan project on your local machine.</p> <p>The example project runs locally using a Postgres SQL engine. Vulcan automatically generates all necessary project files and configurations.</p> <p>To get started, ensure your system meets the prerequisites below, then follow the step-by-step instructions for your operating system.</p>"},{"location":"guides/get-started/docker/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have Docker installed and configured on your system. Follow the instructions below for your operating system.</p> Mac/LinuxWindows <p>1. Verify Docker Installation</p> <p>First, check if Docker Desktop (Mac) or Docker Engine (Linux) is installed and running:</p> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>If both commands return version numbers, Docker is installed. Make sure Docker Desktop is running (you should see the Docker icon in your menu bar or system tray).</p> <p>2. Install Docker (if needed)</p> <ul> <li> <p>Mac: Download and install Docker Desktop for Mac</p> </li> <li> <p>Linux: Install Docker Engine and Docker Compose following the official Docker installation guide</p> </li> </ul> <p>3. Configure Resources</p> <p>Ensure Docker Desktop has at least 4GB of RAM allocated. You can adjust this in Docker Desktop settings under Resources \u2192 Advanced.</p> <p>1. Verify Docker Installation</p> <p>Check if Docker Desktop for Windows is installed and running:</p> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>If both commands return version numbers, Docker is installed. Make sure Docker Desktop is running (you should see the Docker icon in your system tray).</p> <p>2. Install Docker (if needed)</p> <p>If Docker is not installed, download and install Docker Desktop for Windows</p> <p>3. Configure Resources</p> <p>Ensure Docker Desktop has at least 4GB of RAM allocated. You can adjust this in Docker Desktop settings under Settings \u2192 Resources \u2192 Advanced.</p>"},{"location":"guides/get-started/docker/#vulcan-setup-locally","title":"Vulcan Setup Locally","text":"<p>Follow these steps to set up Vulcan on your local machine. The setup process will create all necessary infrastructure services and prepare your environment for development.</p> Mac/LinuxWindows <p> Download for Mac/Linux</p> <p>The download includes: Docker Compose files, Makefile, and a comprehensive README</p> <p>Step 1: Extract and Navigate</p> <p>Extract the downloaded zip file and open the <code>vulcan-project</code> folder in VS Code or your preferred IDE:</p> <pre><code>cd vulcan-project\n</code></pre> <p>Step 2: Run Setup</p> <p>Important: Before running setup, ensure Docker Desktop is running on your machine and that you are logged into RubikLabs.</p> <p>Execute the setup command:</p> <pre><code>make setup\nor\ndocker network create vulcan\ndocker compose -f docker/docker-compose.infra.yml up -d\ndocker compose -f docker/docker-compose.warehouse.yml up -d\nor\ndocker network create vulcan\ndocker compose -f docker/docker-compose.infra.yml up -d\ndocker compose -f docker/docker-compose.warehouse.yml up -d\n</code></pre> <p>This command creates and starts three essential services:</p> <ul> <li> <p>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, and execution history. This database persists your semantic model, plans, and tracks materialization state.</p> </li> <li> <p>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates. This service provides data retrieval and caching for your workflows.</p> </li> <li> <p>minio-init: Initializes MinIO buckets and policies with the correct configuration. This service runs once to set up the storage infrastructure.</p> </li> </ul> <p>Note: These services are essential for Vulcan's operation and must be running before you can use Vulcan. The setup process typically takes 1-2 minutes to complete.</p> <p>State Connection Default</p> <p>By default, you should use Postgres for your state connection. When configuring your <code>config.yaml</code>, set <code>state_connection</code> to use Postgres. This ensures reliable state management and is the recommended approach for most projects.</p> <p>Verify Services Are Running</p> <p>Before proceeding, verify that all required infrastructure services (engine and storage) are up and running.</p> <p>Check running containers</p> <p>Use Docker directly to confirm that all containers are running:</p> <pre><code>docker ps\n</code></pre> <p>You should see containers corresponding to the following services with a status of <code>Up</code>:</p> <ul> <li>statestore (PostgreSQL) \u2013 State storage service</li> <li>minio \u2013 Object storage service</li> <li>warehouse (PostgreSQL) \u2013 Data warehouse engine</li> </ul> <p>If a container is missing from the list or not in an <code>Up</code> state, it may have stopped or failed to start.</p> <p>Validate container logs</p> <p>To inspect logs for any specific service, use:</p> <pre><code>docker logs &lt;container_name&gt;\n</code></pre> <p>For example:</p> <pre><code>docker logs statestore\ndocker logs minio\ndocker logs warehouse\n</code></pre> <p>Review the logs for errors, crash messages, or failed startup checks.</p> <p>Once all containers are running properly and logs look healthy, proceed to the next step.</p> <p>Step 3: Configure Vulcan CLI Access</p> <p>Create an alias to access the Vulcan CLI. The alias uses an engine-specific Docker image. Postgres is shown by default (recommended for most users). If you're using a different engine, select it from the tabs below:</p> <p>Automatic Updates</p> <p>Docker image versions in this section are automatically synchronized with the engine configuration files. When engine image versions are updated, this section is automatically updated as well.</p> Postgres (Default)BigQueryDatabricksFabricMSSQLMySQLRedshiftSnowflakeSparkTrino <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-postgres:0.228.1.8 vulcan\"\n</code></pre> Image version from Postgres engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-bigquery:0.228.1.8 vulcan\"\n</code></pre> Image version from BigQuery engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-databricks:0.228.1.6 vulcan\"\n</code></pre> Image version from Databricks engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-fabric:0.228.1.6 vulcan\"\n</code></pre> Image version from Fabric engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-mssql:0.228.1.6 vulcan\"\n</code></pre> Image version from MSSQL engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-mysql:0.228.1.6 vulcan\"\n</code></pre> Image version from MySQL engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-redshift:0.228.1.6 vulcan\"\n</code></pre> Image version from Redshift engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-snowflake:0.228.1.8 vulcan\"\n</code></pre> Image version from Snowflake engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-spark:0.228.1.6 vulcan\"\n</code></pre> Image version from Spark engine configuration<p></p> <p></p><pre><code>alias vulcan=\"docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan-trino:0.228.1.6 vulcan\"\n</code></pre> Image version from Trino engine configuration<p></p> <p>Note: This alias is temporary and will be lost when you close your shell session. To make it permanent, add this line to your shell configuration file (<code>~/.bashrc</code> for Bash or <code>~/.zshrc</code> for Zsh), then restart your terminal or run <code>source ~/.zshrc</code> (or <code>source ~/.bashrc</code>).</p> <p>Step 4: Start API Services</p> <p>Start the Vulcan API services:</p> <pre><code>make vulcan-up\nor\ndocker compose -f docker/docker-compose.vulcan.yml up -d\nor\ndocker compose -f docker/docker-compose.vulcan.yml up -d\n</code></pre> <p>This command starts two services:</p> <ul> <li> <p>vulcan-api: A REST API server for querying your semantic model (available at <code>http://localhost:8000</code>)</p> </li> <li> <p>vulcan-transpiler: A service for transpiling semantic queries to SQL</p> </li> </ul> <p>Once these services are running, you're ready to create your first project!</p> <p> Download for Windows</p> <p>The download includes: Docker Compose files, Windows batch scripts, and a comprehensive README</p> <p>Step 1: Extract and Navigate</p> <p>Extract the downloaded zip file and navigate to the <code>vulcan-project</code> directory:</p> <pre><code>cd vulcan-project\n</code></pre> <p>Step 2: Run Setup</p> <p>Important: Before running setup, ensure Docker Desktop for Windows is running and that you are logged into RubikLabs.</p> <p>Execute the setup script:</p> <pre><code>setup.bat\n</code></pre> <p>This script creates and starts three essential services:</p> <ul> <li> <p>statestore (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, and execution history. This database persists your semantic model, plans, and tracks materialization state.</p> </li> <li> <p>minio (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates. This service provides data retrieval and caching for your workflows.</p> </li> <li> <p>minio-init: Initializes MinIO buckets and policies with the correct configuration. This service runs once to set up the storage infrastructure.</p> </li> </ul> <p>Note: These services are essential for Vulcan's operation and must be running before you can use Vulcan. The setup process typically takes 1-2 minutes to complete.</p> <p>State Connection Default</p> <p>By default, you should use Postgres for your state connection. When configuring your <code>config.yaml</code>, set <code>state_connection</code> to use Postgres. This ensures reliable state management and is the recommended approach for most projects.</p> <p>Verify Services Are Running</p> <p>Before proceeding, verify that all required infrastructure services (engine and storage) are up and running.</p> <p>Check running containers</p> <p>Use Docker directly to confirm that all containers are running:</p> <pre><code>docker ps\n</code></pre> <p>You should see containers corresponding to the following services with a status of <code>Up</code>:</p> <ul> <li>statestore (PostgreSQL) \u2013 State storage service</li> <li>minio \u2013 Object storage service</li> <li>warehouse (PostgreSQL) \u2013 Data warehouse engine</li> </ul> <p>If a container is missing from the list or not in an <code>Up</code> state, it may have stopped or failed to start.</p> <p>Validate container logs</p> <p>To inspect logs for any specific service, use:</p> <pre><code>docker logs &lt;container_name&gt;\n</code></pre> <p>For example:</p> <pre><code>docker logs statestore\ndocker logs minio\ndocker logs warehouse\n</code></pre> <p>Review the logs for errors, crash messages, or failed startup checks.</p> <p>Once all containers are running properly and logs look healthy, proceed to the next step.</p> <p>Step 3: Access Vulcan CLI</p> <p>Use the provided batch script to access the Vulcan CLI:</p> <pre><code>vulcan.bat\n</code></pre> <p>This script runs Vulcan commands in a Docker container with the correct network and volume settings.</p> <p>Step 4: Start API Services</p> <p>Start the Vulcan API services:</p> <pre><code>start-vulcan-api.bat\n</code></pre> <p>This command starts two services:</p> <ul> <li> <p>vulcan-api: A REST API server for querying your semantic model (available at <code>http://localhost:8000</code>)</p> </li> <li> <p>vulcan-transpiler: A service for transpiling semantic queries to SQL</p> </li> </ul> <p>Once these services are running, you're ready to create your first project!</p>"},{"location":"guides/get-started/docker/#create-your-first-project","title":"Create Your First Project","text":"<p>Now that your environment is set up, let's create your first Vulcan project. This section walks you through initializing a project, verifying the setup, running your first plan, and querying your data.</p> Mac/LinuxWindows <p>Step 1: Initialize Your Project</p> <p>Initialize a new Vulcan project: Learn more about init</p> <pre><code>vulcan init\n</code></pre> <p>When prompted:</p> <ul> <li> <p>Choose <code>DEFAULT</code> as the project type</p> </li> <li> <p>Select <code>Postgres</code> as your SQL engine</p> </li> </ul> <p>This command creates a complete project structure with 7 directories:</p> <ul> <li> <p><code>models/</code> - Contains <code>.sql</code> and <code>.py</code> files for your data models</p> </li> <li> <p><code>seeds/</code> - CSV files for static datasets</p> </li> <li> <p><code>audits/</code> - Write logic to assert data quality and block downstream models if checks fail</p> </li> <li> <p><code>tests/</code> - Test files for validating your model logic</p> </li> <li> <p><code>macros/</code> - Write custom macros for reusable SQL patterns</p> </li> <li> <p><code>checks/</code> - Write data quality checks</p> </li> <li> <p><code>semantics/</code> - Semantic layer definitions (measures, dimensions, etc.)</p> </li> </ul> <p>Step 2: Verify Your Setup</p> <p>Check your project configuration and connection status: Learn more about info</p> <pre><code>vulcan info\n</code></pre> <p>This command displays:</p> <ul> <li> <p>Connection status to your database</p> </li> <li> <p>Number of models, macros, and other project components</p> </li> <li> <p>Project configuration details</p> </li> </ul> <p>Important: Verify that the setup is correct before proceeding to run plans. If you see any errors, check the Troubleshooting section below.</p> <p>Step 3: Create and Apply Your First Plan</p> <p>Generate a plan for your models: Learn more about plan</p> <pre><code>vulcan plan\n</code></pre> <p>This command performs three key actions:</p> <ol> <li>Validates your models and creates the necessary database objects (tables, views, etc.)</li> <li>Calculates which data intervals need to be backfilled based on your model's <code>start</code> date and <code>cron</code> schedule</li> <li>Prompts you to apply the plan</li> </ol> <p>When prompted, enter <code>y</code> to apply the plan and backfill your models with historical data.</p> <p>Note: The backfill process may take a few minutes depending on the amount of historical data to process.</p> <p>Step 4: Query Your Models</p> <p>Execute SQL queries against your models: Learn more about fetchdf</p> <pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre> <p>This command executes a SQL query and returns the results as a pandas DataFrame.</p> <p>Step 5: Query Using Semantic Layer</p> <p>Use Vulcan's semantic layer to query your data: Learn more about transpile</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre> <p>This command transpiles your semantic query into SQL that can be executed against your data warehouse. The semantic layer provides a business-friendly interface for querying your data models.</p> <p>Step 1: Initialize Your Project</p> <p>Initialize a new Vulcan project: Learn more about init</p> <pre><code>vulcan init\n</code></pre> <p>When prompted:</p> <ul> <li> <p>Choose <code>DEFAULT</code> as the project type</p> </li> <li> <p>Select <code>Postgres</code> as your SQL engine</p> </li> </ul> <p>This command creates a complete project structure with 7 directories:</p> <ul> <li> <p><code>models/</code> - Contains <code>.sql</code> and <code>.py</code> files for your data models</p> </li> <li> <p><code>seeds/</code> - CSV files for static datasets</p> </li> <li> <p><code>audits/</code> - Write logic to assert data quality and block downstream models if checks fail</p> </li> <li> <p><code>tests/</code> - Test files for validating your model logic</p> </li> <li> <p><code>macros/</code> - Write custom macros for reusable SQL patterns</p> </li> <li> <p><code>checks/</code> - Write data quality checks</p> </li> <li> <p><code>semantics/</code> - Semantic layer definitions (measures, dimensions, etc.)</p> </li> </ul> <p>Step 2: Verify Your Setup</p> <p>Check your project configuration and connection status: Learn more about info</p> <pre><code>vulcan info\n</code></pre> <p>This command displays:</p> <ul> <li> <p>Connection status to your database</p> </li> <li> <p>Number of models, macros, and other project components</p> </li> <li> <p>Project configuration details</p> </li> </ul> <p>Important: Verify that the setup is correct before proceeding to run plans. If you see any errors, check the Troubleshooting section below.</p> <p>Step 3: Create and Apply Your First Plan</p> <p>Generate a plan for your models: Learn more about plan</p> <pre><code>vulcan plan\n</code></pre> <p>This command performs three key actions:</p> <ol> <li>Validates your models and creates the necessary database objects (tables, views, etc.)</li> <li>Calculates which data intervals need to be backfilled based on your model's <code>start</code> date and <code>cron</code> schedule</li> <li>Prompts you to apply the plan</li> </ol> <p>When prompted, enter <code>y</code> to apply the plan and backfill your models with historical data.</p> <p>Note: The backfill process may take a few minutes depending on the amount of historical data to process.</p> <p>Step 4: Query Your Models</p> <p>Execute SQL queries against your models: Learn more about fetchdf</p> <pre><code>vulcan fetchdf \"select * from schema.model_name\"\n</code></pre> <p>This command executes a SQL query and returns the results as a pandas DataFrame.</p> <p>Step 5: Query Using Semantic Layer</p> <p>Use Vulcan's semantic layer to query your data: Learn more about transpile</p> <pre><code>vulcan transpile --format sql \"SELECT MEASURE(measure_name) FROM model\"\n</code></pre> <p>This command transpiles your semantic query into SQL that can be executed against your data warehouse. The semantic layer provides a business-friendly interface for querying your data models.</p>"},{"location":"guides/get-started/docker/#stopping-services","title":"Stopping Services","text":"<p>When you're done working with Vulcan, you can stop the services to free up system resources. Use the commands below based on your operating system.</p> Mac/LinuxWindows <p>Stop All Services</p> <p>To stop all running services:</p> <pre><code>make all-down       # Stop all services\nor\ndocker compose -f docker/docker-compose.infra.yml down -v\ndocker compose -f docker/docker-compose.warehouse.yml down -v\nor\ndocker compose -f docker/docker-compose.infra.yml down -v\ndocker compose -f docker/docker-compose.warehouse.yml down -v\n</code></pre> <p>Stop and Clean Up (Warning: This deletes all data)</p> <p>To stop all services and remove volumes (this will delete all data):</p> <pre><code>make all-clean      # Stop and remove volumes (this will delete all data)\n</code></pre> <p>Stop Individual Service Groups</p> <p>You can also stop specific service groups:</p> <pre><code>make vulcan-down     # Stop only Vulcan API services\nor\ndocker compose -f docker/docker-compose.vulcan.yml down -v\nor\ndocker compose -f docker/docker-compose.vulcan.yml down -v\nmake infra-down      # Stop infrastructure services (statestore, minio)\nmake warehouse-down  # Stop warehouse services\n</code></pre> <p>Stop All Services</p> <p>To stop all running services:</p> <pre><code>stop-all.bat           # Stop all services\n</code></pre> <p>Stop and Clean Up (Warning: This deletes all data)</p> <p>To stop all services and remove volumes (this will delete all data):</p> <pre><code>clean.bat              # Stop and remove volumes (this will delete all data)\n</code></pre> <p>Stop Individual Services</p> <p>To stop only the Vulcan API services:</p> <pre><code>vulcan-down.bat        # Stop only Vulcan API services\n</code></pre>"},{"location":"guides/get-started/docker/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during setup or while using Vulcan, refer to the solutions below.</p> Common Issues and Solutions <p>Services Won't Start</p> <p>If services fail to start, ensure Docker Desktop is running with at least 4GB RAM allocated. You can check and adjust this in Docker Desktop settings:</p> <ul> <li> <p>Mac: Docker Desktop \u2192 Settings \u2192 Resources \u2192 Advanced</p> </li> <li> <p>Windows: Docker Desktop \u2192 Settings \u2192 Resources \u2192 Advanced</p> </li> </ul> <p>Network Errors</p> <p>If you encounter network-related errors, ensure the <code>vulcan</code> Docker network exists:</p> Mac/LinuxWindows <p>Check if the network exists: </p><pre><code>docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p>Check if the network exists: </p><pre><code>docker network ls | grep vulcan\n</code></pre> If it doesn't exist, create it: <pre><code>docker network create vulcan\n</code></pre><p></p> <p>Port Conflicts</p> <p>If you see errors about ports already being in use, one of the required ports (5431, 5433, 9000, 9001, or 8000) is likely occupied by another application. You have two options:</p> <ol> <li>Stop the conflicting application using that port</li> <li>Modify the port mappings in the Docker Compose files (<code>docker/docker-compose.infra.yml</code> and <code>docker/docker-compose.warehouse.yml</code>)</li> </ol> <p>Can't Connect to Services</p> <p>If you're unable to connect to Vulcan services, verify that all required services are running:</p> <pre><code>docker compose -f docker/docker-compose.infra.yml ps\ndocker compose -f docker/docker-compose.warehouse.yml ps\n</code></pre> <p>All services should show as \"Up\" or \"running\". If any service shows as \"Exited\" or \"Stopped\", check the logs:</p> <pre><code>docker compose -f docker/docker-compose.infra.yml logs\n</code></pre> <p>Access MinIO Console</p> <p>You can access the MinIO console to manage your object storage:</p> <ul> <li> <p>URL: <code>http://localhost:9001</code></p> </li> <li> <p>Username: <code>admin</code></p> </li> <li> <p>Password: <code>password</code></p> </li> </ul> <p>The MinIO console allows you to browse buckets, upload files, and manage storage policies.</p> <p>Permission denied</p> <p>Create a .logs folder manually and change the permission  </p><pre><code>chmod -R a+w .\n</code></pre><p></p>"},{"location":"guides/get-started/docker/#next-steps","title":"Next Steps","text":"<p>You've set up Vulcan and created your first project. Here are recommended next steps:</p> <ul> <li> <p>Learn more about Vulcan CLI commands - Explore all available commands and their options</p> </li> <li> <p>Explore Vulcan concepts - Deep dive into how models work and how to structure your data pipeline</p> </li> <li> <p>Read the model kinds documentation - Understand different model types and when to use them</p> </li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/","title":"Vulcan Docker Quickstart","text":""},{"location":"guides/get-started/zip-mac/vulcan-project/#vulcan-docker-quickstart","title":"Vulcan Docker Quickstart","text":"<p>This package contains all the Docker Compose files and configuration needed to get started with Vulcan using Docker.</p>"},{"location":"guides/get-started/zip-mac/vulcan-project/#contents","title":"Contents","text":"<ul> <li> <p><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</p> </li> <li> <p><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</p> </li> <li> <p><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</p> </li> <li> <p><code>Makefile</code> - Convenient commands for managing services</p> </li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Docker Desktop installed and running</p> </li> <li> <p>Docker Compose (included with Docker Desktop)</p> </li> <li> <p>At least 4GB of available RAM</p> </li> <li> <p>A terminal/command line interface</p> </li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li>Start all infrastructure: <pre><code>make setup\n</code></pre>    This will:    - Create the Docker network</li> </ol> <ul> <li> <p>Start statestore (PostgreSQL) on port 5431</p> </li> <li> <p>Start MinIO object storage on ports 9000 and 9001</p> </li> <li> <p>Start warehouse database (PostgreSQL) on port 5433</p> </li> </ul> <ol> <li> <p>Access Vulcan: </p><pre><code>alias vulcan=\"docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan\"\n</code></pre><p></p> </li> <li> <p>Initialize your project: </p><pre><code>vulcan init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code># Project identity\nname: orders-analytics\ndisplay_name: Orders Analytics Platform\ntenant: engineering\ndescription: Orders Analytics delivers insights across the full order lifecycle.\n\n# Classification\ntags:\n  - postgres\n  - sales_analytics\n  - demo\n\nterms:\n  - glossary.data_product\n  - glossary.sales_operations\n\n# Metadata\nmetadata:\n  domain: sales_operations\n  use_cases:\n    - Daily sales reporting\n    - Customer analytics\n  limitations:\n    - Demo dataset with synthetic data\n\n# Gateway Connection\ngateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      type: postgres\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\n# Model Defaults (required)\nmodel_defaults:\n  dialect: postgres\n  start: 2024-01-01\n  cron: '@daily'\n\n# Linting Rules\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-mac/vulcan-project/#available-make-commands","title":"Available Make Commands","text":"<ul> <li> <p><code>make setup</code> - Run all setup steps</p> </li> <li> <p><code>make vulcan-up</code> - Start Vulcan API services</p> </li> <li> <p><code>make network</code> - Create the Docker network</p> </li> <li> <p><code>make infra</code> - Start infrastructure services</p> </li> <li> <p><code>make warehouse</code> - Start warehouse database</p> </li> <li> <p><code>make all-down</code> - Stop all services</p> </li> <li> <p><code>make all-clean</code> - Stop all services and remove volumes</p> </li> </ul>"},{"location":"guides/get-started/zip-mac/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li> <p>Statestore: 5431</p> </li> <li> <p>Warehouse: 5433</p> </li> <li> <p>MinIO API: 9000</p> </li> <li> <p>MinIO Console: 9001 (admin/password)</p> </li> <li> <p>Vulcan API: 8000</p> </li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/","title":"Vulcan Docker Quickstart - Windows","text":""},{"location":"guides/get-started/zip-window/vulcan-project/#vulcan-docker-quickstart-windows","title":"Vulcan Docker Quickstart - Windows","text":"<p>This package contains all the Docker Compose files and Windows batch scripts needed to get started with Vulcan using Docker on Windows.</p>"},{"location":"guides/get-started/zip-window/vulcan-project/#contents","title":"Contents","text":"<ul> <li> <p><code>docker/docker-compose.infra.yml</code> - Infrastructure services (statestore, MinIO)</p> </li> <li> <p><code>docker/docker-compose.warehouse.yml</code> - Warehouse database (PostgreSQL)</p> </li> <li> <p><code>docker/docker-compose.vulcan.yml</code> - Vulcan API and transpiler services</p> </li> <li> <p><code>setup.bat</code> - Setup script to start all infrastructure</p> </li> <li> <p><code>vulcan.bat</code> - Wrapper script to run Vulcan CLI commands</p> </li> <li> <p><code>start-vulcan-api.bat</code> - Start Vulcan API services</p> </li> <li> <p><code>stop-all.bat</code> - Stop all services</p> </li> <li> <p><code>clean.bat</code> - Stop all services and remove volumes</p> </li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Docker Desktop for Windows installed and running</p> </li> <li> <p>At least 4GB of available RAM</p> </li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#quick-start","title":"Quick Start","text":"<ol> <li>Run the setup script: <pre><code>setup.bat\n</code></pre>    This will:    - Create the Docker network</li> </ol> <ul> <li> <p>Start statestore (PostgreSQL) on port 5431</p> </li> <li> <p>Start MinIO object storage on ports 9000 and 9001</p> </li> <li> <p>Start warehouse database (PostgreSQL) on port 5433</p> </li> </ul> <ol> <li>Access Vulcan: <pre><code>vulcan.bat\n</code></pre></li> <li> <p>Initialize your project: </p><pre><code>vulcan.bat init\n</code></pre><p></p> </li> <li> <p>Update your <code>config.yaml</code> to match the Docker setup:    </p><pre><code># Project identity\nname: orders-analytics\ndisplay_name: Orders Analytics Platform\ntenant: engineering\ndescription: Orders Analytics delivers insights across the full order lifecycle.\n\n# Classification\ntags:\n  - postgres\n  - sales_analytics\n  - demo\n\nterms:\n  - glossary.data_product\n  - glossary.sales_operations\n\n# Metadata\nmetadata:\n  domain: sales_operations\n  use_cases:\n    - Daily sales reporting\n    - Customer analytics\n  limitations:\n    - Demo dataset with synthetic data\n\n# Gateway Connection\ngateways:\n  default:\n    connection:\n      type: postgres\n      host: warehouse\n      port: 5432\n      database: warehouse\n      user: vulcan\n      password: vulcan\n    state_connection:\n      type: postgres\n      host: statestore\n      port: 5432\n      database: statestore\n      user: vulcan\n      password: vulcan\n\ndefault_gateway: default\n\n# Model Defaults (required)\nmodel_defaults:\n  dialect: postgres\n  start: 2024-01-01\n  cron: '@daily'\n\n# Linting Rules\nlinter:\n  enabled: true\n  rules:\n    - ambiguousorinvalidcolumn\n    - invalidselectstarexpansion\n</code></pre><p></p> </li> <li> <p>Create and apply your first plan: </p><pre><code>vulcan.bat plan\n</code></pre><p></p> </li> </ol>"},{"location":"guides/get-started/zip-window/vulcan-project/#available-scripts","title":"Available Scripts","text":"<ul> <li> <p><code>setup.bat</code> - Create network and start all infrastructure</p> </li> <li> <p><code>start-vulcan-api.bat</code> - Start Vulcan API services</p> </li> <li> <p><code>stop-all.bat</code> - Stop all services</p> </li> <li> <p><code>clean.bat</code> - Stop all services and remove volumes</p> </li> </ul>"},{"location":"guides/get-started/zip-window/vulcan-project/#service-ports","title":"Service Ports","text":"<ul> <li> <p>Statestore: 5431</p> </li> <li> <p>Warehouse: 5433</p> </li> <li> <p>MinIO API: 9000</p> </li> <li> <p>MinIO Console: 9001 (admin/password)</p> </li> <li> <p>Vulcan API: 8000 ```</p> </li> </ul>"}]}