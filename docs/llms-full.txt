# CI/CD

Source: https://tmdc-io.github.io/vulcan-book/ci-cd/ci-cd/

---

# CI/CD

Coming soon...



# CLI Commands

Source: https://tmdc-io.github.io/vulcan-book/cli-command/cli/

---

# CLI Commands

The Vulcan CLI is your primary interface for working with your data pipeline. You'll use it to plan changes, run models, check data quality, and manage your project. Here's everything you need to know about the commands available to you.

```
Usage: vulcan [OPTIONS] COMMAND [ARGS]...

  Vulcan command line tool.

Options:
  --version            Show the version and exit.
  -p, --paths TEXT     Path(s) to the Vulcan config/project.
  --config TEXT        Name of the config object. Only applicable to
                       configuration defined using Python script.
  --gateway TEXT       The name of the gateway.
  --ignore-warnings    Ignore warnings.
  --debug              Enable debug mode.
  --log-to-stdout      Display logs in stdout.
  --log-file-dir TEXT  The directory to write log files to.
  --dotenv PATH        Path to a custom .env file to load environment
                       variables.
  --help               Show this message and exit.

Commands:
  api                     Start the Vulcan API server (models, metrics,...
  audit                   Run audits for the target model(s).
  check_intervals         Show missing intervals in an environment,...
  clean                   Clears the Vulcan cache and any build artifacts.
  create_external_models  Create a schema file containing external model...
  create_test             Generate a unit test fixture for a given model.
  dag                     Render the DAG as an html file.
  destroy                 The destroy command removes all project resources.
  diff                    Show the diff between the local state and the...
  dlt_refresh             Attaches to a DLT pipeline with the option to...
  environments            Prints the list of Vulcan environments with its...
  evaluate                Evaluate a model and return a dataframe with a...
  fetchdf                 Run a SQL query and display the results.
  format                  Format all SQL models and audits.
  graphql                 Manage the GraphQL service (subcommands: up,...
  info                    Print information about a Vulcan project.
  invalidate              Invalidate the target environment, forcing its...
  janitor                 Run the janitor process on-demand.
  lint                    Run the linter for the target model(s).
  migrate                 Migrate Vulcan to the current running version.
  plan                    Apply local changes to the target environment.
  render                  Render a model's query, optionally expanding...
  rollback                Rollback Vulcan to the previous migration.
  run                     Evaluate missing intervals for the target...
  semantic                Semantic layer operations.
  state                   Commands for interacting with state
  table_diff              Show the diff between two tables or a selection...
  table_name              Prints the name of the physical table for the...
  test                    Run model unit tests.
  transpile               Transpile a semantic SQL or REST-style semantic...
  transpiler              Manage the Transpiler service (subcommands: up,...
```

## audit

Run data quality audits for your models. This command executes all the audits you've defined in your models and reports which ones pass or fail. It's perfect for validating data quality before deploying changes.

```
Usage: vulcan audit [OPTIONS]

  Run audits for the target model(s).

Options:
  --model TEXT           A model to audit. Multiple models can be audited.
  -s, --start TEXT       The start datetime of the interval for which this
                         command will be applied.
  -e, --end TEXT         The end datetime of the interval for which this
                         command will be applied.
  --execution-time TEXT  The execution time (defaults to now).
  --help                 Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan audit
      Found 11 audit(s).
      unique_values on model sales.daily_sales ✅ PASS.
      not_null on model sales.daily_sales ✅ PASS.
      positive_values on model sales.daily_sales ✅ PASS.
      positive_values on model sales.daily_sales ✅ PASS.
      unique_values on model raw.raw_products ✅ PASS.
      not_null on model raw.raw_products ✅ PASS.
      unique_values on model raw.raw_customers ✅ PASS.
      not_null on model raw.raw_customers ✅ PASS.
      unique_values on model raw.raw_orders ✅ PASS.
      not_null on model raw.raw_orders ✅ PASS.
      positive_values on model raw.raw_orders ✅ PASS.

      Finished with 0 audit errors and 0 audits skipped.
      Done.
    ```

## check_intervals

Check which time intervals are missing for your models in a given environment. This is super useful for understanding what data needs to be backfilled or processed. By default, it respects signals (like upstream dependencies), but you can disable that if you want to see all missing intervals.

```
Usage: vulcan check_intervals [OPTIONS] [ENVIRONMENT]

  Show missing intervals in an environment, respecting signals.

Options:
  --no-signals         Disable signal checks and only show missing intervals.
  --select-model TEXT  Select specific models to show missing intervals for.
  -s, --start TEXT     The start datetime of the interval for which this
                       command will be applied.
  -e, --end TEXT       The end datetime of the interval for which this command
                       will be applied.
  --help               Show this message and exit.
```


## clean

Clear out Vulcan's cache and any build artifacts. This is handy when you're troubleshooting issues or want to start fresh. Don't worry, it won't delete your models or data, just the cached files.

```
Usage: vulcan clean [OPTIONS]

  Clears the Vulcan cache and any build artifacts.

Options:
  --help  Show this message and exit.
```

## create_external_models

Generate a schema file for external models that Vulcan can reference. This is useful when you're working with tables or views that exist outside of your Vulcan project but need to be referenced in your models.

```
Usage: vulcan create_external_models [OPTIONS]

  Create a schema file containing external model schemas.

Options:
  --help  Show this message and exit.
```
??? example "Example"

    ```
    $ vulcan create_external_models
    ```


## create_test

Generate a unit test fixture for a model. This command creates the test file structure and can even generate sample data based on queries you provide. It's a great way to quickly set up tests for your models without writing all the boilerplate yourself.

```
Usage: vulcan create_test [OPTIONS] MODEL

  Generate a unit test fixture for a given model.

Options:
  -q, --query <TEXT TEXT>...  Queries that will be used to generate data for
                              the model's dependencies.
  -o, --overwrite             When true, the fixture file will be overwritten
                              in case it already exists.
  -v, --var <TEXT TEXT>...    Key-value pairs that will define variables
                              needed by the model.
  -p, --path TEXT             The file path corresponding to the fixture,
                              relative to the test directory. By default, the
                              fixture will be created under the test directory
                              and the file name will be inferred based on the
                              test's name.
  -n, --name TEXT             The name of the test that will be created. By
                              default, it's inferred based on the model's
                              name.
  --include-ctes              When true, CTE fixtures will also be generated.
  --help                      Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan create_test sales.daily_sales --query raw.raw_orders "SELECT * FROM raw.raw_orders"
    ```

## dag

Generate a visual representation of your data pipeline's dependency graph (DAG) as an HTML file. This is super helpful for understanding how your models connect and visualizing the flow of data through your pipeline. You can open the HTML file in any browser to explore the graph interactively.

```
Usage: vulcan dag [OPTIONS] FILE

  Render the DAG as an html file.

Options:
  --select-model TEXT  Select specific models to include in the dag.
  --help               Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan dag ./dag.html
    ```

## destroy

⚠️ **Use with caution!** This command permanently removes all Vulcan-managed resources from your data warehouse, including state tables, the cache, and all project resources. It will delete all tables, views, and schemas that Vulcan manages, as well as any external resources created by other tools within those schemas. This is a destructive operation that can't be undone, so make sure you really want to do this before running it.

```
Usage: vulcan destroy

  Removes all state tables, the Vulcan cache and all project resources, including warehouse objects. This includes all tables, views and schemas managed by Vulcan, as well as any external resources that may have been created by other tools within those schemas.

Options:
  --help               Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan destroy
    [WARNING] This will permanently delete all engine-managed objects, state tables and Vulcan cache.
    The operation may disrupt any currently running or scheduled plans.

    Schemas to be deleted:
      • warehouse.raw
      • warehouse.sales

    Snapshot tables to be deleted:
      • warehouse.vulcan__raw.raw__raw_customers__1474975870
      • warehouse.vulcan__raw.raw__raw_orders__1032938324
      • warehouse.vulcan__raw.raw__raw_products__3337559381
      • warehouse.vulcan__sales.sales__daily_sales__2671854529

    This action will DELETE ALL the above resources managed by Vulcan AND
    potentially external resources created by other tools in these schemas.

    Are you ABSOLUTELY SURE you want to proceed with deletion? [y/n]: y
    Environment 'prod' invalidated.

    Deleted object warehouse.raw
    Deleted object warehouse.sales
    Deleted object warehouse.vulcan__raw.raw__raw_products__3337559381__dev
    Deleted object warehouse.vulcan__raw.raw__raw_customers__1474975870__dev
    Deleted object warehouse.vulcan__sales.sales__daily_sales__2671854529__dev
    Deleted object warehouse.vulcan__sales.sales__daily_sales__2671854529
    Deleted object warehouse.vulcan__raw.raw__raw_customers__1474975870
    Deleted object warehouse.vulcan__raw.raw__raw_products__3337559381
    Deleted object warehouse.vulcan__raw.raw__raw_orders__1032938324__dev
    Deleted object warehouse.vulcan__raw.raw__raw_orders__1032938324
    State tables removed.
    Destroy completed successfully.
    ```


## dlt_refresh

```
Usage: dlt_refresh PIPELINE [OPTIONS]

  Attaches to a DLT pipeline with the option to update specific or all models of the Vulcan project.

Options:
  -t, --table TEXT  The DLT tables to generate Vulcan models from. When none specified, all new missing tables will be generated.
  -f, --force       If set it will overwrite existing models with the new generated models from the DLT tables.
  --help            Show this message and exit.
```

## diff

See exactly what's different between your local project state and a target environment. This is super useful for understanding what changes you're about to deploy before running a plan. It shows you model changes, semantic layer changes, and quality check modifications in a clear diff format.

```
Usage: vulcan diff [OPTIONS] ENVIRONMENT

  Show the diff between the local state and the target environment.

Options:
  --help  Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan diff prod

    Differences from the `prod` environment:

    Models:
    └── Directly Modified:
        └── sales.daily_sales
            --- .../daily_sales.sql

            +++ .../daily_sales.sql

            @@ -20,10 +20,11 @@

              grains (order_date)
            )
            SELECT
              CAST(order_date AS TIMESTAMP) AS order_date,
              CAST(COUNT(order_id) AS INT) AS total_orders,
              CAST(SUM(total_amount) AS DOUBLE PRECISION) AS total_revenue,
            -  CAST(MAX(order_id) AS VARCHAR) AS last_order_id
            +  CAST(MAX(order_id) AS VARCHAR) AS last_order_id,
            +  COUNT(DISTINCT product_id) AS total_products
            FROM raw.raw_orders
            GROUP BY
              order_date
    Semantics:
    └── Indirectly Modified:
        ├── semantic-model:sales.daily_sales
        ├── semantic-metric:order_volume
        └── semantic-metric:revenue_trends
    Quality Checks:
    └── Indirectly Modified:
        ├── check-suite:sales.daily_sales:accuracy
        ├── check-suite:sales.daily_sales:timeliness
        ├── check-suite:sales.daily_sales:completeness
        └── check-suite:sales.daily_sales:validity
    ```

## environments

List all your Vulcan environments and see when they expire. This is helpful for managing development environments and understanding which ones might need cleanup.

```
Usage: vulcan environments [OPTIONS]

  Prints the list of Vulcan environments with its expiry datetime.

Options:
  --help             Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan environments
    Number of Vulcan environments are: 2
    prod - No Expiry
    dev - 2025-12-23 00:00:00
    ```

## evaluate

Preview a model's output without actually materializing it. This is perfect for debugging and quick iteration, you can see what your model would produce without running a full plan or run. By default, it returns up to 1000 rows, but you can adjust that limit.

```
Usage: vulcan evaluate [OPTIONS] MODEL

  Evaluate a model and return a dataframe with a default limit of 1000.

Options:
  -s, --start TEXT       The start datetime of the interval for which this
                         command will be applied.
  -e, --end TEXT         The end datetime of the interval for which this
                         command will be applied.
  --execution-time TEXT  The execution time (defaults to now).
  --limit INTEGER        The number of rows which the query should be limited
                         to.
  --help                 Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan evaluate sales.daily_sales
       order_date  total_orders  total_revenue last_order_id  total_products
    0  2024-01-05             1          70.77          O001               1
    1  2024-01-10             1          44.22          O002               1
    2  2024-01-15             1          65.52          O003               1
    3  2024-01-20             1          79.42          O004               1
    4  2024-02-01             1          91.35          O005               1
    ....
    19 2024-05-15             1          38.38          O020               1
    ```

## fetchdf

Run a raw SQL query against your data warehouse and see the results. This is handy for quick data exploration or debugging queries without opening a separate database client.

```
Usage: vulcan fetchdf [OPTIONS] SQL

  Run a SQL query and display the results.

Options:
  --help  Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan fetchdf "select count(*) from sales.daily_sales"
    ┏━━━━━━━┓
    ┃ count ┃
    ┡━━━━━━━┩
    │ 20    │
    └───────┘
    ```

## format

Automatically format all your SQL models and audits according to your formatting preferences. This helps keep your codebase consistent and readable. You can customize formatting options like indentation, comma placement, and function name casing.

```
Usage: vulcan format [OPTIONS]

  Format all SQL models and audits.

Options:
  -t, --transpile TEXT        Transpile project models to the specified
                              dialect.
  --append-newline            Include a newline at the end of each file.
  --no-rewrite-casts          Preserve the existing casts, without rewriting
                              them to use the :: syntax.
  --normalize                 Whether or not to normalize identifiers to
                              lowercase.
  --pad INTEGER               Determines the pad size in a formatted string.
  --indent INTEGER            Determines the indentation size in a formatted
                              string.
  --normalize-functions TEXT  Whether or not to normalize all function names.
                              Possible values are: 'upper', 'lower'
  --leading-comma             Determines whether or not the comma is leading
                              or trailing in select expressions. Default is
                              trailing.
  --max-text-width INTEGER    The max number of characters in a segment before
                              creating new lines in pretty mode.
  --check                     Whether or not to check formatting (but not
                              actually format anything).
  --help                      Show this message and exit.
```

## info

Get a quick overview of your Vulcan project. This command shows you how many models and macros you have, and it tests your connections to both the data warehouse and state backend. It's a great first command to run when setting up a new project or troubleshooting connection issues.

```
Usage: vulcan info [OPTIONS]

  Print information about a Vulcan project.

  Includes counts of project models and macros and connection tests for the
  data warehouse.

Options:
  --skip-connection  Skip the connection test.
  -v, --verbose      Verbose output.
  --help  Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan info
    Models: 4
    Macros: 0
    Data warehouse connection succeeded
    State backend connection succeeded
    ```

## init

Initialize a new Vulcan project. This sets up the basic project structure and configuration files you'll need to get started. You can choose from different templates (like dbt or DLT) or start with an empty project.

```
Usage: vulcan init [OPTIONS] [ENGINE]

  Create a new Vulcan repository.

Options:
  -t, --template TEXT  Project template. Supported values: dbt, dlt, default,
                       empty.
  --dlt-pipeline TEXT  DLT pipeline for which to generate a Vulcan project.
                       Use alongside template: dlt
  --dlt-path TEXT      The directory where the DLT pipeline resides. Use
                       alongside template: dlt
  --help               Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan init postgres
    ```

## invalidate

Mark an environment for deletion. The janitor process will clean it up on its next run. This is useful when you want to remove a development environment that's no longer needed. By default, the deletion happens asynchronously, but you can use `--sync` to wait for it to complete immediately.

```
Usage: vulcan invalidate [OPTIONS] ENVIRONMENT

  Invalidate the target environment, forcing its removal during the next run
  of the janitor process.

Options:
  -s, --sync  Wait for the environment to be deleted before returning. If not
              specified, the environment will be deleted asynchronously by the
              janitor process. This option requires a connection to the data
              warehouse.
  --help      Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan invalidate dev
    Environment 'dev' invalidated.
    ```

## janitor

Run the janitor process manually to clean up old environments and expired snapshots. Normally, the janitor runs automatically, but sometimes you might want to trigger it immediately to free up space or clean up resources right away.

```
Usage: vulcan janitor [OPTIONS]

  Run the janitor process on-demand.

  The janitor cleans up old environments and expired snapshots.

Options:
  --ignore-ttl  Cleanup snapshots that are not referenced in any environment,
                regardless of when they're set to expire
  --help        Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan janitor
    Deleted object warehouse.sales__dev
    Deleted object warehouse.raw__dev
    Cleanup complete.
    ```

## migrate

Upgrade Vulcan's internal state to match the current version you're running. This is typically needed when you upgrade Vulcan itself. **Important:** This command affects all Vulcan users, so make sure to coordinate with your team and contact your Vulcan administrator before running it.

```
Usage: vulcan migrate [OPTIONS]

  Migrate Vulcan to the current running version.

Options:
  --help  Show this message and exit.
```

!!! danger "Caution"

    The `migrate` command affects all Vulcan users. Contact your Vulcan administrator before running.

## plan

Create and apply a plan that compares your local project state with a target environment and determines what changes need to be made. This is one of the most important commands, it's how you deploy model changes, add new models, and backfill data. The plan shows you exactly what will happen before it happens, so you can review changes before they're applied.

```
Usage: vulcan plan [OPTIONS] [ENVIRONMENT]

  Apply local changes to the target environment.

Options:
  -s, --start TEXT                The start datetime of the interval for which
                                  this command will be applied.
  -e, --end TEXT                  The end datetime of the interval for which
                                  this command will be applied.
  --execution-time TEXT           The execution time (defaults to now).
  --create-from TEXT              The environment to create the target
                                  environment from if it doesn't exist.
                                  Default: prod.
  --skip-tests                    Skip tests prior to generating the plan if
                                  they are defined.
  --skip-linter                   Skip linting prior to generating the plan if
                                  the linter is enabled.
  -r, --restate-model TEXT        Restate data for specified models and models
                                  downstream from the one specified. For
                                  production environment, all related model
                                  versions will have their intervals wiped,
                                  but only the current versions will be
                                  backfilled. For development environment,
                                  only the current model versions will be
                                  affected.
  --no-gaps                       Ensure that new snapshots have no data gaps
                                  when comparing to existing snapshots for
                                  matching models in the target environment.
  --skip-backfill, --dry-run      Skip the backfill step and only create a
                                  virtual update for the plan.
  --empty-backfill                Produce empty backfill. Like --skip-backfill
                                  no models will be backfilled, unlike --skip-
                                  backfill missing intervals will be recorded
                                  as if they were backfilled.
  --forward-only                  Create a plan for forward-only changes.
  --allow-destructive-model TEXT  Allow destructive forward-only changes to
                                  models whose names match the expression.
  --allow-additive-model TEXT     Allow additive forward-only changes to
                                  models whose names match the expression.
  --effective-from TEXT           The effective date from which to apply
                                  forward-only changes on production.
  --no-prompts                    Disable interactive prompts for the backfill
                                  time range. Please note that if this flag is
                                  set and there are uncategorized changes,
                                  plan creation will fail.
  --auto-apply                    Automatically apply the new plan after
                                  creation.
  --no-auto-categorization        Disable automatic change categorization.
  --include-unmodified            Include unmodified models in the target
                                  environment.
  --select-model TEXT             Select specific model changes that should be
                                  included in the plan.
  --backfill-model TEXT           Backfill only the models whose names match
                                  the expression.
  --no-diff                       Hide text differences for changed models.
  --run                           Run latest intervals as part of the plan
                                  application (prod environment only).
  --enable-preview                Enable preview for forward-only models when
                                  targeting a development environment.
  --diff-rendered                 Output text differences for the rendered
                                  versions of the models and standalone
                                  audits.
  --explain                       Explain the plan instead of applying it.
  --ignore-cron                   Run all missing intervals, ignoring
                                  individual cron schedules. Only applies if
                                  --run is set.
  --min-intervals INTEGER         For every model, ensure at least this many
                                  intervals are covered by a missing intervals
                                  check regardless of the plan start date
  -v, --verbose                   Verbose output. Use -vv for very verbose
                                  output.
  --help                          Show this message and exit.
```

## api

Start Vulcan's API server, which provides programmatic access to models, metrics, lineage information, and telemetry. This is useful if you want to integrate Vulcan with other tools or build custom dashboards and applications on top of your data pipeline.

```
Usage: vulcan api [OPTIONS]

  Start the Vulcan API server (models, metrics, lineage, telemetry).

Options:
  --host TEXT        Bind socket to this host. Default: 0.0.0.0
  --port INTEGER     Bind socket to this port. Default: 8000
  --reload           Enable auto-reload on file changes. Default: False
  --workers INTEGER  Number of worker processes. Default: 1
  --help             Show this message and exit.
```

## render

See the actual SQL that Vulcan will execute for a model. This is super helpful for debugging and understanding how Vulcan transforms your model definitions into executable SQL. You can optionally expand referenced models to see the full query with all dependencies inlined.

```
Usage: vulcan render [OPTIONS] MODEL

  Render a model's query, optionally expanding referenced models.

Options:
  -s, --start TEXT            The start datetime of the interval for which
                              this command will be applied.
  -e, --end TEXT              The end datetime of the interval for which this
                              command will be applied.
  --execution-time TEXT       The execution time (defaults to now).
  --expand TEXT               Whether or not to expand materialized models
                              (defaults to False). If True, all referenced
                              models are expanded as raw queries. Multiple
                              model names can also be specified, in which case
                              only they will be expanded as raw queries.
  --dialect TEXT              The SQL dialect to render the query as.
  --no-format                 Disable fancy formatting of the query.
  --max-text-width INTEGER    The max number of characters in a segment before
                              creating new lines in pretty mode.
  --leading-comma             Determines whether or not the comma is leading
                              or trailing in select expressions. Default is
                              trailing.
  --normalize-functions TEXT  Whether or not to normalize all function names.
                              Possible values are: 'upper', 'lower'
  --indent INTEGER            Determines the indentation size in a formatted
                              string.
  --pad INTEGER               Determines the pad size in a formatted string.
  --normalize                 Whether or not to normalize identifiers to
                              lowercase.
  --help                      Show this message and exit.
```

??? example "Example"

    ```
    $ vulcan render sales.daily_sales

    SELECT
      CAST("raw_orders"."order_date" AS TIMESTAMP) AS "order_date",
      CAST(COUNT("raw_orders"."order_id") AS INT) AS "total_orders",
      CAST(SUM("raw_orders"."total_amount") AS DOUBLE PRECISION) AS "total_revenue",
      CAST(MAX("raw_orders"."order_id") AS VARCHAR) AS "last_order_id",
      COUNT(DISTINCT "raw_orders"."product_id") AS "total_products"
    FROM "warehouse"."vulcan__raw"."raw__raw_orders__1032938324" AS "raw_orders" /* warehouse.raw.raw_orders */
    GROUP BY
      "raw_orders"."order_date"
    ORDER BY
      "order_date"
    ```

## rollback

Revert Vulcan's internal state to the previous migration version. This is useful if a migration caused issues and you need to go back. **Important:** Like `migrate`, this command affects all Vulcan users, so coordinate with your team and contact your Vulcan administrator before running it.

```
Usage: vulcan rollback [OPTIONS]

  Rollback Vulcan to the previous migration.

Options:
  --help  Show this message and exit.
```

!!! danger "Caution"

    The `rollback` command affects all Vulcan users. Contact your Vulcan administrator before running.

## run

Process missing time intervals for your models in a target environment. This is different from `plan`, `run` focuses on executing scheduled work based on cron schedules, while `plan` handles deploying changes. Use `run` when you want to process new or missing data without making any model definition changes.

```
Usage: vulcan run [OPTIONS] [ENVIRONMENT]

  Evaluate missing intervals for the target environment.

Options:
  -s, --start TEXT              The start datetime of the interval for which
                                this command will be applied.
  -e, --end TEXT                The end datetime of the interval for which
                                this command will be applied.
  --skip-janitor                Skip the janitor task.
  --ignore-cron                 Run for all missing intervals, ignoring
                                individual cron schedules.
  --select-model TEXT           Select specific models to run. Note: this
                                always includes upstream dependencies.
  --exit-on-env-update INTEGER  If set, the command will exit with the
                                specified code if the run is interrupted by an
                                update to the target environment.
  --no-auto-upstream            Do not automatically include upstream models.
                                Only applicable when --select-model is used.
                                Note: this may result in missing / invalid
                                data for the selected models.
  --help                        Show this message and exit.
```

## state

Manage Vulcan's state database. This includes exporting state for backup or migration purposes, and importing state from another environment. These commands are useful for disaster recovery, environment cloning, or moving state between systems.

```
Usage: vulcan state [OPTIONS] COMMAND [ARGS]...

  Commands for interacting with state

Options:
  --help  Show this message and exit.

Commands:
  export  Export the state database to a file
  import  Import a state export file back into the state database
```

### export

Export Vulcan's state database to a file. This creates a backup of your state that you can use for recovery or to move state between environments. You can export specific environments or all of them.

```
Usage: vulcan state export [OPTIONS]

  Export the state database to a file

Options:
  -o, --output-file FILE  Path to write the state export to  [required]
  --environment TEXT      Name of environment to export. Specify multiple
                          --environment arguments to export multiple
                          environments
  --local                 Export local state only. Note that the resulting
                          file will not be importable
  --no-confirm            Do not prompt for confirmation before exporting
                          existing state
  --help                  Show this message and exit.
```

### import

Import a previously exported state file back into the state database. This is useful for restoring from backups or copying state from one environment to another. By default, it merges with existing state, but you can use `--replace` to completely replace it.

```
Usage: vulcan state import [OPTIONS]

  Import a state export file back into the state database

Options:
  -i, --input-file FILE  Path to the state file  [required]
  --replace              Clear the remote state before loading the file. If
                         omitted, a merge is performed instead
  --no-confirm           Do not prompt for confirmation before updating
                         existing state
  --help                 Show this message and exit.
```

## table_diff

Compare data between two tables or models to see what's different. This is super useful for validating that changes produce the expected results, comparing environments, or debugging data discrepancies. You can compare entire tables or specific models, and customize how the comparison works.

```
Usage: vulcan table_diff [OPTIONS] SOURCE:TARGET [MODEL]

  Show the diff between two tables or a selection of models when they are
  specified.

Options:
  -o, --on TEXT            The column to join on. Can be specified multiple
                           times. The model grain will be used if not
                           specified.
  -s, --skip-columns TEXT  The column(s) to skip when comparing the source and
                           target table.
  --where TEXT             An optional where statement to filter results.
  --limit INTEGER          The limit of the sample dataframe.
  --show-sample            Show a sample of the rows that differ. With many
                           columns, the output can be very wide.
  -d, --decimals INTEGER   The number of decimal places to keep when comparing
                           floating point columns. Default: 3
  --skip-grain-check       Disable the check for a primary key (grain) that is
                           missing or is not unique.
  --warn-grain-check       Warn if any selected model is missing a grain,
                           and compute diffs for the remaining models.
  --temp-schema TEXT       Schema used for temporary tables. It can be
                           `CATALOG.SCHEMA` or `SCHEMA`. Default:
                           `vulcan_temp`
  -m, --select-model TEXT  Specify one or more models to data diff. Use
                           wildcards to diff multiple models. Ex: '*' (all
                           models with applied plan diffs), 'demo.model+'
                           (this and downstream models),
                           'git:feature_branch' (models with direct
                           modifications in this branch only)
  --help                   Show this message and exit.
```

## table_name

Get the actual physical table name that Vulcan uses for a model. This is helpful when you need to reference the table directly in SQL or other tools, since Vulcan's internal naming might differ from your model name.

```
Usage: vulcan table_name [OPTIONS] MODEL_NAME

  Prints the name of the physical table for the given model.

Options:
  --environment, --env TEXT  The environment to source the model version from.
  --prod                     If set, return the name of the physical table
                             that will be used in production for the model
                             version promoted in the target environment.
  --help                     Show this message and exit.
```

## test

Run unit tests for your models. These tests validate that your SQL logic works correctly with the test fixtures you've defined. It's a great way to catch bugs before deploying to production.

```
Usage: vulcan test [OPTIONS] [TESTS]...

  Run model unit tests.

Options:
  -k TEXT              Only run tests that match the pattern of substring.
  -v, --verbose        Verbose output.
  --preserve-fixtures  Preserve the fixture tables in the testing database,
                       useful for debugging.
  --help               Show this message and exit.
```

## semantic

Work with Vulcan's semantic layer. Currently, this includes exporting your semantic models and metrics to CubeJS-compatible YAML schemas, which lets you use your Vulcan semantic layer with other tools that support CubeJS.

```
Usage: vulcan semantic [OPTIONS] {export} [ENVIRONMENT]

  Semantic layer operations.

  This command provides semantic layer export functionality, allowing users to
  convert semantic models and metrics into CubeJS-compatible YAML schemas.

Options:
  -o, --output PATH   Output file path for the CubeJS schema.  [required]
  --strict            Strict mode: export only explicitly defined semantic
                      models.
  --no-auto-measures  Disable automatic generation of measures (e.g., _count)
                      for models with grains.
  --no-confirm        Do not prompt for confirmation before overwriting
                      existing output file.
  --help              Show this message and exit.
```

## transpile

Convert semantic SQL queries or REST-style semantic query payloads into executable, database-specific SQL. This is useful for debugging semantic queries, validating them, or understanding how Vulcan translates business-friendly queries into actual SQL.

```
Usage: vulcan transpile [OPTIONS] [QUERY]

  Transpile a semantic SQL or REST-style semantic query to executable SQL.

Options:
  --format [sql|rest]        Input type: semantic SQL ('sql') or REST-style
                             semantic payload ('rest').  [required]
  --file TEXT                Read query or REST payload from file. Use '-' to
                             read from stdin.
  --user TEXT                User id to propagate in the X-User header
                             (defaults to 'cli').
  --disable-post-processing  Disable post-processing in the Transpiler.
  --style [pretty|compact]   SQL output style: 'pretty' (formatted with
                             indentation), 'compact' (unformatted but
                             processed),
  --help                     Show this message and exit.
```

## transpiler

```
Usage: vulcan transpiler [OPTIONS] {up|down}

  Manage the Transpiler service (subcommands: up, down).

Options:
  --no-detach  Run docker compose in the foreground (omit -d).
  --help       Show this message and exit.
```

## graphql

```
Usage: vulcan graphql [OPTIONS] {up|down}

  Manage the GraphQL service (subcommands: up, down).

Options:
  --no-detach  Run docker compose in the foreground (omit -d).
  --help       Show this message and exit.
```

## lint

Run linting rules on your models to catch potential issues and enforce code quality standards. You can lint specific models or all models in your project. This is super helpful for maintaining consistent code quality and catching common mistakes early.

```
Usage: vulcan lint [OPTIONS]
  Run linter for the target model(s).

Options:
  --model TEXT           A model to lint. Multiple models can be linted.  If no models are specified, every model will be linted.
  --help                 Show this message and exit.

```



# Comparisons

Source: https://tmdc-io.github.io/vulcan-book/comparisons/

---

# Comparisons

**This documentation is a work in progress.**

There are many tools and frameworks in the data ecosystem. This page tries to make sense of it all.

If you are not familiar with Vulcan, it will be helpful to first read about Vulcan to better understand the comparisons. <!-- [Why Vulcan](index.md#why-vulcan) and [What is Vulcan](index.md#what-is-vulcan) -->

## dbt
[dbt](https://www.getdbt.com/) is a tool for data transformations. It is a pioneer in this space and has shown how valuable transformation frameworks can be. Although dbt is a fantastic tool, it has trouble scaling with data and organizational size.

dbt built their product focused on simple data transformations. By default, it fully refreshes data warehouses by executing templated SQL in the correct order.

Over time dbt has seen that data transformations are not enough to operate a scalable and robust data product. As a result, advanced features are patched in, such as state management (defer) and incremental loads, to try to address these needs while pushing the burden of correctness onto users with increased complexity. These "advanced" features make up some of the fundamental building blocks of a DataOps framework.

In other words, the challenge of implementing these features in dbt falls primarily on **you**: more jinja macro blocks, more manual configuration, more custom tooling, and more opportunities for error. We needed an easier, more reliable way, so we designed Vulcan from the ground up to be a robust DataOps framework.

Vulcan aims to be dbt format-compatible. Importing existing dbt projects with minor changes is in development.

### Feature comparisons
| Feature                           | dbt | Vulcan
| -------                           | --- | -------
| Modeling
| `SQL models`                      | ✅ | [✅](components/model/overview.md)
| `Python models`                   | ✅ | [✅✅](components/model/types/python_models.md)
| `Jinja support`                   | ✅ | ✅
| `Jinja macros`                    | ✅ | [✅](components/advanced-features/macros/jinja.md)
| `Python macros`                   | ❌ | [✅](components/advanced-features/macros/built_in.md)
| Validation
| `SQL semantic validation`         | ❌ | [✅](concepts-old/glossary.md#semantic-understanding)
| `Unit tests`                      | ❌ | [✅](components/tests/tests.md)
| `Table diff`                      | ❌ | ✅
| `Data audits`                     | ✅ | [✅](components/audits/audits.md)
| `Schema contracts`                | ✅ | [✅](guides/plan.md)
| `Data contracts`                  | ❌ | [✅](guides/plan.md)
| Deployment
| `Virtual Data Environments`       | ❌ | [✅](concepts-old/environments.md)
| `Open-source CI/CD bot`           | ❌ | ✅
| `Data consistency enforcement`    | ❌ | ✅
| Interfaces
| `CLI`                             | ✅ | [✅](getting_started/cli.md)
| `Paid UI`                         | ✅ | ❌
| `Open-source UI`                  | ❌ | ✅
| `Native Notebook Support`         | ❌ | ✅
| Visualization
| `Documentation generation`        | ✅ | ✅
| `Column-level lineage`            | ❌ | ✅
| Miscellaneous
| `Package manager`                 | ✅ | ❌
| `Multi-repository support`        | ❌ | ✅
| `SQL transpilation`               | ❌ | [✅](components/model/types/sql_models.md#transpilation)

### Environments
Development and staging environments in dbt are costly to make and not fully representative of what will go into production.

The standard approach to creating a new environment in dbt is to rerun your entire warehouse in a new environment. This may work at small scales, but even then it wastes time and money. Here's why:

The first part of running a data transformation system is repeatedly iterating through three steps: create or modify model code, execute the models, evaluate the outputs. Practitioners may repeat these steps many times in a day's work.

These steps incur costs to organizations: compute costs to run the models and staff time spent waiting on them to run. Inefficiencies compound rapidly because the steps are repeated so frequently.
dbt's default full refresh approach leads to the most costly version of this loop: recomputing every model every time.

Vulcan takes another approach. It examines the code modifications and the dependency structure among the models to determine which models are affected -- and executes only those models. This results in the least costly version of the loop: computing only what is required every time through.

This enables Vulcan to provide efficient isolated [Virtual Environments](guides/plan.md#plan-application). Environments in dbt cost compute and storage, but creating a development environment in Vulcan is free -- you can quickly access a full replica of any other environment with a single command.

Additionally, Vulcan ensures that promotion of staging environments to production is predictable and consistent. There is no concept of promotion in dbt, so queries are all rerun when it's time to deploy something. In Vulcan, promotions are simple pointer swaps so there is no wasted compute.

### Incremental models
Implementing incremental models is difficult and error-prone in dbt because it does not keep track of state.

#### Complexity
Since there is no state of which incremental intervals have already run in dbt, users must write and maintain subqueries to find missing date boundaries themselves:

```sql
-- dbt incremental
SELECT *
FROM {{ ref(raw.events) }} e
JOIN {{ ref(raw.event_dims) }} d
  ON e.id = d.id
-- must specify the is_incremental flag because this predicate will fail if the model has never run before
{% if is_incremental() %}
    -- this filter dynamically scans the current model to find the date boundary
    AND d.ds >= (SELECT MAX(ds) FROM {{ this }})
{% endif %}
{% if is_incremental() %}
  WHERE e.ds >= (SELECT MAX(ds) FROM {{ this }})
{% endif %}
```

Manually specifying macros to find date boundaries is repetitive and error-prone.

The example above shows how incremental models behave differently in dbt depending on whether they have been run before. As models become more complex, the cognitive burden of having two run times, "first time full refresh" vs. "subsequent incremental", increases.

Vulcan keeps track of which date ranges exist, producing a simplified and efficient query as follows:

```sql
-- Vulcan incremental
SELECT *
FROM raw.events e
JOIN raw.event_dims d
  -- date ranges are handled automatically by Vulcan
  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds
WHERE d.ds BETWEEN @start_ds AND @end_ds
```

#### Data leakage
dbt does not check whether the data inserted into an incremental table should be there or not. This can lead to problems and consistency issues, such as late-arriving data overriding past partitions. These problems are called "data leakage."

Vulcan wraps all queries in a subquery with a time filter under the hood to enforce that the data inserted for a particular batch is as expected and reproducible every time.

In addition, dbt only supports the 'insert/overwrite' incremental load pattern for systems that natively support it. Vulcan enables 'insert/overwrite' on any system, because it is the most robust approach to incremental loading, while 'Append' pipelines risk data inaccuracy in the variety of scenarios where your pipelines may run more than once for a given date.

This example shows the time filtering subquery Vulcan applies to all queries as a guard against data leakage:
```sql
-- original query
SELECT *
FROM raw.events
JOIN raw.event_dims d
  ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds
WHERE d.ds BETWEEN @start_ds AND @end_ds

-- with automated data leakage guard
SELECT *
FROM (
  SELECT *
  FROM raw.events
  JOIN raw.event_dims d
    ON e.id = d.id AND d.ds BETWEEN @start_ds AND @end_ds
  WHERE d.ds BETWEEN @start_ds AND @end_ds
)
WHERE ds BETWEEN @start_ds AND @end_ds
```

#### Data gaps
The main pattern used to implement incremental models in dbt is checking for the most recent data with MAX(date). This pattern does not catch missing data from the past, or "data gaps."

Vulcan stores each date interval a model has been run with, so it knows exactly what dates are missing:
```
Expected dates: 2022-01-01, 2022-01-02, 2022-01-03
Missing past data: ?, 2022-01-02, 2022-01-03
Data gap: 2022-01-01, ?, 2022-01-03
```

Vulcan will automatically fill these data gaps on the next run.

#### Performance
Subqueries that look for MAX(date) could have a performance impact on the primary query. Vulcan is able to avoid these extra subqueries.

Additionally, dbt expects an incremental model to be able to fully refresh the first time it runs. For some large data sets, this is cost-prohibitive or infeasible.

Vulcan is able to [batch](components/model/overview.md#batch_size) up backfills into more manageable chunks.

### SQL understanding
dbt heavily relies on [Jinja](https://jinja.palletsprojects.com/en/3.1.x/). It has no understanding of SQL and treats all queries as raw strings without context. This means that simple syntax errors like trailing commas are difficult to debug and require a full run to detect.

Vulcan supports Jinja, but it does not rely on it - instead, it parses/understands SQL through [SQLGlot](https://github.com/tobymao/sqlglot). Simple errors can be detected at compile time, so you no longer have to wait minutes or even longer to see that you've referenced a column incorrectly or missed a comma.

Additionally, having a first-class understanding of SQL supercharges Vulcan with features such as transpilation, column-level lineage, and automatic change categorization.

### Testing
Data quality checks such as detecting NULL values and duplicated rows are extremely valuable for detecting upstream data issues and large scale problems. However, they are not meant for testing edge cases or business logic, and they are not sufficient for creating robust data pipelines.

[Unit and integration tests](components/tests/tests.md) are the tools to use to validate business logic. Vulcan encourages users to add unit tests to all of their models to ensure changes don't unexpectedly break assumptions. Unit tests are designed to be fast and self contained so that they can run in continuous integration (CI) frameworks.

### Python models
dbt's Python models only run remotely on adapters of data platforms that have a full Python runtime, limiting the number of users that can take advantage of them and making the models difficult to debug.

Vulcan's [Python models](components/model/types/python_models.md) run locally and can be used with any data warehouse. Breakpoints can be added to debug the model.

### Data contracts
dbt offers manually configured schema contracts that will check the model's schema against the yaml schema at runtime. Models can be versioned to allow downstream teams time to migrate to the latest version, at the risk of a fragmented source of truth during the migration period.

Vulcan provides automatic schema contracts and data contracts via [`vulcan plan`](guides/plan.md), which checks the model's schema and query logic for changes that affect downstream users. `vulcan plan` will show which models have breaking changes and which downstream models are affected.

While breaking changes can be rolled out as separate models to allow for a migration period, Vulcan's [Virtual Preview](concepts-old/glossary.md#virtual-preview) empowers teams to collaborate on migrations before the changes are deployed to prod, maintaining a single source of truth across the business.


# Custom materializations

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/custom_materializations/

---

# Custom materializations

Vulcan comes with a variety of [model kinds](../model/model_kinds.md) that handle the most common ways to evaluate and materialize your data transformations. But what if you need something different?

Sometimes, your specific use case doesn't quite fit any of the built-in model kinds. Maybe you need custom logic for how data gets inserted, or you want to implement a materialization strategy that's unique to your workflow. That's where custom materializations come in, they let you write your own Python code to control exactly how your models get materialized.

!!! warning "Advanced Feature"
    Custom materializations are powerful, but they're also advanced. Before diving in, make sure you've exhausted all other options. If an existing model kind can solve your problem, we want to improve our docs; if a built-in kind is almost what you need, we might be able to enhance it for everyone.

## What is a materialization?

A materialization is the "how" behind your model execution. When Vulcan runs a model, it needs to figure out how to get that data into your database. The materialization is the set of methods that handle executing your transformation logic and managing the resulting data.

Some materializations are straightforward. For example, a `FULL` model kind completely replaces the table each time it runs, so its materialization is essentially just `CREATE OR REPLACE TABLE [name] AS [your query]`.

Other materializations are more complex. An `INCREMENTAL_BY_TIME_RANGE` model needs to figure out which time intervals to process, query only that data, and then merge it into the existing table. That requires more logic.

The materialization logic can also vary by SQL engine. PostgreSQL doesn't support `CREATE OR REPLACE TABLE`, so `FULL` models on Postgres use `DROP` then `CREATE` instead. Vulcan handles all these engine-specific details for built-in model kinds, but with custom materializations, you're in control.

## How custom materializations work

Custom materializations are like creating your own model kind. You define them in Python, give them a name, and then reference that name in your model's `MODEL` block. They can accept configuration arguments that you pass in from your model definition.

Here's what every custom materialization needs:

- **Python code**: Written as a Python class

- **Base class**: Must inherit from Vulcan's `CustomMaterialization` class

- **Insert method**: At minimum, you need to implement the `insert` method

- **Auto-loading**: Vulcan automatically discovers materializations in your `materializations/` directory

You can also:

- Override other methods from `MaterializableStrategy` or `EngineAdapter` classes

- Execute arbitrary SQL using the engine adapter

- Perform Python processing with Pandas or other libraries (though for most cases, you'd want that logic in a [Python model](../model/types/python_models.md) instead)

Vulcan will automatically load any Python files in your project's `materializations/` directory. Or, if you prefer, you can package your materialization as a [Python package](#python-packaging) and install it like any other dependency.

## Creating a custom materialization

To create a custom materialization, just add a `.py` file to your project's `materializations/` folder. Vulcan will automatically import all Python modules in this folder when your project loads, so your materializations will be ready to use.

Your materialization class needs to inherit from `CustomMaterialization` and implement at least the `insert` method. Let's look at some examples to see how this works.

### Simple example

Here's a complete example that shows custom insert logic with some helpful logging:

```python linenums="1"
import typing as t
from sqlalchemy import text
from vulcan import CustomMaterialization
from vulcan import Model

class SimpleCustomMaterialization(CustomMaterialization):
    """Simple custom materialization - demonstrates custom insert logic"""
    
    NAME = "simple_custom"
    
    def insert(
        self,
        table_name: str,
        query_or_df: t.Union[str, t.Any],
        model: Model,
        is_first_insert: bool,
        render_kwargs: t.Dict[str, t.Any],
        **kwargs: t.Any,
    ) -> None:
        """Custom insert logic for tables"""
        
        print(f"Custom materialization: Processing table {table_name}")
        print(f"Model: {model.name}")
        print(f"Is first insert: {is_first_insert}")
        
        if is_first_insert:
            print("Creating table for the first time")
            # Create the table normally using the adapter
            self.adapter.create_table(
                table_name,
                columns=model.columns_to_types,
                target_columns_to_types=model.columns_to_types,
                partitioned_by=model.partitioned_by,
            )
        
        # Insert data with custom logic
        if isinstance(query_or_df, str):
            print("Executing SQL query")
            # Execute the query - Vulcan provides the INSERT INTO ... SELECT query
            self.adapter.execute(text(query_or_df))
        else:
            print("Inserting DataFrame")
            # Insert DataFrame normally - useful for Python models that return DataFrames
            self.adapter.insert_append(table_name, query_or_df)
        
        print(f"Custom materialization completed for {table_name}")
```

Let's break down what's happening here:

| Component | What It Does |
|-----------|--------------|
| `NAME` | The identifier you'll use in your model definition (like `simple_custom`) |
| `table_name` | The target table where your data will be inserted |
| `query_or_df` | Either a SQL query string or a DataFrame (works with Pandas, PySpark, Snowpark) |
| `model` | The full model definition object, gives you access to all model properties |
| `is_first_insert` | `True` if this is the first time inserting data for this model version |
| `render_kwargs` | Dictionary of arguments used to render the model query |
| `self.adapter` | The engine adapter, your interface to execute SQL and interact with the database |

### Minimal example

If you just want a simple full-refresh materialization, here's the minimal version:

```python linenums="1"
from vulcan import CustomMaterialization
from vulcan import Model
import typing as t

class CustomFullMaterialization(CustomMaterialization):
    NAME = "my_custom_full"

    def insert(
        self,
        table_name: str,
        query_or_df: t.Any,
        model: Model,
        is_first_insert: bool,
        render_kwargs: t.Dict[str, t.Any],
        **kwargs: t.Any,
    ) -> None:
        self.adapter.replace_query(table_name, query_or_df)
```

That's it! This will completely replace the table contents each time the model runs, just like a `FULL` model kind.

### Controlling table creation and deletion

You can also customize how tables and views are created and deleted by overriding the `create` and `delete` methods:

```python linenums="1"
from vulcan import CustomMaterialization
from vulcan import Model
import typing as t

class CustomFullMaterialization(CustomMaterialization):
    NAME = "my_custom_full"
    
    def insert(self, table_name: str, query_or_df: t.Any, model: Model, 
               is_first_insert: bool, render_kwargs: t.Dict[str, t.Any], **kwargs: t.Any) -> None:
        self.adapter.replace_query(table_name, query_or_df)

    def create(
        self,
        table_name: str,
        model: Model,
        is_table_deployable: bool,
        render_kwargs: t.Dict[str, t.Any],
        **kwargs: t.Any,
    ) -> None:
        # Custom table/view creation logic
        # Uses self.adapter methods like create_table, create_view, or ctas
        self.adapter.create_table(
            table_name,
            columns=model.columns_to_types,
            target_columns_to_types=model.columns_to_types,
        )

    def delete(self, name: str, **kwargs: t.Any) -> None:
        # Custom table/view deletion logic
        self.adapter.drop_table(name)
```

This gives you full control over the lifecycle of your data objects.

## Using a custom materialization

Once you've created your materialization, using it is straightforward. In your model definition, set the `kind` to `CUSTOM` and specify the `materialization` name (the `NAME` from your Python class):

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.custom_model,
      kind CUSTOM (
        materialization 'simple_custom'
      ),
      grain (customer_id)
    );

    SELECT
      c.customer_id,
      c.name AS customer_name,
      COUNT(DISTINCT o.order_id) AS total_orders,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent
    FROM vulcan_demo.customers c
    LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id
    LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id
    GROUP BY c.customer_id, c.name
    ORDER BY total_spent DESC
    ```

=== "Python"

    ```python linenums="1"
    import typing as t
    import pandas as pd
    from datetime import datetime
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.custom_model_py",
        columns={
            "customer_id": "int",
            "customer_name": "string",
            "total_orders": "int",
            "total_spent": "decimal(10,2)",
        },
        kind=dict(
            name=ModelKindName.CUSTOM,
            materialization="simple_custom",
        ),
        grain=["customer_id"],
        depends_on=["vulcan_demo.customers", "vulcan_demo.orders", "vulcan_demo.order_items"],
    )
    def execute(
        context: ExecutionContext,
        start: datetime,
        end: datetime,
        execution_time: datetime,
        **kwargs: t.Any,
    ) -> pd.DataFrame:
        """Python model using custom materialization with dynamic dependencies"""
        
        # Simple customer summary
        query = """
        SELECT 
            c.customer_id,
            c.name as customer_name,
            COUNT(DISTINCT o.order_id) as total_orders,
            COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_spent
        FROM vulcan_demo.customers c
        LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id
        LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id
        GROUP BY c.customer_id, c.name
        ORDER BY total_spent DESC
        """
        
        # Execute query and return results
        return context.fetchdf(query)
    ```

### Passing properties to the materialization

You can pass configuration to your materialization using `materialization_properties`. This is useful when you want to customize behavior per model:

```sql linenums="1"
MODEL (
  name vulcan_demo.custom_model,
  kind CUSTOM (
    materialization 'simple_custom',
    materialization_properties (
      'config_key' = 'config_value',
      'batch_size' = 1000
    )
  )
);
```

Then access these properties in your materialization code via `model.custom_materialization_properties`:

```python linenums="1"
class SimpleCustomMaterialization(CustomMaterialization):
    NAME = "simple_custom"

    def insert(
        self,
        table_name: str,
        query_or_df: t.Any,
        model: Model,
        is_first_insert: bool,
        render_kwargs: t.Dict[str, t.Any],
        **kwargs: t.Any,
    ) -> None:
        # Access custom properties
        config_value = model.custom_materialization_properties.get("config_key")
        batch_size = model.custom_materialization_properties.get("batch_size", 500)
        
        print(f"Config value: {config_value}, Batch size: {batch_size}")
        
        # Proceed with insert logic
        self.adapter.replace_query(table_name, query_or_df)
```

This lets you create flexible materializations that can adapt to different use cases.

## Extending `CustomKind`

!!! warning
    This is advanced territory. You're working with Vulcan's internals here, so there's extra complexity involved. If the basic custom materialization approach works for you, stick with that. Only dive into this if you really need the extra control.

Most of the time, the standard custom materialization approach is all you need. But sometimes you want tighter integration with Vulcan's internals, maybe you need to validate custom properties before any database connections are made, or you want to leverage functionality that depends on specific properties being present.

In those cases, you can create a subclass of `CustomKind` that Vulcan will use instead of the default. When your project loads, Vulcan will detect your subclass and use it instead of the standard `CustomKind`.

### Creating a custom kind

Here's how you'd create a custom kind that validates a `primary_key` property:

```python linenums="1"
import typing as t
from typing_extensions import Self
from pydantic import model_validator
from sqlglot import exp
from vulcan import CustomKind
from vulcan.utils.pydantic import list_of_fields_validator
from vulcan.utils.errors import ConfigError

class MyCustomKind(CustomKind):

    _primary_key: t.List[exp.Expression]

    @model_validator(mode="after")
    def _validate_model(self) -> Self:
        self._primary_key = list_of_fields_validator(
            self.materialization_properties.get("primary_key"),
            {"dialect": self.dialect}
        )
        if not self.primary_key:
            raise ConfigError("primary_key must be specified")
        return self

    @property
    def primary_key(self) -> t.List[exp.Expression]:
        return self._primary_key
```

### Using the custom kind in a model

Use it in your model like this:

```sql linenums="1"
MODEL (
  name vulcan_demo.my_model,
  kind CUSTOM (
    materialization 'my_custom_full',
    materialization_properties (
      primary_key = (col1, col2)
    )
  )
);
```

### Linking to your materialization

To connect your custom kind to your materialization, specify it as a generic type parameter:

```python linenums="1"
class CustomFullMaterialization(CustomMaterialization[MyCustomKind]):
    NAME = "my_custom_full"

    def insert(
        self,
        table_name: str,
        query_or_df: t.Any,
        model: Model,
        is_first_insert: bool,
        render_kwargs: t.Dict[str, t.Any],
        **kwargs: t.Any,
    ) -> None:
        assert isinstance(model.kind, MyCustomKind)

        self.adapter.merge(
            ...,
            unique_key=model.kind.primary_key
        )
```

When Vulcan loads your materialization, it inspects the type signature for generic parameters that are subclasses of `CustomKind`. If it finds one, it uses your subclass when building `model.kind` instead of the default.

Why would you want this? Two main benefits:

- **Early validation**: Your `primary_key` validation happens at load time, not evaluation time. Issues get caught before you even create a plan.

- **Type safety**: `model.kind` resolves to your custom kind object, so you get access to extra properties without additional validation.

## Sharing custom materializations

Once you've built a custom materialization, you'll probably want to use it across multiple projects. You have a couple of options.

### Copying files

The simplest approach is to copy the materialization code into each project's `materializations/` directory. It works, but it's not the most maintainable approach, you'll need to manually update each copy when you make changes.

If you go this route, we strongly recommend keeping the materialization code in version control and setting up a reliable way to notify users when updates are available.

### Python packaging

A more robust approach is to package your materialization as a Python package. This is especially useful if you're using Airflow or other external schedulers where the scheduler cluster doesn't have direct access to your project's `materializations/` folder.

Package your materialization using [setuptools entrypoints](https://packaging.python.org/en/latest/guides/creating-and-discovering-plugins/#using-package-metadata):

=== "pyproject.toml"

    ```toml
    [project.entry-points."vulcan.materializations"]
    my_materialization = "my_package.my_materialization:CustomFullMaterialization"
    ```

=== "setup.py"

    ```python
    setup(
        ...,
        entry_points={
            "vulcan.materializations": [
                "my_materialization = my_package.my_materialization:CustomFullMaterialization",
            ],
        },
    )
    ```

Once the package is installed, Vulcan automatically discovers and loads your materialization from the entrypoint list. No manual configuration needed!

For more details on Python packaging, check out the Vulcan GitHub [custom_materializations](https://github.com/TobikoData/vulcan/tree/main/examples/custom_materializations) example.



# Built In

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/macros/built_in/

---

# Built In

## Macro systems: two approaches

Vulcan macros work differently than templating systems like [Jinja](https://jinja.palletsprojects.com/en/3.1.x/). Here's the key difference: templating systems are all about string substitution, they scan your code, find special characters, and replace them with other text. That's their whole job.

Templating systems are intentionally language-agnostic. They work for blog posts, HTML, SQL, or pretty much anything. They have control flow (if-then, loops) and other features, but those are just tools to help them substitute the right strings.

Vulcan macros are different. They're built specifically for SQL, and they understand what your SQL actually means. Instead of just swapping strings, Vulcan macros analyze your SQL using the [sqlglot](https://github.com/tobymao/sqlglot) library to build a semantic representation of your query. Then they modify that representation. This means they can do things templating systems can't, like knowing whether something is a column name or a string literal, or understanding the structure of your query.

Plus, you can write macro logic in Python, which gives you way more power than simple string substitution.

### How Vulcan macros work

This section explains what happens under the hood when Vulcan processes your macros. You don't need to read this to use macros, but it's helpful when you're debugging something that's not working as expected.

The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.

In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. *Vulcan macros work by building and modifying the semantic representation of the SQL query.*

After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.

It uses the following five step approach to accomplish this:

1. Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol `@` to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as "placeholder" values to use in subsequent steps.

2. Examine the placeholder values to classify them as one of the following types:

    - Creation of user-defined macro variables with the `@DEF` operator (see more about [user-defined macro variables](#user-defined-variables))

    - Macro variables: [Vulcan pre-defined](./variables.md), [user-defined local](#local-variables), and [user-defined global](#global-variables)

    - Macro functions, both [Vulcan's](#macro-operators) and [user-defined](#user-defined-macro-functions)

3. Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.

4. Execute any macro functions and substitute the returned values.

5. Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).

### Embedding variables in strings

Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.

For context, two commonly used types of string in SQL are:

- String literals, which represent text values and are surrounded by single quotes, such as `'the_string'`

- Identifiers, which reference database objects like column, table, alias, and function names

    - They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect

In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the `MODEL` block (and not the query itself).

For example, consider a project that defines a [gateway variable](#gateway-variables) named `gateway_var`. The project includes a model that references `@gateway_var` as part of the schema in the model's `name`, which is a SQL *identifier*.

This is how we might try to write the model:

``` sql title="Incorrectly rendered to string literal"
MODEL (
  name the_@gateway_var_schema.table
);
```

From Vulcan's perspective, the model schema is the combination of three sub-strings: `the_`, the value of `@gateway_var`, and `_schema`.

Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.

To provide the context Vulcan needs, you must add curly braces to the macro variable reference: `@{gateway_var}` instead of `@gateway_var`:

``` sql title="Correctly rendered to identifier"
MODEL (
  name the_@{gateway_var}_schema.table
);
```

The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.

The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable `my_variable` whose value is `col`.

If we `SELECT` this value with regular macro syntax, it will render to a string literal:

``` sql
SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column
```

`'col'` is surrounded with single quotes, and the SQL engine will use that string as the column's data value.

If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:

``` sql
SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column
```

`col` is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named `col`.

## User-defined variables

Vulcan supports four kinds of user-defined macro variables: [global](#global-variables), [gateway](#gateway-variables), [blueprint](#blueprint-variables), and [local](#local-variables).

Here's how they're organized:

- **Global and gateway variables** are defined in your project configuration file and can be used in any model

- **Blueprint and local variables** are defined in a specific model and only work in that model

What happens if you have variables with the same name at different levels? The most specific one wins. Local variables override blueprint or gateway variables, gateway variables override global variables, and so on. This lets you set defaults globally but override them when needed.

### Global variables

Global variables live in your project configuration file under the [`variables` key](../../../references/configuration.md#variables). They're perfect for values you want to use across multiple models.

You can store numbers (`int`, `float`), booleans (`bool`), strings (`str`), or even lists and dictionaries containing these types.

Access them in your models using either `@VAR_NAME` (the simple syntax) or `@VAR('var_name')` (the function syntax). The function syntax lets you provide a default value as the second argument, useful if the variable might not be defined.

For example, this Vulcan configuration key defines six variables of different data types:

=== "YAML"

    ```yaml linenums="1"
    variables:
      int_var: 1
      float_var: 2.0
      bool_var: true
      str_var: "cat"
      list_var: [1, 2, 3]
      dict_var:
        key1: 1
        key2: 2
    ```

=== "Python"

    ``` python linenums="1"
    variables = {
        "int_var": 1,
        "float_var": 2.0,
        "bool_var": True,
        "str_var": "cat",
        "list_var": [1, 2, 3],
        "dict_var": {"key1": 1, "key2": 2},
    }

    config = Config(
        variables=variables,
        ... # other Config arguments
    )
    ```

A model definition could access the `int_var` value in a `WHERE` clause like this:

```sql linenums="1"
SELECT *
FROM table
WHERE int_variable = @INT_VAR
```

Alternatively, the same variable can be accessed by passing the variable name into the `@VAR()` macro function. Note that the variable name is in single quotes in the call `@VAR('int_var')`:

```sql linenums="1"
SELECT *
FROM table
WHERE int_variable = @VAR('int_var')
```

A default value can be passed as a second argument to the `@VAR()` macro function, which will be used as a fallback value if the variable is missing from the configuration file.

In this example, the `WHERE` clause would render to `WHERE some_value = 0` because no variable named `missing_var` was defined in the project configuration file:

```sql linenums="1"
SELECT *
FROM table
WHERE some_value = @VAR('missing_var', 0)
```

A similar API is available for [Python macro functions](#accessing-global-variable-values) via the `evaluator.var` method and [Python models](../../model/types/python_models.md#user-defined-variables) via the `context.var` method.

### Gateway variables

Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's `variables` key:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        variables:
          int_var: 1
        ...
    ```

=== "Python"

    ``` python linenums="1"
    gateway_variables = {
      "int_var": 1
    }

    config = Config(
        gateways={
          "my_gateway": GatewayConfig(
            variables=gateway_variables
            ... # other GatewayConfig arguments
            ),
          }
    )
    ```

Access them in models using the same methods as [global variables](#global-variables).

Gateway-specific variable values take precedence over variables with the same name specified in the root `variables` key.

### Blueprint variables

Blueprint macro variables are defined in a model. Blueprint variable values take precedence over [global](#global-variables) or [gateway-specific](#gateway-variables) variables with the same name.

Blueprint variables are defined as a property of the `MODEL` statement, and serve as a mechanism for [creating model templates](../../model/types/sql_models.md):

```sql linenums="1"
MODEL (
  name @customer.some_table,
  kind FULL,
  blueprints (
    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),
    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')
  )
);

SELECT
  @field_a,
  @{field_b} AS field_b,
  @field_c AS @{field_c}
FROM @customer.some_source

/*
When rendered for customer1.some_table:
SELECT
  x,
  y AS field_b,
  'foo' AS foo
FROM customer1.some_source

When rendered for customer2.some_table:
SELECT
  z,
  w AS field_b,
  'bar' AS bar
FROM customer2.some_source
*/
```

Note the use of both regular `@field_a` and curly brace syntax `@{field_b}` macro variable references in the model query. Both of these will be rendered as identifiers. In the case of `field_c`, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax `@field_c` and if we want to use the string as an identifier then we use the curly braces `@{field_c}`. Learn more [above](#embedding-variables-in-strings)

Blueprint variables can be accessed using the syntax shown above, or through the `@BLUEPRINT_VAR()` macro function, which also supports specifying default values in case the variable is undefined (similar to `@VAR()`).

### Local variables

Local macro variables are defined in a model. Local variable values take precedence over [global](#global-variables), [blueprint](#blueprint-variables), or [gateway-specific](#gateway-variables) variables with the same name.

Define your own local macro variables with the `@DEF` macro operator. For example, you could set the macro variable `macro_var` to the value `1` with:

```sql linenums="1"
@DEF(macro_var, 1);
```

Vulcan has three basic requirements for using the `@DEF` operator:

1. The `MODEL` statement must end with a semi-colon `;`
2. All `@DEF` uses must come after the `MODEL` statement and before the SQL query
3. Each `@DEF` use must end with a semi-colon `;`

For example, consider the following model `vulcan_example.full_model` from the [Vulcan quickstart guide](../../../guides/get-started/docker.md):

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
GROUP BY item_id
```

This model could be extended with a user-defined macro variable to filter the query results based on `item_size` like this:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
); -- NOTE: semi-colon at end of MODEL statement

@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
WHERE
  item_size > @size -- Reference to macro variable `@size` defined above with `@DEF()`
GROUP BY item_id
```

This example defines the macro variable `size` with `@DEF(size, 1)`. When the model is run, Vulcan will substitute in the number `1` where `@size` appears in the `WHERE` clause.

### Macro functions

In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:

```sql linenums="1"
MODEL(...);

@DEF(
  rank_to_int,
  x -> case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end
);

SELECT
  id,
  cust_rank_1,
  cust_rank_2,
  cust_rank_3
  @rank_to_int(cust_rank_1) as cust_rank_1_int,
  @rank_to_int(cust_rank_2) as cust_rank_2_int,
  @rank_to_int(cust_rank_3) as cust_rank_3_int
FROM
  some.model
```

Multiple arguments can be expressed in a macro function as well:

```sql linenums="1"
@DEF(pythag, (x,y) -> sqrt(pow(x, 2) + pow(y, 2)));

SELECT
  sideA,
  sideB,
  @pythag(sideA, sideB) AS sideC
FROM
  some.triangle
```

```sql linenums="1"
@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -> (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);

SELECT
  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,
  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,
  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,
FROM
  some.revenue
```

You can nest macro functions like so:

```sql linenums="1"
MODEL (
  name dummy.model,
  kind FULL
);

@DEF(area, r -> pi() * r * r);
@DEF(container_volume, (r, h) -> @area(@r) * h);

SELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume
```

## Macro operators

Vulcan's macro system comes with a bunch of operators that let you add dynamic behavior to your models. These are the built-in tools that make your SQL adapt to different situations.

### @EACH

`@EACH` is like a `for` loop for your SQL. It takes a list of items and applies a function to each one, transforming them into whatever you need.

??? info "Learn more about `for` loops and `@EACH`"

    Before diving into the `@EACH` operator, let's dissect a `for` loop to understand its components.

    `for` loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a `for` loop in Python:

    ```python linenums="1"
    for number in [4, 5, 6]:
        print(number)
    ```

    This for loop prints each number present in the brackets:

    ```python linenums="1"
    4
    5
    6
    ```

    The first line of the example sets up the loop, doing two things:

    1. Telling Python that code inside the loop will refer to each item as `number`
    2. Telling Python to step through the list of items in brackets

    The second line tells Python what action should be taken for each item. In this case, it prints the item.

    The loop executes one time for each item in the list, substituting in the item for the word `number` in the code. For example, the first time through the loop the code would execute as `print(4)` and the second time as `print(5)`.

    The Vulcan `@EACH` operator is used to implement the equivalent of a `for` loop in Vulcan macros.

    `@EACH` gets its name from the fact that a loop performs the action "for each" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.

`@EACH` takes two arguments: a list of items and a function definition.

```sql linenums="1"
@EACH([list of items], [function definition])
```

The function definition is specified inline. This example specifies the identity function, returning the input unmodified:

```sql linenums="1"
SELECT
  @EACH([4, 5, 6], number -> number)
FROM table
```

The loop is set up by the first argument: `@EACH([4, 5, 6]` tells Vulcan to step through the list of items in brackets.

The second argument `number -> number` tells Vulcan what action should be taken for each item using an "anonymous" function (aka "lambda" function). The left side of the arrow states what name the code on the right side will refer to each item as (like `name` in `for [name] in [items]` in a Python `for` loop).

The right side of the arrow specifies what should be done to each item in the list. `number -> number` tells `@EACH` that for each item `number` it should return that item (e.g., `1`).

Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If `@EACH` is used in the `SELECT` clause of a SQL statement:

1. It prints the item
2. It knows fields are separated by commas in `SELECT`, so it automatically separates the printed items with commas

Because of the automatic print and comma-separation, the anonymous function `number -> number` tells `@EACH` that for each item `number` it should print the item and separate the items with commas. Therefore, the complete output from the example is:

```sql linenums="1"
SELECT
  4,
  5,
  6
FROM table
```

This basic example is too simple to be useful. Many uses of `@EACH` will involve using the values as one or both of a literal value and an identifier.

For example, a column `favorite_number` in our data might contain values `4`, `5`, and `6`, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,
  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,
  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6
FROM table
```

In that SQL query each number is being used in two distinct ways. For example, `4` is being used:

1. As a literal numeric value in `favorite_number = 4`
2. As part of a column name in `favorite_4`

We describe each of these uses separately.

For the literal numeric value, `@EACH` substitutes in the exact value that is passed in the brackets, *including quotes*. For example, consider this query similar to the `CASE WHEN` example above:

```sql linenums="1"
SELECT
  @EACH([4,5,6], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)
FROM table
```

It renders to this SQL:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column
FROM table
```

Note that the number `4`, `5`, and `6` are unquoted in *both* the input `@EACH` array in brackets and the resulting SQL query.

We can instead quote them in the input `@EACH` array:

```sql linenums="1"
SELECT
  @EACH(['4','5','6'], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)
FROM table
```

And they will be quoted in the resulting SQL query:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column
FROM table
```

We can place the array values at the end of a column name by using the Vulcan macro operator `@` inside the `@EACH` function definition:

```sql linenums="1"
SELECT
  @EACH(['4','5','6'], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)
FROM table
```

This query will render to:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,
  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,
  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6
FROM table
```

This syntax works regardless of whether the array values are quoted or not.

!!! note "Embedding macros in strings"

    You can put macro values at the end of a column name using `column_@x`, but if you want to put the variable anywhere else in the identifier, use curly braces `@{}` to avoid confusion. For example: `@{x}_column` or `my_@{x}_column` work great.

    Learn more about embedding macros in strings [above](#embedding-variables-in-strings)

### @IF

`@IF` lets you conditionally include parts of your SQL based on a logical condition. It's like an if-then statement, but for your query.

It has three parts:

1. A condition that evaluates to `TRUE` or `FALSE` (written in SQL)
2. What to return if the condition is `TRUE`
3. What to return if the condition is `FALSE` (this is optional, if you omit it and the condition is false, nothing gets included)

These elements are specified as:

```sql linenums="1"
@IF([logical condition], [value if TRUE], [value if FALSE])
```

The value to return if the condition is `FALSE` is optional - if it is not provided and the condition is `FALSE`, then the macro has no effect on the resulting query.

The logical condition should be written *in SQL* and is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL executor. It supports the following operators:

- Equality: `=` for equals, `!=` or `<>` for not equals

- Comparison: `<`, `>`, `<=`, `>=`,

- Between: `[number] BETWEEN [low number] AND [high number]`

- Membership: `[item] IN ([comma-separated list of items])`

For example, the following simple conditions are all valid SQL and evaluate to `TRUE`:

- `'a' = 'a'`

- `'a' != 'b'`

- `0 < 1`

- `1 >= 1`

- `2 BETWEEN 1 AND 3`

- `'a' IN ('a', 'b')`

`@IF` can be used to modify any part of a SQL query. For example, this query conditionally includes `sensitive_col` in the query results:

```sql linenums="1"
SELECT
  col1,
  @IF(1 > 0, sensitive_col)
FROM table
```

Because `1 > 0` evaluates to `TRUE`, the query is rendered as:

```sql linenums="1"
SELECT
  col1,
  sensitive_col
FROM table
```

Note that `@IF(1 > 0, sensitive_col)` does not include the third argument specifying a value if `FALSE`. Had the condition evaluated to `FALSE`, `@IF` would return nothing and only `col1` would be selected.

Alternatively, we could specify that `nonsensitive_col` be returned if the condition evaluates to `FALSE`:

```sql linenums="1"
SELECT
  col1,
  @IF(1 > 2, sensitive_col, nonsensitive_col)
FROM table
```

Because `1 > 2` evaluates to `FALSE`, the query is rendered as:

```sql linenums="1"
SELECT
  col1,
  nonsensitive_col
FROM table
```

[Macro rendering](#vulcan-macro-approach) occurs before the `@IF` condition is evaluated. For example, Vulcan doesn't evaluate the condition `my_column > @my_value` until it has first substituted the number `@my_value` represents.

Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro "has side effects"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.

#### Pre/post-statements

`@IF` may be used to conditionally execute pre/post-statements:

```sql linenums="1"
@IF([logical condition], [statement to execute if TRUE]);
```

The `@IF` statement itself must end with a semi-colon, but the inner statement argument must not.

This example conditionally executes a pre/post-statement depending on the model's [runtime stage](./variables.md#predefined-variables), accessed via the pre-defined macro variable `@runtime_stage`. The `@IF` post-statement will only be executed at model evaluation time:

```sql linenums="1" hl_lines="17-20"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  grain item_id,
  audits (assert_positive_order_ids),
);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
GROUP BY item_id
ORDER BY item_id;

@IF(
  @runtime_stage = 'evaluating',
  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR
);
```

NOTE: alternatively, we could alter a column's type if the `@runtime_stage = 'creating'`, but that would only be useful if the model is incremental and the alteration would persist. `FULL` models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.

### @EVAL

`@EVAL` evaluates its arguments with SQLGlot's SQL executor.

It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the [`@IF` operator](#if), but it is not limited to logical conditions.

For example, consider a query adding 5 to a macro variable:

```sql linenums="1"
MODEL (
  ...
);

@DEF(x, 1);

SELECT
  @EVAL(5 + @x) as my_six
FROM table
```

After macro variable substitution, this would render as `@EVAL(5 + 1)` and be evaluated to `6`, resulting in the final rendered query:

```sql linenums="1"
SELECT
  6 as my_six
FROM table
```

### @FILTER

`@FILTER` is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as [`@EACH`](#each) or [`@REDUCE`](#reduce).

The user-specified anonymous function must evaluate to `TRUE` or `FALSE`. `@FILTER` applies the function to each item in the array, only including the item in the output array if it meets the condition.

The anonymous function should be written *in SQL* and is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL executor. It supports standard SQL equality and comparison operators - see [`@IF`](#if) above for more information about supported operators.

For example, consider this `@FILTER` call:

```sql linenums="1"
@FILTER([1,2,3], x -> x > 1)
```

It applies the condition `x > 1` to each item in the input array `[1,2,3]` and returns `[2,3]`.

### @REDUCE

`@REDUCE` is used to combine the items in an array.

The anonymous function specifies how the items in the input array should be combined. In contrast to `@EACH` and `@FILTER`, the anonymous function takes two arguments whose values are named in parentheses.

For example, an anonymous function for `@EACH` might be specified `x -> x + 1`. The `x` to the left of the arrow tells Vulcan that the array items will be referred to as `x` in the code to the right of the arrow.

Because the `@REDUCE` anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, `(x, y) -> x + y` tells Vulcan that items will be referred to as `x` and `y` in the code to the right of the arrow.

Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.

Consider the anonymous function `(x, y) -> x + y`. Conceptually, only the `y` argument corresponds to items in the array; the `x` argument is a temporary value created when the function is evaluated.

For the call `@REDUCE([1,2,3,4], (x, y) -> x + y)`, the anonymous function is applied to the array in the following steps:

1. Take the first two items in the array as `x` and `y`. Apply the function to them: `1 + 2` = `3`.
2. Take the output of step (1) as `x` and the next item in the array `3` as `y`. Apply the function to them: `3 + 3` = `6`.
3. Take the output of step (2) as `x` and the next item in the array `4` as `y`. Apply the function to them: `6 + 4` = `10`.
4. No items remain. Return value from step (3): `10`.

`@REDUCE` will almost always be used with another macro operator. For example, we might want to build a `WHERE` clause from multiple column names:

```sql linenums="1"
SELECT
  my_column
FROM
  table
WHERE
  col1 = 1 and col2 = 1 and col3 = 1
```

We can use `@EACH` to build each column's predicate (e.g., `col1 = 1`) and `@REDUCE` to combine them into a single statement:

```sql linenums="1"
SELECT
  my_column
FROM
  table
WHERE
  @REDUCE(
    @EACH([col1, col2, col3], x -> x = 1), -- Builds each individual predicate `col1 = 1`
    (x, y) -> x AND y -- Combines individual predicates with `AND`
  )
```

### @STAR

`@STAR` is used to return a set of column selections in a query.

`@STAR` is named after SQL's star operator `*`, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one `@STAR` and may also include explicit column selections.

`@STAR` uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.

If the column data types are known, the resulting query `CAST`s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.

`@STAR` supports the following arguments, in this order:

- `relation`: The relation/table whose columns are being selected

- `alias` (optional): The alias of the relation (if it has one)

- `exclude` (optional): A list of columns to exclude

- `prefix` (optional): A string to use as a prefix for all selected column names

- `suffix` (optional): A string to use as a suffix for all selected column names

- `quote_identifiers` (optional): Whether to quote the resulting identifiers, defaults to true

**NOTE**: the `exclude` argument used to be named `except_`. The latter is still supported but we discourage its use because it will be deprecated in the future.

Like all Vulcan macro functions, omitting an argument when calling `@STAR` requires passing subsequent arguments with their name and the special `:=` keyword operator. For example, we might omit the `alias` argument with `@STAR(foo, exclude := [c])`. Learn more about macro function arguments [below](#positional-and-keyword-arguments).

As a `@STAR` example, consider the following query:

```sql linenums="1"
SELECT
  @STAR(foo, bar, [c], 'baz_', '_qux')
FROM foo AS bar
```

The arguments to `@STAR` are:

1. The name of the table `foo` (from the query's `FROM foo`)
2. The table alias `bar` (from the query's `AS bar`)
3. A list of columns to exclude from the selection, containing one column `c`
4. A string `baz_` to use as a prefix for all column names
5. A string `_qux` to use as a suffix for all column names

`foo` is a table that contains four columns: `a` (`TEXT`), `b` (`TEXT`), `c` (`TEXT`) and `d` (`INT`). After macro expansion, if the column types are known the query would be rendered as:

```sql linenums="1"
SELECT
  CAST("bar"."a" AS TEXT) AS "baz_a_qux",
  CAST("bar"."b" AS TEXT) AS "baz_b_qux",
  CAST("bar"."d" AS INT) AS "baz_d_qux"
FROM foo AS bar
```

Note these aspects of the rendered query:

- Each column is `CAST` to its data type in the table `foo` (e.g., `a` to `TEXT`)

- Each column selection uses the alias `bar` (e.g., `"bar"."a"`)

- Column `c` is not present because it was passed to `@STAR`'s `exclude` argument

- Each column alias is prefixed with `baz_` and suffixed with `_qux` (e.g., `"baz_a_qux"`)

Now consider a more complex example that provides different prefixes to `a` and `b` than to `d` and includes an explicit column `my_column`:

```sql linenums="1"
SELECT
  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),
  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),
  my_column
FROM foo AS bar
```

As before, `foo` is a table that contains four columns: `a` (`TEXT`), `b` (`TEXT`), `c` (`TEXT`) and `d` (`INT`). After macro expansion, the query would be rendered as:

```sql linenums="1"
SELECT
  CAST("bar"."a" AS TEXT) AS "ab_pre_a",
  CAST("bar"."b" AS TEXT) AS "ab_pre_b",
  CAST("bar"."d" AS INT) AS "d_pre_d",
  my_column
FROM foo AS bar
```

Note these aspects of the rendered query:

- Columns `a` and `b` have the prefix `"ab_pre_"` , while column `d` has the prefix `"d_pre_"`

- Column `c` is not present because it was passed to the `exclude` argument in both `@STAR` calls

- `my_column` is present in the query

### @GENERATE_SURROGATE_KEY

`@GENERATE_SURROGATE_KEY` generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as [`MD5`](https://en.wikipedia.org/wiki/MD5), on the concatenated column values.

The surrogate key is created by:

1. `CAST`ing each column's value to `TEXT` (or the SQL engine's equivalent type)
2. Replacing `NULL` values with the text `'_vulcan_surrogate_key_null_'` for each column
3. Concatenating the column values after steps (1) and (2)
4. Applying the [`MD5()` hash function](https://en.wikipedia.org/wiki/MD5) to the concatenated value returned by step (3)

For example, the following query:

```sql linenums="1"
SELECT
  @GENERATE_SURROGATE_KEY(a, b, c) AS col
FROM foo
```

would be rendered as:

```sql linenums="1"
SELECT
  MD5(
    CONCAT(
      COALESCE(CAST("a" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("b" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("c" AS TEXT), '_vulcan_surrogate_key_null_')
    )
  ) AS "col"
FROM "foo" AS "foo"
```

By default, the `MD5` function is used, but this behavior can change by setting the `hash_function` argument as follows:

```sql linenums="1"
SELECT
  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col
FROM foo
```

This query will similarly be rendered as:

```sql linenums="1"
SELECT
  SHA256(
    CONCAT(
      COALESCE(CAST("a" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("b" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("c" AS TEXT), '_vulcan_surrogate_key_null_')
    )
  ) AS "col"
FROM "foo" AS "foo"
```

### @SAFE_ADD

`@SAFE_ADD` adds two or more operands, substituting `NULL`s with `0`s. It returns `NULL` if all operands are `NULL`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_ADD(a, b, c)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END
FROM foo
```

### @SAFE_SUB

`@SAFE_SUB` subtracts two or more operands, substituting `NULL`s with `0`s. It returns `NULL` if all operands are `NULL`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_SUB(a, b, c)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END
FROM foo
```

### @SAFE_DIV

`@SAFE_DIV` divides two numbers, returning `NULL` if the denominator is `0`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_DIV(a, b)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  a / NULLIF(b, 0)
FROM foo
```

### @UNION

`@UNION` returns a `UNION` query that selects all columns with matching names and data types from the tables.

Its first argument can be either a condition or the `UNION` "type". If the first argument evaluates to a boolean (`TRUE` or `FALSE`), it's treated as a condition. If the condition is `FALSE`, only the first table is returned. If it's `TRUE`, the union operation is performed.

If the first argument is not a boolean condition, it's treated as the `UNION` "type": either `'DISTINCT'` (removing duplicated rows) or `'ALL'` (returning all rows). Subsequent arguments are the tables to be combined.

Let's assume that:

- `foo` is a table that contains three columns: `a` (`INT`), `b` (`TEXT`), `c` (`TEXT`)

- `bar` is a table that contains three columns: `a` (`INT`), `b` (`INT`), `c` (`TEXT`)

Then, the following expression:

```sql linenums="1"
@UNION('distinct', foo, bar)
```

would be rendered as:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
UNION
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM bar
```

If the union type is omitted, `'ALL'` is used as the default. So the following expression:

```sql linenums="1"
@UNION(foo, bar)
```

would be rendered as:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
UNION ALL
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM bar
```

You can also use a condition to control whether the union happens:

```sql linenums="1"
@UNION(1 > 0, 'all', foo, bar)
```

This would render the same as above. However, if the condition is `FALSE`:

```sql linenums="1"
@UNION(1 > 2, 'all', foo, bar)
```

Only the first table would be selected:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
```

### @HAVERSINE_DISTANCE

`@HAVERSINE_DISTANCE` returns the [haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) between two geographic points.

It supports the following arguments, in this order:

- `lat1`: Latitude of the first point

- `lon1`: Longitude of the first point

- `lat2`: Latitude of the second point

- `lon2`: Longitude of the second point

- `unit` (optional): The measurement unit, currently only `'mi'` (miles, default) and `'km'` (kilometers) are supported

Vulcan macro operators do not accept named arguments. For example, `@HAVERSINE_DISTANCE(lat1=lat_column)` will error.

For example, the following query:

```sql linenums="1"
SELECT
  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist
FROM rides
```

would be rendered as:

```sql linenums="1"
SELECT
  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist
FROM rides
```

### @PIVOT

`@PIVOT` returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a "long" format (multiple values in a single column) to a "wide" format (one value in each of multiple columns).

It supports the following arguments, in this order:

- `column`: The column to pivot

- `values`: The values to use for pivoting (one column is created for each value in `values`)

- `alias` (optional): Whether to create aliases for the resulting columns, defaults to true

- `agg` (optional): The aggregation function to use, defaults to `SUM`

- `cmp` (optional): The comparison operator to use for comparing the column values, defaults to `=`

- `prefix` (optional): A prefix to use for all aliases

- `suffix` (optional): A suffix to use for all aliases

- `then_value` (optional): The value to be used if the comparison succeeds, defaults to `1`

- `else_value` (optional): The value to be used if the comparison fails, defaults to `0`

- `quote` (optional): Whether to quote the resulting aliases, defaults to true

- `distinct` (optional): Whether to apply a `DISTINCT` clause for the aggregation function, defaults to false

Like all Vulcan macro functions, omitting an argument when calling `@PIVOT` requires passing subsequent arguments with their name and the special `:=` keyword operator. For example, we might omit the `agg` argument with `@PIVOT(status, ['cancelled', 'completed'], cmp := '<')`. Learn more about macro function arguments [below](#positional-and-keyword-arguments).

For example, the following query:

```sql linenums="1"
SELECT
  date_day,
  @PIVOT(status, ['cancelled', 'completed'])
FROM rides
GROUP BY 1
```

would be rendered as:

```sql linenums="1"
SELECT
  date_day,
  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS "'cancelled'",
  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS "'completed'"
FROM rides
GROUP BY 1
```

### @DEDUPLICATE

`@DEDUPLICATE` is used to deduplicate rows in a table based on the specified partition and order columns with a window function.

It supports the following arguments, in this order:

- `relation`: The table or CTE name to deduplicate

- `partition_by`: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row

- `order_by`: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: ['<column_name> desc nulls last']

For example, the following query:
```sql linenums="1"
with raw_data as (
@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])
)

select * from raw_data
```

would be rendered as:

```sql linenums="1"
WITH "raw_data" AS (
  SELECT
    *
  FROM "my_table" AS "my_table"
  QUALIFY
    ROW_NUMBER() OVER (PARTITION BY "id", CAST("event_date" AS DATE) ORDER BY "event_date" DESC, "status" ASC) = 1
)
SELECT
  *
FROM "raw_data" AS "raw_data"
```

### @DATE_SPINE

`@DATE_SPINE` returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the [`date_spine`](https://github.com/dbt-labs/dbt-utils?tab=readme-ov-file#date_spine-source) macro in `dbt-utils` which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in `where` clauses across many SQL models.

It supports the following arguments, in this order:

- `datepart`: The datepart to use for the date spine - day, week, month, quarter, year

- `start_date`: The start date for the date spine in format YYYY-MM-DD

- `end_date`: The end date for the date spine in format YYYY-MM-DD

For example, the following query:
```sql linenums="1"
WITH discount_promotion_dates AS (
  @date_spine('day', '2024-01-01', '2024-01-16')
)

SELECT * FROM discount_promotion_dates
```

would be rendered as:

```sql linenums="1"
WITH "discount_promotion_dates" AS (
  SELECT
    "_exploded"."date_day" AS "date_day"
  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS
DATE[])) AS "_exploded"("date_day")
)
SELECT
  "discount_promotion_dates"."date_day" AS "date_day"
FROM "discount_promotion_dates" AS "discount_promotion_dates"
```

Note: This is DuckDB SQL and other dialects will be transpiled accordingly.
- Recursive CTEs (common table expressions) will be used for `Redshift / MySQL / MSSQL`.

- For `MSSQL` in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an `OPTION (MAXRECURSION 0)` clause after the date spine macro logic to remove the limit. This applies for long date ranges.

### @RESOLVE_TEMPLATE

`@resolve_template` is a helper macro intended to be used in situations where you need to gain access to the *components* of the physical object name. It's intended for use in the following situations:

- Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)

- Generating references to engine-specific metadata tables that are derived from the physical table name, such as the [`<table>$properties`](https://trino.io/docs/current/connector/iceberg.html#metadata-tables) metadata table in Trino.

Under the hood, it uses the `@this_model` variable so it can only be used during the `creating` and `evaluation` [runtime stages](./variables.md#runtime-variables). Attempting to use it at the `loading` runtime stage will result in a no-op.

The `@resolve_template` macro supports the following arguments:

 - `template` - The string template to render into an AST node

 - `mode` - What type of SQLGlot AST node to return after rendering the template. Valid values are `literal` or `table`. Defaults to `literal`.

The `template` can contain the following placeholders that will be substituted:

  - `@{catalog_name}` - The name of the catalog, eg `datalake`

  - `@{schema_name}` - The name of the physical schema that Vulcan is using for the model version table, eg `vulcan__landing`

  - `@{table_name}` - The name of the physical table that Vulcan is using for the model version, eg `landing__customers__2517971505`

Note the use of the curly brace syntax `@{}` in the template placeholders - learn more [above](#embedding-variables-in-strings).

The `@resolve_template` macro can be used in a `MODEL` block:

```sql linenums="1" hl_lines="5"
MODEL (
  name datalake.landing.customers,
  ...
  physical_properties (
    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')
  )
);
-- CREATE TABLE "datalake"."vulcan__landing"."landing__customers__2517971505" ...

-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')
```

And also within a query, using `mode := 'table'`:

```sql linenums="1"
SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')
-- SELECT * FROM "datalake"."vulcan__landing"."landing__customers__2517971505$properties"
```

### @AND

`@AND` combines a sequence of operands using the `AND` operator, filtering out any NULL expressions.

For example, the following expression:

```sql linenums="1"
@AND(TRUE, NULL)
```

would be rendered as:

```sql linenums="1"
TRUE
```

### @OR

`@OR` combines a sequence of operands using the `OR` operator, filtering out any NULL expressions.

For example, the following expression:

```sql linenums="1"
@OR(TRUE, NULL)
```

would be rendered as:

```sql linenums="1"
TRUE
```

### SQL clause operators

Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:

- `@WITH`: common table expression `WITH` clause

- `@JOIN`: table `JOIN` clause(s)

- `@WHERE`: filtering `WHERE` clause

- `@GROUP_BY`: grouping `GROUP BY` clause

- `@HAVING`: group by filtering `HAVING` clause

- `@ORDER_BY`: ordering `ORDER BY` clause

- `@LIMIT`: limiting `LIMIT` clause

Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.

#### How SQL clause operators work

The SQL clause operators take a single argument that determines whether the clause is generated.

If the argument is `TRUE` the clause code is generated, if `FALSE` the code is not. The argument should be written *in SQL* and its value is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL engine.

Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.

As an example of SQL clause operators, let's revisit the example model from the [User-defined Variables](#user-defined-variables) section above.

As written, the model will always include the `WHERE` clause. We could make its presence dynamic by using the `@WHERE` macro operator:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(TRUE) item_id > @size
GROUP BY item_id
```

The `@WHERE` argument is set to `TRUE`, so the WHERE code is included in the rendered model:

```sql linenums="1"
SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
WHERE item_id > 1
GROUP BY item_id
```

If the `@WHERE` argument were instead set to `FALSE` the `WHERE` clause would be omitted from the query.

These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.

For example, the `WHERE` clause will be included in this query because 1 less than 2:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(1 < 2) item_id > @size
GROUP BY item_id
```

The operator's argument code can include macro variables.

In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(left_number, 1);
@DEF(right_number, 2);
@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(@left_number < @right_number) item_id > @size
GROUP BY item_id
```

The argument to `@WHERE` will be "1 < 2" as in the previous hard-coded example after the macro variables `left_number` and `right_number` are substituted in.

### SQL clause operator examples

This section provides brief examples of each SQL clause operator's usage.

The examples use variants of this simple select statement:

```sql linenums="1"
SELECT *
FROM all_cities
```

#### @WITH operator

The `@WITH` operator is used to create [common table expressions](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression), or "CTEs."

CTEs are typically used in place of derived tables (subqueries in the `FROM` clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.

```sql linenums="1"
@WITH(True) all_cities as (select * from city)
select *
FROM all_cities
```

renders to

```sql linenums="1"
WITH all_cities as (select * from city)
select *
FROM all_cities
```

#### @JOIN operator

The `@JOIN` operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).

```sql linenums="1"
select *
FROM all_cities
LEFT OUTER @JOIN(True) country
  ON city.country = country.name
```

renders to

```sql linenums="1"
select *
FROM all_cities
LEFT OUTER JOIN country
  ON city.country = country.name
```

The `@JOIN` operator recognizes that `LEFT OUTER` is a component of the `JOIN` specification and will omit it if the `@JOIN` argument evaluates to False.

#### @WHERE operator

The `@WHERE` operator adds a filtering `WHERE` clause(s) to the query when its argument evaluates to True.

```sql linenums="1"
SELECT *
FROM all_cities
@WHERE(True) city_name = 'Toronto'
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
WHERE city_name = 'Toronto'
```

#### @GROUP_BY operator

```sql linenums="1"
SELECT *
FROM all_cities
@GROUP_BY(True) city_id
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
GROUP BY city_id
```

#### @HAVING operator

```sql linenums="1"
SELECT
count(city_pop) as population
FROM all_cities
GROUP BY city_id
@HAVING(True) population > 1000
```

renders to

```sql linenums="1"
SELECT
count(city_pop) as population
FROM all_cities
GROUP BY city_id
HAVING population > 1000
```

#### @ORDER_BY operator

```sql linenums="1"
SELECT *
FROM all_cities
@ORDER_BY(True) city_pop
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
ORDER BY city_pop
```

#### @LIMIT operator

```sql linenums="1"
SELECT *
FROM all_cities
@LIMIT(True) 10
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
LIMIT 10
```

## User-defined macro functions

Macro functions let you write reusable logic that you can call from multiple models. Instead of copying the same code everywhere, you define it once and reuse it.

Vulcan supports macro functions in two languages:

- **SQL functions** use the [Jinja templating system](./jinja.md#user-defined-macro-functions)

- **Python functions** use SQLGlot and give you way more power, you can do complex operations that go beyond what variables and operators can handle alone

### Python macro functions

#### Setup

Python macro functions should be placed in `.py` files in the Vulcan project's `macros` directory. Multiple functions can be defined in one `.py` file, or they can be distributed across multiple files.

An empty `__init__.py` file must be present in the Vulcan project's `macros` directory. It will be created automatically when the project scaffold is created with `vulcan init`.

Each `.py` file containing a macro definition must import Vulcan's `macro` decorator with `from vulcan import macro`.

Python macros are defined as regular python functions adorned with the Vulcan `@macro()` decorator. The first argument to the function must be `evaluator`, which provides the macro evaluation context in which the macro function will run.

#### Inputs and outputs

Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless [argument type annotations are provided](#argument-data-types) in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.

Python macro functions may return values of either `string` or SQLGlot `expression` types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.

Macro functions may [return a list of strings or expressions](#returning-more-than-one-value) that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple `CASE WHEN` statements would be incorporated into the query properly, but a list containing both `CASE WHEN` statements and a `WHERE` clause would not.

#### Macro function basics

This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string `text`.

```python linenums="1"
from vulcan import macro

@macro() # Note parentheses at end of `@macro()` decorator
def print_text(evaluator):
  return 'text'
```

We could use this in a Vulcan SQL model like this:

```sql linenums="1"
SELECT
  @print_text() as my_text
FROM table
```

After processing, it will render to this:

```sql linenums="1"
SELECT
  text as my_text
FROM table
```

Note that the python function returned a string `'text'`, but the rendered query uses `text` as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.

The rendered query will treat `text` as a string if we double-quote the single-quoted value in the function definition as `"'text'"`:

```python linenums="1"
from vulcan import macro

@macro()
def print_text(evaluator):
    return "'text'"
```

When run in the same model query as before, this will render to:

```sql linenums="1"
SELECT
  'text' as my_text
FROM table
```

#### Argument data types

Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information [below](#typed-macros).

As [mentioned above](#inputs-and-outputs), argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).

If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).

For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:

```python linenums="1"
from vulcan import macro

@macro()
def arg_in_123(evaluator, my_arg):
    return my_arg in [1,2,3]
```

When this macro is called, it will return `FALSE` even if an integer was passed in the call. Consider this macro call:

``` sql linenums="1"
SELECT
  @arg_in_123(1)
```

It returns `SELECT FALSE` because:

1. The passed value `1` is parsed by SQLGlot into a SQLGlot expression before the function code executes and
2. There is no matching SQLGlot expression in `[1,2,3]`

However, the macro will treat the argument like a normal Python function does if we annotate `my_arg` with the integer `int` type in the function definition:

```python linenums="1"
from vulcan import macro

@macro()
def arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`
    return my_arg in [1,2,3]
```

Now the macro call will return `SELECT TRUE` because the value is coerced to a Python integer before the function code executes and `1` is in `[1,2,3]`.

If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value `None`.

#### Positional and keyword arguments

In a macro call, the arguments may be provided by position if none are skipped.

For example, consider the `add_args()` function - it has three arguments with default values provided in the function definition:

```python linenums="1"
from vulcan import macro

@macro()
def add_args(
    evaluator,
    argument_1: int = 1,
    argument_2: int = 2,
    argument_3: int = 3
):
    return argument_1 + argument_2 + argument_3
```

An `@add_args` call providing values for all arguments accepts positional arguments like this: `@add_args(5, 6, 7)` (which returns 5 + 6 + 7 = `18`). A call omitting and using the default value for the the final `argument_3` can also use positional arguments: `@add_args(5, 6)` (which returns 5 + 6 + 3 = `14`).

However, skipping an argument requires specifying the names of subsequent arguments (i.e., using "keyword arguments"). For example, skipping the second argument above by just omitting it - `@add_args(5, , 7)` - results in an error.

Unlike Python, Vulcan keyword arguments must use the special operator `:=`. To skip and use the default value for the second argument above, the call must name the third argument: `@add_args(5, argument_3 := 8)` (which returns 5 + 2 + 8 = `15`).

#### Variable-length arguments

The `add_args()` macro defined in the [previous section](#positional-and-keyword-arguments) accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.

The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's "variable-length arguments" to accomplish this:

```python linenums="1"
from vulcan import macro

@macro()
def add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`
    return sum(args)
```

This macro can be called with one or more arguments. For example:

- `@add_args(1)` returns 1

- `@add_args(1, 2)` returns 3

- `@add_args(1, 2, 3)` returns 6

#### Returning more than one value

Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.

For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into `CASE WHEN` statements.

Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code **cannot** treat the input list as a regular Python list.

Two things will happen to the input Python list before the function code is executed:

1. Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:
    - Numbers are parsed into [`Literal` expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)

    - Quoted strings are parsed into [`Literal` expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)

    - Unquoted strings are parsed into [`Column` expressions](https://sqlglot.com/sqlglot/expressions.html#Column)

2. The parsed entries will be contained in a SQLGlot [`Array` expression](https://sqlglot.com/sqlglot/expressions.html#Array), the SQL entity analogous to a Python list

Because the input  `Array` expression named `values` is not a Python list, we cannot iterate over it directly - instead, we iterate over its `expressions` attribute with `values.expressions`:

```python linenums="1"
from vulcan import macro

@macro()
def make_indicators(evaluator, string_column, values):
    cases = []

    for value in values.expressions: # Iterate over `values.expressions`
        cases.append(f"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}")

    return cases
```

We call this function in a model query to create `CASE WHEN` statements for the `vehicle` column values `truck` and `bus` like this:

```sql linenums="1"
SELECT
  @make_indicators(vehicle, [truck, bus])
FROM table
```

Which renders to:

```sql linenums="1"
SELECT
  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,
  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,
FROM table
```

Note that in the call `@make_indicators(vehicle, [truck, bus])` none of the three values is quoted.

Because they are unquoted, SQLGlot will parse them all as `Column` expressions. In the places we used single quotes when building the string (`'{value}'`), they will be single-quoted in the output. In the places we did not quote them (`{string_column} = ` and `{string_column}_{value}`), they will not.

#### Accessing predefined and local variable values

[Pre-defined variables](./variables.md#predefined-variables) and [user-defined local variables](#local-variables) can be accessed within the macro's body via the `evaluator.locals` attribute.

The first argument to every macro function, the macro evaluation context `evaluator`, contains macro variable values in its `locals` attribute. `evaluator.locals` is a dictionary whose key:value pairs are macro variables names and the associated values.

For example, a function could access the predefined `execution_epoch` variable containing the epoch timestamp of when the execution started.

```python linenums="1"
from vulcan import macro

@macro()
def get_execution_epoch(evaluator):
    return evaluator.locals['execution_epoch']
```

The function would return the `execution_epoch` value when called in a model query:

```sql linenums="1"
SELECT
  @get_execution_epoch() as execution_epoch
FROM table
```

The same approach works for user-defined local macro variables, where the key `"execution_epoch"` would be replaced with the name of the user-defined variable to be accessed.

One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:

```python linenums="1"
from vulcan import macro

@macro()
def get_macro_var(evaluator, macro_var):
    return evaluator.locals[macro_var]
```

We could define a local macro variable `my_macro_var` with a value of 1 and pass it to the `get_macro_var` function like this:

```sql linenums="1"
MODEL (...);

@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'

SELECT
  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function
FROM table
```

The model query would render to:

```sql linenums="1"
SELECT
  1 as macro_var_value
FROM table
```

#### Accessing global variable values

[User-defined global variables](#global-variables) can be accessed within the macro's body using the `evaluator.var` method.

If a global variable is not defined, the method will return a Python `None` value. You may provide a different default value as the method's second argument.

For example:

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    var_value = evaluator.var("<var_name>") # Default value is `None`
    another_var_value = evaluator.var("<another_var_name>", "default_value") # Default value is `"default_value"`
    ...
```

#### Accessing model, physical table, and virtual layer view names

All Vulcan models have a name in their `MODEL` specification. We refer to that as the model's "unresolved" name because it may not correspond to any specific object in the SQL engine.

When Vulcan renders and executes a model, it converts the model name into three forms at different stages:

1. The *fully qualified* name

    - If the model name is of the form `schema.table`, Vulcan determines the correct catalog and adds it, like `catalog.schema.table`

    - Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like `"catalog"."schema"."table"`

2. The *resolved* physical table name

    - The qualified name of the model's underlying physical table

3. The *resolved* virtual layer view name

    - The qualified name of the model's virtual layer view in the environment where the model is being executed

You can access any of these three forms in a Python macro through properties of the `evaluation` context object.

Access the unresolved, fully-qualified name through the `this_model_fqn` property.

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    # Example:
    # Name in model definition: landing.customers
    # Value returned here: '"datalake"."landing"."customers"'
    unresolved_model_fqn = evaluator.this_model_fqn
    ...
```

Access the resolved physical table and virtual layer view names through the `this_model` property.

The `this_model` property returns different names depending on the runtime stage:

- `promoting` runtime stage: `this_model` resolves to the virtual layer view name

    - Example

        - Model name is `db.test_model`

        - `plan` is running in the `dev` environment

        - `this_model` resolves to `"catalog"."db__dev"."test_model"` (note the `__dev` suffix in the schema name)

- All other runtime stages: `this_model` resolves to the physical table name

    - Example

        - Model name is `db.test_model`

        - `plan` is running in any environment

        - `this_model` resolves to `"catalog"."vulcan__project"."project__test_model__684351896"`

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    if evaluator.runtime_stage == "promoting":
        # virtual layer view name '"catalog"."db__dev"."test_model"'
        resolved_name = evaluator.this_model
    else:
        # physical table name '"catalog"."vulcan__project"."project__test_model__684351896"'
        resolved_name = evaluator.this_model
    ...
```

#### Accessing model schemas

Model schemas can be accessed within a Python macro function through its evaluation context's `column_to_types()` method, if the column types can be statically determined. For instance, a schema of an [external model](../../model/types/external_models.md) can be accessed only after the `vulcan create_external_models` command has been executed.

This macro function renames the columns of an upstream model by adding a prefix to them:

```python linenums="1"
from sqlglot import exp
from vulcan.core.macros import macro

@macro()
def prefix_columns(evaluator, model_name, prefix: str):
    renamed_projections = []

    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,
    # assuming that it does not contain quotes. If it did, we would have to generate SQL for
    # each part of `model_name` separately and then concatenate these parts, because in that
    # case `model_name.sql()` would produce an invalid lookup key.
    model_name_sql = model_name.sql()

    for name in evaluator.columns_to_types(model_name_sql):
        new_name = prefix + name
        renamed_projections.append(exp.column(name).as_(new_name))

    return renamed_projections
```

This can then be used in a SQL model like this:

```sql linenums="1"
MODEL (
  name schema.child,
  kind FULL
);

SELECT
  @prefix_columns(schema.parent, 'stg_')
FROM
  schema.parent
```

Note that `columns_to_types` expects an _unquoted model name_, such as `schema.parent`. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for `model_name` using the `sql()` method.

Accessing the schema of an upstream model can be useful for various reasons. For example:

- Renaming columns so that downstream consumers are not tightly coupled to external or source tables

- Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)

- Applying transformations to columns, such as masking PII or computing various statistics based on the column types

Thus, leveraging `columns_to_types` can also enable one to write code according to the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.

Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: `__schema_unavailable_at_load__`. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.

#### Accessing snapshots

After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the `get_snapshot` method of `MacroEvaluator`.

This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    if evaluator.runtime_stage == "evaluating":
        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly
        intervals = evaluator.get_snapshot("some_model_name").intervals
        ...
    ...
```

#### Using SQLGlot expressions

Vulcan automatically parses strings returned by Python macro functions into [SQLGlot](https://github.com/tobymao/sqlglot) expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.

For example, consider a macro function that uses the `BETWEEN` operator in the predicate of a `WHERE` clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:

```python linenums="1"
from vulcan import macro, SQL

@macro()
def between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):
    return f"{column_name} BETWEEN {low_val} AND {high_val}"
```

The function could then be called in a query:

```sql linenums="1"
SELECT
  a
FROM table
WHERE @between_where(a, 1, 3)
```

And it would render to:

```sql linenums="1"
SELECT
  a
FROM table
WHERE a BETWEEN 1 and 3
```

Alternatively, the function could return a [SQLGLot expression](https://github.com/tobymao/sqlglot/blob/main/sqlglot/expressions.py) equivalent to that string by using SQLGlot's expression methods for building semantic representations:

```python linenums="1"
from vulcan import macro

@macro()
def between_where(evaluator, column, low_val, high_val):
    return column.between(low_val, high_val)
```

The methods are available because the `column` argument is parsed as a SQLGlot [Column expression](https://sqlglot.com/sqlglot/expressions.html#Column) when the macro function is executed.

Column expressions are sub-classes of the [Condition class](https://sqlglot.com/sqlglot/expressions.html#Condition), so they have builder methods like [`between`](https://sqlglot.com/sqlglot/expressions.html#Condition.between) and [`like`](https://sqlglot.com/sqlglot/expressions.html#Condition.like).

#### Macro pre/post-statements

Macro functions may be used to generate pre/post-statements in a model.

By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.

If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.

Specify that a macro only affects a model's metadata by setting the `@macro()` decorator's `metadata_only` argument to `True`. For example:

```python linenums="1" hl_lines="3"
from vulcan import macro

@macro(metadata_only=True)
def print_message(evaluator, message):
  print(message)
```

### Typed macros

Typed macros bring Python's type hinting to your SQL macros. By specifying what types your macro expects, you make your code more readable, easier to maintain, and less prone to errors. Plus, IDEs can give you better autocomplete and catch mistakes before you run your code.

#### Benefits of Typed Macros

1. **Improved Readability**: By specifying types, the intent of the macro is clearer to other developers or future you.
2. **Reduced Boilerplate**: No need for manual type conversion within the macro function, allowing you to focus on the core logic.
3. **Enhanced Autocompletion**: IDEs can provide better autocompletion and documentation based on the specified types.

#### Defining a Typed Macro

Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:

```python linenums="1"
from vulcan import macro

@macro()
def repeat_string(evaluator, text: str, count: int):
    return text * count
```

This macro takes two arguments: `text` of type `str` and `count` of type `int`, and it returns a string.

Without type hints, the inputs are two SQLGlot `exp.Literal` objects you would need to manually convert to Python `str` and `int` types. With type hints, you can work with them as string and integer types directly.

Let's try to use the macro in a Vulcan model:

```sql linenums="1"
SELECT
  @repeat_string('Vulcan ', 3) as repeated_string
FROM some_table;
```

Unfortunately, this model generates an error when rendered:

```
Error: Invalid expression / Unexpected token. Line 1, Col: 23.
  Vulcan Vulcan Vulcan
```

Why? The macro returned `Vulcan Vulcan Vulcan` as expected, but that string is not valid SQL in the rendered query:

```sql linenums="1" hl_lines="2"
SELECT
  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code
FROM some_table;
```

The problem is a mismatch between our macro's Python return type `str` and the type expected by the parsed SQL query.

Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.

Therefore, we must explicitly convert the output with SQLGlot's `exp.Literal.string()` method:

```python linenums="1" hl_lines="5"
from vulcan import macro

@macro()
def repeat_string(evaluator, text: str, count: int):
    return exp.Literal.string(text * count)
```

Now the query will render with a valid single-quoted string literal:

```sql linenums="1"
SELECT
  'Vulcan Vulcan Vulcan ' AS "repeated_string"
FROM "some_table" AS "some_table"
```

Typed macros coerce the **inputs** to a macro function, but the macro code is responsible for coercing the **output** to the type expected by the query's semantic representation.

#### Supported Types

Vulcan supports common Python types for typed macros including:

- `str` -- This handles string literals and basic identifiers, but won't coerce anything more complicated.

- `int`

- `float`

- `bool`

- `datetime.datetime`

- `datetime.date`

- `SQL` -- When you want the SQL string representation of the argument that's passed in

- `list[T]` - where `T` is any supported type including sqlglot expressions

- `tuple[T]` - where `T` is any supported type including sqlglot expressions

- `T1 | T2 | ...` - where `T1`, `T2`, etc. are any supported types including sqlglot expressions

We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:

- `exp.Table`

- `exp.Column`

- `exp.Literal`

- `exp.Identifier`

While these might be obvious examples, you can effectively coerce an input into _any_ SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.

```python linenums="1"
@macro()
def stamped(evaluator, query: exp.Select) -> exp.Subquery:
    return query.select(exp.Literal.string(str(datetime.now())).as_("stamp")).subquery()

# Coercing to a complex node like `exp.Select` works as expected given a string literal input
# SELECT * FROM @stamped('SELECT a, b, c')
```

When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an `assert` statement. For example:

```python linenums="1"
@macro()
def my_macro(evaluator, table: exp.Table) -> exp.Column:
    assert isinstance(table, exp.Table)
    table.set("catalog", "dev")
    return table

# Works
# SELECT * FROM @my_macro('some.table')
# SELECT * FROM @my_macro(some.table)

# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning
# SELECT * FROM @my_macro('SELECT 1 + 1')
```

In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you **also** get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.

#### Advanced Typed Macros

You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:

```python linenums="1"
from typing import List
from vulcan import macro

@macro()
def sum_integers(evaluator, numbers: List[int]) -> int:
    return sum(numbers)
```

Usage in Vulcan:

```sql linenums="1"
SELECT
  @sum_integers([1, 2, 3, 4, 5]) as total
FROM some_table;
```

Generics can be nested and are resolved recursively allowing for fairly robust type hinting.

See examples of the coercion function in action in the test suite [here](https://github.com/TobikoData/vulcan/blob/main/tests/core/test_macros.py).

#### Conclusion

Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.

## Mixing macro systems

Vulcan supports both Vulcan macros and [Jinja macros](./jinja.md), but we strongly recommend picking one system per model. If you mix them, things can get confusing or break in unexpected ways. Pick the one that fits your needs and stick with it.



# Jinja

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/macros/jinja/

---

# Jinja

Vulcan supports macros from the [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) templating system. If you're already familiar with Jinja (maybe from dbt or other tools), you can use it here.

Jinja works differently than Vulcan's native macros. While Vulcan macros understand the semantic structure of your SQL, Jinja macros are pure string substitution, they assemble SQL text by replacing placeholders, without building a semantic representation of the query.

!!! note "dbt compatibility"
    Vulcan supports the standard Jinja function library, but **not** dbt-specific functions like `{{ ref() }}`. If you're working with a dbt project using the Vulcan adapter, dbt-specific functions will work there, but not in native Vulcan projects.

## The basics

Jinja uses curly braces `{}` to mark macro code. The second character after the opening brace tells Jinja what to do:

- `{{...}}` - **Expressions**: These get replaced with values in your rendered SQL. Use them for variables and function calls.

- `{%...%}` - **Statements**: These control flow and logic. Use them for `if` statements, `for` loops, and setting variables.

- `{#...#}` - **Comments**: These are stripped out and won't appear in your final SQL.

Since Jinja syntax isn't valid SQL, you need to wrap your Jinja queries in special blocks so Vulcan knows to process them. For queries, use `JINJA_QUERY_BEGIN; ...; JINJA_END;`:

```sql linenums="1" hl_lines="5 9"
MODEL (
  name vulcan_example.full_model
);

JINJA_QUERY_BEGIN;

SELECT {{ 1 + 1 }};

JINJA_END;
```

For pre/post-statements (code that runs before or after your query), use `JINJA_STATEMENT_BEGIN; ...; JINJA_END;`:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model
);

JINJA_STATEMENT_BEGIN;
{{ pre_hook() }}
JINJA_END;

JINJA_QUERY_BEGIN;
SELECT {{ 1 + 1 }};
JINJA_END;

JINJA_STATEMENT_BEGIN;
{{ post_hook() }}
JINJA_END;
```

## Using Vulcan's predefined variables

You can use all of Vulcan's [predefined macro variables](./variables.md) in your Jinja code. Some give you information about the Vulcan project itself (like `runtime_stage` or `this_model`), while others are temporal (like `start_ds` and `execution_date` for incremental models).

Access them by putting the variable name (unquoted) inside curly braces:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';

JINJA_END;
```

Notice the single quotes around the variable references? That's because `start_ds` and `end_ds` return string values. For numeric variables like `start_epoch`, you wouldn't need the quotes.

One special case: the `gateway` variable is a function call, so you need parentheses: `{{ gateway() }}` instead of just `{{ gateway }}`.

## User-defined variables

Beyond the predefined variables, you can create your own. Vulcan supports global variables (defined in your project config) and local variables (defined in a specific model).

### Global variables

Global variables are defined in your project configuration file and can be used in any model. Learn more about setting them up in the [Vulcan macros documentation](./built_in.md#global-variables).

Access them using the `{{ var() }}` function. Pass the variable name (in single quotes) as the first argument, and optionally a default value as the second:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE int_variable = {{ var('int_var') }};

JINJA_END;
```

If the variable might not exist, provide a default:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE some_value = {{ var('missing_var', 0) }};

JINJA_END;
```

If `missing_var` isn't defined, this will use `0` as the fallback value.

### Gateway variables

Gateway variables work just like global variables, but they're defined in a specific gateway's configuration. They take precedence over global variables with the same name. Learn more in the [Vulcan macros documentation](./built_in.md#gateway-variables).

Access them the same way as global variables using `{{ var() }}`.

### Blueprint variables

Blueprint variables let you create model templates. They're defined in the `MODEL` block and can be used to generate multiple models from one template:

```sql linenums="1"
MODEL (
  name @customer.some_table,
  kind FULL,
  blueprints (
    (customer := customer1, field_a := x, field_b := y),
    (customer := customer2, field_a := z)
  )
);

JINJA_QUERY_BEGIN;
SELECT
  {{ blueprint_var('field_a') }}
  {{ blueprint_var('field_b', 'default_b') }} AS field_b
FROM {{ blueprint_var('customer') }}.some_source
JINJA_END;
```

Use `{{ blueprint_var() }}` to access them, with an optional default value just like `{{ var() }}`.

### Local variables

Define variables that are only available in the current model using `{% set ... %}`:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

JINJA_QUERY_BEGIN;

{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`

SELECT
  item_id,
  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}
FROM
  vulcan_example.incremental_model
GROUP BY item_id

JINJA_END;
```

The `{% set %}` statement goes after the `MODEL` block and before your SQL query.

Jinja variables can be strings, numbers, or even complex data structures like lists, tuples, or dictionaries. They support Python methods too, so you can call `.upper()` on strings, iterate over lists, and so on.

## Control flow

Jinja gives you control flow operators to make your SQL dynamic.

### For loops

For loops let you iterate over collections to generate repetitive SQL. They start with `{% for ... %}` and end with `{% endfor %}`.

Here's an example that creates indicator columns for different vehicle types:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT
  {% for vehicle_type in ['car', 'truck', 'bus'] %}
    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},
  {% endfor %}
FROM table

JINJA_END;
```

A few things to notice:

- The values in the list are quoted: `['car', 'truck', 'bus']`

- When you use `{{ vehicle_type }}` in the `CASE WHEN`, you need quotes around it: `'{{ vehicle_type }}'`

- When you use it in an identifier name like `vehicle_{{ vehicle_type }}`, no quotes needed

- There's a trailing comma after the `CASE WHEN` line, Vulcan's semantic understanding will remove it automatically

This renders to:

```sql linenums="1"
SELECT
  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,
  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,
  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus
FROM table
```

It's usually better to define your lists separately:

```sql linenums="1"
JINJA_QUERY_BEGIN;

{% set vehicle_types = ['car', 'truck', 'bus'] %}

SELECT
  {% for vehicle_type in vehicle_types %}
    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},
  {% endfor %}
FROM table

JINJA_END;
```

Same result, but easier to maintain.

### If statements

If statements let you conditionally include SQL based on some condition. They start with `{% if ... %}` and end with `{% endif %}`.

The condition needs to evaluate to `True` or `False`. Things like `True`, `1 + 1 == 2`, or `'a' in ['a', 'b']` all work.

Here's an example that conditionally includes a testing column:

```sql linenums="1"
JINJA_QUERY_BEGIN;

{% set testing = True %}

SELECT
  normal_column,
  {% if testing %}
    testing_column
  {% endif %}
FROM table

JINJA_END;
```

Since `testing` is `True`, this renders to:

```sql linenums="1"
SELECT
  normal_column,
  testing_column
FROM table
```

## User-defined macro functions

Macro functions let you reuse code across multiple models. Define them in `.sql` files in your project's `macros` directory (you can put multiple functions in one file or split them up).

Define a function with `{% macro %}` and `{% endmacro %}`:

```sql linenums="1"
{% macro print_text() %}
text
{% endmacro %}
```

Call it in your model with `{{ print_text() }}`, and it gets replaced with `text`.

Functions can take arguments:

```sql linenums="1"
{% macro alias(expression, alias) %}
  {{ expression }} AS {{ alias }}
{% endmacro %}
```

Use it like this:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT
  item_id,
  {{ alias('item_id', 'item_id2')}}
FROM table

JINJA_END;
```

This renders to:

```sql linenums="1"
SELECT
  item_id,
  item_id AS item_id2
FROM table
```

Notice that even though you quoted the arguments in the function call, they're not quoted in the output. Vulcan's semantic understanding recognizes that `item_id` is a column name and handles it appropriately.

If you want to select a string literal instead of a column, use double quotes around the string in the function call:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT
  item_id,
  {{ alias("'item_id'", 'item_id2')}}
FROM table

JINJA_END;
```

This renders to:

```sql linenums="1"
SELECT
  item_id,
  'item_id' AS item_id2
FROM table
```

The double quotes tell Vulcan "this is a string literal, not a column name." You can also use `'"item_id"'` if you want double quotes in the output (useful for some SQL dialects).

## Mixing macro systems

Vulcan supports both Jinja and [Vulcan macros](./built_in.md), but we strongly recommend picking one system per model. Mixing them can lead to confusing behavior or errors.

You can use [predefined Vulcan macro variables](./variables.md) in Jinja queries, but if you're passing them as arguments to a Jinja macro function, use the Jinja syntax `{{ start_ds }}` instead of the Vulcan `@start_ds` syntax. You may need to add quotes depending on what you're doing.



# Overview

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/macros/overview/

---

# Overview

SQL is a [declarative language](https://en.wikipedia.org/wiki/Declarative_programming), which means you describe what you want, not how to get it. This provides clarity, but SQL doesn't have built-in features like variables or control flow (if-then statements, loops) that let your queries adapt to different situations.

The problem? Data models are dynamic. You need different behavior depending on context, maybe filter by a different date each day, or include different columns based on configuration. That's where macros come in.

Macros let you make your SQL dynamic. Instead of hardcoding values, you can use variables that get substituted at runtime. Instead of writing repetitive code, you can use functions that generate SQL for you.

Vulcan supports two macro systems, each with its own strengths:

- **Vulcan macros**: Built specifically for SQL, with semantic understanding of your queries

- **Jinja macros**: The popular templating system, useful if you're already familiar with it

Both systems can use the same [pre-defined variables](./variables.md) that Vulcan provides, like `@execution_ds` for the current execution date or `@this_model` for the current model name.

Next steps:

- [Pre-defined macro variables](./variables.md) - Built-in variables available in both systems

- [Vulcan macros](./built_in.md) - Vulcan's native macro system with SQL-aware features

- [Jinja macros](./jinja.md) - The Jinja templating system for SQL



# Variables

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/macros/variables/

---

# Variables

Macro variables are placeholders that get replaced with actual values when Vulcan renders your SQL. They're what make your queries dynamic, instead of hardcoding values, you use variables that change based on context.

Instead of writing `WHERE date > '2023-01-01'` and manually updating it every day, you write `WHERE date > @execution_ds` and it automatically uses today's date.

!!! note
    This page covers Vulcan's built-in macro variables, the ones that come pre-configured and ready to use. If you want to create your own custom variables, check out the [Vulcan macros page](./built_in.md#user-defined-variables) or [Jinja macros page](./jinja.md#user-defined-variables).

## A quick example

Let's say you have a query that filters by date. Without macros, you'd write something like this:

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > '2023-01-01'
```

Every time you want to change the date, you have to edit the query. That's tedious and error-prone.

With a macro variable, you can make it dynamic:

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > @execution_ds
```

The `@` symbol tells Vulcan "this is a macro variable, replace it with a value before executing." The `@execution_ds` variable is predefined, so Vulcan automatically sets it to the date when execution started.

If you run this model on February 1, 2023, Vulcan renders it as:

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > '2023-02-01'
```

The date updates automatically every time you run it. No manual editing needed!

Vulcan comes with a bunch of predefined variables like this. You can also create your own custom variables if you need something specific, we'll cover those in the macro system pages.

## Predefined variables

Vulcan provides a set of predefined variables that are automatically available in your models. Most of them are related to time (dates, timestamps, etc.), since time-based logic is common in data models.

The time variables follow a consistent naming pattern: they combine a prefix (like `start`, `end`, or `execution`) with a postfix (like `ds`, `ts`, or `epoch`) to create variables like `@start_ds` or `@execution_epoch`.

### Temporal variables

Vulcan uses Python's [datetime module](https://docs.python.org/3/library/datetime.html) under the hood and follows the standard [Unix epoch](https://en.wikipedia.org/wiki/Unix_time) (starting January 1, 1970).

!!! tip "Important"
    All time-related predefined variables use [UTC time zone](https://en.wikipedia.org/wiki/Coordinated_Universal_Time). If you need to work with other timezones, you'll handle that in your query logic.

    Learn more about timezones and incremental models [here](../../model/model_kinds.md#timezones).

**Prefixes** tell you what time period the variable represents:

- `start` - The beginning of the time interval for this model run (inclusive)

- `end` - The end of the time interval for this model run (inclusive)

- `execution` - The exact timestamp when the execution started

**Postfixes** tell you what format the value is in:

- `dt` - A Python datetime object that becomes a SQL `TIMESTAMP`

- `dtntz` - A Python datetime object that becomes a SQL `TIMESTAMP WITHOUT TIME ZONE`

- `date` - A Python date object that becomes a SQL `DATE`

- `ds` - A date string formatted as `'YYYY-MM-DD'` (like `'2023-02-01'`)

- `ts` - An ISO 8601 datetime string: `'YYYY-MM-DD HH:MM:SS'`

- `tstz` - An ISO 8601 datetime string with timezone: `'YYYY-MM-DD HH:MM:SS+00:00'`

- `hour` - An integer from 0-23 representing the hour of the day

- `epoch` - An integer representing seconds since Unix epoch

- `millis` - An integer representing milliseconds since Unix epoch

Here are all the temporal variables you can use:

**dt (datetime objects):**

- `@start_dt`

- `@end_dt`

- `@execution_dt`

**dtntz (datetime without timezone):**

- `@start_dtntz`

- `@end_dtntz`

- `@execution_dtntz`

**date (date objects):**

- `@start_date`

- `@end_date`

- `@execution_date`

**ds (date strings):**

- `@start_ds`

- `@end_ds`

- `@execution_ds`

**ts (timestamp strings):**

- `@start_ts`

- `@end_ts`

- `@execution_ts`

**tstz (timestamp strings with timezone):**

- `@start_tstz`

- `@end_tstz`

- `@execution_tstz`

**hour (hour integers):**

- `@start_hour`

- `@end_hour`

- `@execution_hour`

**epoch (Unix epoch seconds):**

- `@start_epoch`

- `@end_epoch`

- `@execution_epoch`

**millis (Unix epoch milliseconds):**

- `@start_millis`

- `@end_millis`

- `@execution_millis`

### Runtime variables

Beyond time, Vulcan provides variables that give you information about the current execution context:

- **`@runtime_stage`** - A string telling you what stage Vulcan is currently in. Useful for conditionally running code based on whether you're creating tables, evaluating queries, or promoting views. Possible values:

  - `'loading'` - Project is being loaded into Vulcan's runtime

  - `'creating'` - Model tables are being created for the first time

  - `'evaluating'` - Model query is being evaluated and data inserted

  - `'promoting'` - Model is being promoted (view created in virtual layer)

  - `'demoting'` - Model is being demoted (view dropped from virtual layer)

  - `'auditing'` - Audit is being run

  - `'testing'` - Model is being evaluated in a unit test context
  
  Learn more about using this in [pre/post-statements](../../model/types/sql_models.md#optional-prepost-statements).

- **`@gateway`** - The name of the current [gateway](../../../references/configuration.md#gateways) (your database connection)

- **`@this_model`** - The physical table name that the model's view selects from. Useful for creating [generic audits](../../audits/audits.md#generic-audits). When used in [on_virtual_update statements](../../model/types/sql_models.md#optional-on-virtual-update-statements), it contains the qualified view name instead.

- **`@model_kind_name`** - The name of the current model kind (like `'FULL'` or `'INCREMENTAL_BY_TIME_RANGE'`). Useful when you need to control [physical properties in model defaults](../../../references/model_configuration.md#model-defaults) based on the model kind.

!!! note "Embedding variables in strings"
    Sometimes you'll see variables written with curly braces like `@{variable}` instead of just `@variable`. They do different things!

    The curly brace syntax tells Vulcan to treat the rendered value as a SQL identifier (like a table or column name), not a string literal. So if `variable` contains `foo.bar`, then:

    - `@variable` produces `foo.bar` (as a literal value)

    - `@{variable}` produces `"foo.bar"` (as an identifier, with quotes)

    You'll most often use `@{variable}` when you want to interpolate a value into an identifier name, like `@{schema}_table`. The regular `@variable` syntax is for plain value substitution.

    Learn more in the [Vulcan macros documentation](./built_in.md#embedding-variables-in-strings).

#### Before all and after all variables

These variables are available in [`before_all` and `after_all` statements](../../../references/configuration.md#before_all--after_all), as well as in any macros called within those statements:

- **`@this_env`** - The name of the current [environment](../../../references/environments.md)

- **`@schemas`** - A list of schema names in the [virtual layer](../../../references/glossary.md#virtual-layer) for the current environment

- **`@views`** - A list of view names in the [virtual layer](../../../references/glossary.md#virtual-layer) for the current environment

These are useful when you need to perform setup or cleanup operations that depend on the environment context.



# Signals

Source: https://tmdc-io.github.io/vulcan-book/components/advanced-features/signals/

---

# Signals

Vulcan's built-in scheduler knows when to run your models based on their `cron` schedules. If you have a model set to run `@daily`, it checks whether a day has passed since the last run and evaluates the model if needed.

But here's the thing: real-world data doesn't always follow our schedules. Sometimes data arrives late, maybe your upstream system had an issue, or a batch job ran behind schedule. When that happens, your daily model might have already run for the day, and that late data won't get processed until tomorrow's scheduled run.

Signals solve this problem by letting you add custom conditions that must be met before a model runs. They are extra gates that the scheduler checks, beyond "has enough time passed?" and "are upstream dependencies done?"

## What is a signal?

By default, Vulcan's scheduler uses two criteria to decide if a model should run:

1. Has the model's `cron` interval elapsed since the last evaluation?
2. Have all upstream dependencies finished running?

Signals let you add a third criterion: your own custom check. A signal is just a Python function that examines a batch of time intervals and decides whether they're ready for evaluation.

Here's how it works under the hood: The scheduler doesn't actually evaluate "a model", it evaluates a model over specific time intervals. For incremental models, this is obvious (you're processing a date range). But even non-temporal models like `FULL` and `VIEW` are evaluated based on time intervals, their `cron` frequency determines the interval.

The scheduler looks at candidate intervals, groups them into batches (controlled by your model's `batch_size` parameter), and then checks signals to see if those batches are ready. Your signal function gets called with a batch of time intervals and can return:

- `True` if all intervals in the batch are ready

- `False` if none are ready

- A list of specific intervals if only some are ready

!!! note "One model, multiple signals"
    You can specify multiple signals for a single model. When you do, Vulcan requires that **all** signal functions agree an interval is ready before it gets evaluated. It works like an AND gate: every signal must give the green light.

## Defining a signal

To create a signal, add a `signals` directory to your project and create your signal function in `__init__.py` (you can organize signals across multiple Python files if you prefer).

A signal function needs to:

- Accept a batch of time intervals (`DateTimeRanges: t.List[t.Tuple[datetime, datetime]]`)

- Return either a boolean or a list of intervals

- Use the `@signal` decorator

Let's look at some examples, starting simple and building up to more complex use cases.

### Simple example

Here's a basic signal that randomly decides whether intervals are ready (useful for testing, maybe not so much for production!):

```python linenums="1"
import random
import typing as t
from vulcan import signal, DatetimeRanges


@signal()
def random_signal(batch: DatetimeRanges, threshold: float) -> t.Union[bool, DatetimeRanges]:
    return random.random() > threshold
```

This signal takes a `threshold` argument (you'll pass this from your model definition) and returns `True` if a random number exceeds that threshold. Notice how the function signature includes `threshold: float`, Vulcan will automatically extract this from your model definition and pass it to the function. The type inference works the same way as [Vulcan macros](./macros/built_in.md#typed-macros).

To use this signal in a model, add it to the `signals` key in your `MODEL` block:

```sql linenums="1" hl_lines="4-6"
MODEL (
  name example.signal_model,
  kind FULL,
  signals (
    random_signal(threshold := 0.5), # specify threshold value
  )
);

SELECT 1
```

The `signals` key accepts a list of signal calls, each with its own arguments. When you run `vulcan run`, this signal checks if a random number is greater than 0.5. If it is, the model runs; otherwise, it waits.

### Advanced example

Sometimes you want more fine-grained control. Instead of saying "all intervals are ready" or "none are ready," you can return specific intervals from the batch. Here's an example that only allows intervals from at least one week ago:

```python
import typing as t

from vulcan import signal, DatetimeRanges
from vulcan.utils.date import to_datetime


# signal that returns only intervals that are <= 1 week ago
@signal()
def one_week_ago(batch: DatetimeRanges) -> t.Union[bool, DatetimeRanges]:
    dt = to_datetime("1 week ago")

    return [
        (start, end)
        for start, end in batch
        if start <= dt
    ]
```

Instead of returning `True` or `False` for the entire batch, this function filters the batch and returns only the intervals that meet the criteria. It compares each interval's start time to "1 week ago" and includes only those that are old enough.

Use it in an incremental model like this:

```sql linenums="1" hl_lines="7-10"
MODEL (
  name example.signal_model,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column ds,
  ),
  start '2 week ago',
  signals (
    one_week_ago(),
  )
);

SELECT @start_ds AS ds
```

This ensures that only data from at least a week ago gets processed, useful if you want to wait for late-arriving data to stabilize before processing it.

### Accessing execution context

Sometimes you need to check something in your database or access the execution context. You can do that by adding a `context` parameter to your signal function:

```python
import typing as t

from vulcan import signal, DatetimeRanges, ExecutionContext


# add the context argument to your function
@signal()
def one_week_ago(batch: DatetimeRanges, context: ExecutionContext) -> t.Union[bool, DatetimeRanges]:
    return len(context.engine_adapter.fetchdf("SELECT 1")) > 1
```

The `context` parameter gives you access to the engine adapter, so you can query your warehouse, check if certain tables exist, verify data freshness, or perform any other checks you need.

### Testing signals

Signals only evaluate when you run `vulcan run` or use the `check_intervals` command. To test your signals without actually running models:

1. Deploy your changes to an environment: `vulcan plan my_dev`
2. Check which intervals would be evaluated: `vulcan check_intervals my_dev`
   - Use `--select-model` to check specific models

   - Use `--no-signals` to see what would run without signal checks
3. Iterate by making changes to your signal and redeploying

!!! note
    The `check_intervals` command only works with remote models that have been deployed to an environment. Local signal changes won't be tested until you deploy them.

This workflow lets you verify your signal logic before it affects your actual model runs.



# Audits

Source: https://tmdc-io.github.io/vulcan-book/components/audits/audits/

---

# Audits

Audits stop bad data before it causes problems downstream. They run after every model execution and halt your models if something's wrong.

Unlike [tests](../tests/tests.md) (which you run manually to verify logic), audits run automatically whenever you apply a [plan](../../guides/plan.md). They catch data quality issues early, whether they come from external vendors, upstream teams, or your own model changes.

All audits in Vulcan are blocking. When an audit fails, Vulcan stops everything: no plan application, no run execution. This prevents bad data from propagating through your entire pipeline.

A comprehensive suite of audits helps you catch problems upstream, builds trust in your data across the organization, and lets your team work with confidence knowing that invalid data won't slip through.

> **Note:** For incremental by time range models, audits only run on the intervals being processed, not the entire table. This keeps things fast and focused on what actually changed.

## Terminology: Audits and Assertions

Before we dive in, let's clear up some terminology. Vulcan uses two related but distinct concepts:

- **AUDIT** - The validation rule itself (the SQL query that checks for problems)

- **ASSERTION** - Attaching an audit to a model (claiming it should pass)

An audit is the rule ("prices must be positive"), and an assertion is you saying "this model follows that rule."

**In MODEL definitions:**

```sql
-- Define the AUDIT (the rule)
AUDIT (name check_positive_price);
SELECT * FROM @this_model WHERE price <= 0;

-- Make ASSERTIONS about your model (attach the audit)
MODEL (
  name products,
  assertions (check_positive_price)  -- Declaring this audit should pass
);
```

> **Note:** You might see older code using `audits` instead of `assertions` in MODEL definitions. Both work identically, but `assertions` is clearer, you're asserting that your model passes these audits. This documentation uses `assertions` throughout.

## How Audits Work

When an audit fails, Vulcan stops everything. No ifs, ands, or buts. This is by design, it's better to catch problems early than to let bad data flow downstream and cause bigger issues.

Here's what happens when you run a model:

1. **Evaluate the model** - Vulcan runs your model SQL (inserts new data, rebuilds the table, etc.)
2. **Run the audit query** - Vulcan executes your audit SQL against the newly updated table. For incremental models, this only checks the intervals you're processing (keeps things fast!)
3. **Check the results** - If the query returns any rows, the audit fails and everything stops

**Why this matters:** Audits query for bad data. If your audit finds bad data (returns rows), that's a problem. If it finds nothing (returns zero rows), you're good to go.

### Plan vs. Run

The difference between `plan` and `run` matters a lot when it comes to audits:

**`plan`** - The safe way:
- Vulcan evaluates and audits all modified models *before* promoting them to production

- If an audit fails, the plan stops and your production table is untouched

- Invalid data stays in an isolated table and never reaches production

- This is like testing in a sandbox before deploying

**`run`** - The direct way:
- Vulcan evaluates and audits models directly against the production environment

- If an audit fails, the run stops, but the invalid data *is already in production*

- The blocking prevents this bad data from being used to build downstream models

- This is like deploying directly, faster, but riskier

**Which should you use?** For production changes, use `plan`. It's safer and gives you a chance to fix issues before they hit production. Use `run` when you're confident or doing quick iterations.

### Fixing a Failed Audit

So an audit failed. Don't panic! Here's how to fix it:

1. **Find the root cause** - Look at the audit query results. What data failed? Check upstream models and data sources.

2. **Fix the source** - This depends on where the problem came from:
   - **External data source?** Fix it at the source, then run a [restatement plan](../../guides/plan.md#restatement-plans) on the first Vulcan model that ingests it. This will restate all downstream models automatically.

   - **Vulcan model?** Update the model's logic, then apply the change with a `plan`. Vulcan will automatically re-evaluate all downstream models.

The key is fixing the root cause, not just the symptom. If bad data is coming from upstream, fixing it downstream won't help long-term.

## User-Defined Audits

You can write your own audits! They're just SQL queries that should return zero rows. If they return rows, that means they found bad data and the audit fails.

Audits live in `.sql` files in an `audits` directory in your project. You can put multiple audits in one file (organize them however makes sense) or define them inline in your model files.

### Your First Audit

Let's create a simple audit. Here's the basic structure:

```sql linenums="1"
AUDIT (
  name assert_item_price_is_not_null,
  dialect spark
);
SELECT * from sushi.items
WHERE
  ds BETWEEN @start_ds AND @end_ds
  AND price IS NULL;
```

This audit checks that every sushi item has a price. If any items are missing prices (the query returns rows), the audit fails.

**A few things to note:**

- The `name` is what you'll reference when attaching it to a model

- If your query uses a different SQL dialect than your project, specify it with `dialect` (like `spark` in the example)

- The `@start_ds` and `@end_ds` macros are automatically filled in for incremental models

To actually use this audit, attach it to a model:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (assert_item_price_is_not_null)
);
```

This audit runs every time the `sushi.items` model runs.

### Generic Audits

Here's where audits get really powerful. You can create parameterized audits that work across multiple models. This saves you from writing the same audit over and over.

Consider this audit that checks if a column exceeds a threshold:

```sql linenums="1"
AUDIT (
  name does_not_exceed_threshold
);
SELECT * FROM @this_model
WHERE @column >= @threshold;
```

This uses [macros](../advanced-features/macros/overview.md) to make it flexible:

- `@this_model` is a special macro that refers to the model being audited (and handles incremental models correctly)

- `@column` and `@threshold` are parameters you'll specify when you use the audit

Now you can use this same audit for different columns and thresholds:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    does_not_exceed_threshold(column := id, threshold := 1000),
    does_not_exceed_threshold(column := price, threshold := 100)
  )
);
```

You can use the same audit multiple times on the same model with different parameters.

**Default values:**

You can set default values for parameters:

```sql linenums="1"
AUDIT (
  name does_not_exceed_threshold,
  defaults (
    threshold = 10,
    column = id
  )
);
SELECT * FROM @this_model
WHERE @column >= @threshold;
```

If someone uses the audit without specifying parameters, it uses these defaults.

**Global audits:**

You can also apply audits globally using model defaults:

```sql linenums="1"
model_defaults:
  assertions:
    - assert_positive_order_ids

    - does_not_exceed_threshold(column := id, threshold := 1000)
```

This applies these audits to all models by default.

> **Note:** In `model_defaults`, you can use either `audits` or `assertions`, both work for backward compatibility.

### Naming

**Avoid SQL keywords** when naming audit parameters. If you must use a keyword, quote it.

For example, if your audit uses a `values` parameter (which is a SQL keyword), you'll need quotes:

```sql linenums="1" hl_lines="4"
MODEL (
  name sushi.items,
  assertions (
    my_audit(column := a, "values" := (1,2,3))
  )
)
```

It's easier to just avoid keywords in the first place, but if you need them, quotes work fine.

### Inline Audits

You can also define audits right in your model file. This is useful when an audit is specific to one model:

```sql linenums="1"
MODEL (
    name sushi.items,
    assertions(does_not_exceed_threshold(column := id, threshold := 1000), price_is_not_null)
);
SELECT id, price
FROM sushi.seed;

AUDIT (name does_not_exceed_threshold);
SELECT * FROM @this_model
WHERE @column >= @threshold;

AUDIT (name price_is_not_null);
SELECT * FROM @this_model
WHERE price IS NULL;
```

You can define multiple audits in the same file. Just make sure they're defined before (or alongside) the MODEL that uses them.

## Built-in Audits

Vulcan comes with a whole suite of built-in audits that cover most common use cases. These are ready to use, no need to write SQL yourself for these scenarios.

All built-in audits are blocking (they stop execution when they fail), and they're grouped by what they check. Let's walk through them:

### Generic Assertion Audit

#### `forall`

The most flexible built-in audit. It lets you write arbitrary boolean SQL expressions:

```sql linenums="1" hl_lines="4-7"
MODEL (
  name sushi.items,
  assertions (
    forall(criteria := (
      price > 0,
      LENGTH(name) > 0
    ))
  )
);
```

This checks that all rows have a `price` greater than 0 AND a `name` with at least one character. You can add as many criteria as you want, they all need to pass.

### Row Counts and NULL Value Audits

These audits check that you have enough data and that required fields aren't missing.

#### `number_of_rows`

Make sure you have enough rows. Useful for catching cases where a model didn't run properly or data didn't load:

```sql linenums="1"
MODEL (
  name sushi.orders,
  assertions (
    number_of_rows(threshold := 10)
  )
);
```

This ensures your model has more than 10 rows. If you have 10 or fewer, something's probably wrong.

#### `not_null`

The classic "required field" check. Ensures specified columns don't have NULL values:

```sql linenums="1"
MODEL (
  name sushi.orders,
  assertions (
    not_null(columns := (id, customer_id, waiter_id))
  )
);
```

This checks that `id`, `customer_id`, and `waiter_id` are never NULL. If any of them are NULL, the audit fails.

#### `at_least_one`

Sometimes you just need at least one non-NULL value, not all of them. This is useful for optional fields that should have some data:

```sql linenums="1"
MODEL (
  name sushi.customers,
  assertions (
    at_least_one(column := zip)
    )
);
```

This ensures the `zip` column has at least one non-NULL value. Maybe most customers don't have zip codes, but at least some should.

#### `not_null_proportion`

Check that NULL values don't exceed a certain percentage. Useful when some NULLs are okay, but too many is a problem:

```sql linenums="1"
MODEL (
  name sushi.customers,
  assertions (
    not_null_proportion(column := zip, threshold := 0.8)
    )
);
```

This ensures that at least 80% of rows have a zip code. The other 20% can be NULL, but if more than 20% are missing, that's a problem.

### Specific Data Values Audits

These audits check the actual values in your data, not just whether they exist.

#### `not_constant`

Make sure a column has variety. If every row has the same value, something might be wrong:

```sql linenums="1"
MODEL (
  name sushi.customer_revenue_by_day,
  assertions (
    not_constant(column := customer_id)
    )
);
```

This ensures `customer_id` has at least two different non-NULL values. If every row has the same customer ID, that's suspicious.

#### `unique_values`

The classic uniqueness check. Ensures no duplicate values:

```sql linenums="1"
MODEL (
  name sushi.orders,
  assertions (
    unique_values(columns := (id, item_id))
  )
);
```

This checks that `id` and `item_id` each have unique values. No duplicates allowed!

#### `unique_combination_of_columns`

Check uniqueness across multiple columns. Maybe individual columns can repeat, but combinations must be unique:

```sql linenums="1"
MODEL (
  name sushi.orders,
  assertions (
    unique_combination_of_columns(columns := (id, ds))
  )
);
```

This ensures that the combination of `id` and `ds` is unique. So `id` can repeat across different dates, but the same `id` can't appear twice on the same date.

#### `accepted_values`

Make sure values are in an allowed set. Like an enum check:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    accepted_values(column := name, is_in := ('Hamachi', 'Unagi', 'Sake'))
  )
);
```

This ensures that `name` is one of the three allowed values. Anything else fails the audit.

!!! note
    Rows with `NULL` values will pass this audit in most databases. If you want to reject NULLs, combine this with a `not_null` audit.

#### `not_accepted_values`

The opposite, make sure certain values are NOT present:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    not_accepted_values(column := name, is_in := ('Hamburger', 'French fries'))
  )
);
```

This ensures that `name` is never 'Hamburger' or 'French fries'. Useful for catching data that shouldn't be there.

!!! note
    This audit doesn't support rejecting `NULL` values. Use `not_null` if you need to ensure no NULLs.

### Numeric Data Audits

These audits check numeric ranges and distributions.

#### `sequential_values`

Check that values are sequential. Useful for IDs or sequence numbers:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    sequential_values(column := item_id, interval := 1)
  )
);
```

This ensures that `item_id` values are sequential (1, 2, 3, 4...). If you have gaps or duplicates, the audit fails.

#### `accepted_range`

Check that values are within a numeric range:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    accepted_range(column := price, min_v := 1, max_v := 100)
  )
);
```

This ensures all prices are between 1 and 100 (inclusive). Values outside this range fail the audit.

**Exclusive ranges:**

You can make the range exclusive (not including the boundaries):

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    accepted_range(column := price, min_v := 0, max_v := 100, inclusive := false)
  )
);
```

Now prices must be greater than 0 and less than 100 (not equal to the boundaries).

#### `mutually_exclusive_ranges`

Check that ranges don't overlap. Useful for pricing tiers or time slots:

```sql linenums="1"
MODEL (
  name pricing.tier_ranges,
  assertions (
    mutually_exclusive_ranges(lower_bound_column := min_price, upper_bound_column := max_price)
  )
);
```

This ensures that each row's price range [min_price, max_price] doesn't overlap with any other row's range. Perfect for ensuring pricing tiers don't conflict.

### Character Data Audits

These audits check string formats and patterns.

!!! warning
    Different databases may behave differently with character sets or languages. Test your audits!

#### `not_empty_string`

Make sure strings aren't empty. NULL is okay, but empty strings `''` are not:

```sql linenums="1"
MODEL (
  name sushi.items,
  assertions (
    not_empty_string(column := name)
  )
);
```

This ensures no `name` is an empty string. NULL values pass, but `''` fails.

#### `string_length_equal`

Check that all strings have the exact same length:

```sql linenums="1"
MODEL (
  name sushi.customers,
  assertions (
    string_length_equal(column := zip, v := 5)
    )
);
```

This ensures all `zip` values are exactly 5 characters. Useful for fixed-length codes.

#### `string_length_between`

Check that string lengths are within a range:

```sql linenums="1"
MODEL (
  name sushi.customers,
  assertions (
    string_length_between(column := name, min_v := 5, max_v := 50)
    )
);
```

This ensures all `name` values are between 5 and 50 characters (inclusive).

**Exclusive ranges:**

You can make the range exclusive:

```sql linenums="1"
MODEL (
  name sushi.customers,
  assertions (
    string_length_between(column := zip, min_v := 4, max_v := 60, inclusive := false)
    )
);
```

Now names must be longer than 4 characters and shorter than 60 (not equal to the boundaries).

#### `valid_uuid`

Check that values match UUID format:

```sql linenums="1"
MODEL (
  name events.user_sessions,
  assertions (
    valid_uuid(column := uuid)
    )
);
```

This ensures all `uuid` values match the UUID structure (like `550e8400-e29b-41d4-a716-446655440000`).

#### `valid_email`

Check email format:

```sql linenums="1"
MODEL (
  name dim.users,
  assertions (
    valid_email(column := email)
    )
);
```

This ensures all `email` values look like valid email addresses (has `@`, has domain, etc.).

#### `valid_url`

Check URL format:

```sql linenums="1"
MODEL (
  name dim.products,
  assertions (
    valid_url(column := url)
    )
);
```

This ensures all `url` values are valid URLs (starts with `http://`, `https://`, or `ftp://`, etc.).

#### `valid_http_method`

Check that values are valid HTTP methods:

```sql linenums="1"
MODEL (
  name logs.api_requests,
  assertions (
    valid_http_method(column := http_method)
  )
);
```

This ensures `http_method` is one of: `GET`, `POST`, `PUT`, `DELETE`, `PATCH`, `HEAD`, `OPTIONS`, `TRACE`, `CONNECT`.

#### `match_regex_pattern_list`

Check that values match at least one regex pattern:

```sql linenums="1"
MODEL (
  name products.inventory,
  assertions (
    match_regex_pattern_list(column := todo, patterns := ('^\d.*', '.*!$'))
  )
);
```

This ensures all `todo` values match at least one pattern: either start with a digit (`^\d.*`) or end with an exclamation mark (`.*!$`).

#### `not_match_regex_pattern_list`

The opposite, make sure values don't match any pattern:

```sql linenums="1"
MODEL (
  name products.inventory,
  assertions (
    not_match_regex_pattern_list(column := todo, patterns := ('^!.*', '.*\d$'))
  )
);
```

This ensures no `todo` values start with `!` or end with a digit.

#### `match_like_pattern_list`

Check that values match at least one SQL LIKE pattern:

```sql linenums="1"
MODEL (
  name sales.customers,
  assertions (
    match_like_pattern_list(column := name, patterns := ('jim%', 'pam%'))
  )
);
```

This ensures all `name` values start with 'jim' or 'pam'. Uses SQL LIKE syntax, so `%` matches any characters.

#### `not_match_like_pattern_list`

Make sure values don't match any LIKE pattern:

```sql linenums="1"
MODEL (
  name products.catalog,
  assertions (
    not_match_like_pattern_list(column := name, patterns := ('%doe', '%smith'))
  )
);
```

This ensures no `name` values end with 'doe' or 'smith'.

### Statistical Audits

These audits check statistical properties of your data. They're powerful but require some tuning to get the thresholds right.

!!! note
    Statistical audit thresholds usually need fine-tuning through trial and error. Start with wide ranges and tighten them as you learn what's normal for your data.

#### `mean_in_range`

Check that a column's average is within a range:

```sql linenums="1"
MODEL (
  name analytics.customer_metrics,
  assertions (
    mean_in_range(column := age, min_v := 21, max_v := 50)
    )
);
```

This ensures the average `age` is between 21 and 50. Useful for catching when your data distribution shifts unexpectedly.

**Exclusive ranges:**

```sql linenums="1"
MODEL (
  name analytics.customer_metrics,
  assertions (
    mean_in_range(column := age, min_v := 18, max_v := 65, inclusive := false)
    )
);
```

Now the mean must be greater than 18 and less than 65 (not equal to the boundaries).

#### `stddev_in_range`

Check that standard deviation is within a range:

```sql linenums="1"
MODEL (
  name analytics.customer_metrics,
  assertions (
    stddev_in_range(column := age, min_v := 2, max_v := 5)
  )
);
```

This ensures the standard deviation of `age` is between 2 and 5. Useful for detecting when your data becomes more or less spread out than expected.

**Exclusive ranges:**

```sql linenums="1"
MODEL (
  name analytics.customer_metrics,
  assertions (
    stddev_in_range(column := age, min_v := 3, max_v := 6, inclusive := false)
  )
);
```

Now the standard deviation must be greater than 3 and less than 6.

#### `z_score`

Check for statistical outliers. Values with high z-scores are far from the mean:

```sql linenums="1"
MODEL (
  name sales.transactions,
  assertions (
    z_score(column := age, threshold := 3)
    )
);
```

This ensures no `age` values have a z-score greater than 3 (meaning they're more than 3 standard deviations from the mean). Useful for catching outliers that might indicate data quality issues.

The z-score is calculated as: `ABS(([row value] - [column mean]) / NULLIF([column standard deviation], 0))`

#### `kl_divergence`

Check how different two distributions are. Useful for comparing current data to a reference:

```sql linenums="1"
MODEL (
  name analytics.cohort_comparison,
  assertions (
    kl_divergence(column := age, target_column := reference_age, threshold := 0.1)
    )
);
```

This ensures the [symmetrised Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence) (also called "Jeffreys divergence" or "Population Stability Index") between `age` and `reference_age` is less than or equal to 0.1.

Lower values mean the distributions are more similar. This is great for detecting when your data distribution has shifted significantly from a known good reference.

#### `chi_square`

Check the relationship between two categorical columns:

```sql linenums="1"
MODEL (
  name analytics.user_segments,
  assertions (
    chi_square(column := user_state, target_column := user_type, critical_value := 6.635)
    )
);
```

This ensures the [chi-square statistic](https://en.wikipedia.org/wiki/Chi-squared_test) for `user_state` and `user_type` doesn't exceed 6.635.

**Finding critical values:**

You can look up critical values in a [chi-square table](https://www.medcalc.org/manual/chi-square-table.php) or calculate them with Python:

```python linenums="1"
from scipy.stats import chi2

# critical value for p-value := 0.95 and degrees of freedom := 1
chi2.ppf(0.95, 1)
```

This is useful for detecting when the relationship between two categorical variables has changed unexpectedly.

## Running Audits

### The CLI Audit Command

You can run audits manually with the `vulcan audit` command:

```bash
$ vulcan -p project audit --start 2022-01-01 --end 2022-01-02
Found 1 audit(s).
assert_item_price_is_not_null FAIL.

Finished with 1 audit error(s).

Failure in audit assert_item_price_is_not_null for model sushi.items (audits/items.sql).
Got 3 results, expected 0.
SELECT * FROM vulcan.sushi__items__1836721418_83893210 WHERE ds BETWEEN '2022-01-01' AND '2022-01-02' AND price IS NULL
Done.
```

This is useful for testing audits before running a full plan, or for debugging why an audit is failing. The output shows you exactly what query failed and how many rows it found.

### Automated Auditing

When you apply a plan, Vulcan automatically runs all audits for models being evaluated. You don't need to do anything special, just run your plan and audits happen automatically.

If any audit fails, Vulcan halts the models immediately. This prevents bad data from propagating downstream and causing bigger problems. It might be annoying when it happens, but trust us, it's better than finding out later that bad data made it into production.

## Advanced Usage

### Skipping Audits

Sometimes you need to temporarily disable an audit. Maybe you're debugging, or you know there's a temporary data issue you're working on fixing. You can skip audits by setting `skip` to `true`:

```sql linenums="1" hl_lines="3"
AUDIT (
  name assert_item_price_is_not_null,
  skip true
);
SELECT * from sushi.items
WHERE ds BETWEEN @start_ds AND @end_ds AND
   price IS NULL;
```

**Use this sparingly!** Skipped audits won't run, which means they won't catch problems. It's better to fix the underlying issue than to skip the audit. But sometimes you need it for debugging or temporary situations.

## Troubleshooting

### Audit Fails Unexpectedly

**Problem:** Your audit is failing, but you're not sure why.

**Solution:** Run the audit query manually to see what it's finding:

```bash
vulcan -p project audit --start 2022-01-01 --end 2022-01-02 --verbose
```

This will show you the exact query and the rows that failed. Once you see what data is causing the failure, you can either fix the data or adjust the audit.

### Audit Too Strict

**Problem:** Your audit is failing during normal operation, even though the data is actually fine.

**Solution:** Review your thresholds. Maybe your `accepted_range` is too narrow, or your `number_of_rows` threshold is too high. Statistical audits especially need tuning, start with wide ranges and tighten them as you learn what's normal.

### Performance Issues

**Problem:** Audits are slowing down your plan execution.

**Solution:**

- Make sure your audit queries use indexes on the columns they're checking

- For incremental models, audits only run on processed intervals (which helps), but you can also add date filters to your audit queries

- Consider if you really need all those audits, sometimes less is more

### Understanding Audit Results

When an audit fails, Vulcan shows you:

- Which audit failed

- Which model it was attached to

- The exact query that was run

- How many rows were returned (when it expected 0)

Use this information to understand what went wrong. The query results tell you exactly what data failed the check.



# Checks

Source: https://tmdc-io.github.io/vulcan-book/components/checks/checks/

---

# Checks

Quality checks are validation rules that monitor your data quality over time without blocking your models. They warn you when something looks off, but they don't stop execution.

Unlike [audits](../audits/audits.md) (which block models execution when they fail), checks run separately or alongside your models and provide non-blocking validation. They're perfect for tracking trends, detecting anomalies, and building up a historical picture of your data quality.

**What makes checks special:**

- Configured in simple YAML files in the `checks/` directory

- Don't block models (your models keep running even if checks fail)

- Track historical patterns and trends

- Support complex statistical analysis

- Integrate with Activity API for monitoring and alerting

## Checks vs Audits vs Profiles

Before we dive in, let's clear up the confusion around these three data quality mechanisms. They all serve different purposes, and understanding when to use each one will save you headaches later.

| Feature | Audits | Checks | Profiles |
|---------|--------|--------|----------|
| **Purpose** | Critical validation | Monitoring & analysis | Observation & tracking |
| **When runs** | With model (inline) | Separately or with models | With model |
| **Blocks models?** | Yes (always) | No | No |
| **Configuration** | In MODEL DDL or .sql files | YAML files (`checks/`) | In MODEL DDL |
| **Output** | Pass/fail | Pass/fail + samples | Statistical metrics |
| **Best for** | Business rules, data integrity | Trend monitoring, anomalies | Understanding data |
| **Historical tracking** | No | Yes (Activity API) | Yes (`_check_profiles`) |

**The Three-Layer Strategy:**

A layered approach to data quality:

```
┌─────────────────────────────────────────┐
│  AUDITS (Critical - Blocks models)   │
│  • Primary keys must be unique          │
│  • Revenue must be non-negative         │
│  • Foreign key relationships valid      │
└─────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  CHECKS (Monitoring - Non-Blocking)     │
│  • Row count within expected range      │
│  • Anomaly detection on metrics         │
│  • Cross-table consistency              │
└─────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  PROFILES (Observation - Metrics)       │
│  • Track null percentages               │
│  • Monitor column distributions         │
│  • Detect data drift                    │
└─────────────────────────────────────────┘
```

Audits stop bad data at the door. Checks watch for problems but don't interfere. Profiles observe patterns and help you understand what's normal.

## When to Use Checks

**Use Quality Checks for:**

- Monitoring data quality trends over time (is completeness getting worse?)

- Statistical anomaly detection (did revenue suddenly spike?)

- Cross-model validation (do orders match customers?)

- Non-critical validation (warnings, not blockers)

- Complex validation requiring historical context

- Building data quality dashboards

**Use Audits Instead for:**

- Critical business rules that must pass (revenue can't be negative)

- Model-specific validation (runs inline with the model)

- Simple SQL assertions

- Blocking invalid data from flowing downstream

**Use Profiles Instead for:**

- Understanding data characteristics (what does this column look like?)

- Discovering patterns (not validation)

- Detecting data drift over time

- Informing which checks/audits to add

**Example: Revenue validation strategy**

Here's how you'd layer all three for a revenue table:

```sql
-- AUDIT (Critical - blocks if fails)

-- This stops the models if revenue is invalid
MODEL (
  name analytics.revenue,
  assertions (
    not_null(columns := (customer_id, revenue)),
    accepted_range(column := revenue, min_v := 0, max_v := 100000000)
  )
);
```

```yaml
# CHECK (Monitoring - warns if unusual)
# This watches for anomalies but doesn't block
checks:
  analytics.revenue:
    accuracy:
      - anomaly detection for avg(revenue):
          name: revenue_anomaly_detection
      - change for row_count >= -30%:
          name: row_count_drop_alert
```

```sql
-- PROFILE (Observation - tracks over time)

-- This just watches and records what it sees
MODEL (
  name analytics.revenue,
  profiles (revenue, order_count, customer_tier)
);
```

## Quick Start

### Your First Check

Let's create your first check. It's simpler than you might think!

Create a file `checks/customers.yml`:

```yaml
checks:
  analytics.customers:
    completeness:
      - missing_count(email) = 0:
          name: no_missing_emails
          attributes:
            description: "All customers must have an email address"
```

That's it! This check ensures that every customer has an email address. When you run your models, this check will run automatically and warn you if any emails are missing.

**What happens when it runs:**

Checks and profiles run automatically when models are executed, either through a **plan** or **run** command. Here's what the execution output looks like:

```bash
Check Executions (1 Models)
└── hello.subscriptions
    ├── completeness (4/4)
    ├── uniqueness (1/1)
    └── validity (3/3)

Profiled 1 model (3 columns):
  warehouse.hello.subscriptions: 3 columns
```

Here are common patterns you'll use:

### Common Check Patterns

Here are the patterns you'll use most often. Copy these, tweak them for your tables, and you're good to go!

#### Pattern 1: Completeness Checks

Make sure required data is present:

```yaml
checks:
  analytics.orders:
    completeness:
      - missing_count(customer_id) = 0:
          name: customer_id_required
      
      - missing_percent(email) < 5:
          name: email_mostly_complete
      
      - row_count > 1000:
          name: sufficient_orders
```

The first check ensures every order has a customer ID (zero tolerance). The second allows up to 5% missing emails (sometimes that's okay). The third makes sure you have enough data to work with.

#### Pattern 2: Validity Checks

Validate data format and values:

```yaml
checks:
  analytics.users:
    validity:
      - failed rows:
          name: invalid_emails
          fail query: |
            SELECT user_id, email
            FROM analytics.users
            WHERE email NOT LIKE '%@%'
          samples limit: 10
      
      - failed rows:
          name: invalid_ages
          fail query: |
            SELECT user_id, age
            FROM analytics.users
            WHERE age < 0 OR age > 120
```

The `failed rows` check type is flexible. You can write any SQL query. If it returns rows, the check fails and captures those rows as samples.

#### Pattern 3: Uniqueness Checks

Ensure no duplicates:

```yaml
checks:
  analytics.customers:
    uniqueness:
      - duplicate_count(email) = 0:
          name: unique_emails
      
      - duplicate_count(customer_id, order_date) = 0:
          name: unique_customer_date_combination
```

The second example shows composite keys, maybe a customer can have multiple orders, but only one per day.

#### Pattern 4: Anomaly Detection

Detect unusual patterns automatically:

```yaml
checks:
  analytics.daily_revenue:
    accuracy:
      - anomaly detection for row_count:
          name: row_count_anomaly
      
      - anomaly detection for avg(revenue):
          name: revenue_anomaly
```

Anomaly detection learns from historical data and flags when something looks unusual. It needs to run a few times first to build up a baseline, then it detects problems.

#### Pattern 5: Change Monitoring

Track changes over time:

```yaml
checks:
  analytics.orders:
    timeliness:
      - change for row_count >= -50%:
          name: row_count_drop_alert
          attributes:
            description: "Alert if row count drops more than 50%"
```

This compares the current value to the previous run and alerts you if it changes too much. Perfect for catching sudden drops or spikes.

## Check Configuration

### File Structure

Checks live in YAML files in the `checks/` directory. You can organize them however makes sense for your project:

```
project/
├── models/
├── checks/
│   ├── users.yml           # Checks for user tables
│   ├── orders.yml          # Checks for order tables
│   ├── revenue.yml         # Checks for revenue tables
│   └── cross_model.yml     # Checks spanning multiple tables
└── config.yaml
```

**File naming:**

- Must end with `.yml` or `.yaml`

- The name doesn't matter (Vulcan reads all files in the directory)

- Organize by domain or table for clarity, whatever helps you find things

### Basic Check Syntax

Here's the basic structure of a check:

```yaml
checks:
  <fully_qualified_table_name>:
    <dimension>:
      - <check_expression>:
          name: <check_name>
          attributes:
            description: <human_readable_description>
            severity: <warning|error>
            tags: [<tag1>, <tag2>]
```

**Example:**

```yaml
checks:
  analytics.customers:
    completeness:
      - row_count > 100:
          name: sufficient_customers
          attributes:
            description: "At least 100 customers expected in production"
            severity: warning
            tags: [critical, daily]
```

The `name` field is required and should be descriptive. The `attributes` section is optional but useful for documentation and filtering.

### Data Quality Dimensions

Checks are organized by **8 standard dimensions** (based on ODPS v3.1). Each dimension focuses on a different aspect of data quality:

#### 1. Completeness

No missing required data. This is probably the most common dimension you'll use.

```yaml
completeness:
  - missing_count(customer_id) = 0

  - missing_percent(email) < 5

  - row_count > 1000
```

#### 2. Validity

Data conforms to format/syntax. Is that email actually an email? Is that date in the right format?

```yaml
validity:
  - failed rows:
      fail query: |
        SELECT * FROM table
        WHERE email NOT LIKE '%@%'
```

#### 3. Accuracy

Data matches reality. Is the average age reasonable? Is revenue in the expected range?

```yaml
accuracy:
  - anomaly detection for avg(revenue)

  - avg(age) between 18 and 65
```

#### 4. Consistency

Data agrees across sources. Do orders match customers? Are totals consistent?

```yaml
consistency:
  - failed rows:
      fail query: |
        SELECT *
        FROM orders o
        LEFT JOIN customers c ON o.customer_id = c.customer_id
        WHERE c.customer_id IS NULL
```

#### 5. Uniqueness

No duplicates. Is that email really unique? Can customers have multiple orders per day?

```yaml
uniqueness:
  - duplicate_count(email) = 0

  - duplicate_count(order_id) = 0
```

#### 6. Timeliness

Data is current. Is the data fresh? Are updates happening on time?

```yaml
timeliness:
  - change for row_count >= -30%

  - failed rows:
      fail query: |
        SELECT *
        FROM orders
        WHERE updated_at < CURRENT_DATE - INTERVAL '7 days'
```

#### 7. Conformity

Follows standards. Does the zip code have the right format? Are codes valid?

```yaml
conformity:
  - failed rows:
      fail query: |
        SELECT *
        FROM addresses
        WHERE LENGTH(zip_code) != 5
```

#### 8. Coverage

All records are present. Did we get all the data we expected?

```yaml
coverage:
  - row_count >= 95% of historical_avg(row_count)
```

### Filtering Checks

Sometimes you want to apply checks to a subset of your data. Maybe you only care about completed orders, or US customers. That's where filters come in:

```yaml
checks:
  analytics.orders:
    filter: "status = 'completed' AND order_date >= CURRENT_DATE - INTERVAL '30 days'"
    
    completeness:
      - missing_count(customer_id) = 0:
          name: completed_orders_have_customers
```

**Multiple filters:**

You can define the same table multiple times with different filters:

```yaml
checks:
  analytics.customers:
    filter: "country = 'US'"
    completeness:
      - row_count > 1000
  
  analytics.customers:
    filter: "country = 'EU'"
    completeness:
      - row_count > 500
```

This lets you have different expectations for different regions.

### Check Attributes

Add metadata to your checks to make them easier to manage and understand:

```yaml
checks:
  analytics.revenue:
    completeness:
      - row_count > 1000:
          name: sufficient_revenue_data
          attributes:
            description: "Revenue table must have at least 1000 rows for analysis"
            severity: error
            tags: [critical, daily, revenue]
            owner: data-team
            jira: DATA-1234
            sla: "< 1 hour"
```

**Standard attributes:**

- `description` - Human-readable explanation

- `severity` - `error` (default) or `warning` (warnings are less urgent)

- `tags` - List of tags for filtering/organization (find all "critical" checks easily)

- `owner` - Team or person responsible (who do I call when this fails?)

- Custom attributes - Any key-value pairs (add whatever metadata you need)

## Built-in Check Types

Vulcan provides several built-in check types that cover most common scenarios. Let's walk through them:

### Missing Data Checks

#### `missing_count(column)`

Count of NULL values. Simple and straightforward:

```yaml
completeness:
  - missing_count(email) = 0:
      name: no_missing_emails
  
  - missing_count(phone) <= 100:
      name: phone_mostly_complete
```

The first ensures zero missing emails (strict). The second allows up to 100 missing phone numbers (maybe phones are optional for some customers).

#### `missing_percent(column)`

Percentage of NULL values. Useful when you care about proportions rather than absolute counts:

```yaml
completeness:
  - missing_percent(email) < 5:
      name: email_95_percent_complete
  
  - missing_percent(optional_field) < 50:
      name: optional_field_half_complete
```

This is useful when table sizes vary. 5% missing might be fine for a million-row table but concerning for a hundred-row table.

### Row Count Checks

#### `row_count`

Total rows in table. Use this to ensure you have enough data:

```yaml
completeness:
  - row_count > 1000:
      name: sufficient_data
  
  - row_count between 1000 and 100000:
      name: expected_row_range
```

The second example shows a range check, maybe you know your table should be between 1K and 100K rows, and anything outside that range is suspicious.

#### `row_count` with filter

You can also check row counts on filtered data:

```yaml
completeness:
  - row_count > 500:
      name: sufficient_active_users
      filter: "status = 'active'"
```

This checks that you have at least 500 active users, regardless of how many total users you have.

### Duplicate Count Checks

#### `duplicate_count(column)`

Count of duplicate values. Perfect for ensuring uniqueness:

```yaml
uniqueness:
  - duplicate_count(email) = 0:
      name: unique_emails
  
  - duplicate_count(customer_id) = 0:
      name: unique_customer_ids
```

If this returns anything greater than zero, you've got duplicates. The check fails and you can investigate.

#### `duplicate_count(column1, column2)`

Composite key duplicates. Check combinations of columns:

```yaml
uniqueness:
  - duplicate_count(customer_id, order_date) = 0:
      name: unique_customer_date
      attributes:
        description: "Each customer can have at most one order per day"
```

Maybe customers can have multiple orders, but only one per day. This check enforces that business rule.

### Failed Rows Checks

#### SQL-based validation with samples

This is the most flexible check type, you can write any SQL query you want:

```yaml
validity:
  - failed rows:
      name: invalid_revenue
      fail query: |
        SELECT customer_id, revenue, order_date
        FROM analytics.orders
        WHERE revenue < 0 OR revenue > 10000000
      samples limit: 20
      attributes:
        description: "Revenue must be between 0 and 10M"
```

**How it works:**

- `fail query` - A SELECT statement that returns invalid rows

- `samples limit` - How many example rows to capture when the check fails (default: 5)

- Returns empty = check passes (no invalid rows found)

- Returns rows = check fails (captures samples so you can see what's wrong)

**Complex validation:**

You can get fancy with joins and CTEs:

```yaml
validity:
  - failed rows:
      name: orphaned_orders
      fail query: |
        SELECT o.order_id, o.customer_id
        FROM analytics.orders o
        LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id
        WHERE c.customer_id IS NULL
      samples limit: 10
```

This finds orders that reference customers that don't exist, a classic referential integrity check.

### Threshold Checks

#### Numeric aggregations

Check aggregated values against thresholds:

```yaml
accuracy:
  - avg(revenue) between 100 and 10000:
      name: revenue_in_expected_range
  
  - sum(amount) > 1000000:
      name: sufficient_total_revenue
  
  - max(age) <= 120:
      name: age_within_human_range
  
  - min(price) >= 0:
      name: non_negative_prices
```

You can use any aggregation function: `avg`, `sum`, `min`, `max`, `count`, `distinct_count`, etc.

#### Statistical checks

Get fancy with statistical functions:

```yaml
accuracy:
  - stddev(revenue) < 5000:
      name: revenue_low_variance
  
  - percentile(revenue, 95) < 50000:
      name: revenue_95th_percentile_check
```

These detect when your data distribution changes unexpectedly.

### Anomaly Detection

#### ML-based anomaly detection

This is where checks get really powerful. Anomaly detection uses historical check results to learn what's normal and flag unusual patterns:

```yaml
accuracy:
  - anomaly detection for row_count:
      name: row_count_anomaly
      attributes:
        description: "Detect unusual changes in row count"
  
  - anomaly detection for avg(revenue):
      name: revenue_anomaly
  
  - anomaly detection for distinct_count(customer_id):
      name: customer_count_anomaly
```

**How it works:**
1. Collects historical metric values over time (every time the check runs)
2. Builds a statistical model (mean, standard deviation, trends)
3. Compares current value to expected range
4. Flags significant deviations (typically > 3 standard deviations)

**Requirements:**

- Needs historical data (runs multiple times to build a baseline)

- Works best with regular schedules (daily, hourly)

- More accurate after 30+ data points (the more history, the better)

So if you're setting up anomaly detection, be patient, it needs to run a few times before it's useful. But once it has enough data, it's really good at spotting problems you might not think to check for.

### Change Over Time Checks

#### Monitor changes compared to previous run

Track how metrics change between runs:

```yaml
timeliness:
  - change for row_count >= -50%:
      name: row_count_drop_alert
      attributes:
        description: "Alert if row count drops more than 50% from last week"
  
  - change for avg(revenue) >= -20%:
      name: revenue_drop_alert
  
  - change for distinct_count(customer_id) >= 10%:
      name: customer_growth_check
```

**Change calculation:**
```
change = (current_value - previous_value) / previous_value * 100
```

**Examples:**

- `change >= -30%` - Alert if metric drops more than 30% (negative change)

- `change >= 10%` - Alert if metric grows more than 10% (positive change)

- `change between -10% and 10%` - Alert if metric changes more than 10% either way

This catches sudden changes that might indicate a problem or an opportunity.

## Data Profiling

### What is Profiling?

**Profiles automatically collect statistical metrics about your data over time.**

Unlike checks (which validate), profiles **observe and track** data characteristics. They're like a data scientist watching your tables and taking notes:

```sql
MODEL (
  name analytics.customers,
  kind FULL,
  grains (customer_id),
  profiles (revenue, signup_date, customer_tier, order_count)
);
```

**What gets profiled:**

**Table-level metrics:**

- Row count

**Column-level metrics (all columns):**

- Null count & percentage

- Distinct count

- Duplicate count

- Uniqueness percentage

**Numeric columns:**

- Min, max, avg, sum

- Standard deviation, variance

- Histogram buckets

**Text columns:**

- Min, max, avg length

- Most frequent values

Profiles track how things change over time so you can spot trends and drift.

### Profile Configuration

Enable profiling in your MODEL definition:

```sql
MODEL (
  name analytics.revenue_metrics,
  kind INCREMENTAL_BY_TIME_RANGE (time_column metric_date),
  
  -- Profile these columns
  profiles (
    revenue,
    order_count,
    customer_tier,
    region
  )
);
```

Just list the columns you want to profile. Vulcan will automatically collect metrics for them every time the model runs.

### Profile Storage

Profiles are stored in the `_check_profiles` table, which you can query like any other table:

| Column | Meaning |
|--------|---------|
| `id` | Unique identifier for this metric row |
| `run_id` | Identifies which profiling run this metric belongs to |
| `table_name` | Name of the table being profiled |
| `column_name` | Name of the column being profiled (NULL for table-level metrics like row_count) |
| `profile_type` | The type of metric, e.g., row_count, distinct, missing_count, frequent_values, min, max, avg_length, etc. |
| `value_number` | Numeric metric value (for metrics like row_count, distinct, min, max, avg, etc.) |
| `value_text` | Used for text values (rare) |
| `value_json` | JSON-encoded metric (for histograms, frequent values, etc.) |
| `value_type` | Type of value stored (number, json, etc.) |
| `profiled_at` | When the profiling was performed (epoch ms) |
| `created_ts` | When the row was inserted |

### Querying Profiles

#### Track missing count over time

See how null percentages change:

```sql
SELECT
  to_timestamp(profiled_at/1000)::date AS date,
  value_number AS missing_count
FROM _check_profiles
WHERE table_name = 'warehouse.hello.subscriptions'
  AND column_name = 'mrr'
  AND profile_type = 'missing_count'
ORDER BY profiled_at DESC
LIMIT 30;  -- Last 30 days
```

This shows you a time series of missing values for spotting trends.

#### Monitor data drift

Compare current values to historical averages:

```sql
WITH latest_profile AS (
  -- Pick the most recent profiling timestamp for that table/column
  SELECT profiled_at
  FROM _check_profiles
  WHERE table_name = 'warehouse.hello.subscriptions'
    AND column_name = 'mrr'
  ORDER BY profiled_at DESC
  LIMIT 1
),

current AS (
  -- Get the most recent distinct count and average value from that profiling run
  SELECT
    MAX(CASE WHEN profile_type = 'distinct' THEN value_number END)     AS distinct_count,
    MAX(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_value
  FROM _check_profiles p
  JOIN latest_profile l ON p.profiled_at = l.profiled_at
  WHERE p.table_name = 'warehouse.hello.subscriptions'
    AND p.column_name = 'mrr'
),

historical AS (
  -- 30-day historical averages (profiled_at stored as epoch ms → convert to timestamp)
  SELECT
    AVG(CASE WHEN profile_type = 'distinct' THEN value_number END)      AS avg_distinct,
    AVG(CASE WHEN profile_type IN ('avg', 'mean', 'average', 'avg_value') THEN value_number END) AS avg_mrr
  FROM _check_profiles
  WHERE table_name = 'warehouse.hello.subscriptions'
    AND column_name = 'mrr'
    AND to_timestamp(profiled_at/1000) >= CURRENT_DATE - INTERVAL '30 days'
)

SELECT
  c.distinct_count,
  h.avg_distinct,
  CASE
    WHEN h.avg_distinct IS NULL THEN NULL
    ELSE (c.distinct_count - h.avg_distinct) / NULLIF(h.avg_distinct, 0) * 100
  END AS distinct_change_pct,
  c.avg_value,
  h.avg_mrr,
  CASE
    WHEN h.avg_mrr IS NULL THEN NULL
    ELSE (c.avg_value - h.avg_mrr) / NULLIF(h.avg_mrr, 0) * 100
  END AS mrr_change_pct
FROM current c, historical h;
```

This query compares current metrics to 30-day historical averages and calculates percentage changes. Perfect for detecting drift!

### Using Profiles to Inform Checks

**Workflow:**

1. **Enable profiling** on new models (just add `profiles (...)` to your MODEL)
2. **Observe patterns** for 30+ days (let profiles collect data)
3. **Identify anomalies** in profile data (query `_check_profiles` and look for trends)
4. **Create checks** based on observed patterns (now you know what's normal)

**Example:**

```sql
-- Step 1: Enable profiling
MODEL (
  name analytics.orders,
  profiles (order_count, revenue, customer_tier)
);
```

```sql
-- Step 2: Query profiles after 30 days
SELECT
    MIN(value_number) AS min_revenue,
    MAX(value_number) AS max_revenue,
    AVG(value_number) AS typical_revenue,
    STDDEV(value_number) AS revenue_stddev
FROM _check_profiles
WHERE table_name = 'warehouse.hello.subscriptions'
  AND column_name = 'mrr'
  AND profile_type IN ('avg', 'mean', 'average', 'avg_value')
  AND to_timestamp(profiled_at/1000) >= CURRENT_DATE - INTERVAL '30 days';

-- Results:

-- min_revenue: 45000

-- max_revenue: 75000

-- typical_revenue: 58000

-- revenue_stddev: 6000
```

```yaml
# Step 3: Create checks based on observed patterns
checks:
  analytics.orders:
    accuracy:
      - avg(revenue) between 40000 and 80000:
          name: revenue_within_observed_range
          attributes:
            description: "Based on 30-day historical analysis"
      
      - anomaly detection for avg(revenue):
          name: revenue_anomaly_detection
```

Now your checks are informed by actual data patterns, not guesses. Much better!

### Profile Best Practices

**DO:**

- Profile high-value production tables (the ones that matter)

- Profile columns used in downstream analysis (if it's important, profile it)

- Use profiles to understand new data sources (what does this data look like?)

- Query profiles to detect data drift (is something changing?)

- Use profiles to inform check thresholds (data-driven thresholds are better)

**DON'T:**

- Profile sensitive/PII columns (privacy risk, be careful)

- Profile every column (performance overhead, pick what matters)

- Profile temporary/experimental models (waste of resources)

- Use profiles as a replacement for checks (they serve different purposes)

- Profile very high-frequency models (storage cost adds up)

**When to use profiles:**

- Building new models (understand the data first)

- Monitoring production tables (watch for changes)

- Detecting data drift (is the data changing?)

- Informing audit/check strategy (what should we check?)

- Debugging data quality issues (what's normal vs abnormal?)

**When to skip profiles:**

- Temporary models (they won't be around long)

- Models with sensitive data (privacy concerns)

- Very high-frequency models (> 100 runs/day, storage costs)

- Models where you only need pass/fail validation (profiles are overkill)

## Advanced Patterns

### Cross-Model Validation

Validate relationships between models. This ensures referential integrity:

```yaml
# checks/cross_model.yml
checks:
  analytics.orders:
    consistency:
      - failed rows:
          name: orphaned_orders
          fail query: |
            SELECT o.order_id, o.customer_id
            FROM analytics.orders o
            LEFT JOIN analytics.customers c ON o.customer_id = c.customer_id
            WHERE c.customer_id IS NULL
          samples limit: 10
          attributes:
            description: "All orders must have a valid customer"
      
      - failed rows:
          name: revenue_mismatch
          fail query: |
            SELECT
              o.order_id,
              o.revenue as order_revenue,
              r.revenue as revenue_table_revenue
            FROM analytics.orders o
            JOIN analytics.revenue r ON o.order_id = r.order_id
            WHERE ABS(o.revenue - r.revenue) > 0.01
```

The first check finds orders without valid customers (orphaned records). The second ensures revenue matches across tables (consistency check).

### Time-Based Validation

Ensure data timeliness. Is your data fresh? Are updates happening on schedule?

```yaml
checks:
  analytics.orders:
    timeliness:
      - failed rows:
          name: stale_data
          fail query: |
            SELECT *
            FROM analytics.orders
            WHERE updated_at < CURRENT_TIMESTAMP - INTERVAL '24 hours'
              AND status != 'completed'
          attributes:
            description: "Pending orders should update within 24 hours"
      
      - failed rows:
          name: future_dates
          fail query: |
            SELECT *
            FROM analytics.orders
            WHERE order_date > CURRENT_DATE
```

The first check finds stale pending orders (maybe something's stuck). The second catches future dates (data entry errors).

### Statistical Outlier Detection

Custom outlier detection using SQL. Sometimes you need more control than anomaly detection provides:

```yaml
checks:
  analytics.revenue:
    accuracy:
      - failed rows:
          name: revenue_outliers
          fail query: |
            WITH stats AS (
              SELECT
                AVG(revenue) as mean,
                STDDEV(revenue) as stddev
              FROM analytics.revenue
            )
            SELECT r.*,
              (r.revenue - s.mean) / s.stddev as z_score
            FROM analytics.revenue r, stats s
            WHERE ABS((r.revenue - s.mean) / s.stddev) > 3
          samples limit: 20
```

This finds rows where revenue is more than 3 standard deviations from the mean (classic outlier detection). The z-score tells you how extreme each outlier is.

## Best Practices

### Check Organization

Organize your checks in a way that makes sense for your team. Here are two common approaches:

**By domain:**

```
checks/
├── customers/
│   ├── completeness.yml
│   ├── validity.yml
│   └── consistency.yml
├── orders/
│   ├── completeness.yml
│   └── timeliness.yml
└── revenue/
    └── accuracy.yml
```

**By priority:**

```
checks/
├── critical.yml      # Must never fail
├── important.yml     # Should rarely fail
├── monitoring.yml    # Track trends
└── experimental.yml  # Testing new checks
```

Pick whatever works for your team. The important thing is consistency, if everyone knows where to find things, life is easier.

### Naming Conventions

**Use descriptive names:**

```yaml
# Bad - what does "check1" tell you?
checks:
  analytics.customers:
    completeness:
      - missing_count(email) = 0:
          name: check1

# Good - clear and descriptive
checks:
  analytics.customers:
    completeness:
      - missing_count(email) = 0:
          name: no_missing_customer_emails
          attributes:
            description: "All customers must have an email for marketing"
```

**Naming pattern:**

- `<dimension>_<what>_<constraint>` or `<what>_<constraint>`

- Examples:

  - `completeness_email_required` or `no_missing_emails`

  - `validity_email_format` or `valid_email_format`

  - `uniqueness_email_no_duplicates` or `unique_emails`

  - `timeliness_order_within_24hrs` or `orders_update_daily`

The key is that someone reading the name should understand what it checks without looking at the code.

### Threshold Selection

**Start conservative, adjust based on data:**

```yaml
# Step 1: Start with wide range
checks:
  analytics.orders:
    completeness:
      - row_count > 100:
          name: sufficient_orders_v1

# Step 2: Monitor for 30 days, see actual range: 5000-10000

# Step 3: Tighten based on observed patterns
checks:
  analytics.orders:
    completeness:
      - row_count between 4000 and 12000:
          name: sufficient_orders_v2
          attributes:
            description: "Based on 30-day historical analysis"
```

Don't set thresholds based on guesses, let the data tell you what's normal. Use profiles to understand your data first, then set checks based on what you learn.

**Use profiles to inform thresholds:**

```sql
-- Query profiles to understand your data
SELECT
  MIN(value_number) as min_observed,
  MAX(value_number) as max_observed,
  AVG(value_number) as typical,
  STDDEV(value_number) as stddev
FROM check_results
WHERE check_name = 'row_count'
  AND executed_at >= CURRENT_DATE - INTERVAL '90 days';

-- Set threshold as: typical ± 3*stddev
```

This gives you data-driven thresholds instead of wild guesses. Much better!

### Integration Strategy

**Layer validation:**

```sql
-- LAYER 1: Audits (critical - blocks)

-- Stop bad data at the door
MODEL (
  name analytics.orders,
  assertions (
    not_null(columns := (order_id, customer_id)),
    unique_values(columns := (order_id))
  )
);
```

```yaml
# LAYER 2: Checks (monitoring - warns)
# Watch for problems but don't block
checks:
  analytics.orders:
    completeness:
      - row_count between 5000 and 15000:
          name: order_count_in_range
    
    timeliness:
      - change for row_count >= -30%:
          name: order_count_stable
```

```sql
-- LAYER 3: Profiles (observe - tracks)

-- Just watch and learn
MODEL (
  name analytics.orders,
  profiles (order_count, revenue, customer_tier)
);
```

This three-layer approach gives you comprehensive data quality coverage: audits stop problems, checks warn about issues, and profiles help you understand what's normal.

## Troubleshooting

### Check Failures

#### Investigate failed check

When a check fails, you'll want to dig into why:

```bash
# Run specific check with verbose output
vulcan check --select analytics.customers.invalid_emails --verbose
```

This gives you more details about what went wrong.

#### Query failed samples

If your check captures samples (like `failed rows` checks do), you can query them:

```sql
-- Get samples from last failed run
SELECT *
FROM check_samples
WHERE check_name = 'invalid_emails'
  AND status = 'failed'
ORDER BY executed_at DESC
LIMIT 10;
```

This shows you actual rows that failed for debugging.

### Performance Issues

#### Slow check queries

**Problem:** Check takes too long to run

**Solution 1: Add filters**

```yaml
# Slow - scans entire table
checks:
  analytics.orders:
    validity:
      - failed rows:
          fail query: |
            SELECT * FROM analytics.orders
            WHERE email NOT LIKE '%@%'

# Fast - filters to recent data
checks:
  analytics.orders:
    filter: "order_date >= CURRENT_DATE - INTERVAL '30 days'"
    validity:
      - failed rows:
          fail query: |
            SELECT * FROM analytics.orders
            WHERE email NOT LIKE '%@%'
```

Filtering reduces the amount of data the check needs to scan, which makes it faster.

**Solution 2: Add indexes**

```sql
-- Add index on frequently checked columns
CREATE INDEX idx_orders_email ON analytics.orders(email);
CREATE INDEX idx_orders_order_date ON analytics.orders(order_date);
```

Indexes help queries run faster, especially for `failed rows` checks that filter on specific columns.

### False Positives

#### Threshold too strict

**Problem:** Check fails during normal variance

```yaml
# Too strict - exact match is unrealistic
checks:
  analytics.orders:
    completeness:
      - row_count = 10000  # Exact match

# Allow variance - more realistic
checks:
  analytics.orders:
    completeness:
      - row_count between 9000 and 11000  # ±10% variance
```

Real data has variance. Don't set thresholds that are too strict, you'll just get false positives.

#### Use anomaly detection instead

Sometimes strict thresholds aren't the right approach:

```yaml
# Replace strict threshold with ML-based detection
checks:
  analytics.orders:
    accuracy:
      - anomaly detection for row_count:
          name: row_count_anomaly
```

Anomaly detection learns what's normal and adapts to variance, which reduces false positives.

## Summary

Quality checks provide a comprehensive way to monitor data quality over time without blocking your models. Here's what we covered:

### Core Concepts

**1. Quality Checks**
- YAML-configured validation rules

- Non-blocking (don't stop models)

- Track trends over time

- Integrate with Activity API

**2. Check Types**
- Missing data checks (`missing_count`, `missing_percent`)

- Row count checks (`row_count`)

- Duplicate checks (`duplicate_count`)

- Failed rows (SQL-based, flexible)

- Anomaly detection (ML-based, learns from history)

- Change monitoring (compare to previous runs)

**3. Data Profiling**
- Automatic statistical metric collection

- Stored in `_check_profiles` table

- Observe patterns without validation

- Inform check threshold selection

**4. Data Quality Strategy**
- **Audits** - Critical, blocking (stop bad data)

- **Checks** - Monitoring, non-blocking (watch for problems)

- **Profiles** - Observation, tracking (understand what's normal)

Remember: start simple, use profiles to understand your data, then create checks based on what you learn. And don't forget, checks are there to help you, not stress you out. If a check is giving you too many false positives, adjust the threshold or switch to anomaly detection. The goal is better data quality, not perfect check scores.



# Kinds

Source: https://tmdc-io.github.io/vulcan-book/components/model/model_kinds/

---

# Kinds

Model kinds determine how Vulcan loads and processes your data. Each kind is optimized for different use cases. Some rebuild everything from scratch, others update incrementally, and some create views that compute on-demand.


## INCREMENTAL_BY_TIME_RANGE

`INCREMENTAL_BY_TIME_RANGE` models are perfect for time-series data, things like events, logs, transactions, or any data that arrives over time. Instead of rebuilding everything each run (like FULL models do), these models only process the time intervals that are missing or need updating.

If you're processing daily sales data, you don't want to reprocess all of 2023 just to add today's data. With `INCREMENTAL_BY_TIME_RANGE`, Vulcan only processes the new intervals, which saves time and money.

To use this kind, you need to tell Vulcan two things:

1. **Which column has your time data** - So Vulcan knows how to partition and filter
2. **A WHERE clause** - That filters your upstream data by time range using Vulcan's time macros

You specify the time column in your `MODEL` DDL using the `time_column` key. Here's a simple example:

```sql linenums="1"
MODEL (
  name vulcan_demo.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date -- This model's time information is stored in the `order_date` column
  )
);
```

<a id="timezones"></a>
In addition to specifying a time column in the `MODEL` DDL, the model's query must contain a `WHERE` clause that filters the upstream records by time range. Vulcan provides special macros that represent the start and end of the time range being processed: `@start_date` / `@end_date` and `@start_ds` / `@end_ds`. See [Macros](../advanced-features/macros/variables.md) for more information.

??? "Example SQL sequence when applying this model kind (ex: BigQuery)"
    This example demonstrates incremental by time range models.

    Create a model with the following definition and run `vulcan plan dev`:

    ```sql
    MODEL (
      name demo.incrementals_demo,
      kind INCREMENTAL_BY_TIME_RANGE (
        -- How does this model kind behave?

        --   DELETE by time range, then INSERT
        time_column transaction_date,

        -- How do I handle late-arriving data?

        --   Handle late-arriving events for the past 2 (2*1) days based on cron

        --   interval. Each time it runs, it will process today, yesterday, and

        --   the day before yesterday.
        lookback 2,
      ),

      -- Don't backfill data before this date
      start '2024-10-25',

      -- What schedule should I run these at?

      --   Daily at Midnight UTC
      cron '@daily',

      -- Good documentation for the primary key
      grain transaction_id,

      -- How do I test this data?

      --   Validate that the `transaction_id` primary key values are both unique

      --   and non-null. Data audit tests only run for the processed intervals,

      --   not for the entire table.

      -- audits (

      --   UNIQUE_VALUES(columns = (transaction_id)),

      --   NOT_NULL(columns = (transaction_id))

      -- )
    );

    WITH sales_data AS (
      SELECT
        transaction_id,
        product_id,
        customer_id,
        transaction_amount,
        -- How do I account for UTC vs. PST (California baby) timestamps?

        --   Make sure all time columns are in UTC and convert them to PST in the

        --   presentation layer downstream.
        transaction_timestamp,
        payment_method,
        currency
      FROM vulcan-public-demo.tcloud_raw_data.sales  -- Source A: sales data
      -- How do I make this run fast and only process the necessary intervals?

      --   Use our date macros that will automatically run the necessary intervals.

      --   Because Vulcan manages state, it will know what needs to run each time

      --   you invoke `vulcan run`.
      WHERE transaction_timestamp BETWEEN @start_dt AND @end_dt
    ),

    product_usage AS (
      SELECT
        product_id,
        customer_id,
        last_usage_date,
        usage_count,
        feature_utilization_score,
        user_segment
      FROM vulcan-public-demo.tcloud_raw_data.product_usage  -- Source B
      -- Include usage data from the 30 days before the interval
      WHERE last_usage_date BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt
    )

    SELECT
      s.transaction_id,
      s.product_id,
      s.customer_id,
      s.transaction_amount,
      -- Extract the date from the timestamp to partition by day
      DATE(s.transaction_timestamp) as transaction_date,
      -- Convert timestamp to PST using a SQL function in the presentation layer for end users
      DATETIME(s.transaction_timestamp, 'America/Los_Angeles') as transaction_timestamp_pst,
      s.payment_method,
      s.currency,
      -- Product usage metrics
      p.last_usage_date,
      p.usage_count,
      p.feature_utilization_score,
      p.user_segment,
      -- Derived metrics
      CASE
        WHEN p.usage_count > 100 AND p.feature_utilization_score > 0.8 THEN 'Power User'
        WHEN p.usage_count > 50 THEN 'Regular User'
        WHEN p.usage_count IS NULL THEN 'New User'
        ELSE 'Light User'
      END as user_type,
      -- Time since last usage
      DATE_DIFF(s.transaction_timestamp, p.last_usage_date, DAY) as days_since_last_usage
    FROM sales_data s
    LEFT JOIN product_usage p
      ON s.product_id = p.product_id
      AND s.customer_id = p.customer_id
    ```

    Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, `50975949`, is part of the table name.

    ```sql
    CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` (
      `transaction_id` STRING,
      `product_id` STRING,
      `customer_id` STRING,
      `transaction_amount` NUMERIC,
      `transaction_date` DATE OPTIONS (description='We extract the date from the timestamp to partition by day'),
      `transaction_timestamp_pst` DATETIME OPTIONS (description='Convert this to PST using a SQL function'),
      `payment_method` STRING,
      `currency` STRING,
      `last_usage_date` TIMESTAMP,
      `usage_count` INT64,
      `feature_utilization_score` FLOAT64,
      `user_segment` STRING,
      `user_type` STRING OPTIONS (description='Derived metrics'),
      `days_since_last_usage` INT64 OPTIONS (description='Time since last usage')
      )
      PARTITION BY `transaction_date`
    ```

    Vulcan will validate the SQL before processing data (note the `WHERE FALSE LIMIT 0` and the placeholder timestamps).

    ```sql
    WITH `sales_data` AS (
      SELECT
        `sales`.`transaction_id` AS `transaction_id`,
        `sales`.`product_id` AS `product_id`,
        `sales`.`customer_id` AS `customer_id`,
        `sales`.`transaction_amount` AS `transaction_amount`,
        `sales`.`transaction_timestamp` AS `transaction_timestamp`,
        `sales`.`payment_method` AS `payment_method`,
        `sales`.`currency` AS `currency`
      FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`
      WHERE (
        `sales`.`transaction_timestamp` <= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND
        `sales`.`transaction_timestamp` >= CAST('1970-01-01 00:00:00+00:00' AS TIMESTAMP)) AND
        FALSE
    ),
    `product_usage` AS (
      SELECT
        `product_usage`.`product_id` AS `product_id`,
        `product_usage`.`customer_id` AS `customer_id`,
        `product_usage`.`last_usage_date` AS `last_usage_date`,
        `product_usage`.`usage_count` AS `usage_count`,
        `product_usage`.`feature_utilization_score` AS `feature_utilization_score`,
        `product_usage`.`user_segment` AS `user_segment`
      FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`
      WHERE (
        `product_usage`.`last_usage_date` <= CAST('1970-01-01 23:59:59.999999+00:00' AS TIMESTAMP) AND
        `product_usage`.`last_usage_date` >= CAST('1969-12-02 00:00:00+00:00' AS TIMESTAMP)
        ) AND
        FALSE
    )

    SELECT
      `s`.`transaction_id` AS `transaction_id`,
      `s`.`product_id` AS `product_id`,
      `s`.`customer_id` AS `customer_id`,
      CAST(`s`.`transaction_amount` AS NUMERIC) AS `transaction_amount`,
      DATE(`s`.`transaction_timestamp`) AS `transaction_date`,
      DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,
      `s`.`payment_method` AS `payment_method`,
      `s`.`currency` AS `currency`,
      `p`.`last_usage_date` AS `last_usage_date`,
      `p`.`usage_count` AS `usage_count`,
      `p`.`feature_utilization_score` AS `feature_utilization_score`,
      `p`.`user_segment` AS `user_segment`,
      CASE
        WHEN `p`.`feature_utilization_score` > 0.8 AND `p`.`usage_count` > 100 THEN 'Power User'
        WHEN `p`.`usage_count` > 50 THEN 'Regular User'
        WHEN `p`.`usage_count` IS NULL THEN 'New User'
        ELSE 'Light User'
      END AS `user_type`,
      DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`
    FROM `sales_data` AS `s`
    LEFT JOIN `product_usage` AS `p`
      ON `p`.`customer_id` = `s`.`customer_id` AND
      `p`.`product_id` = `s`.`product_id`
    WHERE FALSE
    LIMIT 0
    ```

    Vulcan will merge data into the empty table.

    ```sql
    MERGE INTO `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949` AS `__MERGE_TARGET__` USING (
      WITH `sales_data` AS (
        SELECT
          `transaction_id`,
          `product_id`,
          `customer_id`,
          `transaction_amount`,
          `transaction_timestamp`,
          `payment_method`,
          `currency`
        FROM `vulcan-public-demo`.`tcloud_raw_data`.`sales` AS `sales`
        WHERE `transaction_timestamp` BETWEEN CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)
      ),
      `product_usage` AS (
        SELECT
          `product_id`,
          `customer_id`,
          `last_usage_date`,
          `usage_count`,
          `feature_utilization_score`,
          `user_segment`
        FROM `vulcan-public-demo`.`tcloud_raw_data`.`product_usage` AS `product_usage`
        WHERE `last_usage_date` BETWEEN DATE_SUB(CAST('2024-10-25 00:00:00+00:00' AS TIMESTAMP), INTERVAL '30' DAY) AND CAST('2024-11-04 23:59:59.999999+00:00' AS TIMESTAMP)
      )

      SELECT
        `transaction_id`,
        `product_id`,
        `customer_id`,
        `transaction_amount`,
        `transaction_date`,
        `transaction_timestamp_pst`,
        `payment_method`,
        `currency`,
        `last_usage_date`,
        `usage_count`,
        `feature_utilization_score`,
        `user_segment`,
        `user_type`,
        `days_since_last_usage`
      FROM (
        SELECT
          `s`.`transaction_id` AS `transaction_id`,
          `s`.`product_id` AS `product_id`,
          `s`.`customer_id` AS `customer_id`,
          `s`.`transaction_amount` AS `transaction_amount`,
          DATE(`s`.`transaction_timestamp`) AS `transaction_date`,
          DATETIME(`s`.`transaction_timestamp`, 'America/Los_Angeles') AS `transaction_timestamp_pst`,
          `s`.`payment_method` AS `payment_method`,
          `s`.`currency` AS `currency`,
          `p`.`last_usage_date` AS `last_usage_date`,
          `p`.`usage_count` AS `usage_count`,
          `p`.`feature_utilization_score` AS `feature_utilization_score`,
          `p`.`user_segment` AS `user_segment`,
          CASE
            WHEN `p`.`usage_count` > 100 AND `p`.`feature_utilization_score` > 0.8 THEN 'Power User'
            WHEN `p`.`usage_count` > 50 THEN 'Regular User'
            WHEN `p`.`usage_count` IS NULL THEN 'New User'
            ELSE 'Light User'
          END AS `user_type`,
          DATE_DIFF(`s`.`transaction_timestamp`, `p`.`last_usage_date`, DAY) AS `days_since_last_usage`
        FROM `sales_data` AS `s`
        LEFT JOIN `product_usage` AS `p`
          ON `s`.`product_id` = `p`.`product_id`
          AND `s`.`customer_id` = `p`.`customer_id`
      ) AS `_subquery`
      WHERE `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE)
    ) AS `__MERGE_SOURCE__`
    ON FALSE
    WHEN NOT MATCHED BY SOURCE AND `transaction_date` BETWEEN CAST('2024-10-25' AS DATE) AND CAST('2024-11-04' AS DATE) THEN DELETE
    WHEN NOT MATCHED THEN
      INSERT (
        `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,
        `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,
        `days_since_last_usage`
      )
      VALUES (
        `transaction_id`, `product_id`, `customer_id`, `transaction_amount`, `transaction_date`, `transaction_timestamp_pst`,
        `payment_method`, `currency`, `last_usage_date`, `usage_count`, `feature_utilization_score`, `user_segment`, `user_type`,
        `days_since_last_usage`
      )
    ```

    Vulcan will create a suffixed `__dev` schema based on the name of the plan environment.

    ```sql
    CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`
    ```

    Vulcan will create a view in the virtual layer to pointing to the versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incrementals_demo` AS
    SELECT *
    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incrementals_demo__50975949`
    ```

!!! info "Important: Timezone Requirements"

    Your `time_column` should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.

    **Why UTC?** It's a data engineering best practice, convert everything to UTC when it enters your system, then convert to local timezones only when data leaves for end users. This prevents timezone-related bugs as data flows between models.

    **Important:** The `cron_tz` flag doesn't change this requirement, it only affects when your model runs, not how time intervals are calculated.

    If you absolutely must use a different timezone, you can try to work around it using `lookback`, `allow_partials`, or cron offsets, but UTC is strongly recommended. Trust us on this one, timezone bugs are no fun!


This example implements a complete `INCREMENTAL_BY_TIME_RANGE` model that specifies the time column name `order_date` in the `MODEL` DDL and includes a SQL `WHERE` clause to filter records by time range:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.incremental_by_time_range,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column order_date
      ),
      start '2025-01-01',
      grains (order_date, product_id),
      cron '@daily'
    );

    SELECT
      o.order_date,
      p.product_id,
      p.name AS product_name,
      p.category,
      COUNT(DISTINCT o.order_id) AS order_count,
      SUM(oi.quantity) AS total_quantity,
      SUM(oi.quantity * oi.unit_price) AS total_sales_amount
    FROM vulcan_demo.orders AS o
    JOIN vulcan_demo.order_items AS oi
      ON o.order_id = oi.order_id
    JOIN vulcan_demo.products AS p
      ON oi.product_id = p.product_id
    WHERE
      o.order_date BETWEEN @start_ds AND @end_ds
    GROUP BY
      o.order_date, p.product_id, p.name, p.category
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.incremental_by_time_range_py",
        columns={
            "order_date": "date",
            "product_id": "int",
            "product_name": "string",
            "total_sales_amount": "decimal(10,2)",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="order_date",
        ),
        grains=["order_date", "product_id"],
        depends_on=["vulcan_demo.orders", "vulcan_demo.order_items", "vulcan_demo.products"],
    )
    def execute(context: ExecutionContext, start, end, **kwargs):
        query = f"""
        SELECT o.order_date, p.product_id, p.name AS product_name,
               SUM(oi.quantity * oi.unit_price) AS total_sales_amount
        FROM vulcan_demo.orders o
        JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id
        JOIN vulcan_demo.products p ON oi.product_id = p.product_id
        WHERE o.order_date BETWEEN '{start}' AND '{end}'
        GROUP BY o.order_date, p.product_id, p.name
        """
        return context.fetchdf(query)
    ```

### Time Column

Vulcan needs to know which column in your model's output represents the timestamp or date for each record. This is your `time_column`.

!!! info "Remember: UTC Timezone"

    Your `time_column` should be in UTC timezone. Learn more about why this matters [above](#timezones).

The time column is used to determine which records will be overwritten during data [restatement](../../guides/plan.md#restatement-plans) and provides a partition key for engines that support partitioning (such as Apache Spark). The name of the time column is specified in the `MODEL` DDL `kind` specification:

```sql linenums="1"
MODEL (
  name vulcan_demo.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date -- This model's time information is stored in the `order_date` column
  )
);
```

By default, Vulcan assumes your time column is in `%Y-%m-%d` format (like `2025-01-15`). If your dates are in a different format, you can specify it:

```sql linenums="1"
MODEL (
  name vulcan_demo.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column (order_date, '%Y-%m-%d')
  )
);
```

!!! note "Format String Dialect"

    Use the same SQL dialect for your format string as the one used in your model's query.

**Safety feature:** Vulcan automatically adds a time range filter to your query's output to prevent data leakage. This means even if your `WHERE` clause has a bug, Vulcan won't accidentally store records outside the target interval.

Here's how it works:

- **Your WHERE clause** filters the **input** data as it's read from upstream tables (makes queries faster)

- **Vulcan's automatic filter** filters the **output** data before it's stored (prevents data leakage)

This is especially important when handling late-arriving data, you don't want to accidentally overwrite unrelated records!

Example: sometimes your upstream data uses a different time column than your model. In this case, you filter on the upstream column (`shipped_date`), but Vulcan still adds a filter on your model's time column (`order_date`):

```sql linenums="1"
MODEL (
  name vulcan_demo.shipment_events,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date -- `order_date` is model's time column
  )
);

SELECT
  o.order_date,
  s.shipped_date,
  s.carrier
FROM vulcan_demo.orders AS o
JOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id
WHERE
  s.shipped_date BETWEEN @start_ds AND @end_ds; -- Filter is based on the user-supplied `shipped_date` column
```

At runtime, Vulcan will automatically modify the model's query to look like this:

```sql linenums="1"
SELECT
  o.order_date,
  s.shipped_date,
  s.carrier
FROM vulcan_demo.orders AS o
JOIN vulcan_demo.shipments AS s ON o.order_id = s.order_id
WHERE
  s.shipped_date BETWEEN @start_ds AND @end_ds
  AND o.order_date BETWEEN @start_ds AND @end_ds; -- `order_date` time column filter automatically added by Vulcan
```

### Partitioning

By default, Vulcan automatically adds your `time_column` to the partition key. This lets database engines do partition pruning (skipping partitions that don't match your query), which makes queries faster.

**Why this matters:** If you're querying data from the last 7 days, the engine can skip scanning all the old partitions. That's a huge performance win!

Sometimes you might not want this though, maybe you want to partition exclusively on another column, or you want to partition on `month(time_column)` but your engine doesn't support expression-based partitioning.

To disable automatic time column partitioning, set `partition_by_time_column false`:

```sql linenums="1"
MODEL (
  name vulcan_demo.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    partition_by_time_column false
  ),
  partitioned_by (warehouse_id) -- order_date will no longer be automatically added here and the partition key will just be 'warehouse_id'
);
```

### Idempotency
Make incremental by time range model queries [idempotent](../../references/glossary.md#idempotency) to prevent unexpected results during data [restatement](../../guides/plan.md#restatement-plans).

Make your incremental by time range queries idempotent. This means running the same query multiple times produces the same result, which prevents surprises during data restatement.

**Watch out:** Your upstream models can affect idempotency. If you reference a FULL model (which rebuilds everything each run), your incremental model becomes non-idempotent because that upstream data changes every time. This is usually fine, but it's good to be aware of.

### Materialization strategy
Depending on the target engine, models of the `INCREMENTAL_BY_TIME_RANGE` kind are materialized using the following strategies:

| Engine     | Strategy                                  |
|------------|-------------------------------------------|
| Spark      | INSERT OVERWRITE by time column partition |
| Databricks | INSERT OVERWRITE by time column partition |
| Snowflake  | DELETE by time range, then INSERT         |
| BigQuery   | DELETE by time range, then INSERT         |
| Redshift   | DELETE by time range, then INSERT         |
| Postgres   | DELETE by time range, then INSERT         |
| DuckDB     | DELETE by time range, then INSERT         |

## INCREMENTAL_BY_UNIQUE_KEY

`INCREMENTAL_BY_UNIQUE_KEY` models update data based on a unique key. It works like an upsert operation: if a key exists, update it; if it doesn't, insert it.

Here's how it works:

- **New key?** → Insert the row

- **Existing key?** → Update the row with new data

- **Key missing from new data?** → Leave the existing row alone

**Why use this?** Perfect for dimension tables, customer records, or any data where you want to keep the latest version of each record without rebuilding everything. It's like updating a contact list, you update existing contacts and add new ones, but you don't delete contacts that aren't in your latest import.

This kind is a good fit for datasets that have the following traits:

* Each record has a unique key associated with it.
* There is at most one record associated with each unique key.
* It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their keys match.

A [Slowly Changing Dimension](../../references/glossary.md#slowly-changing-dimension-scd) (SCD) is one approach that fits this description well. See the [SCD Type 2](#scd-type-2) model kind for SCD Type 2 models.

The name of the unique key column must be provided as part of the `MODEL` DDL, as in this example:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.incremental_by_unique_key,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key customer_id
      ),
      start '2025-01-01',
      cron '@daily',
      grains (customer_id)
    );

    SELECT
      c.customer_id,
      c.name AS customer_name,
      c.email,
      COUNT(DISTINCT o.order_id) AS total_orders,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,
      MAX(o.order_date) AS last_order_date
    FROM vulcan_demo.customers AS c
    LEFT JOIN vulcan_demo.orders AS o
      ON c.customer_id = o.customer_id
    LEFT JOIN vulcan_demo.order_items AS oi
      ON o.order_id = oi.order_id
    WHERE
      o.order_date IS NULL OR o.order_date BETWEEN @start_date AND @end_date
    GROUP BY c.customer_id, c.name, c.email
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.incremental_by_unique_key_py",
        columns={
            "customer_id": "int",
            "total_spent": "decimal(10,2)",
            "last_order_date": "date",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,
            unique_key=["customer_id"],
        ),
        grains=["customer_id"],
        depends_on=["vulcan_demo.customers", "vulcan_demo.orders", "vulcan_demo.order_items"],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT c.customer_id,
               SUM(oi.quantity * oi.unit_price) as total_spent,
               MAX(o.order_date) as last_order_date
        FROM vulcan_demo.customers c
        LEFT JOIN vulcan_demo.orders o ON c.customer_id = o.customer_id
        LEFT JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id
        GROUP BY c.customer_id
        """
        return context.fetchdf(query)
    ```

You can use composite keys (multiple columns) too:

```sql linenums="1"
MODEL (
  name vulcan_demo.order_items_agg,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key (order_id, product_id)
  )
);
```

You can also filter upstream records by time range using `@start_date`, `@end_date`, or other time macros (just like `INCREMENTAL_BY_TIME_RANGE`). This is useful when you only want to process records from a specific time period.

**Note:** Vulcan's time macros are always in UTC timezone.

```sql linenums="1"
SELECT
  c.customer_id,
  c.name AS customer_name,
  COUNT(o.order_id) AS total_orders
FROM vulcan_demo.customers AS c
LEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id
WHERE
  o.order_date BETWEEN @start_date AND @end_date
GROUP BY c.customer_id, c.name
```

??? "Example SQL sequence when applying this model kind (ex: BigQuery)"

    Create a model with the following definition and run `vulcan plan dev`:

    ```sql
    MODEL (
      name demo.incremental_by_unique_key_example,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key id
      ),
      start '2020-01-01',
      cron '@daily',
    );

    SELECT
      id,
      item_id,
      event_date
    FROM demo.seed_model
    WHERE
      event_date BETWEEN @start_date AND @end_date
    ```

    Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, `1161945221`, is part of the table name.

    ```sql
    CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` (`id` INT64, `item_id` INT64, `event_date` DATE)
    ```

    Vulcan will validate the model's query before processing data (note the `FALSE LIMIT 0` in the `WHERE` statement and the placeholder dates).

    ```sql
    SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`
    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`
    WHERE (`seed_model`.`event_date` <= CAST('1970-01-01' AS DATE) AND `seed_model`.`event_date` >= CAST('1970-01-01' AS DATE)) AND FALSE LIMIT 0
    ```

    Vulcan will create a versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221` AS
    SELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`
    FROM (SELECT `seed_model`.`id` AS `id`, `seed_model`.`item_id` AS `item_id`, `seed_model`.`event_date` AS `event_date`
    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_model__2834544882` AS `seed_model`
    WHERE `seed_model`.`event_date` <= CAST('2024-10-30' AS DATE) AND `seed_model`.`event_date` >= CAST('2020-01-01' AS DATE)) AS `_subquery`
    ```

    Vulcan will create a suffixed `__dev` schema based on the name of the plan environment.

    ```sql
    CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`
    ```

    Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`incremental_by_unique_key_example` AS
    SELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_by_unique_key_example__1161945221`
    ```

**Note:** Models of the `INCREMENTAL_BY_UNIQUE_KEY` kind are inherently [non-idempotent](../../references/glossary.md#idempotency), which should be taken into consideration during data [restatement](../../guides/plan.md#restatement-plans). As a result, partial data restatement is not supported for this model kind, which means that the entire table will be recreated from scratch if restated.

### Unique Key Expressions

You're not limited to column names. You can use SQL expressions when you need to create a key from multiple columns or transform values. Example using `COALESCE`:

```sql linenums="1"
MODEL (
  name vulcan_demo.customers_unique,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key COALESCE("email", '')
  )
);
```

### When Matched Expression

By default, when a key matches (source and target have the same key), Vulcan updates all columns. But sometimes you want more control, maybe you want to preserve certain values, or only update specific columns.

You can customize this behavior with `when_matched` expressions:

```sql linenums="1"
MODEL (
  name vulcan_demo.customers_update,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key customer_id,
    when_matched (
      WHEN MATCHED THEN UPDATE SET target.email = COALESCE(source.email, target.email)
    )
  )
);
```

**Important:** You must use `source` and `target` aliases to distinguish between the source (new data) and target (existing table) columns.

You can also provide multiple `WHEN MATCHED` expressions for more complex logic:

```sql linenums="1"
MODEL (
  name vulcan_demo.products_update,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key product_id,
    when_matched (
      WHEN MATCHED AND source.price IS NULL THEN UPDATE SET target.price = target.price
      WHEN MATCHED THEN UPDATE SET target.category = COALESCE(source.category, target.category)
    )
  )
);
```

!!! note "Engine Support"

    `when_matched` only works on engines that support the `MERGE` statement. Supported engines include:
    
    - BigQuery

    - Databricks

    - Postgres

    - Redshift (requires `enable_merge: true` in connection config)

    - Snowflake

    - Spark

    **Redshift users:** You need to enable MERGE support by setting `enable_merge: true` in your connection config. It's disabled by default.

```yaml linenums="1"
gateways:
  redshift:
    connection:
      type: redshift
      enable_merge: true
```

Redshift supports only the `UPDATE` or `DELETE` actions for the `WHEN MATCHED` clause and does not allow multiple `WHEN MATCHED` expressions. For further information, refer to the [Redshift documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_MERGE.html#r_MERGE-parameters).

### Merge Filter Expression

MERGE operations can be slow on large tables because they typically scan the entire existing table. If you're only updating a small subset of records, this is wasteful.

**Solution:** Use `merge_filter` to add conditions to the MERGE's `ON` clause. This limits the scan to only the rows that might match, making things much faster.

The `merge_filter` accepts predicates (single or combined with AND) that get added to the MERGE operation:

```sql linenums="1"
MODEL (
  name vulcan_demo.orders_recent,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key order_id,
    merge_filter source._operation IS NULL AND target.order_date > dateadd(day, -7, current_date)
  )
);
```

Just like `when_matched`, use `source` and `target` aliases to reference the source and target tables.

If your dbt project uses `incremental_predicates`, Vulcan automatically converts them to `merge_filter`.

### Materialization strategy
Depending on the target engine, models of the `INCREMENTAL_BY_UNIQUE_KEY` kind are materialized using the following strategies:

| Engine     | Strategy                            |
|------------|-------------------------------------|
| Spark      | not supported                       |
| Databricks | MERGE ON unique key                 |
| Snowflake  | MERGE ON unique key                 |
| BigQuery   | MERGE ON unique key                 |
| Redshift   | MERGE ON unique key                 |
| Postgres   | MERGE ON unique key                 |
| DuckDB     | DELETE ON matched + INSERT new rows |

## FULL

`FULL` models are the simplest kind, they rebuild everything from scratch every time they run. No incremental logic, no time columns, no unique keys. Just run the query and replace the entire table.

**When to use FULL:**

- Small datasets where rebuilding is fast and cheap

- Aggregate tables without a time dimension

- Tables that change completely each run (like a "current state" snapshot)

- Development and testing (simpler is better when you're iterating)

**When NOT to use FULL:**

- Large datasets (you'll wait forever and pay a lot)

- Time-series data (use `INCREMENTAL_BY_TIME_RANGE` instead)

- Tables that only change partially (use incremental kinds)

The trade-off is simplicity vs. performance. For small tables, FULL is perfect. For large tables, incremental kinds will save you time and money.

This example specifies a `FULL` model kind:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.full_model,
      kind FULL,
      start '2025-01-01',
      grains (customer_id)
    );

    SELECT
      c.customer_id,
      c.name AS customer_name,
      c.email,
      COUNT(DISTINCT o.order_id) AS total_orders,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value
    FROM vulcan_demo.customers AS c
    LEFT JOIN vulcan_demo.orders AS o
      ON c.customer_id = o.customer_id
    LEFT JOIN vulcan_demo.order_items AS oi
      ON o.order_id = oi.order_id
    GROUP BY c.customer_id, c.name, c.email
    ORDER BY total_spent DESC
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.full_model_py",
        columns={
            "product_id": "int",
            "product_name": "string",
            "category": "string",
            "total_sales": "decimal(10,2)",
        },
        kind=dict(
            name=ModelKindName.FULL,
        ),
        grains=["product_id"],
        depends_on=["vulcan_demo.products", "vulcan_demo.order_items", "vulcan_demo.orders"],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT p.product_id, p.name AS product_name, p.category,
               COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales
        FROM vulcan_demo.products p
        LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id
        LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id
        GROUP BY p.product_id, p.name, p.category
        ORDER BY total_sales DESC
        """
        return context.fetchdf(query)
    ```

??? "Example SQL sequence when applying this model kind (ex: BigQuery)"

    Create a model with the following definition and run `vulcan plan dev`:

    ```sql
    MODEL (
      name demo.full_model_example,
      kind FULL,
      cron '@daily',
      grain item_id,
    );

    SELECT
      item_id,
      COUNT(DISTINCT id) AS num_orders
    FROM demo.incremental_model
    GROUP BY
      item_id
    ```

    Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, `2345651858`, is part of the table name.

    ```sql
    CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` (`item_id` INT64, `num_orders` INT64)
    ```

    Vulcan will validate the model's query before processing data (note the `WHERE FALSE` and `LIMIT 0`).

    ```sql
    SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`
    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`
    WHERE FALSE
    GROUP BY `incremental_model`.`item_id` LIMIT 0
    ```

    Vulcan will create a versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858` AS
    SELECT CAST(`item_id` AS INT64) AS `item_id`, CAST(`num_orders` AS INT64) AS `num_orders`
    FROM (SELECT `incremental_model`.`item_id` AS `item_id`, COUNT(DISTINCT `incremental_model`.`id`) AS `num_orders`
    FROM `vulcan-public-demo`.`vulcan__demo`.`demo__incremental_model__89556012` AS `incremental_model`
    GROUP BY `incremental_model`.`item_id`) AS `_subquery`
    ```

    Vulcan will create a suffixed `__dev` schema based on the name of the plan environment.

    ```sql
    CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`
    ```

    Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`full_model_example` AS
    SELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__full_model_example__2345651858`
    ```

### Materialization strategy
Depending on the target engine, models of the `FULL` kind are materialized using the following strategies:

| Engine     | Strategy                         |
|------------|----------------------------------|
| Spark      | INSERT OVERWRITE                 |
| Databricks | INSERT OVERWRITE                 |
| Snowflake  | CREATE OR REPLACE TABLE          |
| BigQuery   | CREATE OR REPLACE TABLE          |
| Redshift   | DROP TABLE, CREATE TABLE, INSERT |
| Postgres   | DROP TABLE, CREATE TABLE, INSERT |
| DuckDB     | CREATE OR REPLACE TABLE          |

## VIEW

Unlike the other kinds, `VIEW` models don't store any data. Instead, they create a virtual table (a view) that runs your query every time someone queries it.

**How it works:** When a downstream model or user queries your VIEW model, the database executes your query on-the-fly. No data is pre-computed or stored.

**When to use VIEW:**

- Simple transformations that are fast to compute

- When you want always-fresh data (no caching)

- When storage is expensive but compute is cheap

- For lightweight transformations that don't need materialization

**When NOT to use VIEW:**

- Expensive queries that run frequently (you'll pay the compute cost every time)

- Complex aggregations or joins (materialize these instead)

- Python models (VIEW isn't supported for Python, use SQL)

!!! note "Default Kind"

    `VIEW` is the default model kind if you don't specify one. So if you write a model without a `kind`, it becomes a VIEW automatically.

!!! warning "Performance Consideration"

    Since VIEW queries run every time they're referenced, expensive queries can get costly fast. If your view is referenced by many downstream models, you might be running that expensive query dozens of times. Consider materializing expensive views as FULL or incremental models instead.


This example specifies a `VIEW` model kind:

```sql linenums="1"
MODEL (
  name vulcan_demo.view_model,
  kind VIEW,
  grains (warehouse_performance_key)
);

SELECT
  w.warehouse_id,
  w.name AS warehouse_name,
  r.region_name,
  o.order_date,
  CONCAT(w.warehouse_id::TEXT, '_', o.order_date::TEXT) AS warehouse_performance_key,
  COUNT(DISTINCT o.order_id) AS total_transactions,
  SUM(oi.quantity * oi.unit_price) AS total_sales_amount,
  COUNT(DISTINCT o.customer_id) AS unique_customers
FROM vulcan_demo.warehouses AS w
LEFT JOIN vulcan_demo.regions AS r
  ON w.region_id = r.region_id
LEFT JOIN vulcan_demo.orders AS o
  ON w.warehouse_id = o.warehouse_id
LEFT JOIN vulcan_demo.order_items AS oi
  ON o.order_id = oi.order_id
GROUP BY w.warehouse_id, w.name, r.region_name, o.order_date
```

??? "Example SQL sequence when applying this model kind (ex: BigQuery)"

    Create a model with the following definition and run `vulcan plan dev`:

    ```sql
    MODEL (
      name demo.example_view,
      kind VIEW,
      cron '@daily',
    );

    SELECT
      'hello there' as a_column
    ```

    Vulcan will execute this SQL to create a versioned view in the physical layer. Note that the view's version fingerprint, `1024042926`, is part of the view name.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`
    (`a_column`) AS SELECT 'hello there' AS `a_column`
    ```

    Vulcan will create a suffixed `__dev` schema based on the name of the plan environment.

    ```sql
    CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`
    ```

    Vulcan will create a view in the virtual layer pointing to the versioned view in the physical layer.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`example_view` AS
    SELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__example_view__1024042926`
    ```


### Materialized Views

Want the best of both worlds? You can turn a VIEW into a materialized view by setting `materialized: true`. Materialized views store the query results (like a table) but automatically refresh when the underlying data changes (like a view).

Set it up like this:

```sql linenums="1"
MODEL (
  name vulcan_demo.sales_summary,
  kind VIEW (
    materialized true
  )
);
```

!!! note "Engine Support"

    Materialized views are only supported on:
    
    - BigQuery

    - Databricks

    - Snowflake
    
    On other engines, this flag is ignored and you'll get a regular VIEW.

Vulcan only recreates the materialized view when your query changes or the view doesn't exist. This provides the performance benefits of materialized views without unnecessary refreshes.

## EMBEDDED

`EMBEDDED` models are like reusable SQL snippets. They don't create tables or views, instead, their query gets injected directly into any downstream model that references them, as a subquery.

**Why use this?** If you have common logic that multiple models need (like a CTE that filters active customers), you can define it once in an EMBEDDED model and reuse it everywhere. It's like a macro, but for SQL.

**Perfect for:**

- Common CTEs used across multiple models

- Reusable business logic (like "active customers" or "valid orders")

- Avoiding code duplication

!!! note "Python Models"

    Python models don't support the `EMBEDDED` kind, use a SQL model instead.

This example specifies an `EMBEDDED` model kind:

```sql linenums="1"
MODEL (
  name vulcan_demo.unique_customers,
  kind EMBEDDED
);

SELECT DISTINCT
  customer_id,
  name AS customer_name,
  email
FROM vulcan_demo.customers
```

## SEED
The `SEED` model kind is used to specify seed models for using static CSV datasets in your Vulcan project.

**How it works:** You point to a CSV file, define the schema, and Vulcan loads it into a table. The data only gets reloaded if you change the model definition or update the CSV file.

**Use cases:**

- Reference data (countries, states, categories)

- Lookup tables

- Static configuration data

- Test data

!!! note "Python Models"

    Python models don't support the `SEED` kind, use a SQL model instead.

!!! note "When Data Reloads"

    Seed models are loaded once and stay loaded unless you update the model definition or change the CSV file. This keeps things efficient, no point reloading static data every run!

This example specifies a `SEED` model kind:

```sql linenums="1"
MODEL (
  name vulcan_demo.seed_model,
  kind SEED (
    path '../seeds/seed_data.csv'
  ),
  columns (
    id INT,
    item_id INT,
    event_date DATE
  ),
  grains (id),
  assertions (
    UNIQUE_COMBINATION_OF_COLUMNS(columns := (id, event_date)),
    NOT_NULL(columns := (id, item_id, event_date))
  )
)
```

??? "Example SQL sequence when applying this model kind (ex: BigQuery)"

    Create a model with the following definition and run `vulcan plan dev`:

    ```sql
    MODEL (
      name demo.seed_example,
      kind SEED (
        path '../../seeds/seed_example.csv'
      ),
      columns (
        id INT64,
        item_id INT64,
        event_date DATE
      ),
      grain (id, event_date)
    )
    ```

    Vulcan will execute this SQL to create a versioned table in the physical layer. Note that the table's version fingerprint, `3038173937`, is part of the table name.

    ```sql
    CREATE TABLE IF NOT EXISTS `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` (`id` INT64, `item_id` INT64, `event_date` DATE)
    ```

    Vulcan will upload the seed as a temp table in the physical layer.

    ```sql
    vulcan-public-demo.vulcan__demo.__temp_demo__seed_example__3038173937_9kzbpld7
    ```

    Vulcan will create a versioned table in the physical layer from the temp table.

    ```sql
    CREATE OR REPLACE TABLE `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937` AS
    SELECT CAST(`id` AS INT64) AS `id`, CAST(`item_id` AS INT64) AS `item_id`, CAST(`event_date` AS DATE) AS `event_date`
    FROM (SELECT `id`, `item_id`, `event_date`
    FROM `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`) AS `_subquery`
    ```

    Vulcan will drop the temp table in the physical layer.

    ```sql
    DROP TABLE IF EXISTS `vulcan-public-demo`.`vulcan__demo`.`__temp_demo__seed_example__3038173937_9kzbpld7`
    ```

    Vulcan will create a suffixed `__dev` schema based on the name of the plan environment.

    ```sql
    CREATE SCHEMA IF NOT EXISTS `vulcan-public-demo`.`demo__dev`
    ```

    Vulcan will create a view in the virtual layer pointing to the versioned table in the physical layer.

    ```sql
    CREATE OR REPLACE VIEW `vulcan-public-demo`.`demo__dev`.`seed_example` AS
    SELECT * FROM `vulcan-public-demo`.`vulcan__demo`.`demo__seed_example__3038173937`
    ```

## SCD Type 2

SCD Type 2 is a model kind that supports [slowly changing dimensions](https://en.wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row) (SCDs) in your Vulcan project. SCDs are a common pattern in data warehousing that allow you to track changes to records over time.

Vulcan achieves this by adding a `valid_from` and `valid_to` column to your model. The `valid_from` column is the timestamp that the record became valid (inclusive) and the `valid_to` column is the timestamp that the record became invalid (exclusive). The `valid_to` column is set to `NULL` for the latest record.

Therefore, you can use these models to not only tell you what the latest value is for a given record but also what the values were anytime in the past. Note that maintaining this history does come at a cost of increased storage and compute and this may not be a good fit for sources that change frequently since the history could get very large.

**Note**: Partial data [restatement](../../guides/plan.md#restatement-plans) is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss, so data restatement is disabled for models of this kind by default.

Vulcan supports two ways to detect changes: **By Time** (recommended) or **By Column**. Let's look at both:

### SCD Type 2 By Time (Recommended)

**By Time** is the recommended approach. It works with source tables that have an "Updated At" timestamp column (like `updated_at`, `modified_at`, `last_changed`).

**Why it's recommended:** The timestamp tells you exactly when a record changed, which makes your SCD Type 2 table more accurate. You get precise `valid_from` times based on when the source system actually updated the record.

If your source table has an `updated_at` column, use this approach!

This example specifies a `SCD_TYPE_2_BY_TIME` model kind:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.scd_type2_by_time,
      kind SCD_TYPE_2_BY_TIME (
        unique_key dt
      ),
      grains (dt)
    );

    SELECT
      dd.dt,
      dd.year,
      dd.month,
      dd.day_of_week,
      COUNT(DISTINCT o.order_id) AS total_transactions,
      SUM(oi.quantity) AS total_quantity_sold,
      SUM(oi.quantity * oi.unit_price) AS total_sales_amount,
      CURRENT_TIMESTAMP AS updated_at
    FROM vulcan_demo.dim_dates AS dd
    LEFT JOIN vulcan_demo.orders AS o
      ON dd.dt = o.order_date
    LEFT JOIN vulcan_demo.order_items AS oi
      ON o.order_id = oi.order_id
    GROUP BY dd.dt, dd.year, dd.month, dd.day_of_week
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.scd_type2_by_time_py",
        columns={
            "customer_id": "int",
            "customer_name": "string",
            "email": "string",
            "region_name": "string"
        },
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_TIME,
            unique_key=["customer_id"],
        ),
        grains=["customer_id"],
        depends_on=["vulcan_demo.customers", "vulcan_demo.regions"],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT c.customer_id, c.name as customer_name, c.email, r.region_name
        FROM vulcan_demo.customers c
        LEFT JOIN vulcan_demo.regions r ON c.region_id = r.region_id
        """
        return context.fetchdf(query)
    ```

Vulcan will materialize this table with the following structure:
```sql linenums="1"
TABLE db.menu_items (
  id INT,
  name STRING,
  price DOUBLE,
  updated_at TIMESTAMP,
  valid_from TIMESTAMP,
  valid_to TIMESTAMP
);
```

The `updated_at` column name can also be changed by adding the following to your model definition:
```sql linenums="1" hl_lines="5"
MODEL (
  name db.menu_items,
  kind SCD_TYPE_2_BY_TIME (
    unique_key id,
    updated_at_name my_updated_at -- Name for `updated_at` column
  )
);

SELECT
  id,
  name,
  price,
  my_updated_at
FROM
  stg.current_menu_items;
```

Vulcan will materialize this table with the following structure:
```sql linenums="1"
TABLE db.menu_items (
  id INT,
  name STRING,
  price DOUBLE,
  my_updated_at TIMESTAMP,
  valid_from TIMESTAMP,
  valid_to TIMESTAMP
);
```

### SCD Type 2 By Column

**By Column** works when your source table doesn't have an "Updated At" timestamp. Instead, Vulcan compares the values in specific columns between runs and detects changes.

**How it works:** You specify which columns to watch (or use `*` to watch all columns). When Vulcan detects a change in any of those columns, it records `valid_from` as the execution time when the change was detected.

**Use this when:** Your source system doesn't track update timestamps, but you still want to maintain history. The trade-off is that `valid_from` times are based on when Vulcan detected the change, not when the source system actually changed it.

This example specifies a `SCD_TYPE_2_BY_COLUMN` model kind:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.scd_type2_by_column,
      kind SCD_TYPE_2_BY_COLUMN (
        unique_key ARRAY[product_id],
        columns ARRAY[product_name, category, price]
      ),
      grains (product_id)
    );

    SELECT
      p.product_id,
      p.name AS product_name,
      p.category,
      p.price,
      s.name AS supplier_name,
      r.region_name
    FROM vulcan_demo.products AS p
    LEFT JOIN vulcan_demo.suppliers AS s
      ON p.supplier_id = s.supplier_id
    LEFT JOIN vulcan_demo.regions AS r
      ON s.region_id = r.region_id
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.scd_type2_by_column_py",
        columns={
            "product_id": "int",
            "product_name": "string",
            "category": "string",
            "price": "decimal(10,2)"
        },
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_COLUMN,
            unique_key=["product_id"],
            columns=["product_name", "category", "price"],
        ),
        grains=["product_id"],
        depends_on=["vulcan_demo.products"],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT product_id, name as product_name, category, price
        FROM vulcan_demo.products
        """
        return context.fetchdf(query)
    ```

Vulcan will materialize this table with the following structure:
```sql linenums="1"
TABLE db.menu_items (
  id INT,
  name STRING,
  price DOUBLE,
  valid_from TIMESTAMP,
  valid_to TIMESTAMP
);
```

### Change Column Names

Vulcan automatically adds `valid_from` and `valid_to` columns to your table. If you want different names (maybe to match your existing schema conventions), you can customize them:
```sql linenums="1" hl_lines="5-6"
MODEL (
  name db.menu_items,
  kind SCD_TYPE_2_BY_TIME (
    unique_key id,
    valid_from_name my_valid_from, -- Name for `valid_from` column
    valid_to_name my_valid_to -- Name for `valid_to` column
  )
);
```

Vulcan will materialize this table with the following structure:
```sql linenums="1"
TABLE db.menu_items (
  id INT,
  name STRING,
  price DOUBLE,
  updated_at TIMESTAMP,
  my_valid_from TIMESTAMP,
  my_valid_to TIMESTAMP
);
```

### Deletes

A "hard delete" is when a record disappears from your source table entirely. How should SCD Type 2 handle this?

**Default behavior (`invalidate_hard_deletes: false`):**

* `valid_to` column will continue to be set to `NULL` (therefore still considered "valid")
* If the record is added back, then the `valid_to` column will be set to the `valid_from` of the new record.

When a record is added back, the new record will be inserted into the table with `valid_from` set to:

* SCD_TYPE_2_BY_TIME: the largest of either the `updated_at` timestamp of the new record or the `valid_from` timestamp of the deleted record in the SCD Type 2 table
* SCD_TYPE_2_BY_COLUMN: the `execution_time` when the record was detected again

**With `invalidate_hard_deletes: true`:**

* `valid_to` is set to the execution time when Vulcan detected the missing record
* If the record comes back later, `valid_to` stays unchanged (you'll have a gap in history)

**Which should you use?**

- **`false` (default):** Missing records are still considered "valid" (no gaps in history). Use this if you think missing records might be temporary or if you prefer continuous history.

- **`true`:** Deletes are accurately tracked with precise timestamps. Use this if you want to know exactly when records were deleted, even if it creates gaps in history.

With `false`, missing records are still considered valid. With `true`, missing records are treated as deleted at that time.

### Example of SCD Type 2 By Time in Action

Let's walk through a real example. Say you're tracking a restaurant menu, and you start with this source data (with `invalidate_hard_deletes: true`):

| ID | Name             | Price |     Updated At      |
|----|------------------|:-----:|:-------------------:|
| 1  | Chicken Sandwich | 10.99 | 2020-01-01 00:00:00 |
| 2  | Cheeseburger     | 8.99  | 2020-01-01 00:00:00 |
| 3  | French Fries     | 4.99  | 2020-01-01 00:00:00 |

The target table, which is currently empty, will be materialized with the following data:

| ID | Name             | Price |     Updated At      |     Valid From      | Valid To |
|----|------------------|:-----:|:-------------------:|:-------------------:|:--------:|
| 1  | Chicken Sandwich | 10.99 | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 |   NULL   |
| 2  | Cheeseburger     | 8.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 |   NULL   |
| 3  | French Fries     | 4.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 |   NULL   |

Now lets say that you update the source table with the following data:

| ID | Name             | Price |     Updated At      |
|----|------------------|:-----:|:-------------------:|
| 1  | Chicken Sandwich | 12.99 | 2020-01-02 00:00:00 |
| 3  | French Fries     | 4.99  | 2020-01-01 00:00:00 |
| 4  | Milkshake        | 3.99  | 2020-01-02 00:00:00 |

Summary of Changes:

* The price of the Chicken Sandwich was increased from $10.99 to $12.99.
* Cheeseburger was removed from the menu.
* Milkshakes were added to the menu.

Assuming your models ran at `2020-01-02 11:00:00`, target table will be updated with the following data:

| ID | Name             | Price |     Updated At      |     Valid From      |      Valid To       |
|----|------------------|:-----:|:-------------------:|:-------------------:|:-------------------:|
| 1  | Chicken Sandwich | 10.99 | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 | 2020-01-02 00:00:00 |
| 1  | Chicken Sandwich | 12.99 | 2020-01-02 00:00:00 | 2020-01-02 00:00:00 |        NULL         |
| 2  | Cheeseburger     | 8.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 3  | French Fries     | 4.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 |        NULL         |
| 4  | Milkshake        | 3.99  | 2020-01-02 00:00:00 | 2020-01-02 00:00:00 |        NULL         |

For our final pass, lets say that you update the source table with the following data:

| ID | Name                | Price |     Updated At      |
|----|---------------------|:-----:|:-------------------:|
| 1  | Chicken Sandwich    | 14.99 | 2020-01-03 00:00:00 |
| 2  | Cheeseburger        | 8.99  | 2020-01-03 00:00:00 |
| 3  | French Fries        | 4.99  | 2020-01-01 00:00:00 |
| 4  | Chocolate Milkshake | 3.99  | 2020-01-02 00:00:00 |

Summary of changes:

* The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)
* Cheeseburger was added back to the menu with original name and price.
* Milkshake name was updated to be "Chocolate Milkshake".

Target table will be updated with the following data:

| ID | Name                | Price |     Updated At      |     Valid From      |      Valid To       |
|----|---------------------|:-----:|:-------------------:|:-------------------:|:-------------------:|
| 1  | Chicken Sandwich    | 10.99 | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 | 2020-01-02 00:00:00 |
| 1  | Chicken Sandwich    | 12.99 | 2020-01-02 00:00:00 | 2020-01-02 00:00:00 | 2020-01-03 00:00:00 |
| 1  | Chicken Sandwich    | 14.99 | 2020-01-03 00:00:00 | 2020-01-03 00:00:00 |        NULL         |
| 2  | Cheeseburger        | 8.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 2  | Cheeseburger        | 8.99  | 2020-01-03 00:00:00 | 2020-01-03 00:00:00 |        NULL         |
| 3  | French Fries        | 4.99  | 2020-01-01 00:00:00 | 1970-01-01 00:00:00 |        NULL         |
| 4  | Milkshake           | 3.99  | 2020-01-02 00:00:00 | 2020-01-02 00:00:00 | 2020-01-03 00:00:00 |
| 4  | Chocolate Milkshake | 3.99  | 2020-01-03 00:00:00 | 2020-01-03 00:00:00 |        NULL         |

**Notice:** `Cheeseburger` was deleted from `2020-01-02 11:00:00` to `2020-01-03 00:00:00`. If you query the table for that time range, you won't see it, which accurately reflects that it wasn't on the menu during that period.

This is the most accurate representation based on your source data. If `Cheeseburger` had been added back with its original `updated_at` timestamp (`2020-01-01`), Vulcan would have set the new record's `valid_from` to `2020-01-02 11:00:00` (when it was detected again), filling the gap. But since the timestamp didn't change, it's likely the item was removed in error, and the gap accurately represents that.


### Example of SCD Type 2 By Column in Action

Now let's see how **By Column** works. Same restaurant menu example, but this time the source table doesn't have an `updated_at` column. We'll configure the model to watch `Name` and `Price` for changes.

Starting data:

| ID | Name             | Price |
|----|------------------|:-----:|
| 1  | Chicken Sandwich | 10.99 |
| 2  | Cheeseburger     | 8.99  |
| 3  | French Fries     | 4.99  |

After the first run, your SCD Type 2 table looks like this:

| ID | Name             | Price |     Valid From      | Valid To |
|----|------------------|:-----:|:-------------------:|:--------:|
| 1  | Chicken Sandwich | 10.99 | 1970-01-01 00:00:00 |   NULL   |
| 2  | Cheeseburger     | 8.99  | 1970-01-01 00:00:00 |   NULL   |
| 3  | French Fries     | 4.99  | 1970-01-01 00:00:00 |   NULL   |

Now lets say that you update the source table with the following data:

| ID | Name             | Price |
|----|------------------|:-----:|
| 1  | Chicken Sandwich | 12.99 |
| 3  | French Fries     | 4.99  |
| 4  | Milkshake        | 3.99  |

Summary of Changes:

* The price of the Chicken Sandwich was increased from $10.99 to $12.99.
* Cheeseburger was removed from the menu.
* Milkshakes were added to the menu.

Assuming your models ran at `2020-01-02 11:00:00`, target table will be updated with the following data:

| ID | Name             | Price |     Valid From      |      Valid To       |
|----|------------------|:-----:|:-------------------:|:-------------------:|
| 1  | Chicken Sandwich | 10.99 | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 1  | Chicken Sandwich | 12.99 | 2020-01-02 11:00:00 |        NULL         |
| 2  | Cheeseburger     | 8.99  | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 3  | French Fries     | 4.99  | 1970-01-01 00:00:00 |        NULL         |
| 4  | Milkshake        | 3.99  | 2020-01-02 11:00:00 |        NULL         |

For our final pass, lets say that you update the source table with the following data:

| ID | Name                | Price |
|----|---------------------|:-----:|
| 1  | Chicken Sandwich    | 14.99 |
| 2  | Cheeseburger        | 8.99  |
| 3  | French Fries        | 4.99  |
| 4  | Chocolate Milkshake | 3.99  |

Summary of changes:

* The price of the Chicken Sandwich was increased from $12.99 to $14.99 (must be good!)
* Cheeseburger was added back to the menu with original name and price.
* Milkshake name was updated to be "Chocolate Milkshake".

After running at `2020-01-03 11:00:00`, your final SCD Type 2 table:

| ID | Name                | Price |     Valid From      |      Valid To       |
|----|---------------------|:-----:|:-------------------:|:-------------------:|
| 1  | Chicken Sandwich    | 10.99 | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 1  | Chicken Sandwich    | 12.99 | 2020-01-02 11:00:00 | 2020-01-03 11:00:00 |
| 1  | Chicken Sandwich    | 14.99 | 2020-01-03 11:00:00 |        NULL         |
| 2  | Cheeseburger        | 8.99  | 1970-01-01 00:00:00 | 2020-01-02 11:00:00 |
| 2  | Cheeseburger        | 8.99  | 2020-01-03 11:00:00 |        NULL         |
| 3  | French Fries        | 4.99  | 1970-01-01 00:00:00 |        NULL         |
| 4  | Milkshake           | 3.99  | 2020-01-02 11:00:00 | 2020-01-03 11:00:00 |
| 4  | Chocolate Milkshake | 3.99  | 2020-01-03 11:00:00 |        NULL         |

**Notice:** `Cheeseburger` was deleted from `2020-01-02 11:00:00` to `2020-01-03 11:00:00`. Query the table for that time range, and you won't see it, which accurately reflects that it wasn't on the menu during that period.

### Shared Configuration Options

| Name                    | Description                                                                                                                                                                                                                                                                                                       | Type                      |
|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|
| unique_key              | Unique key used for identifying rows between source and target                                                                                                                                                                                                                                                    | List of strings or string |
| valid_from_name         | The name of the `valid_from` column to create in the target table. Default: `valid_from`                                                                                                                                                                                                                          | string                    |
| valid_to_name           | The name of the `valid_to` column to create in the target table. Default: `valid_to`                                                                                                                                                                                                                              | string                    |
| invalidate_hard_deletes | If set to `true`, when a record is missing from the source table it will be marked as invalid. Default: `false`                                                                                                                                                                                                   | bool                      |
| batch_size              | The maximum number of intervals that can be evaluated in a single backfill task. If this is `None`, all intervals will be processed as part of a single task. See [Processing Source Table with Historical Data](#processing-source-table-with-historical-data) for more info on this use case. (Default: `None`) | int                       |

!!! info "BigQuery Data Types"

    On BigQuery, `valid_from` and `valid_to` columns default to `DATETIME`. If you want `TIMESTAMP` instead, specify it in your model definition:

    ```sql linenums="1" hl_lines="5"
    MODEL (
      name db.menu_items,
      kind SCD_TYPE_2_BY_TIME (
        unique_key id,
        time_data_type TIMESTAMP
      )
    );
    ```

    This might work on other engines too, but it's only been tested on BigQuery.

### SCD Type 2 By Time Configuration Options

| Name                     | Description                                                                                                                                                                        | Type                      |
|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|
| updated_at_name          | The name of the column containing a timestamp to check for new or updated records. Default: `updated_at`                                                                           | string                    |
| updated_at_as_valid_from | By default, for new rows `valid_from` is set to `1970-01-01 00:00:00`. This changes the behavior to set it to the valid of `updated_at` when the row is inserted. Default: `false` | bool                      |

### SCD Type 2 By Column Configuration Options

| Name                         | Description                                                                                                                                                                                                                                                                                                                                  | Type                      |
|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|
| columns                      | The name of the columns to check for changes. `*` to represent that all columns should be checked.                                                                                                                                                                                                                                           | List of strings or string |
| execution_time_as_valid_from | By default, when the model is first loaded `valid_from` is set to `1970-01-01 00:00:00` and future new rows will have `execution_time` of when the models ran. This changes the behavior to always use `execution_time`. Default: `false`                                                                                                  | bool                      |
| updated_at_name              | If sourcing from a table that includes as timestamp to use as valid_from, set this property to that column. See [Processing Source Table with Historical Data](#processing-source-table-with-historical-data) for more info on this use case. (Default: `None`) | int                       |


### Processing Source Table with Historical Data

Most of the time, you're creating history for a table that doesn't have it. Like the restaurant menu, it just shows what's available now, but you want to track what was available over time. For this use case, leave `batch_size` as `None` (the default).

**But what if your source already has history?** Some systems create "daily snapshot" tables that contain historical records. If you're sourcing from one of these, set `batch_size` to `1` to process each interval sequentially (one day at a time if you're using `@daily` cron).

**Why sequential?** SCD Type 2 needs to compare each day's snapshot to the previous day to detect changes. Processing them in order ensures the history is captured correctly.

#### Example - Source from Daily Snapshot Table

```sql linenums="1"
MODEL (
    name db.table,
    kind SCD_TYPE_2_BY_COLUMN (
        unique_key id,
        columns [some_value],
        updated_at_name ds,
        batch_size 1
    ),
    start '2025-01-01',
    cron '@daily'
);
SELECT
    id,
    some_value,
    ds
FROM
    source_table
WHERE
    ds between @start_ds and @end_ds
```

This processes each day sequentially, checking if `some_value` changed. When a change is detected, `valid_from` is set to match the `ds` column value (except for the very first record, which gets `1970-01-01 00:00:00`).

If the source data was the following:

| id | some_value |     ds      |
|----|------------|:-----------:|
| 1  | 1          | 2025-01-01  |
| 1  | 2          | 2025-01-02  |
| 1  | 3          | 2025-01-03  |
| 1  | 3          | 2025-01-04  |

Then the resulting SCD Type 2 table would be:

| id | some_value |     ds      |     valid_from      |      valid_to       |
|----|------------|:-----------:|:-------------------:|:-------------------:|
| 1  | 1          | 2025-01-01  | 1970-01-01 00:00:00 | 2025-01-02 00:00:00 |
| 1  | 2          | 2025-01-02  | 2025-01-02 00:00:00 | 2025-01-03 00:00:00 |
| 1  | 3          | 2025-01-03  | 2025-01-03 00:00:00 |        NULL         |

### Querying SCD Type 2 Models

Even though SCD Type 2 models track history, querying the current version is still simple. Here are some common patterns:

#### Querying the Current Version

Want just the latest version of each record? Filter for `valid_to IS NULL`:

```sql linenums="1"
SELECT
  *
FROM
  menu_items
WHERE
  valid_to IS NULL;
```

You can also create a view that adds an `is_current` flag to make it even easier for downstream consumers:

```sql linenums="1"
SELECT
  *,
  valid_to IS NULL AS is_current
FROM
  menu_items;
```

#### Querying for a Specific Point in Time

Want to see what the menu looked like on a specific date? Filter by `valid_from` and `valid_to`:

```sql linenums="1"
SELECT
  *
FROM
  menu_items
WHERE
  id = 1
  AND '2020-01-02 01:00:00' >= valid_from
  AND '2020-01-02 01:00:00' < COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));
```

Here's how you'd use it in a join (to get the menu item price that was valid when an order was placed):

```sql linenums="1"
SELECT
  *
FROM
  orders
INNER JOIN
  menu_items
  ON orders.menu_item_id = menu_items.id
  AND orders.created_at >= menu_items.valid_from
  AND orders.created_at < COALESCE(menu_items.valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP));
```

You can create a view that handles the `COALESCE` automatically, making point-in-time queries even easier:

```sql linenums="1"
SELECT
  id,
  name,
  price,
  updated_at,
  valid_from,
  COALESCE(valid_to, CAST('2199-12-31 23:59:59+00:00' AS TIMESTAMP)) AS valid_to
  valid_to IS NULL AS is_current,
FROM
  menu_items;
```

Want to make `valid_to` inclusive so users can use `BETWEEN`? Adjust it like this:
```sql linenums="1"
SELECT
  id,
  name,
  price,
  updated_at,
  valid_from,
  COALESCE(valid_to, CAST('2200-01-01 00:00:00+00:00' AS TIMESTAMP)) - INTERVAL 1 SECOND AS valid_to
  valid_to IS NULL AS is_current,
```

!!! note "Timestamp Precision"

    This example uses second precision, so we subtract 1 second. Adjust the subtraction based on your timestamp precision (milliseconds, microseconds, etc.).

#### Querying for Deleted Records

To find records that were deleted, query for IDs that don't have a current version (`valid_to IS NULL`). Here's how:

```sql linenums="1"
SELECT
  id,
  MAX(CASE WHEN valid_to IS NULL THEN 0 ELSE 1 END) AS is_deleted
FROM
  menu_items
GROUP BY
  id
```

### Reset SCD Type 2 Model (Clearing History)

By default, SCD Type 2 models protect your history, once it's gone, you can't recreate it. But sometimes you need to start fresh (maybe you're fixing a bug, or the history got corrupted).

**Warning:** This will delete all historical data. Make sure you really want to do this!

To reset history, follow these steps:

```sql linenums="1" hl_lines="5"
MODEL (
  name db.menu_items,
  kind SCD_TYPE_2_BY_TIME (
    unique_key id,
    disable_restatement false
  )
);
```

Plan/apply this change to production.
Then you will want to [restate the model](../../guides/plan.md#restatement-plans).

!!! warning "Data Loss Warning"

    This will permanently remove all historical data. In most cases, you cannot recover it. Make absolutely sure this is what you want!

4. Once complete, remove `disable_restatement` from your model definition (sets it back to `true`) to prevent accidental data loss in the future

```sql linenums="1"
MODEL (
  name db.menu_items,
  kind SCD_TYPE_2_BY_TIME (
    unique_key id,
  )
);
```

5. Plan and apply this change to production

## EXTERNAL

The EXTERNAL model kind is used to specify [external models](./types/external_models.md) that store metadata about external tables. External models are special; they are not specified in .sql files like the other model kinds. They are optional but useful for propagating column and type information for external tables queried in your Vulcan project.

## MANAGED

!!! warning

    Managed models are still under development and the API / semantics may change as support for more engines is added

**Note:** Python models do not support the `MANAGED` model kind - use a SQL model instead.

The `MANAGED` model kind is used to create models where the underlying database engine manages the data lifecycle.

These models don't get updated with new intervals or refreshed when `vulcan run` is called. Responsibility for keeping the *data* up to date falls on the engine.

You can control how the engine creates the managed model by using the [`physical_properties`](./overview.md#physical_properties) to pass engine-specific parameters for adapter to use when issuing commands to the underlying database.

Due to there being no standard, each vendor has a different implementation with different semantics and different configuration parameters. Therefore, `MANAGED` models are not as portable between database engines as other Vulcan model types. In addition, due to their black-box nature, Vulcan has limited visibility into the integrity and state of the model.

We would recommend using standard Vulcan model types in the first instance. However, if you do need to use Managed models, you still gain other Vulcan benefits like the ability to use them in [virtual environments](./overview.md#build-a-virtual-environment).

See [Managed Models](./types/managed_models.md) for more information on which engines are supported and which properties are available.

## INCREMENTAL_BY_PARTITION

Models of the `INCREMENTAL_BY_PARTITION` kind are computed incrementally based on partition. A set of columns defines the model's partitioning key, and a partition is the group of rows with the same partitioning key value.

!!! question "Should you use this model kind?"

    Any model kind can use a partitioned **table** by specifying the [`partitioned_by` key](./overview.md#partitioned_by) in the `MODEL` DDL.

    The "partition" in `INCREMENTAL_BY_PARTITION` is about how the data is **loaded** when the model runs.

    `INCREMENTAL_BY_PARTITION` models are inherently [non-idempotent](../../references/glossary.md#idempotency), so restatements and other actions can cause data loss. This makes them more complex to manage than other model kinds.

    In most scenarios, an `INCREMENTAL_BY_TIME_RANGE` model can meet your needs and will be easier to manage. The `INCREMENTAL_BY_PARTITION` model kind should only be used when the data must be loaded by partition (usually for performance reasons).

This model kind is designed for the scenario where data rows should be loaded and updated as a group based on their shared value for the partitioning key.

It may be used with any SQL engine. Vulcan will automatically create partitioned tables on engines that support explicit table partitioning (e.g., [BigQuery](https://cloud.google.com/bigquery/docs/creating-partitioned-tables), [Databricks](https://docs.databricks.com/en/sql/language-manual/sql-ref-partition.html)).

New rows are loaded based on their partitioning key value:

- If a partitioning key in newly loaded data is not present in the model table, the new partitioning key and its data rows are inserted.

- If a partitioning key in newly loaded data is already present in the model table, **all the partitioning key's existing data rows in the model table are replaced** with the partitioning key's data rows in the newly loaded data.

- If a partitioning key is present in the model table but not present in the newly loaded data, the partitioning key's existing data rows are not modified and remain in the model table.

This kind should only be used for datasets that have the following traits:

* The dataset's records can be grouped by a partitioning key.
* Each record has a partitioning key associated with it.
* It is appropriate to upsert records, so existing records can be overwritten by new arrivals when their partitioning keys match.
* All existing records associated with a given partitioning key can be removed or overwritten when any new record has the partitioning key value.

The column defining the partitioning key is specified in the model's `MODEL` DDL `partitioned_by` key. This example shows the `MODEL` DDL for an `INCREMENTAL_BY_PARTITION` model:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name vulcan_demo.partition,
      kind INCREMENTAL_BY_PARTITION,
      partitioned_by ARRAY[warehouse_id, category],
      grains (partitioned_analysis_key)
    );

    SELECT
      w.warehouse_id,
      w.name AS warehouse_name,
      p.category,
      o.order_date,
      CONCAT(w.warehouse_id::TEXT, '_', p.category, '_', o.order_date::TEXT) AS partitioned_analysis_key,
      COUNT(DISTINCT o.order_id) AS total_transactions,
      SUM(oi.quantity * oi.unit_price) AS total_sales_amount,
      COUNT(DISTINCT o.customer_id) AS unique_customers
    FROM vulcan_demo.orders AS o
    JOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id
    JOIN vulcan_demo.products AS p ON oi.product_id = p.product_id
    JOIN vulcan_demo.warehouses AS w ON o.warehouse_id = w.warehouse_id
    GROUP BY w.warehouse_id, w.name, p.category, o.order_date
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "vulcan_demo.partition_py",
        columns={
            "warehouse_id": "int",
            "order_date": "date",
            "daily_revenue": "decimal(10,2)",
        },
        partitioned_by=["warehouse_id"],
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_PARTITION,
        ),
        grains=["warehouse_id", "order_date"],
        depends_on=["vulcan_demo.orders", "vulcan_demo.order_items"],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT o.warehouse_id, o.order_date,
               SUM(oi.quantity * oi.unit_price) as daily_revenue
        FROM vulcan_demo.orders o
        JOIN vulcan_demo.order_items oi ON o.order_id = oi.order_id
        GROUP BY o.warehouse_id, o.order_date
        """
        return context.fetchdf(query)
    ```

You can use multiple columns for composite partition keys:

```sql linenums="1"
MODEL (
  name vulcan_demo.events,
  kind INCREMENTAL_BY_PARTITION,
  partitioned_by (warehouse_id, category)
);
```

Some engines support expression-based partitioning. Here's a BigQuery example that partitions by month:

```sql linenums="1"
MODEL (
  name vulcan_demo.events,
  kind INCREMENTAL_BY_PARTITION,
  partitioned_by DATETIME_TRUNC(order_date, MONTH)
);
```

!!! warning "Only Full Restatements Supported"

    Partial data [restatements](../../guides/plan.md#restatement-plans) are used to reprocess part of a table's data (usually a limited time range).

    Partial data restatement is not supported for `INCREMENTAL_BY_PARTITION` models. If you restate an `INCREMENTAL_BY_PARTITION` model, its entire table will be recreated from scratch.

    Restating `INCREMENTAL_BY_PARTITION` models may lead to data loss and should be performed with care.

### Example

Here's a practical example that shows how to limit which partitions get updated using a CTE. This is a common pattern to avoid full restatements:

```sql linenums="1"
MODEL (
  name demo.incremental_by_partition_demo,
  kind INCREMENTAL_BY_PARTITION,
  partitioned_by user_segment,
);

-- This is the source of truth for what partitions need to be updated and will join to the product usage data

-- This could be an INCREMENTAL_BY_TIME_RANGE model that reads in the user_segment values last updated in the past 30 days to reduce scope

-- Use this strategy to reduce full restatements
WITH partitions_to_update AS (
  SELECT DISTINCT
    user_segment
  FROM demo.incremental_by_time_range_demo  -- upstream table tracking which user segments to update
  WHERE last_updated_at BETWEEN DATE_SUB(@start_dt, INTERVAL 30 DAY) AND @end_dt
),

product_usage AS (
  SELECT
    product_id,
    customer_id,
    last_usage_date,
    usage_count,
    feature_utilization_score,
    user_segment
  FROM vulcan-public-demo.tcloud_raw_data.product_usage
  WHERE user_segment IN (SELECT user_segment FROM partitions_to_update) -- partition filter applied here
)

SELECT
  product_id,
  customer_id,
  last_usage_date,
  usage_count,
  feature_utilization_score,
  user_segment,
  CASE
    WHEN usage_count > 100 AND feature_utilization_score > 0.7 THEN 'Power User'
    WHEN usage_count > 50 THEN 'Regular User'
    WHEN usage_count IS NULL THEN 'New User'
    ELSE 'Light User'
  END as user_type
FROM product_usage
```

**Note**: Partial data [restatement](../../guides/plan.md#restatement-plans) is not supported for this model kind, which means that the entire table will be recreated from scratch if restated. This may lead to data loss.

### Materialization strategy
Depending on the target engine, models of the `INCREMENTAL_BY_PARTITION` kind are materialized using the following strategies:

| Engine     | Strategy                                |
|------------|-----------------------------------------|
| Databricks | REPLACE WHERE by partitioning key       |
| Spark      | INSERT OVERWRITE by partitioning key    |
| Snowflake  | DELETE by partitioning key, then INSERT |
| BigQuery   | DELETE by partitioning key, then INSERT |
| Redshift   | DELETE by partitioning key, then INSERT |
| Postgres   | DELETE by partitioning key, then INSERT |
| DuckDB     | DELETE by partitioning key, then INSERT |

## INCREMENTAL_UNMANAGED

`INCREMENTAL_UNMANAGED` models are for append-only tables. They're "unmanaged" because Vulcan doesn't try to deduplicate or manage the data, it just runs your query and appends whatever it gets to the table.

**How it works:** Every time the model runs, Vulcan executes your query and appends the results to the table. No deduplication, no updates, no deletes, just append, append, append.

!!! question "Should You Use This?"

    **Use it for:** Data Vault patterns, event logs, audit trails, or any scenario where you need true append-only behavior.

    **Don't use it for:** Most other cases. `INCREMENTAL_BY_TIME_RANGE` or `INCREMENTAL_BY_UNIQUE_KEY` give you much more control and are usually better choices.

**When to use:**

- Data Vault hubs, links, or satellites

- Event logs where every event should be preserved

- Audit trails

- Any pattern that requires true append-only semantics

Here's how you'd set one up:

```sql linenums="1"
MODEL (
  name vulcan_demo.incremental_unmanaged,
  kind INCREMENTAL_UNMANAGED,
  cron '@daily',
  start '2025-01-01',
  grains (shipment_id)
);

/* Append-only shipment event log */
SELECT
  s.shipment_id,
  s.order_id,
  s.shipped_date,
  s.carrier,
  o.customer_id,
  c.name AS customer_name,
  o.order_date,
  (s.shipped_date - o.order_date::DATE)::INT AS days_to_ship,
  CURRENT_TIMESTAMP AS shipment_event_timestamp
FROM vulcan_demo.shipments AS s
JOIN vulcan_demo.orders AS o ON s.order_id = o.order_id
JOIN vulcan_demo.customers AS c ON o.customer_id = c.customer_id
ORDER BY s.shipped_date DESC
```

**Note:** Since it's unmanaged, `INCREMENTAL_UNMANAGED` doesn't support `batch_size` or `batch_concurrency` properties. Vulcan just runs your query and appends the results, no batching or concurrency control.

!!! warning "Only Full Restatements Supported"

    Similar to `INCREMENTAL_BY_PARTITION`, attempting to [restate](../../guides/plan.md#restatement-plans) an `INCREMENTAL_UNMANAGED` model will trigger a full restatement. That is, the model will be rebuilt from scratch rather than from a time slice you specify.

    Be very careful when restating these models!



# Overview

Source: https://tmdc-io.github.io/vulcan-book/components/model/overview/

---

# Overview

Models transform raw data into tables and views. Define what you want (the metadata) and how to make it (the SQL query), and Vulcan handles the rest.

Models live in `.sql` and `.py` files in the `models/` directory of your project. Vulcan automatically figures out how your models relate to each other by parsing your SQL, so you don't have to manually configure dependencies. Write your SQL, and Vulcan handles the lineage.

Every model has two parts:

- **DDL (Data Definition Language)** - The `MODEL` block that tells Vulcan what this model is (name, schedule, how to materialize it, etc.)

- **DML (Data Manipulation Language)** - The `SELECT` query that does the actual transformation work

The DDL defines the model metadata. The DML contains the transformation logic.

## Model Structure

You can write models in SQL or Python. Both work the same way conceptually, they just look different. Let's see both:

=== "SQL Model"

    ```sql linenums="1"
    MODEL (
      name sales.daily_sales,
      kind FULL,
      cron '@daily',
      grain order_date
    );

    SELECT
      CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
      COUNT(order_id)::INTEGER AS total_orders,
      SUM(total_amount)::FLOAT AS total_revenue,
      MAX(order_id)::VARCHAR AS last_order_id
    FROM raw.raw_orders
    GROUP BY order_date
    ORDER BY order_date
    ```

    **Breaking it down:**

    - **Lines 1-6**: The DDL (`MODEL` block) - tells Vulcan this is a daily sales model that runs every day

    - **Lines 8-17**: The DML (`SELECT` query) - the actual transformation that aggregates orders by date

=== "Python Model"

    ```python linenums="1"
    import typing as t
    import pandas as pd
    from datetime import datetime
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
      "sales.daily_sales_py",
      columns={
        "order_date": "timestamp",
        "total_orders": "int",
        "total_revenue": "decimal(18,2)",
        "last_order_id": "string",
      },
      kind=dict(name=ModelKindName.FULL),
      grains=["order_date"],
      depends_on=["raw.raw_orders"],
      cron='@daily',
    )
    def execute(
      context: ExecutionContext,
      start: datetime,
      end: datetime,
      execution_time: datetime,
      **kwargs: t.Any,
    ) -> pd.DataFrame:

      query = """
      SELECT
        CAST(order_date AS TIMESTAMP) AS order_date,
        COUNT(order_id)::INTEGER AS total_orders,
        SUM(total_amount)::NUMERIC(18,2) AS total_revenue,
        MAX(order_id)::VARCHAR AS last_order_id
      FROM raw.raw_orders
      GROUP BY order_date
      ORDER BY order_date
      """

      return context.fetchdf(query)
    ```

    **Breaking it down:**

    - **Lines 7-20**: The DDL (`@model` decorator) - same metadata as SQL, just Python syntax

    - **Lines 21-40**: The DML (function body) - runs the SQL and returns a DataFrame

Both formats do the same thing. Choose the one you prefer.

## DDL: The MODEL Block

The `MODEL` block is where you tell Vulcan about your model. It's the first thing in your file (after any comments) and uses a simple, declarative syntax.

### Basic Syntax

Here's what a `MODEL` block looks like:

```sql linenums="1"
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date
);
```

This tells Vulcan:

- **`name`** - What to call this model (schema.table format)

- **`kind`** - How to materialize it (FULL rebuilds everything, INCREMENTAL only updates changes, etc.)

- **`cron`** - When to run it (`@daily` means every day)

- **`grain`** - What makes each row unique (in this case, `order_date`)

### Common Properties

Here are the properties you'll use most often:

| Property | What It Does | Example |
| -------- | ------------ | ------- |
| `name` | Fully qualified model name (schema.table) | `sales.daily_sales` |
| `kind` | Materialization strategy | `FULL`, `INCREMENTAL`, `VIEW` |
| `cron` | When to run (scheduling) | `'@daily'`, `'0 0 * * *'` |
| `grain` | Column(s) that make rows unique | `order_date` or `(customer_id, order_date)` |
| `owner` | Who owns this model (for governance) | `analytics_team` |
| `description` | Human-readable description | `'Daily sales aggregates'` |

!!! info "More DDL Properties"
    There are more properties available beyond these common ones. Check out the [Model Properties](./properties.md) reference for the complete list of all available model properties and their configurations.

## DML: The SELECT Query

The `SELECT` query is where the magic happens. This is your transformation logic, the SQL that actually does the work.

```sql linenums="1"
SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

This query:

- Reads from `raw.raw_orders`

- Groups by `order_date`

- Counts orders, sums revenue, finds the latest order ID

- Returns the results ordered by date

Pretty standard SQL! Vulcan will automatically figure out that this model depends on `raw.raw_orders` and build the dependency graph for you.

## Conventions

Vulcan tries to be smart and infer as much as possible from your SQL. This means you don't have to write a bunch of YAML config files, just write SQL and Vulcan figures it out. But to do this, your SQL needs to follow some conventions.

### SQL Model Conventions

#### Unique Column Names

Your final `SELECT` needs unique column names. No duplicates allowed!

```sql linenums="1"
-- Good: Each column has a unique name
SELECT
  order_date::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue
FROM raw.raw_orders
GROUP BY order_date
```

If you have duplicate column names, Vulcan won't know which one you mean, and that causes problems.

#### Explicit Types

Cast your types explicitly. This prevents surprises and ensures your schema is consistent:

```sql linenums="1"
-- Explicit casting ensures consistent schema
SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,  -- explicit timestamp
  COUNT(order_id)::INTEGER AS total_orders,                 -- explicit integer
  SUM(total_amount)::FLOAT AS total_revenue,                -- explicit float
  MAX(order_id)::VARCHAR AS last_order_id                   -- explicit varchar
FROM raw.raw_orders
GROUP BY order_date
```

Vulcan uses PostgreSQL-style casting (`x::int`), but don't worry, it automatically converts this to whatever your execution engine needs. So you write `::INTEGER` and Vulcan handles the rest.

**Why this matters:** Without explicit types, you might get `FLOAT` when you expected `INTEGER`, or `VARCHAR` when you wanted `TIMESTAMP`. Explicit casting prevents these surprises.

#### Inferrable Names

Your columns need names that Vulcan can figure out. If Vulcan can't infer a name, you need to add an alias:

```sql linenums="1"
SELECT
  1,                              -- not inferrable (what do you call this?)
  total_amount + 1,               -- not inferrable (needs an alias)
  SUM(total_amount),              -- not inferrable (needs an alias)
  order_date,                     -- inferrable as order_date
  order_date::TIMESTAMP,          -- inferrable as order_date
  total_amount + 1 AS adjusted,   -- explicitly named
  SUM(total_amount) AS revenue    -- explicitly named
```

If you forget an alias, Vulcan's formatter will add one automatically when it renders your SQL. But it's better to be explicit, you'll know what the column is called!

#### Column Descriptions

Document your columns! There are two ways to do this:

**Option 1: In the DDL (Recommended)**

```sql linenums="1" hl_lines="7-12"
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date,
  description 'Aggregated daily sales metrics',
  column_descriptions (
    order_date = 'The date of the sales transactions',
    total_orders = 'Count of orders placed on this date',
    total_revenue = 'Sum of all order amounts for this date',
    last_order_id = 'The most recent order ID for this date'
  )
);
```

This keeps all your documentation in one place, in the MODEL block.

!!! note "Priority"
    If you use `column_descriptions` in the DDL, Vulcan will use those and ignore any inline comments in your query. DDL descriptions take priority, so if you define descriptions in both places, the DDL version wins.

**Option 2: Inline Comments**

If you don't specify `column_descriptions` in the DDL, Vulcan will automatically pick up comments from your query:

```sql linenums="1" hl_lines="9-12"
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date
);

SELECT
  order_date::TIMESTAMP AS order_date,           -- The date of sales transactions
  COUNT(order_id)::INTEGER AS total_orders,      -- Number of orders placed
  SUM(total_amount)::FLOAT AS total_revenue,     -- Total revenue for the day
  MAX(order_id)::VARCHAR AS last_order_id        -- Most recent order ID
FROM raw.raw_orders
GROUP BY order_date
```

Vulcan registers these comments as column descriptions in your database.

**Table comments:** If you put a comment before the `MODEL` block, Vulcan will use it as the table description. But if you also specify `description` in the MODEL block, that takes priority.

### Python Model Conventions

Python models work a bit differently because Python doesn't have the same type inference capabilities as SQL.

#### Explicit Column Definitions

You **must** define your columns explicitly in the `@model` decorator:

```python linenums="1" hl_lines="3-8"
@model(
    "sales.daily_sales_py",
    columns={
        "order_date": "timestamp",
        "total_orders": "int",
        "total_revenue": "decimal(18,2)",
        "last_order_id": "string",
    },
    kind=dict(name=ModelKindName.FULL),
)
```

Vulcan can't infer types from Python code the way it can from SQL, so you need to tell it explicitly.

#### Explicit Dependencies

Unlike SQL models (where Vulcan figures out dependencies automatically), Python models need you to list them:

```python linenums="1" hl_lines="4"
@model(
    "sales.daily_sales_py",
    columns={...},
    depends_on=["raw.raw_orders"],  # Must explicitly list upstream models
)
```

This is because Vulcan can't parse your Python code to find `FROM` clauses and joins. You need to tell it what this model depends on.

#### Column Descriptions

Python models can't use inline comments for column descriptions. Instead, specify them in the decorator:

```python linenums="1" hl_lines="8-13"
@model(
    "sales.daily_sales_py",
    columns={
        "order_date": "timestamp",
        "total_orders": "int",
        "total_revenue": "decimal(18,2)",
    },
    column_descriptions={
        "order_date": "The date of sales transactions",
        "total_orders": "Number of orders placed on this date",
        "total_revenue": "Total revenue for the day",
    },
)
```

!!! warning "Column name validation"
    Vulcan will error if you put a column name in `column_descriptions` that doesn't exist in `columns`. This prevents typos and keeps things consistent, if you describe a column, it better exist!

#### Return Type

Your `execute` function must return a pandas DataFrame, and the columns must match what you defined in `columns`:

```python linenums="1"
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:  # Must return a DataFrame
    query = "SELECT ..."
    return context.fetchdf(query)
```

The DataFrame columns need to match your `columns` definition exactly, same names, compatible types.

!!! info "Learn more"
    See [Python Models](./types/python_models.md) for detailed information, advanced patterns, and more examples.

## Comment Registration

Vulcan registers comments (descriptions) in your database so they show up in your BI tools and data catalogs.

### How Comments Get Registered

**Model-level comments:**

- If you put a comment before the `MODEL` block, Vulcan uses it as the table comment

- If you also specify `description` in the MODEL block, that takes priority

**Column-level comments:**

- Use `column_descriptions` in the DDL (recommended)

- Or use inline comments in your SELECT query (if `column_descriptions` isn't specified)

### What Gets Registered

Not everything gets comments registered:

- **Physical tables** - Comments are registered (tables in the `vulcan__[project schema]` schema)

- **Production views** - Comments are registered

- **Temporary tables** - No comments (they're temporary)

- **Non-production views** - No comments (keeps things clean)

**Note:** Some engines automatically pass comments from physical tables to views that select from them. So even if Vulcan didn't explicitly register a comment on a view, it might still show up if the engine does this automatically.

### Engine Support

Different databases support comments differently. Some can register comments in the `CREATE` statement (one command), others need separate commands for each comment.

Here's what each engine supports:

| Engine        | `TABLE` comments | `VIEW` comments |
| ------------- | ---------------- | --------------- |
| Postgres      | Y                | Y               |
| Snowflake     | Y                | Y               |
| Spark         | Y                | Y               |

If your engine doesn't support comments, Vulcan will skip registration (no errors, it just won't register them).

## Macros

Macros are like variables for your SQL. They let you parameterize queries and avoid repetition. Vulcan provides several built-in macros (like `@start_ds` and `@end_ds` for incremental models), and you can define your own.

Macros use the `@` prefix. For example, `@this_model` refers to the current model being processed, and `@start_ds` is the start date for incremental processing.

See the [macros documentation](../advanced-features/macros/overview.md) for details.



# Properties

Source: https://tmdc-io.github.io/vulcan-book/components/model/properties/

---

# Properties

The `MODEL` DDL statement has properties you can use to control how your model behaves. Configure scheduling, storage, validation, and more.

This page is a complete reference for all available properties. It covers what each one does, when to use it, and shows examples.

---

## Quick Reference

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| `name` | Fully qualified model name (`schema.model` or `catalog.schema.model`) | `str` | N* |
| `project` | Project name for multi-repo deployments | `str` | N |
| `kind` | Model kind (VIEW, FULL, INCREMENTAL, etc.) | `str` \| `dict` | N |
| `cron` | Schedule expression for model refresh | `str` | N |
| `cron_tz` | Timezone for the cron schedule | `str` | N |
| `interval_unit` | Temporal granularity of data intervals | `str` | N |
| `start` | Earliest date/time to process | `str` \| `int` | N |
| `end` | Latest date/time to process | `str` \| `int` | N |
| `grain` | Column(s) defining row uniqueness | `str` \| `array` | N |
| `grains` | Multiple unique key definitions | `array` | N |
| `owner` | Model owner for governance | `str` | N |
| `description` | Model description (registered as table comment) | `str` | N |
| `column_descriptions` | Column-level comments | `dict` | N |
| `columns` | Explicit column names and types | `array` | N |
| `dialect` | SQL dialect of the model | `str` | N |
| `tags` | Labels for organizing models | `array[str]` | N |
| `assertions`  | Audits to run after model evaluation | `array` | N |
| `profiles` | Columns to track statistical metrics over time | `array` | N |
| `depends_on` | Explicit model dependencies | `array[str]` | N |
| `references` | Non-unique join relationship columns | `array` | N |
| `partitioned_by` | Partition key column(s) | `str` \| `array` | N |
| `clustered_by` | Clustering column(s) | `str` | N |
| `table_format` | Table format (iceberg, hive, delta) | `str` | N |
| `storage_format` | Storage format (parquet, orc) | `str` | N |
| `physical_properties` | Engine-specific table/view properties | `dict` | N |
| `virtual_properties` | Engine-specific view layer properties | `dict` | N |
| `session_properties` | Engine session properties | `dict` | N |
| `stamp` | Arbitrary version string | `str` | N |
| `enabled` | Whether model is enabled | `bool` | N |
| `allow_partials` | Allow partial data intervals | `bool` | N |
| `gateway` | Specific gateway for execution | `str` | N |
| `optimize_query` | Enable query optimization | `bool` | N |
| `formatting` | Enable model formatting | `bool` | N |
| `ignored_rules` | Linter rules to ignore | `str` \| `array` | N |

**Note:** Required unless [name inference](#model-naming) is enabled.

---

## General Properties

### name

Your model's name is how it's identified in the data warehouse. It needs at least a schema (`schema.model`), and you can optionally include a catalog (`catalog.schema.model`).

**Format:** `schema.model` or `catalog.schema.model`

This becomes the production table/view name that other models and users will reference.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,        -- schema.model format
    );
    
    -- Or with catalog
    MODEL (
      name catalog.sales.daily_sales -- catalog.schema.model format
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",  # schema.model format
    )
    def execute(context, **kwargs):
        ...
    
    # Or with catalog
    @model(
        "catalog.sales.daily_sales",  # catalog.schema.model format
    )
    ```

!!! note "Environment Prefixing"

    In non-production environments, Vulcan automatically prefixes your model names. So `sales.daily_sales` becomes `sales__dev.daily_sales` in the dev environment. This keeps your dev and prod data separate without you having to think about it.

### project

If you're running multiple Vulcan projects in the same repository (multi-repo setup), use `project` to specify which project this model belongs to. This helps Vulcan organize and isolate models from different projects.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      project 'analytics_project',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        project="analytics_project",
    )
    ```

### kind

The `kind` property determines how your model is computed and stored. Do you want to rebuild everything each run? Update incrementally? Create a view? This is where you decide.

For all the details on each kind and when to use them, check out the [Model Kinds](model_kinds.md) documentation.

=== "SQL"

    ```sql
    -- VIEW (default for SQL)
    MODEL (
      name sales.daily_sales,
      kind VIEW,
    );
    
    -- FULL
    MODEL (
      name sales.daily_sales,
      kind FULL,
    );
    
    -- Incremental with properties
    MODEL (
      name sales.events,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column event_ts,
      ),
    );
    
    -- SEED
    MODEL (
      name raw.holidays,
      kind SEED (
        path 'seeds/holidays.csv',
      ),
    );
    ```

=== "Python"

    ```python
    from vulcan import ModelKindName
    
    # FULL (default for Python)
    @model(
        "sales.daily_sales",
        kind=dict(name=ModelKindName.FULL),
    )
    
    # Incremental
    @model(
        "sales.events",
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="event_ts",
        ),
    )
    
    # SCD Type 2
    @model(
        "dim.customers",
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_TIME,
            unique_key=["customer_id"],
        ),
    )
    ```

### cron

Controls when your model runs. You can use standard cron expressions or Vulcan's shortcuts for common schedules.

**Why this matters:** Without a schedule, your model only runs when you manually trigger it. Set a cron, and Vulcan will automatically process new data on schedule.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      cron '@daily',          -- Daily at midnight UTC
    );
    
    MODEL (
      name sales.hourly_metrics,
      cron '@hourly',         -- Every hour
    );
    
    MODEL (
      name sales.custom_schedule,
      cron '0 6 * * *',       -- Custom: every day at 6 AM UTC
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        cron="@daily",
    )
    
    @model(
        "sales.hourly_metrics",
        cron="@hourly",
    )
    
    @model(
        "sales.custom_schedule",
        cron="0 6 * * *",  # Every day at 6 AM UTC
    )
    ```

**Cron shortcuts:** Vulcan provides convenient shortcuts:

- `@hourly` - Every hour

- `@daily` - Every day at midnight UTC

- `@weekly` - Once per week

- `@monthly` - Once per month

These are much easier than writing `0 * * * *`!

### cron_tz

Sets the timezone for your cron schedule. This only affects **when** the model runs, not how time intervals are calculated (those are always UTC).

**Example:** If you set `cron '@daily'` and `cron_tz 'America/Los_Angeles'`, your model runs at midnight Pacific time, but the time intervals it processes are still in UTC.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      cron '@daily',
      cron_tz 'America/Los_Angeles',  -- Runs at midnight Pacific time
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        cron="@daily",
        cron_tz="America/Los_Angeles",
    )
    ```

### interval_unit

Controls the granularity of time intervals for incremental models. By default, Vulcan figures this out from your `cron` expression, but you can override it if needed.

**Supported values:** `year`, `month`, `day`, `hour`, `half_hour`, `quarter_hour`, `five_minute`

**When to override:** If your cron runs daily but you want to process hourly intervals, set `interval_unit 'hour'`. This is useful when you want finer-grained control over incremental processing.

=== "SQL"

    ```sql
    MODEL (
      name sales.hourly_metrics,
      cron '30 7 * * *',      -- Run daily at 7:30 AM
      interval_unit 'hour',   -- Process hourly intervals (not daily)
      );
    ```

=== "Python"

    ```python
    from vulcan import IntervalUnit
    
    @model(
        "sales.hourly_metrics",
        cron="30 7 * * *",
        interval_unit=IntervalUnit.HOUR,
    )
    ```

### start

Sets the earliest date/time your model should process. This is useful for limiting backfills or defining when your model's data begins.

You can use:

- **Absolute dates:** `'2024-01-01'`

- **Relative expressions:** `'1 year ago'`

- **Epoch milliseconds:** `1704067200000`

=== "SQL"

    ```sql
    -- Absolute date
    MODEL (
      name sales.daily_sales,
      start '2024-01-01',
    );
    
    -- Relative expression
    MODEL (
      name sales.recent_sales,
      start '1 year ago',
    );
    
    -- Epoch milliseconds
    MODEL (
      name sales.events,
      start 1704067200000,
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        start="2024-01-01",
    )
    
    @model(
        "sales.recent_sales",
        start="1 year ago",
    )
    ```

### end

Sets the latest date/time your model should process. Uses the same format as `start`. This is handy for historical models or limiting processing to a specific time range.

=== "SQL"

    ```sql
    MODEL (
      name sales.historical_sales,
      start '2020-01-01',
      end '2023-12-31',
      );
    ```

=== "Python"

    ```python
    @model(
        "sales.historical_sales",
        start="2020-01-01",
        end="2023-12-31",
    )
    ```

### grain / grains

Defines the column(s) that make each row unique. This is like a primary key, it tells Vulcan what identifies a single row in your table.

**Why this matters:** Tools like `table_diff` use grains to compare tables. It also helps Vulcan understand your data structure for better optimization and validation.

You can specify a single grain (`grain order_id`) or multiple grains (`grains (order_id, (customer_id, order_date))`).

=== "SQL"

    ```sql
    -- Single column grain
    MODEL (
      name sales.daily_sales,
      grain order_date,
    );
    
    -- Composite grain
    MODEL (
      name sales.customer_daily,
      grain (customer_id, order_date),
    );
    
    -- Multiple grains
    MODEL (
      name sales.orders,
      grains (
        order_id,
        (customer_id, order_date)
      ),
    );
    ```

=== "Python"

    ```python
    # Single grain
    @model(
        "sales.daily_sales",
        grains=["order_date"],
    )
    
    # Composite grain
    @model(
        "sales.customer_daily",
        grains=[("customer_id", "order_date")],
    )
    
    # Multiple grains
    @model(
        "sales.orders",
        grains=[
            "order_id",
            ("customer_id", "order_date"),
        ],
    )
    ```

### owner

Sets the owner of the model, usually a team name or individual. This is used for governance, notifications, and knowing who to contact when something breaks.

**Example:** `owner 'analytics_team'` or `owner 'data_engineers'`

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      owner 'analytics_team',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        owner="analytics_team",
    )
    ```

### description

A human-readable description of what your model does. Vulcan automatically registers this as a table comment in your SQL engine (if it supports comments), so it shows up in your BI tools and data catalogs.

**Pro tip:** Write descriptions that explain the business purpose, not just the technical details. Future you (and your teammates) will thank you!

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      description 'Aggregated daily sales metrics including total orders and revenue',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        description="Aggregated daily sales metrics including total orders and revenue",
    )
    ```

### column_descriptions

Document your columns! This property lets you add descriptions for each column, which get registered as column comments in your database.

**Why document columns?** When someone queries your table in a BI tool, they'll see what each column means. It's like inline documentation that travels with your data.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      column_descriptions (
        order_date = 'The date of sales transactions',
        total_orders = 'Count of orders placed on this date',
        total_revenue = 'Sum of all order amounts',
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        columns={
            "order_date": "timestamp",
            "total_orders": "int",
            "total_revenue": "decimal(18,2)",
        },
        column_descriptions={
            "order_date": "The date of sales transactions",
            "total_orders": "Count of orders placed on this date",
            "total_revenue": "Sum of all order amounts",
        },
    )
    ```

!!! warning "Priority"
    If `column_descriptions` is present, [inline column comments](./overview.md#inline-column-comments) will not be registered.

### columns

Explicitly defines your model's column names and data types. When you use this, Vulcan won't try to infer types from your query, it'll use exactly what you specify.

**When to use:**

- Python models (required, Vulcan can't infer types from Python code)

- Seed models (you need to define the CSV schema)

- When you want strict type control

=== "SQL"

    ```sql
    MODEL (
      name sales.national_holidays,
      kind SEED (path 'holidays.csv'),
      columns (
        holiday_name VARCHAR,
        holiday_date DATE
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        columns={
            "order_date": "timestamp",
            "total_orders": "int",
            "total_revenue": "decimal(18,2)",
            "last_order_id": "string",
        },
    )
    def execute(context, **kwargs) -> pd.DataFrame:
        ...
    ```

!!! note "Python Models"
This is required for [Python models](../model/types/python_models.md) since Vulcan can't infer column types from Python code. You must explicitly define your schema.

### dialect

Specifies the SQL dialect your model uses. Defaults to whatever you set in `model_defaults`.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      dialect 'snowflake',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        dialect="snowflake",
    )
    ```

### tags

Labels for organizing and filtering models.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      tags ['sales', 'daily', 'core'],
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        tags=["sales", "daily", "core"],
    )
    ```

### assertions

Attach [assertions](../audits/audits.md) directly to your model. These validations run after each model evaluation and will block the models if they fail.

**Why use assertions?** They're your safety net, they catch bad data before it flows downstream. If revenue can't be negative, assert it. If customer IDs must be unique, assert it. Fail fast, fix fast.

Think of assertions as "this data must be true" validations that run automatically.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      assertions (
        not_null(columns := (order_date, customer_id)),
        unique_values(columns := (order_id)),
        accepted_range(column := price, min_v := 0, max_v := 1000),
        forall(criteria := (price > 0, quantity >= 1))
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        assertions=[
            ("not_null", {"columns": ["order_date", "customer_id"]}),
            ("unique_values", {"columns": ["order_id"]}),
            ("accepted_range", {"column": "price", "min_v": 0, "max_v": 1000}),
        ],
    )
    ```

### profiles

Enable automatic data profiling for specific columns. Profiles track statistical metrics over time (like null percentages, distinct counts, distributions) without blocking your models.

**How it works:** Vulcan collects metrics each run and stores them in the `_check_profiles` table. You can query this to see how your data changes over time, detect data drift, understand patterns, and decide which checks or audits to add.

**Use cases:**

- Track null percentages over time

- Monitor distinct value counts

- Detect data drift

- Understand column distributions

- Inform which checks/audits to create

Think of profiles as your data observability layer, they watch and learn, but don't block.
=== "SQL"

    ```sql
    MODEL (
      name vulcan_demo.full_model,
      kind FULL,
      grains (customer_id),
      profiles (customer_id, customer_name, email, total_orders, total_spent, avg_order_value)
    );

    SELECT
      c.customer_id,
      c.name AS customer_name,
      c.email,
      COUNT(DISTINCT o.order_id) AS total_orders,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent,
      COALESCE(SUM(oi.quantity * oi.unit_price), 0) / NULLIF(COUNT(DISTINCT o.order_id), 0) AS avg_order_value
    FROM vulcan_demo.customers AS c
    LEFT JOIN vulcan_demo.orders AS o ON c.customer_id = o.customer_id
    LEFT JOIN vulcan_demo.order_items AS oi ON o.order_id = oi.order_id
    GROUP BY c.customer_id, c.name, c.email
    ```

=== "Python"

    ```python
    @model(
        "vulcan_demo.full_model_py",
        columns={
            "customer_id": "int",
            "customer_name": "string",
            "email": "string",
            "total_orders": "int",
            "total_spent": "decimal(10,2)",
            "avg_order_value": "decimal(10,2)",
        },
        kind="full",
        grains=["customer_id"],
        profiles=["customer_id", "customer_name", "email", "total_orders", "total_spent", "avg_order_value"],
    )
    def execute(context, **kwargs):
        ...
    ```


### depends_on

Explicitly declare model dependencies. Vulcan automatically infers dependencies from SQL queries, but sometimes you need to add extra ones.

**When to use:**

- Python models (required, Vulcan can't parse Python to find dependencies)

- Hidden dependencies (like a macro that references another model)

- External dependencies that aren't in your SQL

**Note:** Dependencies you declare here are added to the ones Vulcan infers, they don't replace them.

=== "SQL"

    ```sql
    MODEL (
      name sales.summary,
      depends_on ['sales.daily_sales', 'sales.products'],
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.summary",
        depends_on=["sales.daily_sales", "sales.products"],
    )
    ```

!!! note "Python Models"

    Python models **require** `depends_on` since Vulcan can't automatically infer dependencies from Python code. You need to tell it explicitly what your model depends on.

### references

Declare non-unique join relationships to other models. These help Vulcan understand how models relate to each other for better lineage and optimization.

**Example:** If your `orders` table has a `customer_id` that joins to `customers.customer_id`, you'd add `customer_id` to references. This tells Vulcan about the relationship even though `customer_id` isn't unique in the orders table.

=== "SQL"

    ```sql
    MODEL (
      name sales.orders,
      references (
        customer_id,
        guest_id AS account_id,  -- Alias for joining to account_id grain
      ),
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.orders",
        references=[
            "customer_id",
            ("guest_id", "account_id"),  # Alias
        ],
    )
    ```

---

## Storage Properties

These properties control how your data is physically stored in the database. They're engine-specific, so check your engine's documentation for what's supported.

### partitioned_by

Defines the partition key for your table. Partitioning splits your table into chunks based on column values, which makes queries faster (the engine can skip irrelevant partitions).

**Supported engines:** Spark, BigQuery, Databricks, and others that support table partitioning.

**Why partition?** If you're querying data from the last 7 days and your table is partitioned by date, the engine only scans 7 partitions instead of scanning the entire table. That's a huge performance win!

=== "SQL"

    ```sql
    -- Single column partition
    MODEL (
      name sales.events,
      partitioned_by event_date,
    );
    
    -- Partition with transformation (BigQuery)
    MODEL (
      name sales.events,
      partitioned_by TIMESTAMP_TRUNC(event_ts, DAY),
    );
    
    -- Multi-column partition
    MODEL (
      name sales.events,
      partitioned_by (year, month, day),
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        partitioned_by=["event_date"],
    )
    
    # Multi-column
    @model(
        "sales.events",
        partitioned_by=["year", "month", "day"],
    )
    ```

### clustered_by

Sets clustering columns for engines that support it (like BigQuery). Clustering organizes data within partitions based on column values, which makes range queries and filters faster.

**How it works:** Data is physically stored sorted by the clustering columns. When you filter on those columns, the engine can skip reading irrelevant data blocks.

**Example:** If you cluster by `customer_id`, queries filtering by customer will be faster because related data is stored together.

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      partitioned_by event_date,
      clustered_by (customer_id, product_id),
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        partitioned_by=["event_date"],
        clustered_by=["customer_id", "product_id"],
    )
    ```

### table_format

Specifies the table format for engines that support multiple formats. Different formats have different features and performance characteristics.

**Supported formats:** `iceberg`, `hive`, `delta`

**When to use:** If your engine supports multiple formats, choose based on your needs:

- **Iceberg:** Great for time travel and schema evolution

- **Delta:** Good for ACID transactions and time travel

- **Hive:** Traditional format, widely supported

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      table_format 'iceberg',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        table_format="iceberg",
    )
    ```

### storage_format

Sets the physical file format for your table's data files. This affects compression, query performance, and storage costs.

**Common formats:** `parquet`, `orc`

**Parquet** is usually the best choice, it's columnar (great for analytics), has good compression, and is widely supported. **ORC** is another option, especially if you're using Hive.

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      storage_format 'parquet',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        storage_format="parquet",
    )
    ```

---

## Engine Properties

These properties let you pass engine-specific settings to Vulcan. Each engine has different capabilities, so these properties vary by engine.

### physical_properties

Pass engine-specific properties directly to the physical table/view creation. This is where you set things like retention policies, labels, or other engine-specific features.

**Use cases:**

- Set table retention (BigQuery: `partition_expiration_days`)

- Add labels or tags (BigQuery, Snowflake)

- Configure table type (Snowflake: `TRANSIENT` tables)

- Any other engine-specific table settings

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      physical_properties (
        partition_expiration_days = 7,
        require_partition_filter = true,
        creatable_type = TRANSIENT,  -- Creates TRANSIENT TABLE
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        physical_properties={
            "partition_expiration_days": 7,
            "require_partition_filter": True,
            "creatable_type": "TRANSIENT",
        },
    )
    ```

### virtual_properties

Pass engine-specific properties to the virtual layer view. This is useful for things like view-level security, labels, or other view-specific settings.

**Use cases:**

- Create secure views (Snowflake: `SECURE` views)

- Add labels to views

- Set view-level permissions

- Configure view-specific engine features

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      virtual_properties (
        creatable_type = SECURE,  -- Creates SECURE VIEW
        labels = [('team', 'analytics')]
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        virtual_properties={
            "creatable_type": "SECURE",
            "labels": [("team", "analytics")],
        },
    )
    ```

### session_properties

Set session-level properties that apply when Vulcan executes your model. These affect how queries run but don't change the table structure.

**Use cases:**

- Set query timeouts

- Configure parallelism

- Adjust memory limits

- Set engine-specific session variables

**Example:** If you have a large query that needs more time, set `query_timeout: 3600` to give it an hour instead of the default timeout.

=== "SQL"

    ```sql
    MODEL (
      name sales.large_query,
      session_properties (
        query_timeout = 3600,
        max_parallel_workers = 8,
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.large_query",
        session_properties={
            "query_timeout": 3600,
            "max_parallel_workers": 8,
        },
    )
    ```

### gateway

Specifies which gateway to use for executing this model. Useful when you have multiple database connections and want to route specific models to specific databases.

**When to use:** Multi-warehouse setups, isolated environments, or when you need to run certain models on a different database than the default.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      gateway 'warehouse_gateway',
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        gateway="warehouse_gateway",
    )
    ```

---

## Behavior Properties

These properties control how Vulcan behaves when processing your model.

### stamp

Force a new model version without changing the definition. This is like a version tag, useful for tracking deployments or forcing a refresh.

**When to use:** When you want to create a new version for tracking purposes, or when you need to force downstream models to rebuild even though this model's definition hasn't changed.

=== "SQL"

    ```sql
    MODEL (
      name sales.daily_sales,
      stamp 'v2.1.0',  -- Force new version
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.daily_sales",
        stamp="v2.1.0",
    )
    ```

### enabled

Control whether the model is active. Set to `false` to disable a model without deleting it.

**When to use:**

- Temporarily disable a model while debugging

- Deprecate a model but keep it for reference

- Skip models during development

**Default:** `true` (models are enabled by default)

=== "SQL"

    ```sql
    MODEL (
      name sales.deprecated_model,
      enabled false,  -- Model will be ignored
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.deprecated_model",
        enabled=False,
    )
    ```

### allow_partials

Allow processing of incomplete data intervals. By default, Vulcan waits for complete intervals before processing (keeps data quality high). Set this to `true` if you need to process partial intervals.

**When to use:**

- Real-time or near-real-time models

- When you need data ASAP, even if it's incomplete

- Streaming data scenarios

**Trade-off:** You lose the ability to distinguish between "missing data" (models issue) and "partial interval" (expected). Use with caution!

**Default:** `false` (wait for complete intervals)

### optimize_query

Enable or disable query optimization. Vulcan optimizes queries by default (rewrites them for better performance), but sometimes you want to disable this.

**When to disable:**

- The optimizer is breaking your query

- You have engine-specific optimizations you want to preserve

- Debugging query issues

**Default:** `true` (optimize queries)

=== "SQL"

    ```sql
    MODEL (
      name sales.complex_query,
      optimize_query false,  -- Disable optimization
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.complex_query",
        optimize_query=False,
    )
    ```

### formatting

Control whether Vulcan formats this model when you run `vulcan format`. Set to `false` if you want to preserve custom formatting.

**When to disable:**

- Legacy models with specific formatting requirements

- Models where formatting breaks something

- When you prefer manual formatting control

**Default:** `true` (format models automatically)

=== "SQL"

    ```sql
    MODEL (
      name sales.legacy_model,
      formatting false,  -- Skip formatting
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.legacy_model",
        formatting=False,
    )
    ```

### ignored_rules

Tell Vulcan's linter to ignore specific rules for this model. Useful when you have a legitimate reason to break a rule, or when a rule doesn't apply to your use case.

You can ignore specific rules (`['rule_name', 'another_rule']`) or all rules (`'ALL'`).

**Use sparingly:** If you're ignoring lots of rules, maybe the rules need updating, or maybe the model needs refactoring.

=== "SQL"

    ```sql
    -- Ignore specific rules
    MODEL (
      name sales.legacy_model,
      ignored_rules ['rule_name', 'another_rule'],
    );
    
    -- Ignore all rules
    MODEL (
      name sales.legacy_model,
      ignored_rules 'ALL',
    );
    ```

=== "Python"

    ```python
    # Ignore specific rules
    @model(
        "sales.legacy_model",
        ignored_rules=["rule_name", "another_rule"],
    )
    
    # Ignore all rules
    @model(
        "sales.legacy_model",
        ignored_rules="ALL",
    )
    ```

---

## Incremental Model Properties

These properties are specified inside the `kind` definition for incremental models. They control how incremental models behave, things like handling schema changes, restatements, and batch processing.

For the full picture on incremental models, check out the [Model Kinds](model_kinds.md) documentation.

### Common Incremental Properties

These properties work with all incremental model kinds. They're your toolkit for controlling incremental behavior:

| Property | Description | Type | Default |
|----------|-------------|:----:|:-------:|
| `forward_only` | All changes should be [forward-only](../../references/plans.md#forward-only-plans) | `bool` | `false` |
| `on_destructive_change` | Behavior for destructive schema changes | `str` | `error` |
| `on_additive_change` | Behavior for additive schema changes | `str` | `allow` |
| `disable_restatement` | Disable [data restatement](../../references/plans.md#restatement-plans) | `bool` | `false` |
| `auto_restatement_cron` | Cron expression for automatic restatement | `str` | - |

**Values for `on_destructive_change` / `on_additive_change`:**

- `allow` - Let the change happen (default for additive)

- `warn` - Allow but warn about it

- `error` - Block the change (default for destructive)

- `ignore` - Pretend it didn't happen

**Why this matters:** Schema changes can break downstream models. These settings let you control how strict Vulcan should be when your schema evolves.

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column event_ts,
        forward_only true,
        on_destructive_change 'warn',
        on_additive_change 'allow',
        disable_restatement false,
      )
    );
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="event_ts",
            forward_only=True,
            on_destructive_change="warn",
            on_additive_change="allow",
            disable_restatement=False,
        ),
    )
    ```

---

### INCREMENTAL_BY_TIME_RANGE

Properties for models that update incrementally based on a time column. These control how time-based incremental processing works.

For the full guide on `INCREMENTAL_BY_TIME_RANGE` models, see the [Model Kinds documentation](model_kinds.md#incremental_by_time_range).

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| **`time_column`** | Column containing each row's timestamp (should be UTC) | `str` | **Y** |
| `format` | Format of the time column's data | `str` | N |
| `batch_size` | Maximum intervals per backfill task | `int` | N |
| `batch_concurrency` | Maximum concurrent batches | `int` | N |
| `lookback` | Prior intervals to include for late-arriving data | `int` | N |
| `auto_restatement_intervals` | Number of last intervals to auto-restate | `int` | N |

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column event_ts,
        time_column (event_ts, '%Y-%m-%d'),  -- With format
        batch_size 12,
        batch_concurrency 4,
        lookback 7,
        auto_restatement_cron '@weekly',
        auto_restatement_intervals 7,
      )
    );
    
    SELECT
      event_ts::TIMESTAMP AS event_ts,
      event_type::VARCHAR AS event_type,
      user_id::INTEGER AS user_id
    FROM raw.events
    WHERE event_ts BETWEEN @start_ts AND @end_ts;
    ```

=== "Python"

    ```python
    from vulcan import ModelKindName
    
    @model(
        "sales.events",
        columns={
            "event_ts": "timestamp",
            "event_type": "varchar",
            "user_id": "int",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="event_ts",
            batch_size=12,
            batch_concurrency=4,
            lookback=7,
        ),
        depends_on=["raw.events"],
    )
    def execute(context, start, end, **kwargs) -> pd.DataFrame:
        query = f"""
        SELECT event_ts, event_type, user_id
        FROM raw.events
        WHERE event_ts BETWEEN '{start}' AND '{end}'
        """
        return context.fetchdf(query)
    ```

!!! info "Important: UTC Timezone"

    Your `time_column` should be in UTC timezone. This ensures Vulcan's scheduler and time macros work correctly.

---

### INCREMENTAL_BY_UNIQUE_KEY

Properties for models that update based on unique keys (upsert operations). These control MERGE behavior and key handling.

For details on `INCREMENTAL_BY_UNIQUE_KEY` models, see the [Model Kinds documentation](model_kinds.md#incremental_by_unique_key).

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| **`unique_key`** | Column(s) containing each row's unique key | `str` \| `array` | **Y** |
| `when_matched` | SQL logic to update columns on match (MERGE engines only) | `str` | N |
| `merge_filter` | Predicates for ON clause of MERGE operation | `str` | N |
| `batch_size` | Maximum intervals per backfill task | `int` | N |
| `lookback` | Prior intervals to include for late-arriving data | `int` | N |

=== "SQL"

    ```sql
    -- Single unique key
    MODEL (
      name sales.customers,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key customer_id,
      )
    );
    
    -- Composite unique key
    MODEL (
      name sales.order_items,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key (order_id, item_id),
      )
    );
    
    -- With MERGE options
    MODEL (
      name sales.customers,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key customer_id,
        when_matched WHEN MATCHED THEN UPDATE SET 
          name = source.name,
          updated_at = source.updated_at,
        auto_restatement_cron '@weekly',
      )
    );
    ```

=== "Python"

    ```python
    # Single unique key
    @model(
        "sales.customers",
        columns={
            "customer_id": "int",
            "name": "varchar",
            "email": "varchar",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,
            unique_key="customer_id",
        ),
        depends_on=["raw.customers"],
    )
    
    # Composite unique key
    @model(
        "sales.order_items",
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,
            unique_key=["order_id", "item_id"],
        ),
    )
    ```

!!! note "Batch Concurrency"

    `batch_concurrency` isn't supported for this kind because MERGE operations can't safely run in parallel. Vulcan processes these models sequentially to avoid conflicts.

---

### INCREMENTAL_BY_PARTITION

Properties for models that update by partition. This kind uses the `partitioned_by` property (from the General Properties section) as its partition key.

**Note:** There are no additional kind-specific properties, just use `partitioned_by` to define your partition columns.

For details on `INCREMENTAL_BY_PARTITION` models, see the [Model Kinds documentation](model_kinds.md#incremental_by_partition).

=== "SQL"

    ```sql
    MODEL (
      name sales.events,
      kind INCREMENTAL_BY_PARTITION,
      partitioned_by event_date,
    );
    
    SELECT
      event_date::DATE AS event_date,
      event_type::VARCHAR AS event_type,
      COUNT(*)::INTEGER AS event_count
    FROM raw.events
    GROUP BY event_date, event_type;
    ```

=== "Python"

    ```python
    @model(
        "sales.events",
        columns={
            "event_date": "date",
            "event_type": "varchar",
            "event_count": "int",
        },
        kind=dict(name=ModelKindName.INCREMENTAL_BY_PARTITION),
        partitioned_by=["event_date"],
        depends_on=["raw.events"],
    )
    ```

---

### SCD_TYPE_2

Properties for Slowly Changing Dimension Type 2 models, which track historical changes to your data.

For the complete guide on SCD Type 2 models, see the [Model Kinds documentation](model_kinds.md#scd-type-2).

#### Common SCD Type 2 Properties

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| **`unique_key`** | Column(s) containing each row's unique key | `array` | **Y** |
| `valid_from_name` | Column for valid from date | `str` | N (default: `valid_from`) |
| `valid_to_name` | Column for valid to date | `str` | N (default: `valid_to`) |
| `invalidate_hard_deletes` | Mark missing records as invalid | `bool` | N (default: `true`) |

#### SCD_TYPE_2_BY_TIME

Properties for SCD Type 2 models that detect changes using an `updated_at` timestamp column. This is the recommended approach when your source table has update timestamps.

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| `updated_at_name` | Column containing updated at date | `str` | N (default: `updated_at`) |
| `updated_at_as_valid_from` | Use `updated_at` value as `valid_from` for new rows | `bool` | N (default: `false`) |

=== "SQL"

    ```sql
    MODEL (
      name dim.customers,
      kind SCD_TYPE_2_BY_TIME (
        unique_key customer_id,
        updated_at_name last_modified,
        valid_from_name effective_from,
        valid_to_name effective_to,
        invalidate_hard_deletes true,
        updated_at_as_valid_from true,
      )
    );
    
    SELECT
      customer_id::INTEGER AS customer_id,
      name::VARCHAR AS name,
      email::VARCHAR AS email,
      last_modified::TIMESTAMP AS last_modified
    FROM raw.customers;
    ```

=== "Python"

    ```python
    @model(
        "dim.customers",
        columns={
            "customer_id": "int",
            "name": "varchar",
            "email": "varchar",
            "last_modified": "timestamp",
        },
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_TIME,
            unique_key=["customer_id"],
            updated_at_name="last_modified",
            valid_from_name="effective_from",
            valid_to_name="effective_to",
            invalidate_hard_deletes=True,
        ),
        depends_on=["raw.customers"],
    )
    ```

#### SCD_TYPE_2_BY_COLUMN

Properties for SCD Type 2 models that detect changes by comparing column values. Use this when your source table doesn't have an `updated_at` column.

| Property | Description | Type | Required |
|----------|-------------|:----:|:--------:|
| **`columns`** | Columns to check for changes (`*` for all) | `str` \| `array` | **Y** |
| `execution_time_as_valid_from` | Use execution time as `valid_from` for new rows | `bool` | N (default: `false`) |

=== "SQL"

    ```sql
    -- Track specific columns
    MODEL (
      name dim.products,
      kind SCD_TYPE_2_BY_COLUMN (
        unique_key product_id,
        columns (name, price, category),
        execution_time_as_valid_from true,
      )
    );
    
    -- Track all columns
    MODEL (
      name dim.products,
      kind SCD_TYPE_2_BY_COLUMN (
        unique_key product_id,
        columns '*',
      )
    );
    ```

=== "Python"

    ```python
    # Track specific columns
    @model(
        "dim.products",
        columns={
            "product_id": "int",
            "name": "varchar",
            "price": "decimal(10,2)",
            "category": "varchar",
        },
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_COLUMN,
            unique_key=["product_id"],
            columns=["name", "price", "category"],
            execution_time_as_valid_from=True,
        ),
        depends_on=["raw.products"],
    )
    
    # Track all columns
    @model(
        "dim.products",
        kind=dict(
            name=ModelKindName.SCD_TYPE_2_BY_COLUMN,
            unique_key=["product_id"],
            columns="*",
        ),
    )
    ```

---
<!-- 
## Model Defaults

Configure default values for all models in your project's `config.yaml`:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      start: 2024-01-01
      cron: '@daily'
      owner: data_team
      physical_properties:
        partition_expiration_days: 7
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
      model_defaults=ModelDefaultsConfig(
        dialect="snowflake",
        start="2024-01-01",
        cron="@daily",
        owner="data_team",
        physical_properties={
          "partition_expiration_days": 7,
        },
      ),
    )
    ```

**Supported default properties:**

- `kind`, `dialect`, `cron`, `owner`, `start`

- `table_format`, `storage_format`

- `physical_properties`, `virtual_properties`, `session_properties`

- `on_destructive_change`, `on_additive_change`

- `assertions`, `optimize_query`, `allow_partials`, `enabled`, `interval_unit`

- `pre_statements`, `post_statements`, `on_virtual_update`

### Overriding Defaults

Model-specific properties override defaults. To unset a default, use `None`:

```sql
MODEL (
  name sales.daily_sales,
  physical_properties (
    partition_expiration_days = 14,      -- Override
    project_level_property = None,       -- Unset
  )
);
```

--- -->

## Model Naming

By default, you need to specify the `name` property in every model. But if you organize your models in a directory structure that matches your schema names, you can enable automatic name inference.

**How it works:** With `infer_names: true`, a model at `models/sales/daily_sales.sql` automatically gets the name `sales.daily_sales`. The directory structure becomes your schema, and the filename becomes your model name.

Enable it in your config:

```yaml
model_defaults:
  dialect: snowflake
  
# Enable name inference
infer_names: true
```

**When to use:** If your project structure matches your schema structure, this saves you from typing `name` in every model. Pretty convenient!

Learn more in the [configuration guide](../../references/configuration.md#model-naming).



# Statements

Source: https://tmdc-io.github.io/vulcan-book/components/model/statements/

---

# Statements

Statements let you run SQL commands at specific points during model execution. You can run code before your query, after it completes, or when views are created.

**Why use statements?** They're perfect for:
- Setting session parameters (timeouts, memory limits)

- Loading UDFs or creating temporary tables

- Creating indexes or clustering

- Running data quality checks

- Logging anomalies or errors

- Granting permissions on views

You can define statements at the model level (for specific needs) or at the project level via `model_defaults` (for consistent behavior across all models).

**Statement types:**

- **Pre-statements**: Run before the main model query executes

- **Post-statements**: Run after the main model query completes

- **On-virtual-update statements**: Run when views are created or updated in the virtual layer

!!! warning "Concurrency Considerations"

    Pre-statements should generally only prepare the main query. Avoid creating or altering physical tables in pre-statements, if multiple models run concurrently, you could get race conditions or unpredictable behavior. Stick to session settings, UDFs, and temporary objects.

## Model Defaults

You can define statements at the project level using `model_defaults` in your configuration. Use this for setting up common behavior across all models, like session timeouts or default permissions.

**How it works:** Default statements run first, then model-specific statements. So if you set a default timeout in `model_defaults` and a model-specific timeout in a model, the model-specific one runs after (and can override the default).

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      pre_statements:
        - "SET query_timeout = 300000"
      post_statements:
        - "@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)"
      on_virtual_update:
        - "GRANT SELECT ON @this_model TO ROLE analyst_role"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
      model_defaults=ModelDefaultsConfig(
        dialect="snowflake",
        pre_statements=[
          "SET query_timeout = 300000",
        ],
        post_statements=[
          "@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)",
        ],
        on_virtual_update=[
          "GRANT SELECT ON @this_model TO ROLE analyst_role",
        ],
      ),
    )
    ```

## Pre-statements

Pre-statements run before your main model query executes. They're perfect for setting up the environment your query needs.

**Common use cases:**

- Loading JARs or UDFs that your query uses

- Creating temporary tables or caching data

- Setting session parameters (timeouts, memory, etc.)

- Initializing variables or settings

Pre-statements run in the setup phase before your main query runs.

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name analytics.orders,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column order_date
      ),
      start '2020-01-01',
      cron '@daily'
    );

    /* Pre-statement: Create table for anomaly tracking */
    CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (
      anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,
      order_id VARCHAR,
      anomaly_type VARCHAR,
      details VARCHAR,
      captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    /* Pre-statement: Set session variables using Jinja */
    JINJA_STATEMENT_BEGIN;
    {% if start_date is none or end_date is none %}
      SET start_date = DATE '{{ start }}';
      SET end_date = CURRENT_DATE;
    {% endif %}
    JINJA_END;

    /* Main model query */
    SELECT
      order_id::VARCHAR AS order_id,
      order_date::DATE AS order_date,
      customer_id::VARCHAR AS customer_id,
      total_amount::FLOAT AS total_amount
    FROM demo.raw_data.orders
    WHERE
      order_date BETWEEN @start_date AND @end_date;
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName
    from sqlglot import exp

    @model(
        "analytics.orders_py",
        columns={
            "order_id": "varchar",
            "order_date": "date",
            "customer_id": "varchar",
            "total_amount": "float",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="order_date",
        ),
        pre_statements=[
            "SET query_timeout = 300000",
            """CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (
                anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,
                order_id VARCHAR,
                anomaly_type VARCHAR,
                details VARCHAR
            )""",
            exp.Cache(this=exp.table_("orders_cache"), expression=exp.select("*").from_("demo.raw_data.orders")),
        ],
    )
    def execute(context: ExecutionContext, start, end, **kwargs):
        query = f"""
        SELECT order_id, order_date, customer_id, total_amount
        FROM demo.raw_data.orders
        WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'
        """
        return context.fetchdf(query)
    ```

## Post-statements

Post-statements run after your model query completes. They're great for cleanup, optimization, or validation tasks.

**Important:** When you use post-statements in SQL models, your main query **must end with a semicolon**. This tells Vulcan where the query ends and the statements begin.

**Common use cases:**

- Creating indexes or clustering (for query performance)

- Running data quality checks or validations

- Logging anomalies or errors to tracking tables

- Conditional table alterations (like setting retention policies)

**Think of it as:** The "cleanup and optimization" phase after your data is loaded.

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name analytics.orders,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column order_date
      )
    );

    SELECT
      order_id,
      order_date,
      customer_id,
      quantity,
      unit_price,
      total_amount
    FROM demo.raw_data.orders
    WHERE
      order_date BETWEEN @start_date AND @end_date;

    /* Post-statement: Conditional retention policy (only on table creation) */
    @IF(
      @runtime_stage IN ('creating'),
      ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30
    );

    /* Post-statement: Add clustering for query performance */
    ALTER TABLE @this_model CLUSTER BY (order_date, customer_id);

    /* Post-statement: Capture data anomalies - negative quantities */
    INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
    SELECT
      order_id,
      'NEGATIVE_QUANTITY',
      CONCAT('Quantity=', quantity)
    FROM @this_model
    WHERE quantity < 0;

    /* Post-statement: Capture data anomalies - total mismatch */
    INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
    SELECT
      order_id,
      'TOTAL_MISMATCH',
      CONCAT(
        'calculated=', ROUND(unit_price * quantity, 2),
        '; actual=', ROUND(total_amount, 2)
      )
    FROM @this_model
    WHERE ABS((unit_price * quantity) - total_amount) > 0.01;
    ```

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "analytics.orders_py",
        columns={
            "order_id": "varchar",
            "order_date": "date",
            "customer_id": "varchar",
            "total_amount": "float",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="order_date",
        ),
        post_statements=[
            "@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)",
            "ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)",
            """INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
               SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)
               FROM @this_model WHERE quantity < 0""",
        ],
    )
    def execute(context: ExecutionContext, start, end, **kwargs):
        query = f"""
        SELECT order_id, order_date, customer_id, total_amount
        FROM demo.raw_data.orders
        WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'
        """
        return context.fetchdf(query)
    ```

## On-virtual-update Statements

On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.

**Common use cases:**

- Granting permissions on views (so users can query them)

- Setting up access controls or row-level security

- Applying column masking policies

- Any view-level configuration

**Think of it as:** The "access control" phase, setting up who can see what.

**Note:** These statements run at the virtual layer, so table names (including `@this_model`) resolve to view names, not physical table names.

=== "SQL"

    Use `ON_VIRTUAL_UPDATE_BEGIN` and `ON_VIRTUAL_UPDATE_END` to define these statements:

    ```sql linenums="1"
    MODEL (
      name analytics.customers,
      kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key customer_id
      )
    );

    SELECT
      customer_id,
      full_name,
      email,
      customer_segment
    FROM demo.raw_data.customers;

    /* Post-statement: Apply masking policy */
    JINJA_STATEMENT_BEGIN;
    ALTER TABLE {{ this_model }} MODIFY COLUMN email SET MASKING POLICY demo.security.mask_email_policy;
    JINJA_END;

    /* On-virtual-update: Grant permissions when view is created/updated */
    ON_VIRTUAL_UPDATE_BEGIN;
    JINJA_STATEMENT_BEGIN;
    GRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;
    JINJA_END;
    ON_VIRTUAL_UPDATE_END;
    ```

=== "Python"

    Use the `on_virtual_update` argument in the `@model` decorator:

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName

    @model(
        "analytics.customers_py",
        columns={
            "customer_id": "varchar",
            "full_name": "varchar",
            "email": "varchar",
            "customer_segment": "varchar",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_UNIQUE_KEY,
            unique_key=["customer_id"],
        ),
        post_statements=[
            "@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 7)",
        ],
        on_virtual_update=[
            "GRANT SELECT ON @this_model TO ROLE view_only_role",
        ],
    )
    def execute(context: ExecutionContext, **kwargs):
        query = """
        SELECT customer_id, CONCAT(first_name, ' ', last_name) AS full_name,
               email, customer_segment
        FROM demo.raw_data.customers
        """
        return context.fetchdf(query)
    ```

## Complete example

Here's a complete example showing all statement types:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      name analytics.orders,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column order_date
      ),
      start '2020-01-01',
      cron '@daily',
      grains (order_id),
      description 'Orders fact table with incremental loading'
    );

    /* ============ PRE-STATEMENTS ============ */

    /* Create anomaly tracking table */
    CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (
      anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,
      order_id VARCHAR,
      anomaly_type VARCHAR,
      details VARCHAR,
      captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    /* ============ MAIN QUERY ============ */

    SELECT
      order_id::VARCHAR AS order_id,
      order_date::DATE AS order_date,
      customer_id::VARCHAR AS customer_id,
      product_id::VARCHAR AS product_id,
      quantity::INT AS quantity,
      unit_price::FLOAT AS unit_price,
      discount::FLOAT AS discount,
      tax::FLOAT AS tax,
      shipping_cost::FLOAT AS shipping_cost,
      total_amount::FLOAT AS total_amount
    FROM demo.raw_data.orders
    WHERE
      order_date BETWEEN @start_date AND @end_date;

    /* ============ POST-STATEMENTS ============ */

    /* Conditional: Set retention only on table creation */
    @IF(
      @runtime_stage IN ('creating'),
      ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30
    );

    /* Add clustering for performance */
    ALTER TABLE @this_model CLUSTER BY (order_date, customer_id);

    /* Data quality: Log negative quantities */
    INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
    SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)
    FROM @this_model
    WHERE quantity < 0;

    /* Data quality: Log total mismatches */
    INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
    SELECT
      order_id,
      'TOTAL_MISMATCH',
      CONCAT(
        'calc=', ROUND(unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0), 2),
        '; total=', ROUND(total_amount, 2)
      )
    FROM @this_model
    WHERE ABS(
      (unit_price * quantity * (1 - COALESCE(discount, 0)) + COALESCE(tax, 0) + COALESCE(shipping_cost, 0))
      - total_amount
    ) > 0.01;

    /* ============ ON-VIRTUAL-UPDATE ============ */

    ON_VIRTUAL_UPDATE_BEGIN;
    JINJA_STATEMENT_BEGIN;
    GRANT SELECT ON VIEW {{ this_model }} TO ROLE view_only_role;
    JINJA_END;
    ON_VIRTUAL_UPDATE_END;
    ```

=== "Python"

    ```python linenums="1"
    import typing as t
    import pandas as pd
    from datetime import datetime
    from vulcan import ExecutionContext, model
    from vulcan import ModelKindName
    from sqlglot import exp

    @model(
        "analytics.orders_py",
        columns={
            "order_id": "varchar",
            "order_date": "date",
            "customer_id": "varchar",
            "product_id": "varchar",
            "quantity": "int",
            "unit_price": "float",
            "discount": "float",
            "tax": "float",
            "shipping_cost": "float",
            "total_amount": "float",
        },
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="order_date",
        ),
        grains=["order_id"],
        depends_on=["demo.raw_data.orders"],
        description="Orders fact table with incremental loading",
        pre_statements=[
            """CREATE TABLE IF NOT EXISTS analytics._orders_anomalies (
                anomaly_id BIGINT GENERATED ALWAYS AS IDENTITY,
                order_id VARCHAR,
                anomaly_type VARCHAR,
                details VARCHAR,
                captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )""",
        ],
        post_statements=[
            "@IF(@runtime_stage = 'creating', ALTER TABLE @this_model SET DATA_RETENTION_TIME_IN_DAYS = 30)",
            "ALTER TABLE @this_model CLUSTER BY (order_date, customer_id)",
            """INSERT INTO analytics._orders_anomalies (order_id, anomaly_type, details)
               SELECT order_id, 'NEGATIVE_QUANTITY', CONCAT('Quantity=', quantity)
               FROM @this_model WHERE quantity < 0""",
        ],
        on_virtual_update=[
            "GRANT SELECT ON @this_model TO ROLE view_only_role",
        ],
    )
    def execute(
        context: ExecutionContext,
        start: datetime,
        end: datetime,
        execution_time: datetime,
        **kwargs: t.Any,
    ) -> pd.DataFrame:
        query = f"""
        SELECT
            order_id, order_date, customer_id, product_id,
            quantity, unit_price, discount, tax, shipping_cost, total_amount
        FROM demo.raw_data.orders
        WHERE order_date BETWEEN '{start.date()}' AND '{end.date()}'
        """
        return context.fetchdf(query)
    ```

## Useful macros and variables

| Macro/Variable | Description |
|----------------|-------------|
| `@this_model` | References the current model's table/view |
| `@runtime_stage` | Current execution stage: `'creating'`, `'evaluating'`, or `'testing'` |
| `@IF(condition, statement)` | Conditionally execute a statement |
| `@start_date`, `@end_date` | Time range macros for incremental models |
| `{{ this_model }}` | Jinja equivalent of `@this_model` |

For more information on macros, see the [Macro Variables](../advanced-features/macros/variables.md) documentation.



# External

Source: https://tmdc-io.github.io/vulcan-book/components/model/types/external_models/

---

# External

Sometimes your models need to query tables that exist outside your Vulcan project, maybe a third-party data source, a table managed by another system, or a read-only database. These are "external" tables.

Vulcan doesn't manage external tables (you can't create or update them), but it can use metadata about them to make your life easier. By defining external models, you give Vulcan information about column names and types, which enables better column-level lineage and query optimization.

Even though Vulcan can't manage them, knowing their schema helps with:
- Column-level lineage (see how data flows through external tables)

- Query optimization (Vulcan can make better decisions)

- Documentation (your data catalog knows what's in those tables)

Vulcan stores this metadata as `EXTERNAL` models.

## How External Models Work

`EXTERNAL` models are metadata-only, they just describe a table's schema (column names and types). There's no query for Vulcan to run, and Vulcan doesn't manage the data.

**Important limitations:**

- Vulcan doesn't know what data is in the table (or if it even exists)

- If someone alters the external table, Vulcan won't detect it

- If all data is deleted, Vulcan won't know

- Vulcan never modifies external tables

The querying model's [`kind`](../../model/model_kinds.md), [`cron`](../../model/overview.md#cron), and previously loaded time intervals determine when Vulcan will query the `EXTERNAL` model.

**When external tables get queried:** Only when a Vulcan model references them. The querying model's `kind`, `cron`, and time intervals determine when the external table is actually queried. Vulcan doesn't proactively query external tables, it only queries them as part of executing your models.

## Creating External Models

External models are defined in YAML files. You have two options:

1. **Let Vulcan generate it** (easiest) - Use the `create_external_models` CLI command
2. **Write it yourself** - Hand-craft the YAML if you need more control

The main file is `external_models.yaml` (or `schema.yaml`) in your project root. You can also add more files in the `external_models/` directory.

Let's say you have a model that queries external tables. Here's an example:

```sql
MODEL (
  name vulcan_demo.full_model,
  kind FULL
);

SELECT
  c.customer_id,
  c.name AS customer_name,
  c.email,
  COUNT(DISTINCT o.order_id) AS total_orders,
  COALESCE(SUM(oi.quantity * oi.unit_price), 0) AS total_spent
FROM vulcan_demo.customers AS c
LEFT JOIN vulcan_demo.orders AS o
  ON c.customer_id = o.customer_id
LEFT JOIN vulcan_demo.order_items AS oi
  ON o.order_id = oi.order_id
GROUP BY c.customer_id, c.name, c.email
```

The following sections show you how to create external models for these tables. You can define all external models in `external_models.yaml`, or split them across multiple files in the `external_models/` directory (useful for organization or when Vulcan regenerates the main file).

### Using CLI

Instead of creating the `external_models.yaml` file manually, Vulcan can generate it for you with the [create_external_models](../../../cli-command/cli.md#create_external_models) CLI command.

The command identifies all external tables referenced in your Vulcan project, fetches their column information from the SQL engine's metadata, and then stores the information in the `external_models.yaml` file.

If Vulcan does not have access to an external table's metadata, the table will be omitted from the file and Vulcan will issue a warning.

`create_external_models` solely queries SQL engine metadata and does not query external tables themselves.

### Gateway-specific external models

In some use-cases such as [isolated systems with multiple gateways](../../../guides-old/isolated_systems.md#multiple-gateways), there are external models that only exist on a certain gateway.

**Gateway names are case-insensitive in external model configurations.** You can specify the gateway name using any case (e.g., `gateway: dev`, `gateway: DEV`, `gateway: Dev`) and Vulcan will handle the matching correctly.

Consider the following model that queries an external table with a dynamic database based on the current gateway:

```bash
vulcan create_external_models
```

**What it does:**

- Scans your project for references to external tables

- Fetches column information from your SQL engine's metadata

- Writes everything to `external_models.yaml`

**Important:** This command only queries metadata (table schemas), not the actual data. It's fast and safe.

**If Vulcan can't access a table's metadata:** That table gets skipped and Vulcan warns you. You'll need to define it manually (see the "Writing YAML by hand" section below).

### Gateway-Specific External Models

If you're using [isolated systems with multiple gateways](../../../guides-old/isolated_systems.md#multiple-gateways), you might have external tables that only exist on specific gateways.

**Example:** Your model uses a gateway variable to select different databases:

```sql
MODEL (
  name vulcan_demo.customer_summary,
  kind FULL
);

SELECT * FROM @{gateway}_db.customers;
```

When you run with `--gateway dev`, it queries `dev_db.customers`. When you run with `--gateway prod`, it queries `prod_db.customers`. These are different tables with potentially different schemas!

**Solution:** Run `create_external_models` with the `--gateway` flag:

```bash
vulcan --gateway dev create_external_models
```

This sets `gateway: dev` on the external model, so it only loads when that gateway is active. Do this for each gateway that has different external tables.

!!! note "Case-Insensitive Gateway Names"

    Gateway names are case-insensitive in external model configs. `gateway: dev`, `gateway: DEV`, and `gateway: Dev` all work the same.

### Writing YAML by Hand

Sometimes you need to define external models manually, maybe Vulcan can't access the metadata, or you want more control. Here's the structure:

```yaml
- name: '"warehouse"."vulcan_demo"."customers"'
  description: "Customer dimension table from external system"
  gateway: dev  # Optional: only load for this gateway
  columns:
    customer_id: INT
    region_id: INT
    name: TEXT
    email: TEXT
- name: '"warehouse"."vulcan_demo"."orders"'
  columns:
    order_id: INT
    customer_id: INT
    order_date: TIMESTAMP
    warehouse_id: INT
```

**What you need:**

- `name`: Fully qualified table name (with quotes if needed for case sensitivity)

- `columns`: Dictionary of column names to data types

**Optional fields:**

- `description`: Human-readable description

- `gateway`: Gateway name (for gateway-specific tables)

**Pro tip:** Use triple-quoted names if your table names have special characters or need case sensitivity. The exact format depends on your SQL engine.

### Using the `external_models` Directory

Here's a common problem: You run `vulcan create_external_models` and it generates `external_models.yaml`. But some tables need manual definitions (maybe Vulcan can't access their metadata). If you add them to `external_models.yaml` and run the command again, your manual changes get overwritten!

**Solution:** Put manual definitions in the `external_models/` directory:

```
external_models.yaml              # Auto-generated by Vulcan
external_models/manual_tables.yaml # Your manual definitions
external_models/legacy_tables.yaml # More manual definitions
```

**How it works:**

- Vulcan loads `external_models.yaml` first (or `schema.yaml`)

- Then it loads all `.yaml` files from `external_models/`

- Everything gets merged together

**Best practice:** Use `create_external_models` to manage the main file, and put any tables that need manual definitions in the `external_models/` directory. That way you can regenerate the main file without losing your manual work!

### External Assertions

You can define [assertions](../../audits/audits.md) on external models! This is super useful for validating upstream data quality before your internal models run.

**Why this matters:** If your external data source has quality issues, you want to catch them early, before they flow into your models and cause bigger problems downstream.

Here's how you'd add assertions to an external model:

```yaml
- name: '"warehouse"."vulcan_demo"."customers"'
  description: Table containing customer information
  assertions:
    - name: not_null
      columns: "[customer_id, email]"
    - name: unique_values
      columns: "[customer_id]"
  columns:
    customer_id: INT
    region_id: INT
    name: TEXT
    email: TEXT
- name: '"warehouse"."vulcan_demo"."orders"'
  description: Table containing order transactions
  assertions:
    - name: not_null
      columns: "[order_id, customer_id, order_date]"
    - name: accepted_range
      column: order_id
      min_v: "1"
  columns:
    order_id: INT
    customer_id: INT
    order_date: TIMESTAMP
    warehouse_id: INT
```



# Managed

Source: https://tmdc-io.github.io/vulcan-book/components/model/types/managed_models/

---

# Managed

Most Vulcan models manage their own data, you run `vulcan run`, and Vulcan updates the tables. Managed models are different: the database engine handles data updates automatically in the background.

**How it works:** You define a query, and the engine monitors upstream tables. When source data changes, the engine automatically refreshes your managed table. No manual `REFRESH` commands needed, it just happens.

Use this for scenarios where you need always-fresh data without managing refresh schedules yourself. The engine handles the complexity of incremental updates, change detection, and refresh timing.

**Best use case:** Managed models are typically built on [External Models](./external_models.md) rather than other Vulcan models. Since Vulcan already keeps its models up to date, the main benefit comes when you're reading from external tables that aren't tracked by Vulcan. The engine keeps your managed table in sync with those external sources automatically.

!!! warning "Python Models Not Supported"

    Python models don't support the `MANAGED` [model kind](../model_kinds.md). You'll need to use a SQL model instead.

## Difference from Materialized Views

What's the difference between a managed model and a materialized view?

Vulcan already supports [materialized views](../model_kinds.md#materialized-views), but they have limitations:

- Some engines only allow materialized views from a single base table

- Materialized views aren't automatically refreshed, you need to run `REFRESH MATERIALIZED VIEW` manually

- You're responsible for scheduling refreshes

**Managed models are different:**

- **Automatic updates** - The engine refreshes data when source tables change

- **Smart refresh** - The engine understands your query and can do incremental or full refreshes as needed

- **No manual commands** - Everything happens in the background

**In some engines, there's no difference** (they're the same thing). In others, managed models give you more automation and flexibility.

## Lifecycle in Vulcan

Managed models follow the same lifecycle as other Vulcan models:

- Virtual environments create pointers to model snapshots

- Model changes create new snapshots

- Upstream changes trigger new snapshots

- You can deploy and rollback like any other model

- Snapshots get cleaned up when TTL expires

**Cost consideration:** Managed models usually cost more than regular tables. For example, Snowflake charges extra for Dynamic Tables. To save money, Vulcan uses regular tables for dev previews (in forward-only plans) and only creates managed tables when deploying to production.

!!! warning "Dev vs Prod Differences"

    Since dev uses regular tables and prod uses managed tables, it's possible to write a query that works in dev but fails in prod. This happens if you use features available to regular tables but not managed tables.

    We think the cost savings are worth it, but if this causes issues, let us know!

## Supported Engines

Currently, Vulcan supports managed models on:

| Engine | Implementation |
|--------|----------------|
| [Snowflake](../../../configurations/engines/snowflake/snowflake.md) | [Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro) |

To create a managed model, use the [`MANAGED`](../model_kinds.md#managed) model kind.

### Snowflake

On Snowflake, managed models are implemented as [Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro). Dynamic Tables automatically refresh when their source data changes, which is exactly what managed models need.

Here's how you'd create one:

```sql linenums="1"
MODEL (
  name db.events,
  kind MANAGED,
  physical_properties (
    warehouse = datalake,
    target_lag = '2 minutes',
    data_retention_time_in_days = 2
  )
);

SELECT
  event_date::DATE as event_date,
  event_payload::TEXT as payload
FROM raw_events
```

results in:

```sql linenums="1"
CREATE OR REPLACE DYNAMIC TABLE db.events
  WAREHOUSE = "datalake",
  TARGET_LAG = '2 minutes'
  DATA_RETENTION_TIME_IN_DAYS = 2
AS SELECT
  event_date::DATE as event_date,
  event_payload::TEXT as payload
FROM raw_events
```

!!! note "No Intervals"

    Vulcan doesn't create intervals or run this model on a schedule. You don't need `WHERE` clauses with date filters like you would for incremental models. Snowflake handles all the refreshing automatically, you just define the query and let Snowflake do its thing.

#### Table Properties

Dynamic Tables have properties that control refresh frequency, initial data population, retention, and more. You can find the complete list in the [Snowflake documentation](https://docs.snowflake.com/sql-reference/sql/create-dynamic-table).

In Vulcan, you set these properties using [`physical_properties`](../overview.md#physical_properties) in your model definition. Here are the key ones:

| Snowflake Property              | Required | Notes
| ------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| target_lag                      | Y        |                                                                                                                                         |
| warehouse                       | N        | In Snowflake, this is a required property. However, if not specified, then Vulcan will use the result of `select current_warehouse()`. |
| refresh_mode                    | N        |                                                                                                                                         |
| initialize                      | N        |                                                                                                                                         |
| data_retention_time_in_days     | N        |                                                                                                                                         |
| max_data_extension_time_in_days | N        |                                                                                                                                         |

The following Dynamic Table properties can be set directly on the model:

| Snowflake Property | Required   | Notes                                                                                                                                                                   |
| ------------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| cluster by         | N          | `clustered_by` is a [standard model property](../overview.md#clustered_by), so set `clustered_by` on the model to add a `CLUSTER BY` clause to the Dynamic Table |



# Python

Source: https://tmdc-io.github.io/vulcan-book/components/model/types/python_models/

---

# Python

Sometimes you need Python instead of SQL. Use Python models for machine learning, calling external APIs, or implementing complex business logic that's difficult to express in SQL.

Vulcan supports Python models. As long as your function returns a Pandas, Spark, Bigframe, or Snowpark DataFrame, you can use Python.

**When to use Python models:**

- Building machine learning workflows

- Integrating with external APIs

- Complex transformations that are easier in Python

- Data processing that benefits from Python libraries


!!! info "Unsupported Model Kinds"

    Python models don't support these [model kinds](../model_kinds.md). If you need one of these, use a SQL model instead:
        
        - `VIEW` - Views need to be SQL

        - `SEED` - Seed models load CSV files (SQL only)

        - `MANAGED` - Managed models require SQL

        - `EMBEDDED` - Embedded models inject SQL subqueries

## Definition

Create a Python model by adding a `.py` file to your `models/` directory and defining an `execute` function.

Here's what a basic Python model looks like:

```python linenums="1"
import typing as t
import pandas as pd
from datetime import datetime
from vulcan import ExecutionContext, model
from vulcan import ModelKindName

@model(
    "sales.daily_sales_py",
    columns={
        "order_date": "timestamp",
        "total_orders": "int",
        "total_revenue": "decimal(18,2)",
        "last_order_id": "string",
    },
    kind=dict(
        name=ModelKindName.FULL,
    ),
    grains=["order_date"],
    depends_on=["raw.raw_orders"],
    cron='@daily',
    description="Daily sales aggregated by order_date.",
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    """FULL model - rebuilds entire daily_sales table each run"""

    query = """
    SELECT
      CAST(order_date AS TIMESTAMP) AS order_date,
      COUNT(order_id) AS total_orders,
      SUM(total_amount) AS total_revenue,
      MAX(order_id) AS last_order_id
    FROM raw.raw_orders
    GROUP BY order_date
    ORDER BY order_date
    """

    return context.fetchdf(query)
```

**How it works:**

The `@model` decorator captures your model's metadata (just like the `MODEL` DDL in SQL models). You specify column names and types in the `columns` argument, this is required because Vulcan needs to create the table before your function runs.

**Function signature:** Your `execute` function receives:

- `context: ExecutionContext` - For running queries and getting time intervals

- `start`, `end` - Time range for incremental models

- `execution_time` - When the model is running

- `**kwargs` - Any other runtime variables

**Return types:** You can return Pandas, PySpark, Bigframe, or Snowpark DataFrames. If your output is large, you can also use Python generators to return data in chunks for memory management.

## `@model` Specification

The `@model` decorator accepts the same properties as SQL models, just use Python syntax instead of SQL DDL. `name`, `kind`, `cron`, `grains`, etc. They all work the same way.

Python model `kind`s are specified with a Python dictionary containing the kind's name and arguments. All model kind arguments are listed in the [models configuration reference page](../../../references/model_configuration.md#model-kind-properties).

```python
from vulcan import ModelKindName

@model(
    "sales.daily_sales",
    kind=dict(
        name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
        time_column="order_date",
    ),
)
```

All model kind properties are documented in the [model configuration reference](../../../references/model_configuration.md#model-kind-properties).

Supported `kind` dictionary `name` values are:

- `ModelKindName.VIEW`

- `ModelKindName.FULL`

- `ModelKindName.SEED`

- `ModelKindName.INCREMENTAL_BY_TIME_RANGE`

- `ModelKindName.INCREMENTAL_BY_UNIQUE_KEY`

- `ModelKindName.INCREMENTAL_BY_PARTITION`

- `ModelKindName.SCD_TYPE_2_BY_TIME`

- `ModelKindName.SCD_TYPE_2_BY_COLUMN`

- `ModelKindName.EMBEDDED`

- `ModelKindName.CUSTOM`

- `ModelKindName.MANAGED`

- `ModelKindName.EXTERNAL`

## Execution Context

Python models can do anything you want, but it is strongly recommended for all models to be [idempotent](../../../references/glossary.md#idempotency). Python models can fetch data from upstream models or even data outside of Vulcan.

**Fetching data:** Use `context.fetchdf()` to run SQL queries and get DataFrames:

```python
df = context.fetchdf("SELECT * FROM vulcan_demo.products")
```

**Resolving table names:** Use `context.resolve_table()` to get the correct table name for the current environment (handles dev/prod prefixes automatically):

```python
table = context.resolve_table("vulcan_demo.products")
df = context.fetchdf(f"SELECT * FROM {table}")
```

**Best practice:** Make your models [idempotent](../../../references/glossary.md#idempotency), running them multiple times should produce the same result. This makes debugging and restatements much easier.

```python linenums="1"
df = context.fetchdf("SELECT * FROM vulcan_demo.products")
```

## Optional Pre/Post-Statements

You can run SQL commands before and after your Python model executes. This is useful for setting session parameters, creating indexes, or running data quality checks.

**Pre-statements:** Run before your `execute` function
**Post-statements:** Run after your `execute` function completes

You can pass SQL strings, SQLGlot expressions, or macro calls as lists to `pre_statements` and `post_statements`.

!!! warning "Concurrency"

    Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings, UDFs, and temporary objects in pre-statements.

**Project-level defaults:** You can also define pre/post-statements in `model_defaults` for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the [model configuration reference](../../../references/model_configuration.md#model-defaults).

``` python linenums="1" hl_lines="8-12"
@model(
    "vulcan_demo.model_with_statements",
    kind="full",
    columns={
        "id": "int",
        "name": "text",
    },
    pre_statements=[
        "SET GLOBAL parameter = 'value';",
        exp.Cache(this=exp.table_("x"), expression=exp.select("1")),
    ],
    post_statements=["@CREATE_INDEX(@this_model, id)"],
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:

    return pd.DataFrame([
        {"id": 1, "name": "name"}
    ])

```

The previous example's `post_statements` called user-defined Vulcan macro `@CREATE_INDEX(@this_model, id)`.

We could define the `CREATE_INDEX` macro in the project's `macros` directory like this. The macro creates a table index on a single column, conditional on the [runtime stage](../../advanced-features/macros/variables.md#runtime-variables) being `creating` (table creation time).


``` python linenums="1"
@macro()
def create_index(
    evaluator: MacroEvaluator,
    model_name: str,
    column: str,
):
    if evaluator.runtime_stage == "creating":
        return f"CREATE INDEX idx ON {model_name}({column});"
    return None
```

**Alternative approach:** Instead of using the `@model` decorator's `pre_statements` and `post_statements`, you can execute SQL directly in your function using `context.engine_adapter.execute()`.

**Important:** If you want post-statements to run after your function completes, you need to use `yield` instead of `return`. Post-statements specified after a `yield` will execute after the function finishes.

This example function includes both pre- and post-statements:

``` python linenums="1" hl_lines="9-10 12 17-18"
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:

    # pre-statement
    context.engine_adapter.execute("SET GLOBAL parameter = 'value';")

    # post-statement requires using `yield` instead of `return`
    yield pd.DataFrame([
        {"id": 1, "name": "name"}
    ])

    # post-statement
    context.engine_adapter.execute("CREATE INDEX idx ON vulcan_demo.model_with_statements (id);")
```

## Optional On-Virtual-Update Statements

On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view pointing to it is set up.

**Common use case:** Granting permissions on views so users can query them.

You can set `on_virtual_update` in the `@model` decorator to a list of SQL strings, SQLGlot expressions, or macro calls.

**Project-level defaults:** You can also define on-virtual-update statements at the project level using `model_defaults` in your configuration. These will be applied to all models in your project (including Python models) and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the [model configuration reference](../../../references/model_configuration.md#model-defaults).

``` python linenums="1" hl_lines="8"
@model(
    "vulcan_demo.model_with_grants",
    kind="full",
    columns={
        "id": "int",
        "name": "text",
    },
    on_virtual_update=["GRANT SELECT ON VIEW @this_model TO ROLE dev_role"],
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:

    return pd.DataFrame([
        {"id": 1, "name": "name"}
    ])
```

!!! note "Virtual Layer Resolution"

    These statements run at the virtual layer, so table names resolve to view names, not physical table names. For example, in a `dev` environment, `vulcan_demo.model_with_grants` and `@this_model` resolve to `vulcan_demo__dev.model_with_grants` (the view), not the physical table.

## Dependencies

In order to fetch data from an upstream model, you first get the table name using `context`'s `resolve_table` method. This returns the appropriate table name for the current runtime [environment](../../../references/environments.md):

```python linenums="1"
table = context.resolve_table("vulcan_demo.products")
df = context.fetchdf(f"SELECT * FROM {table}")
```

The `resolve_table` method will automatically add the referenced model to the Python model's dependencies.

The only other way to set dependencies of models in Python models is to define them explicitly in the `@model` decorator using the keyword `depends_on`. The dependencies defined in the model decorator take precedence over any dynamic references inside the function.

```python linenums="1"
@model(
    "vulcan_demo.full_model_py",
    columns={
        "product_id": "int",
        "product_name": "string",
        "category": "string",
        "total_sales": "decimal(10,2)",
    },
    kind=dict(
        name=ModelKindName.FULL,
    ),
    grains=["product_id"],
    depends_on=["vulcan_demo.products", "vulcan_demo.order_items", "vulcan_demo.orders"],
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    # Dependencies are explicitly declared above
    query = """
    SELECT 
        p.product_id,
        p.name AS product_name,
        p.category,
        COALESCE(SUM(oi.quantity * oi.unit_price), 0) as total_sales
    FROM vulcan_demo.products p
    LEFT JOIN vulcan_demo.order_items oi ON p.product_id = oi.product_id
    LEFT JOIN vulcan_demo.orders o ON oi.order_id = o.order_id
    GROUP BY p.product_id, p.name, p.category
    ORDER BY total_sales DESC
    """
    
    return context.fetchdf(query)
```

You can use [global variables](../../../references/configuration.md#variables) or [blueprint variables](#python-model-blueprinting) in `resolve_table` calls. Here's how:

```python linenums="1"
@model(
    "@schema_name.test_model2",
    kind="FULL",
    columns={"id": "INT"},
)
def execute(context, **kwargs):
    table = context.resolve_table(f"{context.var('schema_name')}.test_model1")
    select_query = exp.select("*").from_(table)
    return context.fetchdf(select_query)
```

## Returning Empty DataFrames

Python models can't return empty DataFrames directly. If your model might return empty data, use `yield` instead of `return`:

**Why?** This allows Vulcan to handle the empty case properly. If you `return` an empty DataFrame, Vulcan will error. If you `yield` an empty generator or conditionally yield, it works fine.

```python linenums="1" hl_lines="10-13"
@model(
    "vulcan_demo.empty_df_model"
)
def execute(
    context: ExecutionContext,
) -> pd.DataFrame:

    [...code creating df...]

    if df.empty:
        yield from ()
    else:
        yield df
```

## User-defined variables

[User-defined global variables](../../../references/configuration.md#variables) can be accessed from within the Python model with the `context.var` method.

For example, this model access the user-defined variables `var` and `var_with_default`. It specifies a default value of `default_value` if `variable_with_default` resolves to a missing value.

```python linenums="1" hl_lines="11 12"
@model(
    "vulcan_demo.model_with_vars",
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    var_value = context.var("var")
    var_with_default_value = context.var("var_with_default", "default_value")
    ...
```

Alternatively, you can access global variables via `execute` function arguments, where the name of the argument corresponds to the name of a variable key.

For example, this model specifies `my_var` as an argument to the `execute` method. The model code can reference the `my_var` object directly:

```python linenums="1" hl_lines="9 12"
@model(
    "vulcan_demo.model_with_arg_vars",
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    my_var: Optional[str] = None,
    **kwargs: t.Any,
) -> pd.DataFrame:
    my_var_plus1 = my_var + 1
    ...
```

Make sure the argument has a default value if it's possible for the variable to be missing.

Note that arguments must be specified explicitly - variables cannot be accessed using `kwargs`.

## Python Model Blueprinting

Python models can serve as templates for creating multiple models. This is called "blueprinting", you define one model template, and Vulcan creates multiple models from it.

**How it works:** You parameterize the model name with a variable (using `@{variable}` syntax) and provide a list of mappings in `blueprints`. Vulcan creates one model for each mapping.

**Use case:** When you have similar models that differ only by a few parameters (like different schemas, regions, or customers).

Here's an example that creates two models:

```python linenums="1"
import typing as t
from datetime import datetime

import pandas as pd
from vulcan import ExecutionContext, model

@model(
    "@{customer}.some_table",
    kind="FULL",
    blueprints=[
        {"customer": "customer1", "field_a": "x", "field_b": "y"},
        {"customer": "customer2", "field_a": "z", "field_b": "w"},
    ],
    columns={
        "field_a": "text",
        "field_b": "text",
        "customer": "text",
    },
)
def entrypoint(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    return pd.DataFrame(
        {
            "field_a": [context.blueprint_var("field_a")],
            "field_b": [context.blueprint_var("field_b")],
            "customer": [context.blueprint_var("customer")],
        }
    )
```

**Important:** Notice the `@{customer}` syntax in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal). Learn more about this syntax [here](../../advanced-features/macros/built_in.md#embedding-variables-in-strings).

**Dynamic blueprints:** You can generate blueprints dynamically using macros. This is handy when your blueprint list comes from external sources (like CSV files or API calls):

```python
@model(
    "@{customer}.some_table",
    blueprints="@gen_blueprints()",  # Macro generates the list
    ...
)
```

For example, the definition of the `gen_blueprints` may look like this:

```python linenums="1"
from vulcan import macro

@macro()
def gen_blueprints(evaluator):
    return (
        "((customer := customer1, field_a := x, field_b := y),"
        " (customer := customer2, field_a := z, field_b := w))"
    )
```

It's also possible to use the `@EACH` macro, combined with a global list variable (`@values`):

```python linenums="1"

@model(
    "@{customer}.some_table",
    blueprints="@EACH(@values, x -> (customer := schema_@x))",
    ...
)
...
```

## Using Macros in Model Properties

Python models support macro variables in model properties, but there's a gotcha when macros appear inside strings.

**The issue:** Cron expressions often use `@` (like `@daily`, `@hourly`), which conflicts with Vulcan's macro syntax.

**The solution:** Wrap the entire expression in quotes and prefix with `@`:

```python linenums="1"
# Correct: Wrap the cron expression containing a macro variable
@model(
    "vulcan_demo.scheduled_model",
    cron="@'*/@{mins} * * * *'",  # Note the @'...' syntax
    ...
)

# This also works with blueprint variables
@model(
    "@{customer}.scheduled_model",
    cron="@'0 @{hour} * * *'",
    blueprints=[
        {"customer": "customer_1", "hour": 2}, # Runs at 2 AM
        {"customer": "customer_2", "hour": 8}, # Runs at 8 AM
    ],
    ...
)

```

This is necessary because cron expressions often use `@` for aliases (like `@daily`, `@hourly`), which can conflict with Vulcan's macro syntax.

## Examples

Here are some practical examples showing different ways to use Python models.

### Basic

A simple Python model that returns a static Pandas DataFrame. All the [metadata properties](../overview.md#model-properties) work the same as SQL models, just use Python syntax.

```python linenums="1"
import typing as t
from datetime import datetime

import pandas as pd
from sqlglot.expressions import to_column
from vulcan import ExecutionContext, model

@model(
    "vulcan_demo.basic_model",
    owner="data_team",
    cron="@daily",
    columns={
        "id": "int",
        "name": "text",
    },
    column_descriptions={
        "id": "Unique ID",
        "name": "Name corresponding to the ID",
    },
    audits=[
        ("not_null", {"columns": [to_column("id")]}),
    ],
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:

    return pd.DataFrame([
        {"id": 1, "name": "name"}
    ])
```

### SQL Query and Pandas

A more realistic example: query upstream models, do some pandas processing, and return the result. This shows how you'd typically use Python models in practice:

```python linenums="1"
import typing as t
from datetime import datetime

import pandas as pd
from vulcan import ExecutionContext, model

@model(
    "vulcan_demo.sql_pandas_model",
    columns={
        "product_id": "int",
        "product_name": "text",
        "total_sales": "decimal(10,2)",
    },
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    # get the upstream model's name and register it as a dependency
    products_table = context.resolve_table("vulcan_demo.products")
    order_items_table = context.resolve_table("vulcan_demo.order_items")

    # fetch data from the model as a pandas DataFrame
    df = context.fetchdf(f"""
        SELECT 
            p.product_id,
            p.name AS product_name,
            SUM(oi.quantity * oi.unit_price) as total_sales
        FROM {products_table} p
        LEFT JOIN {order_items_table} oi ON p.product_id = oi.product_id
        GROUP BY p.product_id, p.name
    """)

    # do some pandas stuff
    df['total_sales'] = df['total_sales'].fillna(0)
    return df
```

### PySpark

If you're using Spark, use the PySpark DataFrame API instead of Pandas. PySpark DataFrames compute in a distributed fashion (across your Spark cluster), which is much faster for large datasets.

**Why PySpark over Pandas:** Pandas loads everything into memory on a single machine. PySpark distributes the work across your cluster, so you can handle much larger datasets.

```python linenums="1"
import typing as t
from datetime import datetime

import pandas as pd
from pyspark.sql import DataFrame, functions

from vulcan import ExecutionContext, model

@model(
    "vulcan_demo.pyspark_model",
    columns={
        "customer_id": "int",
        "customer_name": "text",
        "region": "text",
    },
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> DataFrame:
    # get the upstream model's name and register it as a dependency
    table = context.resolve_table("vulcan_demo.customers")

    # use the spark DataFrame api to add the region column
    df = context.spark.table(table).withColumn("region", functions.lit("North"))

    # returns the pyspark DataFrame directly, so no data is computed locally
    return df
```


### Snowpark

If you're using Snowflake, use the Snowpark DataFrame API. Like PySpark, Snowpark DataFrames compute on Snowflake's servers (not locally), which is much more efficient.

**Why Snowpark over Pandas:** All computation happens in Snowflake, so you're not moving data to your local machine. Faster, cheaper, and can handle huge datasets.

```python linenums="1"
import typing as t
from datetime import datetime

import pandas as pd
from snowflake.snowpark.dataframe import DataFrame

from vulcan import ExecutionContext, model

@model(
    "vulcan_demo.snowpark_model",
    columns={
        "id": "int",
        "name": "text",
        "country": "text",
    },
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> DataFrame:
    # returns the snowpark DataFrame directly, so no data is computed locally
    df = context.snowpark.create_dataframe([[1, "a", "usa"], [2, "b", "cad"]], schema=["id", "name", "country"])
    df = df.filter(df.id > 1)
    return df
```

### Bigframe

If you're using BigQuery, use the [Bigframe](https://cloud.google.com/bigquery/docs/use-bigquery-dataframes#pandas-examples) DataFrame API. Bigframe looks like Pandas but runs everything in BigQuery.

**Why Bigframe over Pandas:** All computation happens in BigQuery, so you get BigQuery's scale and performance. Plus, you can use BigQuery remote functions (like in the example below) for custom Python logic.

```python linenums="1"
import typing as t
from datetime import datetime

from bigframes.pandas import DataFrame

from vulcan import ExecutionContext, model


def get_bucket(num: int):
    if not num:
        return "NA"
    boundary = 10
    return "at_or_above_10" if num >= boundary else "below_10"


@model(
    "vulcan_demo.bigframe_model",
    columns={
        "title": "text",
        "views": "int",
        "bucket": "text",
    },
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> DataFrame:
    # Create a remote function to be used in the Bigframe DataFrame
    remote_get_bucket = context.bigframe.remote_function([int], str)(get_bucket)

    # Returns the Bigframe DataFrame handle, no data is computed locally
    df = context.bigframe.read_gbq("bigquery-samples.wikipedia_pageviews.200809h")

    df = (
        # This runs entirely on the BigQuery engine lazily
        df[df.title.str.contains(r"[Gg]oogle")]
        .groupby(["title"], as_index=False)["views"]
        .sum(numeric_only=True)
        .sort_values("views", ascending=False)
    )

    return df.assign(bucket=df["views"].apply(remote_get_bucket))
```

### Batching

If your Python model outputs a huge DataFrame and you can't use Spark/Bigframe/Snowpark, you can batch the output using Python generators.

**The problem:** With Pandas, everything loads into memory. If your output is too large, you'll run out of memory.

**The solution:** Use `yield` to return DataFrames in chunks. Vulcan processes them one at a time, so you never have more than one chunk in memory at once.

Here's how you'd do it:

```python linenums="1" hl_lines="20"
@model(
    "vulcan_demo.batching_model",
    columns={
        "customer_id": "int",
    },
)
def execute(
    context: ExecutionContext,
    start: datetime,
    end: datetime,
    execution_time: datetime,
    **kwargs: t.Any,
) -> pd.DataFrame:
    # get the upstream model's table name
    table = context.resolve_table("vulcan_demo.customers")

    for i in range(3):
        # run 3 queries to get chunks of data and not run out of memory
        df = context.fetchdf(f"SELECT customer_id from {table} WHERE customer_id = {i}")
        yield df
```

## Serialization

Vulcan executes Python models locally (wherever Vulcan is running) using a custom serialization framework. This means your Python code runs on your machine or CI/CD environment, not in the database.

**Why this matters:** You have full access to Python libraries, can make API calls, do ML processing, etc. The database just receives the final DataFrame.



# SQL

Source: https://tmdc-io.github.io/vulcan-book/components/model/types/sql_models/

---

# SQL

SQL models are the most common type of model you'll write. You can define them using SQL directly, or use Python to generate SQL dynamically.

SQL models are simple, powerful, and work with any SQL database. Most of your data transformations will be SQL models.

## SQL-Based Definition

SQL-based models are the most common type. They use regular SQL with additional Vulcan features.

**Structure:** A SQL model file has these parts (in order):

1. The `MODEL` DDL (metadata and configuration)
2. Optional pre-statements (setup SQL)
3. A single query (your transformation logic)
4. Optional post-statements (cleanup/optimization SQL)
5. Optional on-virtual-update statements (view permissions, etc.)

**Creating a SQL model:** Add a `.sql` file to your `models/` directory (or a subdirectory). The filename doesn't matter to Vulcan, but it's conventional to name it after your model. For example, `sales.daily_sales` → `daily_sales.sql`.

### Example

Here's a simple SQL model to get you started:

```sql linenums="1"
-- This is the MODEL DDL, where you specify model metadata and configuration information.
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date
);

/*
  This is the single query that defines the model's logic.
  Although it is not required, it is considered best practice to explicitly
  specify the type for each one of the model's columns through casting.
*/
SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

### `MODEL` DDL

The `MODEL` DDL is where you define your model's metadata, name, kind, schedule, owner, and more. It must be the first statement in your SQL file.

The `MODEL` DDL tells Vulcan everything it needs to know about your model. For a complete list of all available properties, see the [Model Properties](../properties.md) documentation.

### Optional Pre/Post-Statements

Pre-statements run before your query, post-statements run after. Use them for setup, cleanup, and optimization tasks.

**Common use cases:**

- Pre-statements: Set session parameters, load UDFs, cache tables

- Post-statements: Create indexes, run data quality checks, set retention policies

**Important:** Pre/post-statements must end with semicolons. If you have post-statements, your main query must also end with a semicolon (so Vulcan knows where the query ends).

!!! warning "Concurrency"

    Be careful with pre-statements that create or alter physical tables, if multiple models run concurrently, you could get conflicts. Stick to session settings and temporary objects.

```sql linenums="1"
MODEL (
  name sales.daily_sales,
  kind FULL
);

-- Pre-statement: Cache a table for use in the query
CACHE TABLE countries AS SELECT * FROM raw.countries;

-- The model query (must end with semi-colon when post-statements are present)
SELECT
  order_date::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue
FROM raw.raw_orders
GROUP BY order_date;

-- Post-statement: Clean up the cached table
UNCACHE TABLE countries;
```

**Project-level defaults:** You can define pre/post-statements in `model_defaults` for consistent behavior across all models. Default statements run first, then model-specific ones. Learn more in the [model configuration reference](../../../references/model_configuration.md#model-defaults).

!!! warning "Statements Run Twice"

Pre/post-statements are evaluated twice: when a model's table is created and when its query logic is evaluated. Executing statements more than once can have unintended side-effects, so you can [conditionally execute](../../advanced-features/macros/built_in.md#prepost-statements) them based on Vulcan's [runtime stage](../../advanced-features/macros/variables.md#runtime-variables).

    **Solution:** Use conditional execution with `@IF` and `@runtime_stage` to control when statements run. For example, only run a post-statement when the query is actually being evaluated:

We can condition the post-statement to only run after the model query is evaluated using the [`@IF` macro operator](../../advanced-features/macros/built_in.md#if) and [`@runtime_stage` macro variable](../../advanced-features/macros/variables.md#runtime-variables) like this:

```sql linenums="1" hl_lines="14-17"
MODEL (
  name sales.daily_sales,
  kind FULL
);

CACHE TABLE countries AS SELECT * FROM raw.countries;

SELECT
  order_date::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders
FROM raw.raw_orders
GROUP BY order_date;

@IF(
  @runtime_stage = 'evaluating',
  UNCACHE TABLE countries
);
```

**Important:** The SQL command inside `@IF()` doesn't end with a semicolon. The semicolon goes after the `@IF()` macro's closing parenthesis.

### Optional On-Virtual-Update Statements

On-virtual-update statements run when views are created or updated in the virtual layer. This happens after your model's physical table is created and the view is set up.

**Common use case:** Granting permissions on views so users can query them.

**Project-level defaults:** You can also define on-virtual-update statements at the project level using `model_defaults` in your configuration. These will be applied to all models in your project and merged with any model-specific statements. Default statements are executed first, followed by model-specific statements. Learn more about this in the [model configuration reference](../../../references/model_configuration.md#model-defaults).

**Syntax:** Wrap these statements in `ON_VIRTUAL_UPDATE_BEGIN;` ... `ON_VIRTUAL_UPDATE_END;` blocks:

```sql linenums="1" hl_lines="10-15"
MODEL (
  name sales.daily_sales,
  kind FULL
);

SELECT
  order_date::TIMESTAMP,
  COUNT(order_id)::INTEGER AS total_orders
FROM raw.raw_orders
GROUP BY order_date;

ON_VIRTUAL_UPDATE_BEGIN;
GRANT SELECT ON VIEW @this_model TO ROLE role_name;
JINJA_STATEMENT_BEGIN;
GRANT SELECT ON VIEW {{ this_model }} TO ROLE admin;
JINJA_END;
ON_VIRTUAL_UPDATE_END;
```

**Jinja support:** You can use [Jinja expressions](../../advanced-features/macros/jinja.md) in these statements. Just wrap them in `JINJA_STATEMENT_BEGIN;` ... `JINJA_END;` blocks (as shown in the example above).

!!! note "Virtual Layer Resolution"

    These statements run at the virtual layer, so table names resolve to view names, not physical table names. In a `dev` environment, `sales.daily_sales` and `@this_model` resolve to `sales__dev.daily_sales` (the view), not the physical table.

### The Model Query

Your model must contain a standalone query. This can be:

- A single `SELECT` statement

- Multiple `SELECT` statements combined with `UNION`, `INTERSECT`, or `EXCEPT`

The result of this query becomes your model's table or view data. Pretty straightforward!

### SQL Model Blueprinting

SQL models can serve as templates for creating multiple models. This is called "blueprinting", define one template, get multiple models.

**How it works:** Parameterize your model name with a variable (using `@{variable}` syntax) and provide a list of mappings in `blueprints`. Vulcan creates one model for each mapping.

**Use case:** When you have similar models that differ only by parameters (like different regions, schemas, or customers).

Here's an example that creates four models from one template:

```sql linenums="1"
MODEL (
  name vulcan_demo.fct_daily_sales__@{region},
  kind VIEW,
  blueprints (
    (region := 'north'),
    (region := 'south'),
    (region := 'east'),
    (region := 'west')
  ),
  grains region_id
);

SELECT
  *
FROM vulcan_demo.fct_daily_sales
@WHERE(TRUE)
  LOWER(region_name) = LOWER(@region)
```

Vulcan creates these four models from that template:

```sql linenums="1"
-- This uses the first variable mapping
MODEL (
  name vulcan_demo.fct_daily_sales__north,
  kind VIEW
);

SELECT
  *
FROM vulcan_demo.fct_daily_sales
WHERE
  LOWER(region_name) = LOWER('north')

-- This uses the second variable mapping
MODEL (
  name vulcan_demo.fct_daily_sales__south,
  kind VIEW
);

SELECT
  *
FROM vulcan_demo.fct_daily_sales
WHERE
  LOWER(region_name) = LOWER('south')
```

**Important syntax:** Notice `@{region}` in the model name. The curly braces tell Vulcan to treat the variable value as a SQL identifier (not a string literal).

You can see the different behavior in the WHERE clause. `@region` (without braces) is resolved to the string literal `'north'` (with single quotes) because the blueprint value is quoted. Learn more about the curly brace syntax [here](../../advanced-features/macros/built_in.md#embedding-variables-in-strings).


**Dynamic blueprints:** You can generate blueprints using macros. This is handy when your blueprint list comes from external sources (CSV files, APIs, etc.):

```sql
MODEL (
  name vulcan_demo.fct_daily_sales__@{region},
  blueprints @gen_blueprints(),  -- Macro generates the list
  ...
);
```

Here's how you might define the macro:

```python linenums="1"
from vulcan import macro

@macro()
def gen_blueprints(evaluator):
    return (
        "((region := 'north'),"
        " (region := 'south'),"
        " (region := 'east'),"
        " (region := 'west'))"
    )
```

You can also use the `@EACH` macro with a global list variable:

```sql linenums="1"
MODEL (
  name vulcan_demo.fct_daily_sales__@{region},
  kind VIEW,
  blueprints @EACH(@values, x -> (region := @x)),
);

SELECT
  *
FROM vulcan_demo.fct_daily_sales
@WHERE(TRUE)
  LOWER(region_name) = LOWER(@region)
```

## Python-Based Definition

You can also define SQL models using Python! This is useful when:

- Your query is too complex for clean SQL

- You need heavy dynamic logic (would require lots of macros)

- You want to generate SQL programmatically

**How it works:** You write Python code that generates SQL, and Vulcan executes it. You still get SQL models (they run SQL queries), but you write them in Python.

For the complete guide on Python-based SQL models, including the `@model` decorator, execution context, and examples, see the [Python Models](python_models.md) page.

## Automatic Dependencies

One of Vulcan's superpowers: it parses your SQL and automatically figures out dependencies. No need to manually specify what your model depends on, Vulcan just knows!

**How it works:** Vulcan analyzes your `FROM` and `JOIN` clauses and builds a dependency graph. When you run `vulcan plan`, it ensures upstream models run first.

**Example:** This query automatically depends on `raw.raw_orders`:

```sql
SELECT order_date, COUNT(order_id) AS total_orders
FROM raw.raw_orders
GROUP BY order_date
```

Vulcan will make sure `raw.raw_orders` runs before this model. Pretty neat!

**External dependencies:** If you reference tables that aren't Vulcan models, Vulcan can handle them too, either implicitly (through execution order) or via [signals](../../advanced-features/signals.md).

**Manual dependencies:** Sometimes you need to add extra dependencies manually (maybe a hidden dependency or a macro that references another model). Use the `depends_on` property in your `MODEL` DDL for that.

## Conventions

Vulcan follows some conventions to keep things consistent and reliable. Here are the key ones:

### Explicit Type Casting

Vulcan encourages explicit type casting for all columns. This helps Vulcan understand your data types and prevents incorrect inference.

**Format:** Use `column_name::data_type` syntax (works in any SQL dialect):

```sql
SELECT
  order_date::DATE AS order_date,
  total_orders::INTEGER AS total_orders,
  revenue::DECIMAL(10,2) AS revenue
```

**Why this matters:** Explicit types make your models more predictable and help Vulcan optimize queries better.

### Explicit SELECTs

Avoid `SELECT *` when possible. It's convenient, but dangerous, if an upstream source adds or removes columns, your model's output changes unexpectedly.

**Best practice:** List every column you need explicitly. If you're querying external sources, use [`create_external_models`](../../../getting_started/cli.md#create_external_models) to capture their schema, or define them as [external models](../model_kinds.md#external).

**Why avoid `SELECT *` on external sources:** It prevents Vulcan from optimizing queries and determining column-level lineage. Define external models instead!

### Encoding

SQL model files must be UTF-8 encoded. Using other encodings can cause parsing errors or unexpected behavior.

## Transpilation

Vulcan uses [SQLGlot](https://github.com/tobymao/sqlglot) to parse and transpile SQL. This gives you some superpowers:

**Write in any dialect, run on any engine:** Write PostgreSQL-style SQL, and Vulcan converts it to BigQuery. Or write Snowflake SQL and run it on Spark. Pretty cool!

**Use advanced syntax:** You can use features from one dialect even if your engine doesn't support them. For example, `x::int` (PostgreSQL syntax) works even on engines that only support `CAST(x AS INT)`. SQLGlot handles the conversion.

**Formatting flexibility:** Trailing commas, extra whitespace, minor formatting differences, SQLGlot normalizes them all. Write SQL however you like, and Vulcan makes it consistent.

## Macros

Standard SQL is powerful, but real-world data pipelines need dynamic components. Date filters that change each run, conditional logic, reusable query patterns, macros give you all of this.

**Macro variables:** Vulcan provides automatic date/time variables for incremental models. Use `@start_date`, `@end_date`, `@start_ds`, `@end_ds` and Vulcan fills them in with the current time range. No more hardcoding dates!

**Custom macros:** For complex logic or reusable patterns, Vulcan supports a powerful [macro syntax](../../advanced-features/macros/overview.md) and [Jinja templates](https://jinja.palletsprojects.com/en/3.1.x/). Write macros once, use them everywhere.

**Why macros matter:** They make your SQL more maintainable. Instead of copy-pasting complex logic, define it once as a macro and reuse it. Your queries stay clean and readable.

Learn more about macros in the [Macros documentation](../../advanced-features/macros/overview.md).



# Business Metrics

Source: https://tmdc-io.github.io/vulcan-book/components/semantics/business_metrics/

---

# Business Metrics

Business metrics are where your semantic layer really shines. They combine measures (the calculations) with dimensions (the attributes) and time (the when) to create complete analytical definitions that are ready for time-series analysis.

Semantic models provide the building blocks (measures, dimensions, joins), and business metrics combine those blocks into something you can analyze over time. They're pre-configured for dashboards, reports, and APIs, no SQL required.

## What are business metrics?

Business metrics are complete analytical definitions that:

- **Combine measures with time**: Let you analyze trends at different time granularities (daily, weekly, monthly, etc.)

- **Include dimensions**: Enable slicing and dicing by business attributes (customer tier, region, product category, etc.)

- **Ready for analysis**: Pre-configured so they can power dashboards, reports, and APIs directly

- **Examples**: `monthly_revenue_by_tier`, `daily_active_users`, `customer_acquisition_trend`

They're the bridge between your technical data models and the business questions people actually want to answer.

## Basic structure

A business metric brings together three things:

- **Measure**: The calculation you want to perform (like `orders.total_revenue`)

- **Time**: The time dimension for your analysis (like `orders.order_date`)

- **Dimensions**: Optional attributes for grouping and filtering (like `customers.customer_tier`)

Here's the simplest possible metric:

```yaml
metrics:
  monthly_revenue:
    measure: orders.total_revenue      # Which measure to calculate
    time: orders.order_date            # Time dimension for analysis
    description: "Monthly revenue trends"
```

That's it! This metric is now ready to be queried at any time granularity you want.

## Simple metric

Let's start with the basics, a metric that just has a measure and time:

```yaml
metrics:
  daily_revenue:
    measure: orders.total_revenue
    time: orders.order_date
    description: "Daily revenue trends"
```

Even though it's called `daily_revenue`, you're not locked into daily granularity. You can query this same metric at different time intervals (day, week, month, quarter, year) without redefining it. The metric definition stays the same; you just change the granularity when you query it.

## Metric with dimensions

Add dimensions to enable slicing and grouping:

```yaml
metrics:
  revenue_by_tier:
    measure: orders.total_revenue
    time: orders.order_date
    dimensions:
      - customers.customer_tier      # Group by tier

      - customers.country            # And country
    description: "Revenue trends by customer tier and country"
```

Now you can answer questions like:

- What's our revenue by tier over time?

- How does revenue vary by country?

- What's the revenue breakdown by tier and country together?

The dimensions give you flexibility to analyze the metric from different angles.

## Cross-model metrics

You're not limited to one model. Combine measures and dimensions from multiple models:

```yaml
metrics:
  product_revenue_by_customer_segment:
    measure: orders.total_revenue      # From orders
    time: orders.order_date            # From orders
    dimensions:
      - products.category              # From products

      - products.brand

      - customers.customer_tier        # From customers

      - customers.region
    description: "Product revenue segmented by customer demographics"
```

This metric pulls the measure from `orders`, time from `orders`, product dimensions from `products`, and customer dimensions from `customers`. As long as you've defined the proper joins between these semantic models, Vulcan will handle the cross-model logic for you.

**Important:** Make sure your semantic models have the right joins defined, or cross-model metrics won't work.

## Reference format

Always use **dot notation** with semantic model aliases when referencing measures, dimensions, and time:

```yaml
# Good: Use aliases
measure: orders.total_revenue     # alias.measure_name
time: orders.order_date           # alias.column_name
dimensions:
  - customers.customer_tier       # alias.column_name

# Bad: Don't use physical names
measure: analytics.fact_orders.revenue
time: order_date  # Missing alias
```

The dot notation (`orders.total_revenue`) tells Vulcan which semantic model to look in and what to reference. Physical table names won't work here, you need the semantic aliases.

## Time granularity

Define metrics once, then query them at any time granularity:

```yaml
metrics:
  revenue_trends:
    measure: orders.total_revenue
    time: orders.order_date
    description: "Revenue at any time granularity"
```

The same metric can be queried with different granularities:

- Daily: `granularity=day`

- Weekly: `granularity=week`

- Monthly: `granularity=month`

- Quarterly: `granularity=quarter`

- Yearly: `granularity=year`

You don't need separate metric definitions for each granularity, just change the query parameter.

## Complete example

Here's a more complete example showing different types of metrics:

```yaml
metrics:
  # Simple revenue metric
  daily_revenue:
    measure: orders.total_revenue
    time: orders.order_date
    description: "Daily revenue trends"
    tags: [revenue, financial, kpi]
  
  # Customer acquisition
  customer_acquisition_trend:
    measure: customers.new_signups
    time: customers.signup_date
    dimensions:
      - customers.signup_channel

      - customers.customer_tier

      - customers.country
    description: "Customer acquisition by channel, tier, and geography"
    tags: [acquisition, growth, customer]
  
  # Cross-model metric
  product_performance:
    measure: orders.total_revenue
    time: orders.order_date
    dimensions:
      - products.category

      - products.brand

      - customers.customer_tier
    description: "Product revenue by category, brand, and customer segment"
    tags: [revenue, products, segmentation]
```

Notice how each metric has a clear purpose, good description, and relevant tags. The tags help organize and discover metrics later.

## Benefits

### Time-series analysis

Metrics are built for analyzing trends over time:

- **Flexible granularity**: Query the same metric at different time intervals without redefinition

- **Consistent definitions**: Same calculation logic applies across all time periods

- **Trend analysis**: Built-in support for comparing periods (month-over-month, year-over-year, etc.)

### Self-service analytics

Business users can query metrics without writing SQL:

- **Simple API**: Query metrics by name with a time range and dimensions

- **Consistent results**: Same metric definition is used everywhere, so everyone gets the same answer

- **No SQL required**: Complex joins and aggregations are abstracted away

### Single source of truth

Centralized metric definitions mean:

- **Define once**: Create metric definitions in YAML files

- **Use everywhere**: Same metrics power dashboards, reports, and APIs

- **Version controlled**: Metric definitions live alongside your code, so changes are tracked

## Best practices

### Descriptive names

Make your metric names self-explanatory:

```yaml
# Good: Self-explanatory
metrics:
  monthly_revenue_by_tier: ...
  daily_active_users: ...

# Bad: Vague
metrics:
  metric_1: ...
  rev: ...
```

Good names make it obvious what the metric measures and how it's broken down.

### Include essential dimensions

Think about what business questions people will want to answer, and include those dimensions:

```yaml
# Good: Key business dimensions
metrics:
  revenue_analysis:
    measure: orders.total_revenue
    time: orders.order_date
    dimensions:
      - customers.customer_tier

      - customers.region

      - products.category

# Too few: Limited analysis
metrics:
  revenue:
    measure: orders.total_revenue
    time: orders.order_date
    # Missing dimensions - can't slice and dice!
```

Dimensions are what make metrics useful. Without them, you can only see the overall trend, not the breakdowns that drive business decisions.

### Document business context

Add descriptions and metadata to help people understand what the metric means:

```yaml
metrics:
  net_revenue_retention:
    measure: subscriptions.nrr
    time: subscriptions.cohort_month
    description: "Net Revenue Retention: expansion minus churn"
    meta:
      business_owner: "Finance Team"
      calculation: "(Starting MRR + Expansion - Churn) / Starting MRR"
      benchmark: ">110% is good for SaaS"
```

The `meta` section is perfect for business context, calculation details, benchmarks, and ownership information. This helps people understand not just what the metric is, but what it means and how to interpret it.

## Integration with semantic models

Metrics build on top of semantic models:

1. **Semantic models** define measures, dimensions, and joins
2. **Metrics** combine these components with time for analysis
3. **APIs** expose metrics for querying and visualization

The semantic layer provides the foundation (the building blocks), and metrics add the time-series analytical capabilities (the finished product).

## Next steps

- Learn about [Semantic Models](models.md) that provide the foundation for metrics

- See the [Semantics Overview](overview.md) for the complete picture

- Explore metric definitions in your project's `semantics/` directory



# Semantic Models

Source: https://tmdc-io.github.io/vulcan-book/components/semantics/models/

---

# Semantic Models

Semantic models are your bridge between technical data structures and business understanding. They take your physical Vulcan models (the tables and columns in your database) and map them to business concepts that make sense to analysts, product managers, and other non-technical users.

Semantic models are a translation layer. Your database might have tables named `dim_customers` or `fact_orders` (technical naming), but your semantic layer can expose them as `customers` and `orders` (business-friendly naming). Semantic models define what you can do with the data: dimensions for grouping, measures for calculations, segments for filtering, and joins for combining models.

## What are semantic models?

Semantic models bridge the gap between technical table structures and business understanding:

- **Reference physical models**: Each semantic model references a Vulcan model defined in your `models/` directory

- **Provide business aliases**: Hide technical naming (like `dim_customers` or `fact_orders`) behind consumer-friendly names

- **Expose analytical capabilities**: Define dimensions, measures, segments, and joins for each model

They're the foundation of your semantic layer, everything else (business metrics, semantic queries) builds on top of semantic models.

## Basic structure

A semantic model maps a physical Vulcan model to a semantic representation. Here's the basic structure:

```yaml
models:
  analytics.customers:  # Physical model name (dictionary key)
    alias: customers     # Business-friendly semantic alias
    description: "Customer master data"
    dimensions: {...}    # Optional: control which columns are exposed
    measures: {...}      # Optional: aggregated calculations
    segments: {...}      # Optional: reusable filter conditions
    joins: {...}         # Optional: relationships to other models
```

The physical model name (`analytics.customers`) is the key, and everything else defines how it appears in the semantic layer.

## Dimensions

Dimensions are attributes you use for grouping and filtering. They answer "by what?" questions, like "revenue by customer tier" or "orders by country."

**The good news:** All columns from your Vulcan model automatically become dimensions. You don't have to define them manually unless you want to control which ones are exposed or add enhancements.

Here's how you can control dimensions:

```yaml
# All columns from analytics.customers automatically become dimensions:
# - customers.customer_id
# - customers.customer_tier
# - customers.signup_date
# - customers.country

# You can control which columns are exposed:
dimensions:
  excludes:
    - password_hash       # Hide sensitive data

    - internal_notes
  
  # Enhance dimensions with additional capabilities:
  enhancements:
    - name: start_date
      granularities:
        - name: monthly
          interval: "1 month"
          description: "Monthly subscription cohorts"
        - name: quarterly
          interval: "3 months"
          description: "Quarterly cohorts"
```

Use `excludes` to hide sensitive or internal columns. Use `enhancements` to add time granularities for cohort analysis, useful for subscription or signup dates.

## Measures

Measures are aggregated calculations that answer "how much?" or "how many?" questions. They're what you use to calculate totals, averages, counts, and other aggregations.

You define measures using SQL expressions with aggregations like `SUM()`, `COUNT()`, `AVG()`, etc.:

```yaml
measures:
  total_revenue:
    type: sum
    expression: "{customers.amount}"
    description: "Total revenue from all orders"
    format: currency
  
  avg_order_value:
    type: number
    expression: "SUM({customers.total_revenue}) / NULLIF(COUNT(*), 0)"
    format: currency
    description: "Average order value"
  
  active_customers:
    type: count_distinct
    expression: "{customers.customer_id}"
    filters:
      - "{customers.status} = 'active'"
    description: "Number of active customers"
```

Notice the curly braces around column references like `{customers.amount}`? That's the semantic reference syntax. We'll talk more about that in the best practices section.

Measures can have filters (like `active_customers` above), which let you calculate metrics on subsets of data. They can also have formatting hints (like `currency`) to help visualization tools display them correctly.

## Segments

Segments are reusable filter conditions that answer "which ones?" questions. They define meaningful subsets of your data that you can use across multiple queries and metrics.

Segments are saved filters. Instead of writing `WHERE status = 'active'` every time, you define an `active_customers` segment once and reuse it:

```yaml
segments:
  active_customers:
    expression: "{customers.status} = 'active'"
    description: "Customers with active subscriptions"
  
  high_value:
    expression: "{customers.total_spent} > 10000"
    description: "Customers who spent over $10K"
  
  recent_signups:
    expression: "{customers.signup_date} >= CURRENT_DATE - INTERVAL '30 days'"
    description: "Customers who signed up in last 30 days"
```

Segments make your semantic layer more consistent, everyone uses the same definition of "active customers" or "high value," so there's no confusion about what those terms mean.

## Joins

Joins define relationships between semantic models. They're what enable cross-model analysis, like combining order data with customer data or product data.

You define the relationship type (`one_to_one`, `one_to_many`, `many_to_one`) and the join expression:

```yaml
joins:
  customers:
    type: many_to_one
    expression: "{orders.customer_id} = {customers.customer_id}"
    description: "Order's customer"
  
  products:
    type: many_to_one
    expression: "{orders.product_id} = {products.product_id}"
    description: "Ordered product"
```

The relationship type helps Vulcan understand the cardinality, which is important for aggregations and preventing double-counting. The expression is the actual SQL join condition, using semantic references with curly braces.

## Cross-model analysis

Once you've defined joins, you can reference columns and measures from other models in your current model's definitions. This is where semantic models really shine, you can build complex analytical definitions that span multiple models.

### Referencing joined model fields

You can use columns from joined models in measure expressions and filters:

```yaml
measures:
  enterprise_revenue:
    type: sum
    expression: "{orders.amount}"
    filters:
      - "{customers.customer_tier} = 'Enterprise'"
    description: "Revenue from Enterprise customers"
```

Even though `enterprise_revenue` is defined on the `orders` model, it filters by `customers.customer_tier` from the joined `customers` model. Vulcan handles the join logic automatically.

### Proxy dimensions

Proxy dimensions let you expose measures from joined models as dimensions on the current model. This is useful when you want to filter or group by aggregated values from other models:

```yaml
dimensions:
  proxies:
    - name: plan_average_monthly_price
      measure: subscription_plans.avg_monthly_price
    
    - name: plan_average_annual_price
      measure: subscription_plans.avg_annual_price
```

**Requirements:**

- The referenced model must be joined to the current model

- The measure must exist on the target model

- Use the format `model_alias.measure_name`

Proxy dimensions are powerful, they let you analyze one model using aggregated values from another model, all without writing complex SQL.

## Complete example

Here's a complete semantic model definition that brings it all together:

```yaml
models:
  analytics.customers:
    alias: customers
    
    dimensions:
      excludes:
        - password_hash

        - internal_notes
      enhancements:
        - name: signup_date
          granularities:
            - name: monthly
              interval: "1 month"
              description: "Monthly signup cohorts"
            - name: quarterly
              interval: "3 months"
              description: "Quarterly signup cohorts"
    
    measures:
      total_customers:
        type: count
        expression: "{customers.customer_id}"
        description: "Total number of customers"
      
      active_customers:
        type: count_distinct
        expression: "{customers.customer_id}"
        filters:
          - "{customers.status} = 'active'"
        description: "Number of active customers"
    
    segments:
      active:
        expression: "{customers.status} = 'active'"
        description: "Active customers"
      
      high_value:
        expression: "{customers.total_spent} > 10000"
        description: "High-value customers"
    
    joins:
      orders:
        type: one_to_many
        expression: "{customers.customer_id} = {orders.customer_id}"
        description: "Customer's orders"
```

This semantic model:
- Exposes customer dimensions (with some exclusions and enhancements)

- Defines customer measures (total and active counts)

- Creates reusable segments (active and high-value customers)

- Joins to orders for cross-model analysis

## Best practices

### Use business-friendly aliases

Your aliases should make sense to business users, not just developers:

```yaml
# Good: Consumer-friendly
alias: customers
alias: orders
alias: subscriptions

# Bad: Technical naming
alias: dim_customers
alias: fact_orders
```

The whole point of semantic models is to hide technical complexity. Don't bring it back with technical naming!

### Design models with semantics in mind

When you're building your Vulcan models, think about how they'll be used semantically:

```sql
-- Good: Clean column names, business-friendly
MODEL (name analytics.customers);
SELECT
  customer_id,
  customer_tier,      -- Good dimension name
  signup_date,        -- Good time dimension
  total_spent         -- Good for segments
FROM raw.customers;
```

Clean, descriptive column names make semantic models easier to build and use. Avoid abbreviations and technical jargon.

### Document business logic

Add descriptions and metadata to help people understand what measures and segments mean:

```yaml
measures:
  total_revenue:
    type: sum
    expression: "{orders.amount}"
    description: "Total revenue from all completed orders"
    meta:
      business_owner: "Finance Team"
      calculation_method: "Sum of order amounts excluding refunds"
```

The `meta` section is perfect for business context, ownership, calculation details, and other information that helps people understand and trust the metric.

### Use curly braces for references

When referencing any column or measure anywhere in your semantic model definitions, always use curly braces `{}`:

```yaml
# Good: Use curly braces for all references
measures:
  total_revenue:
    type: sum
    expression: "{orders.amount}"  # Column reference with curly braces
  
  active_customers:
    type: count_distinct
    expression: "{customers.customer_id}"  # Column reference with curly braces
    filters:
      - "{customers.status} = 'active'"  # Column reference in filter

segments:
  high_value:
    expression: "{customers.total_spent} > 10000"  # Column reference with curly braces

joins:
  customers:
    type: many_to_one
    expression: "{orders.customer_id} = {customers.customer_id}"  # Both references use curly braces
```

**Why use curly braces?**
- Clear distinction between semantic references and SQL functions

- Consistent syntax across all semantic model definitions

- Prevents ambiguity in complex expressions

- Required for cross-model references (e.g., `{customers.customer_tier}`)

It's a best practice that makes your semantic models more maintainable and less error-prone.

## Validation

Vulcan automatically validates semantic model definitions when you create a plan. It checks:

- All column references in measures exist

- All column references in segments exist

- Join expressions reference valid columns

- Cross-model references have valid join paths

- Semantic aliases are properly defined

If something's wrong, you'll know about it before you try to use the semantic layer. This catches errors early and keeps your semantic models reliable.

## Next steps

- Learn about [Business Metrics](./business_metrics.md) that combine measures with time and dimensions

- Explore semantic model examples in your project's `semantics/` directory

- See the [Semantics Overview](overview.md) for the complete picture



# Overview

Source: https://tmdc-io.github.io/vulcan-book/components/semantics/overview/

---

# Overview

The semantic layer translates technical data models into business-friendly interfaces. It takes your SQL tables and makes them understandable to people who don't write SQL, turning `analytics.daily_revenue_metrics` into "Monthly Revenue by Customer Tier" that anyone can query.

Your models are the engine (they do the work), and the semantic layer is the dashboard (it makes things usable). It adds business context, consistent definitions, and a friendly interface so people can use your data.

## What is the Semantic Layer?

The semantic layer bridges the gap between "here's a table with columns" and "here's what this means for the business." It provides a consistent, business-friendly interface to your data that enables self-service analytics while keeping a single source of truth for your business logic.

Without a semantic layer, every time someone wants to analyze revenue, they have to remember which table has it, what the column is called, how to join it with other tables, and how to calculate it correctly. With a semantic layer, they ask for "revenue" and it works.

### Key Benefits

The semantic layer helps everyone in your organization work with data more effectively:

**For Developers:**

- **Define metrics once, use everywhere** - Write the calculation once, use it in dashboards, APIs, and reports

- **Version-controlled business logic** - Your metric definitions live in code, so changes are tracked and reviewable

- **Consistent calculations** - No more "which revenue calculation should I use?", there's one definition

**For Business Users:**

- **Self-service analytics** - Query data without writing SQL (or even knowing what SQL is)

- **Consistent metric definitions** - Everyone uses the same definition of "revenue" or "active users"

- **Trusted, validated data** - Metrics are defined by the data team, so you know they're correct

- **Works everywhere** - Use the same metrics in Tableau, Power BI, Python, or APIs

**For Organizations:**

- **Single source of truth** - One place where "revenue" is defined, not scattered across 20 different dashboards

- **Faster time to insights** - Business users can answer questions themselves instead of waiting for the data team

- **Reduced data team bottleneck** - Less "can you build me a dashboard?" requests

- **Better data governance** - Centralized definitions make it easier to audit and maintain data quality

## Core Components

The semantic layer has two main pieces that work together. Think of them as building blocks, you start with semantic models, then build metrics on top.

### Semantic Models

Semantic models are like wrappers around your Vulcan models. They take your technical tables and expose them in a business-friendly way. For detailed information, check out the [Semantic Models](models.md) documentation.

Here's what semantic models do:

- **Map physical models** - Reference your Vulcan models from the `models/` directory

- **Expose dimensions** - All model columns automatically become dimensions (things you can filter and group by)

- **Define measures** - Aggregated calculations like `SUM(amount)` or `COUNT(*)`

- **Create segments** - Reusable filter conditions (like "high-value customers" or "active users")

- **Establish joins** - Relationships between models so you can analyze across tables

Here's a simple example:

```yaml
models:
  analytics.customers:
    alias: customers
    measures:
      total_customers:
        type: count
        expression: "COUNT(*)"
```

This takes your `analytics.customers` model and exposes a `total_customers` measure that anyone can use. Business users can query "total customers" without knowing which table it comes from or how to write the SQL.

### Business Metrics

Business metrics combine measures with dimensions and time to create complete analytical definitions. They're like pre-built queries that are ready to use. Learn more in the [Business Metrics](./business_metrics.md) guide.

Here's what makes metrics powerful:

- **Time-series analysis** - Metrics include time dimensions so you can see trends over time

- **Flexible granularity** - Query the same metric at different time intervals (day, week, month, etc.)

- **Multi-dimensional** - Slice and dice by business attributes (customer tier, region, product category, etc.)

- **Ready for dashboards** - Pre-configured for visualization tools

Here's what a metric looks like:

```yaml
metrics:
  monthly_revenue:
    measure: orders.total_revenue
    time: orders.order_date
    dimensions:
      - customers.customer_tier
```

This creates a `monthly_revenue` metric that:

- Uses the `total_revenue` measure from the orders model

- Groups by `order_date` (time dimension)

- Can be sliced by `customer_tier` (business dimension)

Anyone can query "monthly revenue by customer tier" without writing SQL. They reference the metric name, and Vulcan handles the complexity.

## How It Works

Setting up your semantic layer is straightforward. Here's the workflow:

1. **Define semantic models** - Create YAML files that reference your Vulcan models
2. **Add measures and dimensions** - Define what can be calculated and filtered
3. **Create joins** - Connect models so you can analyze across tables
4. **Define metrics** - Combine measures with time and dimensions for analysis
5. **Validate** - Vulcan automatically validates your semantic definitions when you create a plan
6. **Query** - Use the semantic layer via APIs or export to BI tools

The validation step is important, Vulcan checks that your measures reference real columns, joins are valid, and metrics make sense. It'll catch errors before you try to use them, which saves you from debugging issues later.

## File Organization

Semantic layer definitions are YAML files in the `semantics/` directory. You can organize them however makes sense for your team:

```
project/
├── models/           # Vulcan data models (.sql files)
│   ├── customers.sql
│   ├── orders.sql
│   └── events.sql
│
├── semantics/        # Semantic layer definitions (YAML)
│   ├── customers.yml
│   ├── orders.yml
│   └── metrics.yml
│
└── config.yaml
```

**File naming:** The filename doesn't matter, Vulcan automatically merges all YAML files in the `semantics/` directory. Organize by domain (like `customers.yml`, `orders.yml`) or by model (like `revenue_metrics.yml`), whatever makes sense for your team.

## Integration with Models

Here's the key insight: **Model columns automatically become dimensions.** The semantic layer adds measures, segments, joins, and metrics on top of your existing models. It builds on what you already have, it doesn't replace anything.

When you're designing Vulcan models, keep the semantic layer in mind:

```sql
-- Good: Clean column names, business-friendly
MODEL (name analytics.customers);
SELECT
  customer_id,
  customer_tier,      -- Good dimension name (can filter/group by this)
  signup_date,        -- Good time dimension (can analyze trends)
  total_spent         -- Good for segments (can create "high-value" segment)
FROM raw.customers;
```

This model will automatically expose:

- `customer_tier` as a dimension (filter by tier, group by tier)

- `signup_date` as a time dimension (analyze trends over time)

- `total_spent` as a dimension (create segments like "high-value customers")

Then you can add measures and metrics on top. The semantic layer builds on your models, it doesn't replace them. Your models stay exactly as they are; the semantic layer just makes them more accessible.

## Next Steps

Next steps:

- **[Semantic Models](models.md)** - Map physical models to business concepts

- **[Business Metrics](./business_metrics.md)** - Create time-series analytical definitions

- **[Transpiling Semantic Queries](../../guides/transpiling_semantics.md)** - See how semantic queries get converted to SQL

- **Check your project** - Look at the `semantics/` directory in your Vulcan project for examples

The semantic layer makes your data accessible to everyone, not just SQL experts. Start with semantic models, add measures, then build metrics.



# Tests

Source: https://tmdc-io.github.io/vulcan-book/components/tests/tests/

---

# Tests

Tests are your safety net for data transformations. Just like software engineers write unit tests to catch bugs before they ship, you can write tests to verify that your models transform data correctly, catching problems before they reach production and cause headaches.

Tests are executable documentation. They show exactly how your model should behave with specific inputs, and they fail if something changes unexpectedly. Unlike [audits](../audits/audits.md) (which check data quality at runtime), tests verify the logic of your models against predefined inputs and expected outputs.

## Why Testing Matters

Data models are tricky beasts. Small errors can snowball into significant business impacts. A small change in one model can cascade into big problems downstream. Here's why testing is worth your time:

- **Catch breaking changes** - Refactor with confidence knowing tests will flag unintended behavior changes

- **Document expected behavior** - Tests serve as executable specifications (better than comments that get outdated!)

- **Faster debugging** - When something breaks, tests pinpoint exactly which transformation failed

- **Data quality assurance** - Verify that aggregations, joins, and calculations produce correct results

- **Confidence in changes** - Make updates knowing you'll catch regressions before they hit production

Tests run either on demand (like in CI/CD pipelines) or automatically when you create a new [plan](../../guides/plan.md).

## Creating Tests

Tests live in YAML files in the `tests/` folder of your project. The filename must start with `test` and end with `.yaml` or `.yml`. You can put multiple tests in one file (organize them however makes sense).

At minimum, a test needs three things:

- **model** - Which model you're testing

- **inputs** - Mock data for upstream dependencies (what goes in)

- **outputs** - Expected results from the model's query (what should come out)

Let's start with a simple example.

### Your First Test

Here's a model that aggregates orders by date:

```sql linenums="1"
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date
);

SELECT
  CAST(order_date AS TIMESTAMP) AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

Now let's write a test to verify it works correctly:

```yaml linenums="1"
test_daily_sales_aggregation:
  model: sales.daily_sales
  description: >
    Test that daily_sales correctly aggregates orders by date.

  inputs:
    raw.raw_orders:
      rows:
        - order_id: O001
          order_date: '2024-03-15'
          customer_id: C001
          product_id: P001
          total_amount: 50.00
        - order_id: O002
          order_date: '2024-03-15'
          customer_id: C002
          product_id: P002
          total_amount: 75.00
        - order_id: O003
          order_date: '2024-03-16'
          customer_id: C001
          product_id: P003
          total_amount: 100.00

  outputs:
    query:
      rows:
        - order_date: "2024-03-15"
          total_orders: 2
          total_revenue: 125.00
          last_order_id: "O002"
        - order_date: "2024-03-16"
          total_orders: 1
          total_revenue: 100.00
          last_order_id: "O003"
```

This test gives the model three orders (two on March 15, one on March 16) and checks that:

- Orders are correctly grouped by date

- `total_orders` counts distinct orders per day (should be 2 for March 15, 1 for March 16)

- `total_revenue` sums the amounts correctly (50 + 75 = 125 for March 15)

- `last_order_id` returns the maximum order ID per day (O002 for March 15, O003 for March 16)

If any of these expectations don't match, the test fails and tells you what went wrong.

### Testing Models with Multiple Dependencies

Real-world models often join multiple tables. Here's how you'd test a more complex model that joins customers, orders, and order items:

```yaml linenums="1"
test_full_model_basic:
  model: vulcan_demo.full_model
  description: |
    Validates aggregates and averages:
    - DISTINCT order counting

    - SUM(quantity * unit_price)

    - avg_order_value = total_spent / total_orders, or NULL when total_orders = 0

  inputs:
    vulcan_demo.customers:
      - customer_id: 1
        name: Alice
        email: alice@example.com
      - customer_id: 2
        name: Bob
        email: bob@example.com
      - customer_id: 3
        name: Charlie
        email: charlie@example.com

    vulcan_demo.orders:
      # Alice has 2 orders
      - order_id: 1001
        customer_id: 1
      - order_id: 1002
        customer_id: 1
      # Bob has 1 order
      - order_id: 2001
        customer_id: 2
      # Charlie has 0 orders (no rows)

    vulcan_demo.order_items:
      # Order 1001: 2*50 + 1*25 = 125
      - order_id: 1001
        product_id: 501
        quantity: 2
        unit_price: 50
      - order_id: 1001
        product_id: 502
        quantity: 1
        unit_price: 25
      # Order 1002: 1*200 = 200 → Alice total = 325
      - order_id: 1002
        product_id: 503
        quantity: 1
        unit_price: 200
      # Order 2001: 2*5 = 10 → Bob total = 10
      - order_id: 2001
        product_id: 504
        quantity: 2
        unit_price: 5

  outputs:
    query:
      rows:
        - customer_id: 1
          customer_name: Alice
          email: alice@example.com
          total_orders: 2
          total_spent: 325
          avg_order_value: 162.5
        - customer_id: 2
          customer_name: Bob
          email: bob@example.com
          total_orders: 1
          total_spent: 10
          avg_order_value: 10.0
        - customer_id: 3
          customer_name: Charlie
          email: charlie@example.com
          total_orders: 0
          total_spent: 0
          avg_order_value: null  # Division by zero handled
```

Notice how we're providing mock data for all three upstream tables. The test verifies that the model correctly:

- Joins customers with orders and order items

- Counts distinct orders per customer

- Calculates total spent (quantity × unit_price summed across all items)

- Handles division by zero (Charlie has no orders, so avg_order_value should be NULL)

The comments in the YAML help explain the test data, which makes it easier to understand what's being tested.

### Testing Incremental Models

Incremental models are a bit special because they filter data by time range. You'll need to set `start` and `end` dates using the `vars` attribute:

```yaml linenums="1"
test_incremental_by_time_range_basic:
  model: vulcan_demo.incremental_by_time_range
  description: |
    Validates per-(order_date, product_id) aggregates over a fixed two-day window.
    Checks DISTINCT order counts, quantity and revenue sums, and AVG(unit_price).
  vars:
    start: '2025-01-01'
    end: '2025-01-02'

  inputs:
    vulcan_demo.products:
      - product_id: 10
        name: Widget
        category: Electronics
      - product_id: 20
        name: Gizmo
        category: Home

    vulcan_demo.orders:
      - order_id: 1001
        customer_id: 9001
        warehouse_id: 1
        order_date: '2025-01-01'
      - order_id: 1002
        customer_id: 9002
        warehouse_id: 1
        order_date: '2025-01-01'
      - order_id: 1003
        customer_id: 9003
        warehouse_id: 2
        order_date: '2025-01-02'

    vulcan_demo.order_items:
      # 2025-01-01
      - order_id: 1001
        product_id: 10
        quantity: 2
        unit_price: 50
      - order_id: 1001
        product_id: 20
        quantity: 1
        unit_price: 200
      - order_id: 1002
        product_id: 10
        quantity: 1
        unit_price: 60
      # 2025-01-02
      - order_id: 1003
        product_id: 10
        quantity: 5
        unit_price: 40

  outputs:
    query:
      rows:
        - order_date: '2025-01-01'
          product_id: 20
          product_name: Gizmo
          category: Home
          order_count: 1
          total_quantity: 1
          total_sales_amount: 200
          avg_unit_price: 200
        - order_date: '2025-01-01'
          product_id: 10
          product_name: Widget
          category: Electronics
          order_count: 2
          total_quantity: 3
          total_sales_amount: 160
          avg_unit_price: 55
        - order_date: '2025-01-02'
          product_id: 10
          product_name: Widget
          category: Electronics
          order_count: 1
          total_quantity: 5
          total_sales_amount: 200
          avg_unit_price: 40
```

The `vars` section tells Vulcan what time range to use when running the model. This is important because incremental models filter by `@start_ds` and `@end_ds` macros, and you need to control those in your test.

### Testing CTEs

You can also test individual CTEs (Common Table Expressions) within your model. This is useful for debugging complex queries step by step.

Say you have a model with a CTE:

```sql linenums="1"
WITH filtered_orders_cte AS (
  SELECT id, item_id
  FROM vulcan_demo.incremental_model
  WHERE item_id = 1
)
SELECT
  item_id,
  COUNT(DISTINCT id) AS num_orders
FROM filtered_orders_cte
GROUP BY item_id
```

You can test both the CTE and the final query:

```yaml linenums="1"
test_model_with_cte:
  model: vulcan_demo.full_model
  inputs:
    vulcan_demo.incremental_model:
      rows:
        - id: 1
          item_id: 1
        - id: 2
          item_id: 1
        - id: 3
          item_id: 2
  outputs:
    ctes:
      filtered_orders_cte:
        rows:
          - id: 1
            item_id: 1
          - id: 2
            item_id: 1
    query:
      rows:
        - item_id: 1
          num_orders: 2
```

This verifies that:

1. The CTE correctly filters to `item_id = 1` (should return rows with id 1 and 2)
2. The final query correctly counts distinct orders (should be 2)

Testing CTEs separately makes it easier to pinpoint where things go wrong in complex queries.

## Supported Data Formats

Vulcan gives you flexibility in how you define test data. Pick whatever format works best for your situation:

### YAML Dictionaries (Default)

The most common format, just list your rows as YAML dictionaries:

```yaml linenums="1"
inputs:
  vulcan_demo.orders:
    rows:
      - order_id: 1001
        customer_id: 1
        order_date: '2025-01-01'
```

This works well for small datasets and when you want everything in one place.

### CSV Format

If you have lots of data, CSV might be easier to read and write:

```yaml linenums="1"
inputs:
  vulcan_demo.orders:
    format: csv
    rows: |
      order_id,customer_id,order_date
      1001,1,2025-01-01
      1002,2,2025-01-01
```

You can also customize CSV parsing with `csv_settings` if you need different separators or other options.

### SQL Queries

Sometimes you want more control over how data is generated. Use a SQL query:

```yaml linenums="1"
inputs:
  vulcan_demo.orders:
    query: |
      SELECT 1001 AS order_id, 1 AS customer_id, '2025-01-01' AS order_date
      UNION ALL
      SELECT 1002 AS order_id, 2 AS customer_id, '2025-01-01' AS order_date
```

This is useful when you need to generate test data programmatically or when the data structure is complex.

### External Files

For large test datasets, store them in separate files:

```yaml linenums="1"
inputs:
  vulcan_demo.orders:
    format: csv
    path: fixtures/orders_test_data.csv
```

This keeps your test files clean and makes it easy to reuse test data across multiple tests.

## Omitting Columns

For wide tables, you don't need to specify every column. You can omit columns (they'll be treated as `NULL`) or use partial matching to only test the columns you care about:

```yaml linenums="1"
outputs:
  query:
    partial: true  # Only test specified columns
    rows:
      - customer_id: 1
        total_spent: 325
```

This is useful when you have a table with 50 columns but only care about testing a few of them.

**Apply partial matching globally:**

```yaml linenums="1"
outputs:
  partial: true
  query:
    rows:
      - customer_id: 1
        total_spent: 325
```

This applies partial matching to all outputs in the test, which is convenient when you're only testing a subset of columns.

## Freezing Time

If your model uses `CURRENT_TIMESTAMP` or similar functions, you'll want to freeze time in your tests to make them deterministic. Otherwise, your tests will fail every time you run them because the timestamp changes!

```yaml linenums="1"
test_with_timestamp:
  model: vulcan_demo.audit_log
  outputs:
    query:
      - event: "login"
        created_at: "2023-01-01 12:05:03"
  vars:
    execution_time: "2023-01-01 12:05:03"
```

Setting `execution_time` in `vars` makes `CURRENT_TIMESTAMP` and `CURRENT_DATE` return fixed values, so your tests are predictable and repeatable.

## Running Tests

### Command Line

Run tests from the command line:

```bash
# Run all tests
vulcan test

# Run specific test file
vulcan test tests/test_daily_sales.yaml

# Run specific test
vulcan test tests/test_daily_sales.yaml::test_daily_sales_aggregation

# Run tests matching a pattern
vulcan test tests/test_*
```

The `::` syntax lets you run a specific test from a file when debugging a single failing test.

### Example Output

When tests pass, you'll see something like:

```
$ vulcan test
..
----------------------------------------------------------------------
Ran 2 tests in 0.024s

OK
```

The dots (`.`) indicate passing tests. Simple and clean!

**When tests fail:**

```
$ vulcan test
F
======================================================================
FAIL: test_daily_sales_aggregation (tests/test_daily_sales.yaml)
----------------------------------------------------------------------
AssertionError: Data mismatch (exp: expected, act: actual)

  total_orders
         exp  act
0        3.0  2.0

----------------------------------------------------------------------
Ran 1 test in 0.012s

FAILED (failures=1)
```

The output shows you exactly what didn't match. In this case, `total_orders` was expected to be 3.0 but was actually 2.0. This tells you exactly what to investigate.

## Automatic Test Generation

Writing tests can be tedious, especially when you're just getting started. Vulcan can help by generating tests automatically:

```bash
vulcan create_test vulcan_demo.daily_sales \
  --query raw.raw_orders "SELECT * FROM raw.raw_orders WHERE order_date BETWEEN '2025-01-01' AND '2025-01-02' LIMIT 10" 
```

This creates a test file with actual data from your warehouse, which makes it easy to bootstrap your test suite. You can then tweak the generated test to match your needs.

**Pro tip:** Start with generated tests, then refine them to test edge cases and specific scenarios. It's much faster than writing everything from scratch!

## Troubleshooting

### Preserving Fixtures

When a test fails, you might want to inspect the actual data that was created. Use `--preserve-fixtures` to keep test fixtures around:

```bash
vulcan test --preserve-fixtures
```

Fixtures are created as views in a schema named `vulcan_test_<random_ID>`. You can query these views directly to see what data was actually produced for debugging.

### Type Mismatches

Sometimes Vulcan can't figure out the correct types for your test data. If you're seeing type errors, specify them explicitly:

```yaml linenums="1"
inputs:
  vulcan_demo.orders:
    columns:
      order_id: INT
      order_date: DATE
      total_amount: DECIMAL(10,2)
    rows:
      - order_id: 1001
        order_date: '2025-01-01'
        total_amount: 99.99
```

The `columns` section tells Vulcan exactly what types to use, which helps avoid type inference issues. You can also explicitly cast columns in your model's query to help Vulcan infer types more accurately.

### Test Not Finding Model

**Problem:** Test says it can't find the model.

**Solution:** Make sure the model name in your test matches exactly what's in your `models/` folder. Model names are case-sensitive and must include the schema (like `sales.daily_sales`, not just `daily_sales`).

### Output Order Matters

**Problem:** Test fails even though the data looks correct.

**Solution:** The columns in your expected output must appear in the same order as they're selected in the model's query. Check the `SELECT` statement order and make sure your test rows match.

### Partial Matching Not Working

**Problem:** Partial matching isn't ignoring extra columns.

**Solution:** Make sure you set `partial: true` at the right level. It needs to be under `outputs.query` (or `outputs.ctes.<cte_name>`) for CTE-specific partial matching, or under `outputs` for global partial matching.

## Test Structure Reference

Here's a complete reference of all the fields you can use in a test. Most tests only need `model`, `inputs`, and `outputs`, but it's good to know what else is available.

### `<test_name>`

The unique name of your test. Use descriptive names that explain what you're testing, like `test_daily_sales_aggregation` or `test_customer_revenue_calculation`.

### `<test_name>.model`

The fully qualified name of the model being tested (like `sales.daily_sales`). This model must exist in your project's `models/` folder.

### `<test_name>.description`

An optional description that explains what the test validates. This is helpful for your teammates (and future you) to understand what the test is checking.

### `<test_name>.schema`

The name of the schema that will contain the test fixtures (the views created for this test). If not specified, Vulcan creates a temporary schema.

### `<test_name>.gateway`

The gateway whose `test_connection` will be used to run this test. If not specified, the default gateway is used. Useful when you need to test against a specific database or engine.

### `<test_name>.inputs`

Mock data for upstream models that your target model depends on. If your model has no dependencies, you can omit this.

### `<test_name>.inputs.<upstream_model>`

A model that your target model depends on. Provide mock data for each upstream model.

### `<test_name>.inputs.<upstream_model>.rows`

The rows of test data, defined as an array of dictionaries:

```yaml linenums="1"
    <upstream_model>:
      rows:
        - <column_name>: <column_value>
        ...
```

**Shortcut:** If `rows` is the only key, you can omit it:

```yaml linenums="1"
    <upstream_model>:
      - <column_name>: <column_value>
      ...
```

### `<test_name>.inputs.<upstream_model>.format`

The format of the input data. Options: `yaml` (default) or `csv`.

```yaml linenums="1"
    <upstream_model>:
      format: csv
```

### `<test_name>.inputs.<upstream_model>.csv_settings`

When using CSV format, customize how the CSV is parsed:

```yaml linenums="1"
    <upstream_model>:
      format: csv
      csv_settings: 
        sep: "#"
        skip_blank_lines: true
      rows: |
        <column1_name>#<column2_name>
        <row1_value>#<row1_value>
```

See [pandas read_csv documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) for all supported settings.

### `<test_name>.inputs.<upstream_model>.path`

Load data from an external file:

```yaml linenums="1"
    <upstream_model>:
      path: filepath/test_data.yaml
```

### `<test_name>.inputs.<upstream_model>.columns`

Explicitly specify column types to help Vulcan interpret your data correctly:

```yaml linenums="1"
    <upstream_model>:
      columns:
        <column_name>: <column_type>
        ...
```

This is especially useful when Vulcan can't infer types correctly (like with dates or decimals).

### `<test_name>.inputs.<upstream_model>.query`

Generate input data using a SQL query:

```yaml linenums="1"
    <upstream_model>:
      query: <sql_query>
```

**Note:** You can't use `query` together with `rows`, pick one or the other.

### `<test_name>.outputs`

The expected outputs from your model. This is what you're asserting should be true.

**Important:** Column order matters! The columns in your expected rows must match the order they appear in the model's `SELECT` statement.

### `<test_name>.outputs.partial`

When `true`, only test the columns you specify. Extra columns in the output are ignored. Useful for wide tables where you only care about a few columns.

### `<test_name>.outputs.query`

The expected output of the model's final query. This is optional if you're testing CTEs instead.

### `<test_name>.outputs.query.partial`

Same as `outputs.partial`, but applies only to the query output (not CTEs).

### `<test_name>.outputs.query.rows`

The expected rows from the model's query. Same format as input rows.

### `<test_name>.outputs.query.query`

Generate expected output using a SQL query. Useful when the expected output is complex or when you want to compute it dynamically.

### `<test_name>.outputs.ctes`

Test individual CTEs within your model. This is optional if you're testing the final query output.

### `<test_name>.outputs.ctes.<cte_name>`

The expected output of a specific CTE. Use this to test intermediate steps in complex queries.

### `<test_name>.outputs.ctes.<cte_name>.partial`

Partial matching for a specific CTE.

### `<test_name>.outputs.ctes.<cte_name>.rows`

Expected rows for a specific CTE.

### `<test_name>.outputs.ctes.<cte_name>.query`

Generate expected CTE output using a SQL query.

### `<test_name>.vars`

Set values for macro variables used in your model:

```yaml linenums="1"
  vars:
    start: 2022-01-01
    end: 2022-01-01
    execution_time: 2022-01-01
    <macro_variable_name>: <macro_variable_value>
```

**Special variables:**

- `start` - Overrides `@start_ds` for incremental models

- `end` - Overrides `@end_ds` for incremental models  

- `execution_time` - Overrides `@execution_ds` and makes `CURRENT_TIMESTAMP`/`CURRENT_DATE` return fixed values

These are useful for testing incremental models and making time-dependent tests deterministic.



# Serialization

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/architecture/serialization/

---

# Serialization

Vulcan executes Python code through [macros](../../components/advanced-features/macros/overview.md) and [Python models](components/model/types/python_models.md). Each Python model is stored as a standalone [snapshot](../architecture/snapshots.md), which includes all of the Python code necessary to generate it.

## Serialization format

Rather than using Python's `pickle` format, Vulcan has its own serialization format. This is because `pickle` is not compatible across Python versions, and would, for example, prevent you from developing on Python 3.9 and then running Python 3.10 in production.

Instead, Vulcan stores the string representation of your Python implementation and then re-evaluates it. Given a custom Python function or macro, Vulcan reads the Abstract Syntax Tree (AST) of the function and converts that into a string representation, along with all dependencies and global variables. For more information, refer to [snapshot fingerprinting](../architecture/snapshots.md#fingerprinting).

### Limitations

Vulcan only serializes the Python code you write and does not include libraries, which means the module of your code must match your Vulcan config path. In addition, any references to libraries will be converted to imports, so you must ensure that any libraries you are using are installed everywhere that Vulcan is running.



# Snapshots

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/architecture/snapshots/

---

# Snapshots
A snapshot is a record of a model at a given time. Along with a copy of the model, a snapshot contains everything needed to evaluate the model and render its query. This allows Vulcan to have a consistent view of your project's history and its data as the project and its models evolve and change. Since model queries can have macros, each snapshot stores a copy of all macro definitions and global variables at the time the snapshot is taken. Additionally, snapshots store the intervals of time for which they have data.

## Fingerprinting
Snapshots have unique fingerprints that are derived from their models. Vulcan uses these fingerprints to determine when existing tables can be reused, or whether a backfill is needed as a model's query has changed.

Because Vulcan can understand SQL with SQLGlot, it can generate fingerprints such that superficial changes to a model, such as applying formatting to its query, will not return a new fingerprint.

## Change categories
Refer to [change categories](../../guides/plan.md#change-categories).



# environments

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/environments/

---

## Environments
Environments are isolated namespaces that allow you to test and preview your changes.

Vulcan differentiates between production and development environments. Currently, only the environment with the name `prod` is treated by Vulcan as the production one. Environments with other names are considered to be development ones.

[Models](components/model/overview.md) in development environments get a special suffix appended to the schema portion of their names. For example, to access data for a model with name `db.model_a` in the target environment `my_dev`, the `db__my_dev.model_a` table name should be used in a query. Models in the production environment are referred to by their original names.

## Why use environments
Data pipelines and their dependencies tend to grow in complexity over time, and so assessing the impact of local changes can become quite challenging. Pipeline owners may not be aware of all downstream consumers of their pipelines, or may drastically underestimate the impact a change would have. That's why it is so important to be able to iterate and test model changes using production dependencies and data, while simultaneously avoiding any impact to existing datasets or pipelines that are currently used in production. Recreating the entire data warehouse with given changes would be an ideal solution to fully understand their impact, but this process is usually excessively expensive and time consuming.

Vulcan environments allow you to easily spin up shallow 'clones' of the data warehouse quickly and efficiently. Vulcan understands which models have changed compared to the target environment, and only computes data gaps that have been directly caused by the changes. Any changes or backfills within the target environment **do not impact** other environments. At the same time, any computation that was done in this environment **can be safely reused** in other environments.

## How to use environments
When running the [plan](guides/plan.md) command, the environment name can be supplied in the first argument. An arbitrary string can be used as an environment name. The only special environment name by default is `prod`, which refers to the production environment. Environment with names other than `prod` are considered to be development environments.

By default, the [`vulcan plan`](guides/plan.md) command targets the production (`prod`) environment.

### Example
A custom name can be provided as an argument to create or update a development environment. For example, to target an environment with name `my_dev`, run:

```bash
vulcan plan my_dev
```
A new environment is created automatically the first time a plan is applied to it.

## How environments work
Whenever a model definition changes, a new model snapshot is created with a unique [fingerprint](architecture/snapshots.md#fingerprinting). This fingerprint allows Vulcan to detect if a given model variant exists in other environments or if it's a brand new variant. Because models may depend on other models, the fingerprint of a target model variant also includes fingerprints of its upstream dependencies. If a fingerprint already exists in Vulcan, it is safe to reuse the existing physical table associated with that model variant, since we're confident that the logic that populates that table is exactly the same. This makes an environment a collection of references to model [snapshots](architecture/snapshots.md).

Refer to [plans](guides/plan.md#plan-application) for additional details.

## Date range
A development environment includes a start date and end date. When creating a development environment, the intent is usually to test changes on a subset of data. The size of such a subset is determined by a time range defined through the start and end date of the environment. Both start and end date are provided during the [plan](guides/plan.md) creation.



# Glossary

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/glossary/

---

# Glossary

## Abstract Syntax Tree
A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.

## Backfill
Load or refresh model data, triggered by a vulcan plan command.

## Catalog
A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.

## CI/CD
An engineering process that combines both Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests) in a manner that is scalable, reliable, and secure. Vulcan accomplishes this with [tests](components/tests/tests.md) and [audits](components/audits/audits.md).

## CTE
A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, refer to [tests](components/tests/tests.md).

## DAG
Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.

## Data modeling
Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.

## Data pipeline
The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.

## Data transformation
Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.

## Data warehouse
The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.

## Direct Modification
A change to a model's definition from the user instead of being inherited from an upstream dependency like [Indirect Modification](#indirect-modification).

## ELT
Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.

## ETL
Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.

## Full refresh
In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.

## Idempotency
The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.

## Incremental Loads
Incremental loads are a type of data refresh that only updates the data that has changed since the last refresh. This is significantly faster and more efficient than a full refresh loads. Vulcan encourages developers to incrementally load when possible by offering easy to use variables and macros to help define your incremental models. See [Model Kinds](components/model/model_kinds.md) for more information.

## Indirect Modification
A change to model's upstream dependency and not to the model itself like a [Direct Modification](#direct-modification).

## Integration
Combining data from various sources (such as from a data warehouse) into one unified view.

## Lineage
The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.

## Physical Layer
The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the [Vulcan virtual layer's](#virtual-layer) views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.

## Plan Summaries
An upcoming feature that allows users to see a summary of changes applied to a given environment.

## Semantic Understanding
Vulcan, by leveraging [SQLGlot](https://github.com/tobymao/sqlglot), understands the full meaning of a SQL model. That means it can not only validate that what is written is valid SQL but also transpile (convert) that SQL into other engine dialects if needed.

## Slowly Changing Dimension (SCD)
A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.

## Table
A table is the visual representation of data stored in rows and columns.

## User-Defined Function (UDF)
Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.

## View
A view is the result of a SQL query on a database.

## Virtual Environments
Vulcan's unique approach to environment that allows it to provide both environment isolation and the ability to share tables across environments. This is done in a way to ensure data consistency and accuracy. See [plan application](guides/plan.md#plan-application) for more information.

## Virtual Layer
The virtual layer is Vulcan's abstraction layer over the [physical layer and physical data storage](#physical-layer). While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.

## Virtual Update
Term used to describe a plan that can be applied without having to load any additional data or build any additional tables. See [Virtual Update](guides/plan.md#virtual-update) for more information.

## Virtual Preview
Term used to describe the ability to create an environment without having to build any additional tables. By comparing the version of models in the repo against what currently exists, Vulcan can create an environment that exactly represents what is in the repo by just updating views.



# Jinja

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/macros/jinja_macros/

---

# Jinja

Vulcan supports macros from the [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) templating system.

Jinja's macro approach is pure string substitution. Unlike Vulcan macros, they assemble SQL query text without building a semantic representation.

**NOTE:** Vulcan projects support the standard Jinja function library only - they do **not** support dbt-specific jinja functions like `{{ ref() }}`. dbt-specific functions are allowed in dbt projects being run with the Vulcan adapter.

## Basics

Jinja uses curly braces `{}` to differentiate macro from non-macro text. It uses the second character after the left brace to determine what the text inside the braces will do.

The three curly brace symbols are:

- `{{...}}` creates Jinja expressions. Expressions are replaced by text that is incorporated into the rendered SQL query; they can contain macro variables and functions.
- `{%...%}` creates Jinja statements. Statements give instructions to Jinja, such as setting variable values, control flow with `if`, `for` loops, and defining macro functions.
- `{#...#}` creates Jinja comments. These comments will not be included in the rendered SQL query.

Since Jinja strings are not syntactically valid SQL expressions and cannot be parsed as such, the model query must be wrapped in a special `JINJA_QUERY_BEGIN; ...; JINJA_END;` block in order for Vulcan to detect it:

```sql linenums="1" hl_lines="5 9"
MODEL (
  name vulcan_example.full_model
);

JINJA_QUERY_BEGIN;

SELECT {{ 1 + 1 }};

JINJA_END;
```

Similarly, to use Jinja expressions as part of statements that should be evaluated before or after the model query, the `JINJA_STATEMENT_BEGIN; ...; JINJA_END;` block should be used:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model
);

JINJA_STATEMENT_BEGIN;
{{ pre_hook() }}
JINJA_END;

JINJA_QUERY_BEGIN;
SELECT {{ 1 + 1 }};
JINJA_END;

JINJA_STATEMENT_BEGIN;
{{ post_hook() }}
JINJA_END;
```

## Vulcan predefined variables

Vulcan provides multiple [predefined macro variables](../../components/advanced-features/macros/variables.md) you may reference in jinja code.

Some predefined variables provide information about the Vulcan project itself, like the [`runtime_stage`](../../components/advanced-features/macros/variables.md#runtime-variables) and [`this_model`](../../components/advanced-features/macros/variables.md#runtime-variables) variables.

Other predefined variables are [temporal](../../components/advanced-features/macros/variables.md#temporal-variables), like `start_ds` and `execution_date`. They are used to build incremental model queries and are only available in incremental model kinds.

Access predefined macro variables by passing their unquoted name in curly braces. For example, this demonstrates how to access the `start_ds` and `end_ds` variables:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE time_column BETWEEN '{{ start_ds }}' and '{{ end_ds }}';

JINJA_END;
```

Because the two macro variables return string values, we must surround the curly braces with single quotes `'`. Other macro variables, such as `start_epoch`, return numeric values and do not require the single quotes.

The `gateway` variable uses a slightly different syntax than other predefined variables because it is a function call. Instead of the bare name `{{ gateway }}`, it must include parentheses: `{{ gateway() }}`.

## User-defined variables

Vulcan supports two kinds of user-defined macro variables: global and local.

Global macro variables are defined in the project configuration file and can be accessed in any project model.

Local macro variables are defined in a model definition and can only be accessed in that model.

### Global variables

Learn more about defining global variables in the [Vulcan macros documentation](../../components/advanced-features/macros/built_in.md#global-variables).

Access global variable values in a model definition using the `{{ var() }}` jinja function. The function requires the name of the variable _in single quotes_ as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.

For example, a model would access a global variable named `int_var` like this:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE int_variable = {{ var('int_var') }};

JINJA_END;
```

A default value can be passed as a second argument to the `{{ var() }}` jinja function, which will be used as a fallback value if the variable is missing from the configuration file.

In this example, the `WHERE` clause would render to `WHERE some_value = 0` if no variable named `missing_var` was defined in the project configuration file:

```sql linenums="1"
JINJA_QUERY_BEGIN;

SELECT *
FROM table
WHERE some_value = {{ var('missing_var', 0) }};

JINJA_END;
```

### Gateway variables

Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's `variables` key. Learn more about defining gateway variables in the [Vulcan macros documentation](../../components/advanced-features/macros/built_in.md#gateway-variables).

Access gateway variables in models using the same methods as [global variables](#global-variables).

Gateway-specific variable values take precedence over variables with the same name specified in the configuration file's root `variables` key.

### Blueprint variables

Blueprint variables are defined as a property of the `MODEL` statement, and serve as a mechanism for [creating model templates](../../components/model/types/sql_models.md):

```sql linenums="1"
MODEL (
  name @customer.some_table,
  kind FULL,
  blueprints (
    (customer := customer1, field_a := x, field_b := y),
    (customer := customer2, field_a := z)
  )
);

JINJA_QUERY_BEGIN;
SELECT
  {{ blueprint_var('field_a') }}
  {{ blueprint_var('field_b', 'default_b') }} AS field_b
FROM {{ blueprint_var('customer') }}.some_source
JINJA_END;
```

Blueprint variables can be accessed using the `{{ blueprint_var() }}` macro function, which also supports specifying default values in case the variable is undefined (similar to `{{ var() }}`).


### Local variables

Define your own variables with the Jinja statement `{% set ... %}`. For example, we could specify the name of the `num_orders` column in the `vulcan_example.full_model` like this:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

JINJA_QUERY_BEGIN;

{% set my_col = 'num_orders' %} -- Jinja definition of variable `my_col`

SELECT
  item_id,
  count(distinct id) AS {{ my_col }}, -- Reference to Jinja variable {{ my_col }}
FROM
  vulcan_example.incremental_model
GROUP BY item_id

JINJA_END;
```

Note that the Jinja set statement is written after the `MODEL` statement and before the SQL query.

Jinja variables can be string, integer, or float data types. They can also be an iterable data structure, such as a list, tuple, or dictionary. Each of these data types and structures supports multiple [Python methods](https://jinja.palletsprojects.com/en/3.1.x/templates/#python-methods), such as the `upper()` method for strings.

## Macro operators

### Control flow operators

#### for loops

For loops let you iterate over a collection of items to condense repetitive code and easily change the values used by the code.

Jinja for loops begin with `{% for ... %}` and end with `{% endfor %}`. This example demonstrates creating indicator variables with `CASE WHEN` using a Jinja for loop:

```sql linenums="1"
SELECT
  {% for vehicle_type in ['car', 'truck', 'bus']}
    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},
  {% endfor %}
FROM table
```

Note that the `vehicle_type` values are quoted in the list `['car', 'truck', 'bus']`. Jinja removes those quotes during processing, so the reference `'{{ vehicle_type }}` in the `CASE WHEN` statement must be in quotes. The reference `vehicle_{{ vehicle_type }}` does not require quotes.

Also note that a comma is present at the end of the `CASE WHEN` line. Trailing commas are not valid SQL and would normally require special handling, but Vulcan's semantic understanding of the query allows it to identify and remove the offending comma.

The example renders to this after Vulcan processing:

```sql linenums="1"
SELECT
  CASE WHEN user_vehicle = 'car' THEN 1 ELSE 0 END AS vehicle_car,
  CASE WHEN user_vehicle = 'truck' THEN 1 ELSE 0 END AS vehicle_truck,
  CASE WHEN user_vehicle = 'bus' THEN 1 ELSE 0 END AS vehicle_bus
FROM table
```

In general, it is a best practice to define lists of values separately from their use. We could do that like this:

```sql linenums="1"
{% set vehicle_types = ['car', 'truck', 'bus'] %}

SELECT
  {% for vehicle_type in vehicle_types }
    CASE WHEN user_vehicle = '{{ vehicle_type }}' THEN 1 ELSE 0 END as vehicle_{{ vehicle_type }},
  {% endfor %}
FROM table
```

The rendered query would be the same as before.

#### if

if statements allow you to take an action (or not) based on some condition.

Jinja if statements begin with `{% if ... %}` and end with `{% endif %}`. The starting `if` statement must contain code that evaluates to `True` or `False`. For example, all of `True`, `1 + 1 == 2`, and `'a' in ['a', 'b']` evaluate to `True`.

As an example, you might want a model to only include a column if the model was being run for testing purposes. We can do that by setting a variable indicating whether it's a testing run that determines whether the query includes `testing_column`:

```sql linenums="1"
{% set testing = True %}

SELECT
  normal_column,
  {% if testing %}
    testing_column
  {% endif %}
FROM table
```

Because `testing` is `True`, the rendered query would be:

```sql linenums="1"
SELECT
  normal_column,
  testing_column
FROM table
```

## User-defined macro functions

User-defined macro functions allow the same macro code to be used in multiple models.

Jinja macro functions should be placed in `.sql` files in the Vulcan project's `macros` directory. Multiple functions can be defined in one `.sql` file, or they can be distributed across multiple files.

Jinja macro functions are defined with the `{% macro %}` and `{% endmacro %}` statements. The macro function name and arguments are specified in the `{% macro %}` statement.

For example, a macro function named `print_text` that takes no arguments could be defined with:

```sql linenums="1"
{% macro print_text() %}
text
{% endmacro %}
```

This macro function would be called in a SQL model with `{{ print_text() }}`, which would be substituted with `text`" in the rendered query.

Macro function arguments are placed in the parentheses next to the macro name. For example, this macro generates a SQL column with an alias based on the arguments `expression` and `alias`:

```sql linenums="1"
{% macro alias(expression, alias) %}
  {{ expression }} AS {{ alias }}
{% endmacro %}
```

We might call this macro function in a SQL query like this:

```sql linenums="1"
SELECT
  item_id,
  {{ alias('item_id', 'item_id2')}}
FROM table
```

After processing, it would render to this:

```sql linenums="1"
SELECT
  item_id,
  item_id AS item_id2
FROM table
```

Note that both argument values are quoted in the call `alias('item_id', 'item_id2')` but are not quoted in the rendered query. During the rendering process, Vulcan uses its semantic understanding of the query to build the rendered text - it recognizes that the first argument is a column name and that column aliases are unquoted by default.

In that example, the SQL query selects the column `item_id` with the alias `item_id2`. If instead we wanted to select the *string* `'item_id'` with the name `item_id2`, we would pass the `expression` argument with double quotes around it: `"'item_id'"`:

```sql linenums="1"
SELECT
  item_id,
  {{ alias("'item_id'", 'item_id2')}}
FROM table
```

After processing, it would render to this:

```sql linenums="1"
SELECT
  item_id,
  'item_id' AS item_id2
FROM table
```

The double quotes around `"'item_id'"` signal to Vulcan that it is not a column name.

Some SQL dialects interpret double and single quotes differently. We could replace the rendered single quoted `'item_id'` with double quoted `"item_id"` in the previous example by switching the placement of quotes in the macro function call. Instead of `alias("'item_id'", 'item_id2')` we would use `alias('"item_id"', 'item_id2')`.

## Mixing macro systems

Vulcan supports both the Jinja and [Vulcan](../../components/advanced-features/macros/built_in.md) macro systems. We strongly recommend using only one system in a single model - if both are present, they may fail or behave in unintuitive ways.

[Predefined Vulcan macro variables](../../components/advanced-features/macros/variables.md) can be used in a query containing user-defined Jinja variables and functions. However, predefined variables passed as arguments to a user-defined Jinja macro function must use the Jinja curly brace syntax `{{ start_ds }}` instead of the Vulcan macro `@` prefix syntax `@start_ds`. Note that curly brace syntax may require quoting to generate the equivalent of the `@` syntax.



# Variables

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/macros/macro_variables/

---

# Variables

Macro variables are placeholders whose values are substituted in when the macro is rendered.

They enable dynamic macro behavior - for example, a date parameter's value might be based on when the macro was run.

!!! note

    This page discusses Vulcan's built-in macro variables. Learn more about custom, user-defined macro variables on the [Vulcan macros page](../../components/advanced-features/macros/built_in.md#user-defined-variables).

## Example

Consider a SQL query that filters by date in the `WHERE` clause.

Instead of manually changing the date each time the model is run, you can use a macro variable to make the date dynamic. With the dynamic approach, the date changes automatically based on when the query is run.

This query filters for rows where column `my_date` is after '2023-01-01':

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > '2023-01-01'
```

To make this query's date dynamic you could use the predefined Vulcan macro variable `@execution_ds`:

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > @execution_ds
```

The `@` symbol tells Vulcan that `@execution_ds` is a macro variable that requires substitution before the SQL is executed.

The macro variable `@execution_ds` is predefined, so its value will be automatically set by Vulcan based on when the execution started. If the model was executed on February 1, 2023 the rendered query would be:

```sql linenums="1"
SELECT *
FROM table
WHERE my_date > '2023-02-01'
```

This example used one of Vulcan's predefined variables, but you can also define your own macro variables.

We describe Vulcan's predefined variables below; user-defined macro variables are discussed in the [Vulcan macros](../../components/advanced-features/macros/built_in.md#user-defined-variables) and [Jinja macros](../../components/advanced-features/macros/jinja.md#user-defined-variables) pages.

## Predefined variables
Vulcan comes with predefined variables that can be used in your queries. They are automatically set by the Vulcan runtime.

Most predefined variables are related to time and use a combination of prefixes (start, end, etc.) and postfixes (date, ds, ts, etc.). They are described in the next section; [other predefined variables](#runtime-variables) are discussed in the following section.

### Temporal variables

Vulcan uses the python [datetime module](https://docs.python.org/3/library/datetime.html) for handling dates and times. It uses the standard [Unix epoch](https://en.wikipedia.org/wiki/Unix_time) start of 1970-01-01.

!!! tip "Important"

    Predefined variables with a time component always use the [UTC time zone](https://en.wikipedia.org/wiki/Coordinated_Universal_Time).

    Learn more about timezones and incremental models [here](../../components/model/model_kinds.md#timezones).

Prefixes:

* start - The inclusive starting interval of a model run
* end - The inclusive end interval of a model run
* execution - The timestamp of when the execution started

Postfixes:

* dt - A python datetime object that converts into a native SQL `TIMESTAMP` (or SQL engine equivalent)
* dtntz - A python datetime object that converts into a native SQL `TIMESTAMP WITHOUT TIME ZONE` (or SQL engine equivalent)
* date - A python date object that converts into a native SQL `DATE`
* ds - A date string with the format: '%Y-%m-%d'
* ts - An ISO 8601 datetime formatted string: '%Y-%m-%d %H:%M:%S'
* tstz - An ISO 8601 datetime formatted string with timezone: '%Y-%m-%d %H:%M:%S%z'
* hour - An integer representing the hour of the day, with values 0-23
* epoch - An integer representing seconds since Unix epoch
* millis - An integer representing milliseconds since Unix epoch

All predefined temporal macro variables:

* dt
    * @start_dt
    * @end_dt
    * @execution_dt

* dtntz
    * @start_dtntz
    * @end_dtntz
    * @execution_dtntz

* date
    * @start_date
    * @end_date
    * @execution_date

* ds
    * @start_ds
    * @end_ds
    * @execution_ds

* ts
    * @start_ts
    * @end_ts
    * @execution_ts

* tstz
    * @start_tstz
    * @end_tstz
    * @execution_tstz

* hour
    * @start_hour
    * @end_hour
    * @execution_hour

* epoch
    * @start_epoch
    * @end_epoch
    * @execution_epoch

* millis
    * @start_millis
    * @end_millis
    * @execution_millis

### Runtime variables

Vulcan provides additional predefined variables used to modify model behavior based on information available at runtime.

* @runtime_stage - A string value denoting the current stage of the Vulcan runtime. Typically used in models to conditionally execute pre/post-statements (learn more [here](../../components/model/types/sql_models.md#optional-prepost-statements)). It returns one of these values:
    * 'loading' - The project is being loaded into Vulcan's runtime context.
    * 'creating' - The model tables are being created for the first time. The data may be inserted during table creation.
    * 'evaluating' - The model query logic is evaluated, and the data is inserted into the existing model table.
    * 'promoting' - The model is being promoted in the target environment (view created during virtual layer update).
    * 'demoting' - The model is being demoted in the target environment (view dropped during virtual layer update).
    * 'auditing' - The audit is being run.
    * 'testing' - The model query logic is being evaluated in the context of a unit test.
* @gateway - A string value containing the name of the current [gateway](guides-old/connections.md).
* @this_model - The physical table name that the model's view selects from. Typically used to create [generic audits](../../components/audits/audits.md#generic-audits). When used in [on_virtual_update statements](../../components/model/types/sql_models.md#optional-on-virtual-update-statements), it contains the qualified view name instead.
* @model_kind_name - A string value containing the name of the current model kind. Intended to be used in scenarios where you need to control the [physical properties in model defaults](../../configurations-old/configuration.md#model-defaults).

!!! note "Embedding variables in strings"

    Macro variable references sometimes use the curly brace syntax `@{variable}`, which serves a different purpose than the regular `@variable` syntax.

    The curly brace syntax tells Vulcan that the rendered string should be treated as an identifier, instead of simply replacing the macro variable value.

    For example, if `variable` is defined as `@DEF(`variable`, foo.bar)`, then `@variable` produces `foo.bar`, while `@{variable}` produces `"foo.bar"`. This is because Vulcan converts `foo.bar` into an identifier, using double quotes to correctly include the `.` character in the identifier name.

    In practice, `@{variable}` is most commonly used to interpolate a value within an identifier, e.g., `@{variable}_suffix`, whereas `@variable` is used to do plain substitutions for string literals.

    Learn more in the [Vulcan macros documentation](../../components/advanced-features/macros/built_in.md#embedding-variables-in-strings).

#### Before all and after all variables

The following variables are also available in [`before_all` and `after_all` statements](guides-old/configuration.md#before_all-and-after_all-statements), as well as in macros invoked within them.

* @this_env - A string value containing the name of the current [environment](concepts-old/environments.md).
* @schemas - A list of the schema names of the [virtual layer](concepts-old/glossary.md#virtual-layer) of the current environment.
* @views - A list of the view names of the [virtual layer](concepts-old/glossary.md#virtual-layer) of the current environment.


# Overview

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/macros/overview/

---

# Overview

SQL is a [declarative language](https://en.wikipedia.org/wiki/Declarative_programming). It does not natively have features like variables or control flow logic (if-then, for loops) that allow SQL commands to behave differently in different situations.

However, data pipelines are dynamic and need different behavior depending on context. SQL is made dynamic with *macros*.

Vulcan supports two macro systems: Vulcan macros and the [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) templating system.

Learn more about macros in Vulcan:

- [Pre-defined macro variables](../../components/advanced-features/macros/variables.md) available in both macro systems
- [Vulcan macros](../../components/advanced-features/macros/built_in.md)
- [Jinja macros](../../components/advanced-features/macros/jinja.md)



# Built In

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/macros/vulcan_macros/

---

# Built In

## Macro systems: two approaches

Vulcan macros behave differently than those of templating systems like [Jinja](https://jinja.palletsprojects.com/en/3.1.x/).

Macro systems are based on string substitution. The macro system scans code files, identifies special characters that signify macro content, and replaces the macro elements with other text.

In a general sense, that is the entire functionality of templating systems. They have tools that provide control flow logic (if-then) and other functionality, but *that functionality is solely to support substituting in the correct strings*.

Templating systems are intentionally agnostic to the programming language being templated, and most of them work for everything from blog posts to HTML to SQL.

In contrast, Vulcan macros are designed specifically for generating SQL code. They have *semantic understanding* of the SQL code being created by analyzing it with the Python [sqlglot](https://github.com/tobymao/sqlglot) library, and they allow use of Python code so users can tidily implement sophisticated macro logic.

### Vulcan macro approach

This section describes how Vulcan macros work under the hood. Feel free to skip over this section and return if and when it is useful. This information is **not** required to use Vulcan macros, but it will be useful for debugging any macros exhibiting puzzling behavior.

The critical distinction between the Vulcan macro approach and templating systems is the role string substitution plays. In templating systems, string substitution is the entire and only point.

In Vulcan, string substitution is just one step toward modifying the semantic representation of the SQL query. *Vulcan macros work by building and modifying the semantic representation of the SQL query.*

After processing all the non-SQL text, it uses the substituted values to modify the semantic representation of the query to its final state.

It uses the following five step approach to accomplish this:

1. Parse the text with the appropriate sqlglot SQL dialect (e.g., Postgres, BigQuery, etc.). During the parsing, it detects the special macro symbol `@` to differentiate non-SQL from SQL text. The parser builds a semantic representation of the SQL code's structure, capturing non-SQL text as "placeholder" values to use in subsequent steps.

2. Examine the placeholder values to classify them as one of the following types:

    - Creation of user-defined macro variables with the `@DEF` operator (see more about [user-defined macro variables](#user-defined-variables))
    - Macro variables: [Vulcan pre-defined](../../components/advanced-features/macros/variables.md), [user-defined local](#local-variables), and [user-defined global](#global-variables)
    - Macro functions, both [Vulcan's](#macro-operators) and [user-defined](#user-defined-macro-functions)

3. Substitute macro variable values where they are detected. In most cases, this is direct string substitution as with a templating system.

4. Execute any macro functions and substitute the returned values.

5. Modify the semantic representation of the SQL query with the substituted variable values from (3) and functions from (4).

### Embedding variables in strings

Vulcan always incorporates macro variable values into the semantic representation of a SQL query (step 5 above). To do that, it infers the role each macro variable value plays in the query.

For context, two commonly used types of string in SQL are:

- String literals, which represent text values and are surrounded by single quotes, such as `'the_string'`
- Identifiers, which reference database objects like column, table, alias, and function names
    - They may be unquoted or quoted with double quotes, backticks, or brackets, depending on the SQL dialect

In a normal query, Vulcan can easily determine which role a given string is playing. However, it is more difficult if a macro variable is embedded directly into a string - especially if the string is in the `MODEL` block (and not the query itself).

For example, consider a project that defines a [gateway variable](#gateway-variables) named `gateway_var`. The project includes a model that references `@gateway_var` as part of the schema in the model's `name`, which is a SQL *identifier*.

This is how we might try to write the model:

``` sql title="Incorrectly rendered to string literal"
MODEL (
  name the_@gateway_var_schema.table
);
```

From Vulcan's perspective, the model schema is the combination of three sub-strings: `the_`, the value of `@gateway_var`, and `_schema`.

Vulcan will concatenate those strings, but it does not have the context to know that it is building a SQL identifier and will return a string literal.

To provide the context Vulcan needs, you must add curly braces to the macro variable reference: `@{gateway_var}` instead of `@gateway_var`:

``` sql title="Correctly rendered to identifier"
MODEL (
  name the_@{gateway_var}_schema.table
);
```

The curly braces let Vulcan know that it should treat the string as a SQL identifier, which it will then quote based on the SQL dialect's quoting rules.

The most common use of the curly brace syntax is embedding macro variables into strings, it can also be used to differentiate string literals and identifiers in SQL queries. For example, consider a macro variable `my_variable` whose value is `col`.

If we `SELECT` this value with regular macro syntax, it will render to a string literal:

``` sql
SELECT @my_variable AS the_column; -- renders to SELECT 'col' AS the_column
```

`'col'` is surrounded with single quotes, and the SQL engine will use that string as the column's data value.

If we use curly braces, Vulcan will know that we want to use the rendered string as an identifier:

``` sql
SELECT @{my_variable} AS the_column; -- renders to SELECT col AS the_column
```

`col` is not surrounded with single quotes, and the SQL engine will determine that the query is referencing a column or other object named `col`.

## User-defined variables

Vulcan supports four kinds of user-defined macro variables: [global](#global-variables), [gateway](#gateway-variables), [blueprint](#blueprint-variables) and [local](#local-variables).

Global and gateway macro variables are defined in the project configuration file and can be accessed in any project model. Blueprint and macro variables are defined in a model definition and can only be accessed in that model.

Macro variables with the same name may be specified at any or all of the global, gateway, blueprint and local levels. When variables are specified at multiple levels, the value of the most specific level takes precedence. For example, the value of a local variable takes precedence over the value of a blueprint or gateway variable with the same name, and the value of a gateway variable takes precedence over the value of a global variable.

### Global variables

Global variables are defined in the project configuration file [`variables` key](../../configurations-old/configuration.md#variables).

Global variable values may be any of the following data types or lists or dictionaries containing these types: `int`, `float`, `bool`, `str`.

Access global variable values in a model definition using the `@<VAR_NAME>` macro or the `@VAR()` macro function. The latter function requires the name of the variable _in single quotes_ as the first argument and an optional default value as the second argument. The default value is a safety mechanism used if the variable name is not found in the project configuration file.

For example, this Vulcan configuration key defines six variables of different data types:

=== "YAML"

    ```yaml linenums="1"
    variables:
      int_var: 1
      float_var: 2.0
      bool_var: true
      str_var: "cat"
      list_var: [1, 2, 3]
      dict_var:
        key1: 1
        key2: 2
    ```

=== "Python"

    ``` python linenums="1"
    variables = {
        "int_var": 1,
        "float_var": 2.0,
        "bool_var": True,
        "str_var": "cat",
        "list_var": [1, 2, 3],
        "dict_var": {"key1": 1, "key2": 2},
    }

    config = Config(
        variables=variables,
        ... # other Config arguments
    )
    ```

A model definition could access the `int_var` value in a `WHERE` clause like this:

```sql linenums="1"
SELECT *
FROM table
WHERE int_variable = @INT_VAR
```

Alternatively, the same variable can be accessed by passing the variable name into the `@VAR()` macro function. Note that the variable name is in single quotes in the call `@VAR('int_var')`:

```sql linenums="1"
SELECT *
FROM table
WHERE int_variable = @VAR('int_var')
```

A default value can be passed as a second argument to the `@VAR()` macro function, which will be used as a fallback value if the variable is missing from the configuration file.

In this example, the `WHERE` clause would render to `WHERE some_value = 0` because no variable named `missing_var` was defined in the project configuration file:

```sql linenums="1"
SELECT *
FROM table
WHERE some_value = @VAR('missing_var', 0)
```

A similar API is available for [Python macro functions](#accessing-global-variable-values) via the `evaluator.var` method and [Python models](../../components/model/types/python_models.md#user-defined-variables) via the `context.var` method.

### Gateway variables

Like global variables, gateway variables are defined in the project configuration file. However, they are specified in a specific gateway's `variables` key:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        variables:
          int_var: 1
        ...
    ```

=== "Python"

    ``` python linenums="1"
    gateway_variables = {
      "int_var": 1
    }

    config = Config(
        gateways={
          "my_gateway": GatewayConfig(
            variables=gateway_variables
            ... # other GatewayConfig arguments
            ),
          }
    )
    ```

Access them in models using the same methods as [global variables](#global-variables).

Gateway-specific variable values take precedence over variables with the same name specified in the root `variables` key.

### Blueprint variables

Blueprint macro variables are defined in a model. Blueprint variable values take precedence over [global](#global-variables) or [gateway-specific](#gateway-variables) variables with the same name.

Blueprint variables are defined as a property of the `MODEL` statement, and serve as a mechanism for [creating model templates](../../components/model/types/sql_models.md):

```sql linenums="1"
MODEL (
  name @customer.some_table,
  kind FULL,
  blueprints (
    (customer := customer1, field_a := x, field_b := y, field_c := 'foo'),
    (customer := customer2, field_a := z, field_b := w, field_c := 'bar')
  )
);

SELECT
  @field_a,
  @{field_b} AS field_b,
  @field_c AS @{field_c}
FROM @customer.some_source

/*
When rendered for customer1.some_table:
SELECT
  x,
  y AS field_b,
  'foo' AS foo
FROM customer1.some_source

When rendered for customer2.some_table:
SELECT
  z,
  w AS field_b,
  'bar' AS bar
FROM customer2.some_source
*/
```

Note the use of both regular `@field_a` and curly brace syntax `@{field_b}` macro variable references in the model query. Both of these will be rendered as identifiers. In the case of `field_c`, which in the blueprints is a string, it would be rendered as a string literal when used with the regular macro syntax `@field_c` and if we want to use the string as an identifier then we use the curly braces `@{field_c}`. Learn more [above](#embedding-variables-in-strings)

Blueprint variables can be accessed using the syntax shown above, or through the `@BLUEPRINT_VAR()` macro function, which also supports specifying default values in case the variable is undefined (similar to `@VAR()`).

### Local variables

Local macro variables are defined in a model. Local variable values take precedence over [global](#global-variables), [blueprint](#blueprint-variables), or [gateway-specific](#gateway-variables) variables with the same name.

Define your own local macro variables with the `@DEF` macro operator. For example, you could set the macro variable `macro_var` to the value `1` with:

```sql linenums="1"
@DEF(macro_var, 1);
```

Vulcan has three basic requirements for using the `@DEF` operator:

1. The `MODEL` statement must end with a semi-colon `;`
2. All `@DEF` uses must come after the `MODEL` statement and before the SQL query
3. Each `@DEF` use must end with a semi-colon `;`

For example, consider the following model `vulcan_example.full_model` from the [Vulcan quickstart guide](guides/get-started/docker.md):

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
GROUP BY item_id
```

This model could be extended with a user-defined macro variable to filter the query results based on `item_size` like this:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
); -- NOTE: semi-colon at end of MODEL statement

@DEF(size, 1); -- NOTE: semi-colon at end of @DEF operator

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
WHERE
  item_size > @size -- Reference to macro variable `@size` defined above with `@DEF()`
GROUP BY item_id
```

This example defines the macro variable `size` with `@DEF(size, 1)`. When the model is run, Vulcan will substitute in the number `1` where `@size` appears in the `WHERE` clause.

### Macro functions

In addition to inline user-defined variables, Vulcan also supports inline macro functions. These functions can be used to express more readable and reusable logic than is possible with variables alone. Lets look at an example:

```sql linenums="1"
MODEL(...);

@DEF(
  rank_to_int,
  x -> case when left(x, 1) = 'A' then 1 when left(x, 1) = 'B' then 2 when left(x, 1) = 'C' then 3 end
);

SELECT
  id,
  cust_rank_1,
  cust_rank_2,
  cust_rank_3
  @rank_to_int(cust_rank_1) as cust_rank_1_int,
  @rank_to_int(cust_rank_2) as cust_rank_2_int,
  @rank_to_int(cust_rank_3) as cust_rank_3_int
FROM
  some.model
```

Multiple arguments can be expressed in a macro function as well:

```sql linenums="1"
@DEF(pythag, (x,y) -> sqrt(pow(x, 2) + pow(y, 2)));

SELECT
  sideA,
  sideB,
  @pythag(sideA, sideB) AS sideC
FROM
  some.triangle
```

```sql linenums="1"
@DEF(nrr, (starting_mrr, expansion_mrr, churned_mrr) -> (starting_mrr + expansion_mrr - churned_mrr) / starting_mrr);

SELECT
  @nrr(fy21_mrr, fy21_expansions, fy21_churns) AS fy21_net_retention_rate,
  @nrr(fy22_mrr, fy22_expansions, fy22_churns) AS fy22_net_retention_rate,
  @nrr(fy23_mrr, fy23_expansions, fy23_churns) AS fy23_net_retention_rate,
FROM
  some.revenue
```

You can nest macro functions like so:

```sql linenums="1"
MODEL (
  name dummy.model,
  kind FULL
);

@DEF(area, r -> pi() * r * r);
@DEF(container_volume, (r, h) -> @area(@r) * h);

SELECT container_id, @container_volume((cont_di / 2), cont_hi) AS volume
```

## Macro operators

Vulcan's macro system has multiple operators that allow different forms of dynamic behavior in models.

### @EACH

`@EACH` is used to transform a list of items by applying a function to each of them, analogous to a `for` loop.

??? info "Learn more about `for` loops and `@EACH`"

    Before diving into the `@EACH` operator, let's dissect a `for` loop to understand its components.

    `for` loops have two primary parts: a collection of items and an action that should be taken for each item. For example, here is a `for` loop in Python:

    ```python linenums="1"
    for number in [4, 5, 6]:
        print(number)
    ```

    This for loop prints each number present in the brackets:

    ```python linenums="1"
    4
    5
    6
    ```

    The first line of the example sets up the loop, doing two things:

    1. Telling Python that code inside the loop will refer to each item as `number`
    2. Telling Python to step through the list of items in brackets

    The second line tells Python what action should be taken for each item. In this case, it prints the item.

    The loop executes one time for each item in the list, substituting in the item for the word `number` in the code. For example, the first time through the loop the code would execute as `print(4)` and the second time as `print(5)`.

    The Vulcan `@EACH` operator is used to implement the equivalent of a `for` loop in Vulcan macros.

    `@EACH` gets its name from the fact that a loop performs the action "for each" item in the collection. It is fundamentally equivalent to the Python loop above, but you specify the two loop components differently.

`@EACH` takes two arguments: a list of items and a function definition.

```sql linenums="1"
@EACH([list of items], [function definition])
```

The function definition is specified inline. This example specifies the identity function, returning the input unmodified:

```sql linenums="1"
SELECT
  @EACH([4, 5, 6], number -> number)
FROM table
```

The loop is set up by the first argument: `@EACH([4, 5, 6]` tells Vulcan to step through the list of items in brackets.

The second argument `number -> number` tells Vulcan what action should be taken for each item using an "anonymous" function (aka "lambda" function). The left side of the arrow states what name the code on the right side will refer to each item as (like `name` in `for [name] in [items]` in a Python `for` loop).

The right side of the arrow specifies what should be done to each item in the list. `number -> number` tells `@EACH` that for each item `number` it should return that item (e.g., `1`).

Vulcan macros use their semantic understanding of SQL code to take automatic actions based on where in a SQL query macro variables are used. If `@EACH` is used in the `SELECT` clause of a SQL statement:

1. It prints the item
2. It knows fields are separated by commas in `SELECT`, so it automatically separates the printed items with commas

Because of the automatic print and comma-separation, the anonymous function `number -> number` tells `@EACH` that for each item `number` it should print the item and separate the items with commas. Therefore, the complete output from the example is:

```sql linenums="1"
SELECT
  4,
  5,
  6
FROM table
```

This basic example is too simple to be useful. Many uses of `@EACH` will involve using the values as one or both of a literal value and an identifier.

For example, a column `favorite_number` in our data might contain values `4`, `5`, and `6`, and we want to unpack that column into three indicator (i.e., binary, dummy, one-hot encoded) columns. We could write that by hand as:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END as favorite_4,
  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END as favorite_5,
  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END as favorite_6
FROM table
```

In that SQL query each number is being used in two distinct ways. For example, `4` is being used:

1. As a literal numeric value in `favorite_number = 4`
2. As part of a column name in `favorite_4`

We describe each of these uses separately.

For the literal numeric value, `@EACH` substitutes in the exact value that is passed in the brackets, *including quotes*. For example, consider this query similar to the `CASE WHEN` example above:

```sql linenums="1"
SELECT
  @EACH([4,5,6], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)
FROM table
```

It renders to this SQL:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = 4 THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = 5 THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = 6 THEN 1 ELSE 0 END AS column
FROM table
```

Note that the number `4`, `5`, and `6` are unquoted in *both* the input `@EACH` array in brackets and the resulting SQL query.

We can instead quote them in the input `@EACH` array:

```sql linenums="1"
SELECT
  @EACH(['4','5','6'], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column)
FROM table
```

And they will be quoted in the resulting SQL query:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column,
  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column
FROM table
```

We can place the array values at the end of a column name by using the Vulcan macro operator `@` inside the `@EACH` function definition:

```sql linenums="1"
SELECT
  @EACH(['4','5','6'], x -> CASE WHEN favorite_number = x THEN 1 ELSE 0 END as column_@x)
FROM table
```

This query will render to:

```sql linenums="1"
SELECT
  CASE WHEN favorite_number = '4' THEN 1 ELSE 0 END AS column_4,
  CASE WHEN favorite_number = '5' THEN 1 ELSE 0 END AS column_5,
  CASE WHEN favorite_number = '6' THEN 1 ELSE 0 END AS column_6
FROM table
```

This syntax works regardless of whether the array values are quoted or not.

!!! note "Embedding macros in strings"

    Vulcan macros support placing macro values at the end of a column name using `column_@x`.

    However, if you wish to substitute the variable anywhere else in the identifier, you need to use the more explicit curly brace syntax `@{}` to avoid ambiguity. For example, these are valid uses: `@{x}_column` or `my_@{x}_column`.

    Learn more about embedding macros in strings [above](#embedding-variables-in-strings)

### @IF

Vulcan's `@IF` macro allows components of a SQL query to change based on the result of a logical condition.

It includes three elements:

1. A logical condition that evaluates to `TRUE` or `FALSE`
2. A value to return if the condition is `TRUE`
3. A value to return if the condition is `FALSE` [optional]

These elements are specified as:

```sql linenums="1"
@IF([logical condition], [value if TRUE], [value if FALSE])
```

The value to return if the condition is `FALSE` is optional - if it is not provided and the condition is `FALSE`, then the macro has no effect on the resulting query.

The logical condition should be written *in SQL* and is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL executor. It supports the following operators:

- Equality: `=` for equals, `!=` or `<>` for not equals
- Comparison: `<`, `>`, `<=`, `>=`,
- Between: `[number] BETWEEN [low number] AND [high number]`
- Membership: `[item] IN ([comma-separated list of items])`

For example, the following simple conditions are all valid SQL and evaluate to `TRUE`:

- `'a' = 'a'`
- `'a' != 'b'`
- `0 < 1`
- `1 >= 1`
- `2 BETWEEN 1 AND 3`
- `'a' IN ('a', 'b')`

`@IF` can be used to modify any part of a SQL query. For example, this query conditionally includes `sensitive_col` in the query results:

```sql linenums="1"
SELECT
  col1,
  @IF(1 > 0, sensitive_col)
FROM table
```

Because `1 > 0` evaluates to `TRUE`, the query is rendered as:

```sql linenums="1"
SELECT
  col1,
  sensitive_col
FROM table
```

Note that `@IF(1 > 0, sensitive_col)` does not include the third argument specifying a value if `FALSE`. Had the condition evaluated to `FALSE`, `@IF` would return nothing and only `col1` would be selected.

Alternatively, we could specify that `nonsensitive_col` be returned if the condition evaluates to `FALSE`:

```sql linenums="1"
SELECT
  col1,
  @IF(1 > 2, sensitive_col, nonsensitive_col)
FROM table
```

Because `1 > 2` evaluates to `FALSE`, the query is rendered as:

```sql linenums="1"
SELECT
  col1,
  nonsensitive_col
FROM table
```

[Macro rendering](#vulcan-macro-approach) occurs before the `@IF` condition is evaluated. For example, Vulcan doesn't evaluate the condition `my_column > @my_value` until it has first substituted the number `@my_value` represents.

Your macro might do things besides returning a value, such as printing a message or executing a statement (i.e., the macro "has side effects"). The side effect code will always run during the rendering step. To prevent this, modify the macro code to condition the side effects on the evaluation stage.

#### Pre/post-statements

`@IF` may be used to conditionally execute pre/post-statements:

```sql linenums="1"
@IF([logical condition], [statement to execute if TRUE]);
```

The `@IF` statement itself must end with a semi-colon, but the inner statement argument must not.

This example conditionally executes a pre/post-statement depending on the model's [runtime stage](../../components/advanced-features/macros/variables.md#predefined-variables), accessed via the pre-defined macro variable `@runtime_stage`. The `@IF` post-statement will only be executed at model evaluation time:

```sql linenums="1" hl_lines="17-20"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  grain item_id,
  audits (assert_positive_order_ids),
);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
GROUP BY item_id
ORDER BY item_id;

@IF(
  @runtime_stage = 'evaluating',
  ALTER TABLE vulcan_example.full_model ALTER item_id TYPE VARCHAR
);
```

NOTE: alternatively, we could alter a column's type if the `@runtime_stage = 'creating'`, but that would only be useful if the model is incremental and the alteration would persist. `FULL` models are rebuilt on each evaluation, so changes made at their creation stage will be overwritten each time the model is evaluated.

### @EVAL

`@EVAL` evaluates its arguments with SQLGlot's SQL executor.

It allows you to execute mathematical or other calculations in SQL code. It behaves similarly to the first argument of the [`@IF` operator](#if), but it is not limited to logical conditions.

For example, consider a query adding 5 to a macro variable:

```sql linenums="1"
MODEL (
  ...
);

@DEF(x, 1);

SELECT
  @EVAL(5 + @x) as my_six
FROM table
```

After macro variable substitution, this would render as `@EVAL(5 + 1)` and be evaluated to `6`, resulting in the final rendered query:

```sql linenums="1"
SELECT
  6 as my_six
FROM table
```

### @FILTER

`@FILTER` is used to subset an input array of items to only those meeting the logical condition specified in the anonymous function. Its output can be consumed by other macro operators such as [`@EACH`](#each) or [`@REDUCE`](#reduce).

The user-specified anonymous function must evaluate to `TRUE` or `FALSE`. `@FILTER` applies the function to each item in the array, only including the item in the output array if it meets the condition.

The anonymous function should be written *in SQL* and is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL executor. It supports standard SQL equality and comparison operators - see [`@IF`](#if) above for more information about supported operators.

For example, consider this `@FILTER` call:

```sql linenums="1"
@FILTER([1,2,3], x -> x > 1)
```

It applies the condition `x > 1` to each item in the input array `[1,2,3]` and returns `[2,3]`.

### @REDUCE

`@REDUCE` is used to combine the items in an array.

The anonymous function specifies how the items in the input array should be combined. In contrast to `@EACH` and `@FILTER`, the anonymous function takes two arguments whose values are named in parentheses.

For example, an anonymous function for `@EACH` might be specified `x -> x + 1`. The `x` to the left of the arrow tells Vulcan that the array items will be referred to as `x` in the code to the right of the arrow.

Because the `@REDUCE` anonymous function takes two arguments, the text to the left of the arrow must contain two comma-separated names in parentheses. For example, `(x, y) -> x + y` tells Vulcan that items will be referred to as `x` and `y` in the code to the right of the arrow.

Even though the anonymous function takes only two arguments, the input array can contain as many items as necessary.

Consider the anonymous function `(x, y) -> x + y`. Conceptually, only the `y` argument corresponds to items in the array; the `x` argument is a temporary value created when the function is evaluated.

For the call `@REDUCE([1,2,3,4], (x, y) -> x + y)`, the anonymous function is applied to the array in the following steps:

1. Take the first two items in the array as `x` and `y`. Apply the function to them: `1 + 2` = `3`.
2. Take the output of step (1) as `x` and the next item in the array `3` as `y`. Apply the function to them: `3 + 3` = `6`.
3. Take the output of step (2) as `x` and the next item in the array `4` as `y`. Apply the function to them: `6 + 4` = `10`.
4. No items remain. Return value from step (3): `10`.

`@REDUCE` will almost always be used with another macro operator. For example, we might want to build a `WHERE` clause from multiple column names:

```sql linenums="1"
SELECT
  my_column
FROM
  table
WHERE
  col1 = 1 and col2 = 1 and col3 = 1
```

We can use `@EACH` to build each column's predicate (e.g., `col1 = 1`) and `@REDUCE` to combine them into a single statement:

```sql linenums="1"
SELECT
  my_column
FROM
  table
WHERE
  @REDUCE(
    @EACH([col1, col2, col3], x -> x = 1), -- Builds each individual predicate `col1 = 1`
    (x, y) -> x AND y -- Combines individual predicates with `AND`
  )
```

### @STAR

`@STAR` is used to return a set of column selections in a query.

`@STAR` is named after SQL's star operator `*`, but it allows you to programmatically generate a set of column selections and aliases instead of just selecting all available columns. A query may use more than one `@STAR` and may also include explicit column selections.

`@STAR` uses Vulcan's knowledge of each table's columns and data types to generate the appropriate column list.

If the column data types are known, the resulting query `CAST`s columns to their data type in the source table. Otherwise, the columns will be listed without any casting.

`@STAR` supports the following arguments, in this order:

- `relation`: The relation/table whose columns are being selected
- `alias` (optional): The alias of the relation (if it has one)
- `exclude` (optional): A list of columns to exclude
- `prefix` (optional): A string to use as a prefix for all selected column names
- `suffix` (optional): A string to use as a suffix for all selected column names
- `quote_identifiers` (optional): Whether to quote the resulting identifiers, defaults to true

**NOTE**: the `exclude` argument used to be named `except_`. The latter is still supported but we discourage its use because it will be deprecated in the future.

Like all Vulcan macro functions, omitting an argument when calling `@STAR` requires passing subsequent arguments with their name and the special `:=` keyword operator. For example, we might omit the `alias` argument with `@STAR(foo, exclude := [c])`. Learn more about macro function arguments [below](#positional-and-keyword-arguments).

As a `@STAR` example, consider the following query:

```sql linenums="1"
SELECT
  @STAR(foo, bar, [c], 'baz_', '_qux')
FROM foo AS bar
```

The arguments to `@STAR` are:

1. The name of the table `foo` (from the query's `FROM foo`)
2. The table alias `bar` (from the query's `AS bar`)
3. A list of columns to exclude from the selection, containing one column `c`
4. A string `baz_` to use as a prefix for all column names
5. A string `_qux` to use as a suffix for all column names

`foo` is a table that contains four columns: `a` (`TEXT`), `b` (`TEXT`), `c` (`TEXT`) and `d` (`INT`). After macro expansion, if the column types are known the query would be rendered as:

```sql linenums="1"
SELECT
  CAST("bar"."a" AS TEXT) AS "baz_a_qux",
  CAST("bar"."b" AS TEXT) AS "baz_b_qux",
  CAST("bar"."d" AS INT) AS "baz_d_qux"
FROM foo AS bar
```

Note these aspects of the rendered query:

- Each column is `CAST` to its data type in the table `foo` (e.g., `a` to `TEXT`)
- Each column selection uses the alias `bar` (e.g., `"bar"."a"`)
- Column `c` is not present because it was passed to `@STAR`'s `exclude` argument
- Each column alias is prefixed with `baz_` and suffixed with `_qux` (e.g., `"baz_a_qux"`)

Now consider a more complex example that provides different prefixes to `a` and `b` than to `d` and includes an explicit column `my_column`:

```sql linenums="1"
SELECT
  @STAR(foo, bar, exclude := [c, d], 'ab_pre_'),
  @STAR(foo, bar, exclude := [a, b, c], 'd_pre_'),
  my_column
FROM foo AS bar
```

As before, `foo` is a table that contains four columns: `a` (`TEXT`), `b` (`TEXT`), `c` (`TEXT`) and `d` (`INT`). After macro expansion, the query would be rendered as:

```sql linenums="1"
SELECT
  CAST("bar"."a" AS TEXT) AS "ab_pre_a",
  CAST("bar"."b" AS TEXT) AS "ab_pre_b",
  CAST("bar"."d" AS INT) AS "d_pre_d",
  my_column
FROM foo AS bar
```

Note these aspects of the rendered query:

- Columns `a` and `b` have the prefix `"ab_pre_"` , while column `d` has the prefix `"d_pre_"`
- Column `c` is not present because it was passed to the `exclude` argument in both `@STAR` calls
- `my_column` is present in the query

### @GENERATE_SURROGATE_KEY

`@GENERATE_SURROGATE_KEY` generates a surrogate key from a set of columns. The surrogate key is a sequence of alphanumeric digits returned by a hash function, such as [`MD5`](https://en.wikipedia.org/wiki/MD5), on the concatenated column values.

The surrogate key is created by:
1. `CAST`ing each column's value to `TEXT` (or the SQL engine's equivalent type)
2. Replacing `NULL` values with the text `'_vulcan_surrogate_key_null_'` for each column
3. Concatenating the column values after steps (1) and (2)
4. Applying the [`MD5()` hash function](https://en.wikipedia.org/wiki/MD5) to the concatenated value returned by step (3)

For example, the following query:

```sql linenums="1"
SELECT
  @GENERATE_SURROGATE_KEY(a, b, c) AS col
FROM foo
```

would be rendered as:

```sql linenums="1"
SELECT
  MD5(
    CONCAT(
      COALESCE(CAST("a" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("b" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("c" AS TEXT), '_vulcan_surrogate_key_null_')
    )
  ) AS "col"
FROM "foo" AS "foo"
```

By default, the `MD5` function is used, but this behavior can change by setting the `hash_function` argument as follows:

```sql linenums="1"
SELECT
  @GENERATE_SURROGATE_KEY(a, b, c, hash_function := 'SHA256') AS col
FROM foo
```

This query will similarly be rendered as:

```sql linenums="1"
SELECT
  SHA256(
    CONCAT(
      COALESCE(CAST("a" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("b" AS TEXT), '_vulcan_surrogate_key_null_'),
      '|',
      COALESCE(CAST("c" AS TEXT), '_vulcan_surrogate_key_null_')
    )
  ) AS "col"
FROM "foo" AS "foo"
```

### @SAFE_ADD

`@SAFE_ADD` adds two or more operands, substituting `NULL`s with `0`s. It returns `NULL` if all operands are `NULL`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_ADD(a, b, c)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) + COALESCE(b, 0) + COALESCE(c, 0) END
FROM foo
```

### @SAFE_SUB

`@SAFE_SUB` subtracts two or more operands, substituting `NULL`s with `0`s. It returns `NULL` if all operands are `NULL`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_SUB(a, b, c)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  CASE WHEN a IS NULL AND b IS NULL AND c IS NULL THEN NULL ELSE COALESCE(a, 0) - COALESCE(b, 0) - COALESCE(c, 0) END
FROM foo
```

### @SAFE_DIV

`@SAFE_DIV` divides two numbers, returning `NULL` if the denominator is `0`.

For example, the following query:

```sql linenums="1"
SELECT
  @SAFE_DIV(a, b)
FROM foo
```
would be rendered as:

```sql linenums="1"
SELECT
  a / NULLIF(b, 0)
FROM foo
```

### @UNION

`@UNION` returns a `UNION` query that selects all columns with matching names and data types from the tables.

Its first argument can be either a condition or the `UNION` "type". If the first argument evaluates to a boolean (`TRUE` or `FALSE`), it's treated as a condition. If the condition is `FALSE`, only the first table is returned. If it's `TRUE`, the union operation is performed.

If the first argument is not a boolean condition, it's treated as the `UNION` "type": either `'DISTINCT'` (removing duplicated rows) or `'ALL'` (returning all rows). Subsequent arguments are the tables to be combined.

Let's assume that:

- `foo` is a table that contains three columns: `a` (`INT`), `b` (`TEXT`), `c` (`TEXT`)
- `bar` is a table that contains three columns: `a` (`INT`), `b` (`INT`), `c` (`TEXT`)

Then, the following expression:

```sql linenums="1"
@UNION('distinct', foo, bar)
```

would be rendered as:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
UNION
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM bar
```

If the union type is omitted, `'ALL'` is used as the default. So the following expression:

```sql linenums="1"
@UNION(foo, bar)
```

would be rendered as:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
UNION ALL
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM bar
```

You can also use a condition to control whether the union happens:

```sql linenums="1"
@UNION(1 > 0, 'all', foo, bar)
```

This would render the same as above. However, if the condition is `FALSE`:

```sql linenums="1"
@UNION(1 > 2, 'all', foo, bar)
```

Only the first table would be selected:

```sql linenums="1"
SELECT
  CAST(a AS INT) AS a,
  CAST(c AS TEXT) AS c
FROM foo
```

### @HAVERSINE_DISTANCE

`@HAVERSINE_DISTANCE` returns the [haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) between two geographic points.

It supports the following arguments, in this order:

- `lat1`: Latitude of the first point
- `lon1`: Longitude of the first point
- `lat2`: Latitude of the second point
- `lon2`: Longitude of the second point
- `unit` (optional): The measurement unit, currently only `'mi'` (miles, default) and `'km'` (kilometers) are supported

Vulcan macro operators do not accept named arguments. For example, `@HAVERSINE_DISTANCE(lat1=lat_column)` will error.

For example, the following query:

```sql linenums="1"
SELECT
  @HAVERSINE_DISTANCE(driver_y, driver_x, passenger_y, passenger_x, 'mi') AS dist
FROM rides
```

would be rendered as:

```sql linenums="1"
SELECT
  7922 * ASIN(SQRT((POWER(SIN(RADIANS((passenger_y - driver_y) / 2)), 2)) + (COS(RADIANS(driver_y)) * COS(RADIANS(passenger_y)) * POWER(SIN(RADIANS((passenger_x - driver_x) / 2)), 2)))) * 1.0 AS dist
FROM rides
```

### @PIVOT

`@PIVOT` returns a set of columns as a result of pivoting an input column on the specified values. This operation is sometimes described a pivoting from a "long" format (multiple values in a single column) to a "wide" format (one value in each of multiple columns).

It supports the following arguments, in this order:

- `column`: The column to pivot
- `values`: The values to use for pivoting (one column is created for each value in `values`)
- `alias` (optional): Whether to create aliases for the resulting columns, defaults to true
- `agg` (optional): The aggregation function to use, defaults to `SUM`
- `cmp` (optional): The comparison operator to use for comparing the column values, defaults to `=`
- `prefix` (optional): A prefix to use for all aliases
- `suffix` (optional): A suffix to use for all aliases
- `then_value` (optional): The value to be used if the comparison succeeds, defaults to `1`
- `else_value` (optional): The value to be used if the comparison fails, defaults to `0`
- `quote` (optional): Whether to quote the resulting aliases, defaults to true
- `distinct` (optional): Whether to apply a `DISTINCT` clause for the aggregation function, defaults to false

Like all Vulcan macro functions, omitting an argument when calling `@PIVOT` requires passing subsequent arguments with their name and the special `:=` keyword operator. For example, we might omit the `agg` argument with `@PIVOT(status, ['cancelled', 'completed'], cmp := '<')`. Learn more about macro function arguments [below](#positional-and-keyword-arguments).

For example, the following query:

```sql linenums="1"
SELECT
  date_day,
  @PIVOT(status, ['cancelled', 'completed'])
FROM rides
GROUP BY 1
```

would be rendered as:

```sql linenums="1"
SELECT
  date_day,
  SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) AS "'cancelled'",
  SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) AS "'completed'"
FROM rides
GROUP BY 1
```

### @DEDUPLICATE

`@DEDUPLICATE` is used to deduplicate rows in a table based on the specified partition and order columns with a window function.

It supports the following arguments, in this order:

- `relation`: The table or CTE name to deduplicate
- `partition_by`: column names, or expressions to use to identify a window of rows out of which to select one as the deduplicated row
- `order_by`: A list of strings representing the ORDER BY clause, optional - you can add nulls ordering like this: ['<column_name> desc nulls last']

For example, the following query:
```sql linenums="1"
with raw_data as (
@deduplicate(my_table, [id, cast(event_date as date)], ['event_date DESC', 'status ASC'])
)

select * from raw_data
```

would be rendered as:

```sql linenums="1"
WITH "raw_data" AS (
  SELECT
    *
  FROM "my_table" AS "my_table"
  QUALIFY
    ROW_NUMBER() OVER (PARTITION BY "id", CAST("event_date" AS DATE) ORDER BY "event_date" DESC, "status" ASC) = 1
)
SELECT
  *
FROM "raw_data" AS "raw_data"
```

### @DATE_SPINE

`@DATE_SPINE` returns the SQL required to build a date spine. The spine will include the start_date (if it is aligned to the datepart), AND it will include the end_date. This is different from the [`date_spine`](https://github.com/dbt-labs/dbt-utils?tab=readme-ov-file#date_spine-source) macro in `dbt-utils` which will NOT include the end_date. It's typically used to join in unique, hard-coded, date ranges to with other tables/views, so people don't have to constantly adjust date ranges in `where` clauses across many SQL models.

It supports the following arguments, in this order:

- `datepart`: The datepart to use for the date spine - day, week, month, quarter, year
- `start_date`: The start date for the date spine in format YYYY-MM-DD
- `end_date`: The end date for the date spine in format YYYY-MM-DD

For example, the following query:
```sql linenums="1"
WITH discount_promotion_dates AS (
  @date_spine('day', '2024-01-01', '2024-01-16')
)

SELECT * FROM discount_promotion_dates
```

would be rendered as:

```sql linenums="1"
WITH "discount_promotion_dates" AS (
  SELECT
    "_exploded"."date_day" AS "date_day"
  FROM UNNEST(CAST(GENERATE_SERIES(CAST('2024-01-01' AS DATE), CAST('2024-01-16' AS DATE), INTERVAL '1' DAY) AS
DATE[])) AS "_exploded"("date_day")
)
SELECT
  "discount_promotion_dates"."date_day" AS "date_day"
FROM "discount_promotion_dates" AS "discount_promotion_dates"
```

Note: This is DuckDB SQL and other dialects will be transpiled accordingly.
- Recursive CTEs (common table expressions) will be used for `Redshift / MySQL / MSSQL`.
- For `MSSQL` in particular, there's a recursion limit of approximately 100. If this becomes a problem, you can add an `OPTION (MAXRECURSION 0)` clause after the date spine macro logic to remove the limit. This applies for long date ranges.

### @RESOLVE_TEMPLATE

`@resolve_template` is a helper macro intended to be used in situations where you need to gain access to the *components* of the physical object name. It's intended for use in the following situations:

- Providing explicit control over table locations on a per-model basis for engines that decouple storage and compute (such as Athena, Trino, Spark etc)
- Generating references to engine-specific metadata tables that are derived from the physical table name, such as the [`<table>$properties`](https://trino.io/docs/current/connector/iceberg.html#metadata-tables) metadata table in Trino.

Under the hood, it uses the `@this_model` variable so it can only be used during the `creating` and `evaluation` [runtime stages](../../components/advanced-features/macros/variables.md#runtime-variables). Attempting to use it at the `loading` runtime stage will result in a no-op.

The `@resolve_template` macro supports the following arguments:

 - `template` - The string template to render into an AST node
 - `mode` - What type of SQLGlot AST node to return after rendering the template. Valid values are `literal` or `table`. Defaults to `literal`.

The `template` can contain the following placeholders that will be substituted:

  - `@{catalog_name}` - The name of the catalog, eg `datalake`
  - `@{schema_name}` - The name of the physical schema that Vulcan is using for the model version table, eg `vulcan__landing`
  - `@{table_name}` - The name of the physical table that Vulcan is using for the model version, eg `landing__customers__2517971505`

Note the use of the curly brace syntax `@{}` in the template placeholders - learn more [above](#embedding-variables-in-strings).

The `@resolve_template` macro can be used in a `MODEL` block:

```sql linenums="1" hl_lines="5"
MODEL (
  name datalake.landing.customers,
  ...
  physical_properties (
    location = @resolve_template('s3://warehouse-data/@{catalog_name}/prod/@{schema_name}/@{table_name}')
  )
);
-- CREATE TABLE "datalake"."vulcan__landing"."landing__customers__2517971505" ...
-- WITH (location = 's3://warehouse-data/datalake/prod/vulcan__landing/landing__customers__2517971505')
```

And also within a query, using `mode := 'table'`:

```sql linenums="1"
SELECT * FROM @resolve_template('@{catalog_name}.@{schema_name}.@{table_name}$properties', mode := 'table')
-- SELECT * FROM "datalake"."vulcan__landing"."landing__customers__2517971505$properties"
```

### @AND

`@AND` combines a sequence of operands using the `AND` operator, filtering out any NULL expressions.

For example, the following expression:

```sql linenums="1"
@AND(TRUE, NULL)
```

would be rendered as:

```sql linenums="1"
TRUE
```

### @OR

`@OR` combines a sequence of operands using the `OR` operator, filtering out any NULL expressions.

For example, the following expression:

```sql linenums="1"
@OR(TRUE, NULL)
```

would be rendered as:

```sql linenums="1"
TRUE
```

### SQL clause operators

Vulcan's macro system has six operators that correspond to different clauses in SQL syntax. They are:

- `@WITH`: common table expression `WITH` clause
- `@JOIN`: table `JOIN` clause(s)
- `@WHERE`: filtering `WHERE` clause
- `@GROUP_BY`: grouping `GROUP BY` clause
- `@HAVING`: group by filtering `HAVING` clause
- `@ORDER_BY`: ordering `ORDER BY` clause
- `@LIMIT`: limiting `LIMIT` clause

Each of these operators is used to dynamically add the code for its corresponding clause to a model's SQL query.

#### How SQL clause operators work

The SQL clause operators take a single argument that determines whether the clause is generated.

If the argument is `TRUE` the clause code is generated, if `FALSE` the code is not. The argument should be written *in SQL* and its value is evaluated with [SQLGlot's](https://github.com/tobymao/sqlglot) SQL engine.

Each SQL clause operator may only be used once in a query, but any common table expressions or subqueries may contain their own single use of the operator as well.

As an example of SQL clause operators, let's revisit the example model from the [User-defined Variables](#user-defined-variables) section above.

As written, the model will always include the `WHERE` clause. We could make its presence dynamic by using the `@WHERE` macro operator:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(TRUE) item_id > @size
GROUP BY item_id
```

The `@WHERE` argument is set to `TRUE`, so the WHERE code is included in the rendered model:

```sql linenums="1"
SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
WHERE item_id > 1
GROUP BY item_id
```

If the `@WHERE` argument were instead set to `FALSE` the `WHERE` clause would be omitted from the query.

These operators aren't too useful if the argument's value is hard-coded. Instead, the argument can consist of code executable by the SQLGlot SQL executor.

For example, the `WHERE` clause will be included in this query because 1 less than 2:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(1 < 2) item_id > @size
GROUP BY item_id
```

The operator's argument code can include macro variables.

In this example, the two numbers being compared are defined as macro variables instead of being hard-coded:

```sql linenums="1"
MODEL (
  name vulcan_example.full_model,
  kind FULL,
  cron '@daily',
  audits (assert_positive_order_ids),
);

@DEF(left_number, 1);
@DEF(right_number, 2);
@DEF(size, 1);

SELECT
  item_id,
  count(distinct id) AS num_orders,
FROM
  vulcan_example.incremental_model
@WHERE(@left_number < @right_number) item_id > @size
GROUP BY item_id
```

The argument to `@WHERE` will be "1 < 2" as in the previous hard-coded example after the macro variables `left_number` and `right_number` are substituted in.

### SQL clause operator examples

This section provides brief examples of each SQL clause operator's usage.

The examples use variants of this simple select statement:

```sql linenums="1"
SELECT *
FROM all_cities
```

#### @WITH operator

The `@WITH` operator is used to create [common table expressions](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression), or "CTEs."

CTEs are typically used in place of derived tables (subqueries in the `FROM` clause) to make SQL code easier to read. Less commonly, recursive CTEs support analysis of hierarchical data with SQL.

```sql linenums="1"
@WITH(True) all_cities as (select * from city)
select *
FROM all_cities
```

renders to

```sql linenums="1"
WITH all_cities as (select * from city)
select *
FROM all_cities
```

#### @JOIN operator

The `@JOIN` operator specifies joins between tables or other SQL objects; it supports different join types (e.g., INNER, OUTER, CROSS, etc.).

```sql linenums="1"
select *
FROM all_cities
LEFT OUTER @JOIN(True) country
  ON city.country = country.name
```

renders to

```sql linenums="1"
select *
FROM all_cities
LEFT OUTER JOIN country
  ON city.country = country.name
```

The `@JOIN` operator recognizes that `LEFT OUTER` is a component of the `JOIN` specification and will omit it if the `@JOIN` argument evaluates to False.

#### @WHERE operator

The `@WHERE` operator adds a filtering `WHERE` clause(s) to the query when its argument evaluates to True.

```sql linenums="1"
SELECT *
FROM all_cities
@WHERE(True) city_name = 'Toronto'
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
WHERE city_name = 'Toronto'
```

#### @GROUP_BY operator

```sql linenums="1"
SELECT *
FROM all_cities
@GROUP_BY(True) city_id
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
GROUP BY city_id
```

#### @HAVING operator

```sql linenums="1"
SELECT
count(city_pop) as population
FROM all_cities
GROUP BY city_id
@HAVING(True) population > 1000
```

renders to

```sql linenums="1"
SELECT
count(city_pop) as population
FROM all_cities
GROUP BY city_id
HAVING population > 1000
```

#### @ORDER_BY operator

```sql linenums="1"
SELECT *
FROM all_cities
@ORDER_BY(True) city_pop
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
ORDER BY city_pop
```

#### @LIMIT operator

```sql linenums="1"
SELECT *
FROM all_cities
@LIMIT(True) 10
```

renders to

```sql linenums="1"
SELECT *
FROM all_cities
LIMIT 10
```

## User-defined macro functions

User-defined macro functions allow the same macro code to be used in multiple models.

Vulcan supports user-defined macro functions written in two languages - SQL and Python:

- SQL macro functions must use the [Jinja templating system](../../components/advanced-features/macros/jinja.md#user-defined-macro-functions).
- Python macro functions use the SQLGlot library to allow more complex operations than macro variables and operators provide alone.

### Python macro functions

#### Setup

Python macro functions should be placed in `.py` files in the Vulcan project's `macros` directory. Multiple functions can be defined in one `.py` file, or they can be distributed across multiple files.

An empty `__init__.py` file must be present in the Vulcan project's `macros` directory. It will be created automatically when the project scaffold is created with `vulcan init`.

Each `.py` file containing a macro definition must import Vulcan's `macro` decorator with `from vulcan import macro`.

Python macros are defined as regular python functions adorned with the Vulcan `@macro()` decorator. The first argument to the function must be `evaluator`, which provides the macro evaluation context in which the macro function will run.

#### Inputs and outputs

Python macros parse all arguments passed to the macro call with SQLGlot before they are used in the function body. Therefore, unless [argument type annotations are provided](#argument-data-types) in the function definition, the macro function code must process SQLGlot expressions and may need to extract the expression's attributes/contents for use.

Python macro functions may return values of either `string` or SQLGlot `expression` types. Vulcan will automatically parse returned strings into a SQLGlot expression after the function is executed so they can be incorporated into the model query's semantic representation.

Macro functions may [return a list of strings or expressions](#returning-more-than-one-value) that all play the same role in the query (e.g., specifying column definitions). For example, a list containing multiple `CASE WHEN` statements would be incorporated into the query properly, but a list containing both `CASE WHEN` statements and a `WHERE` clause would not.

#### Macro function basics

This example demonstrates the core requirements for defining a python macro - it takes no user-supplied arguments and returns the string `text`.

```python linenums="1"
from vulcan import macro

@macro() # Note parentheses at end of `@macro()` decorator
def print_text(evaluator):
  return 'text'
```

We could use this in a Vulcan SQL model like this:

```sql linenums="1"
SELECT
  @print_text() as my_text
FROM table
```

After processing, it will render to this:

```sql linenums="1"
SELECT
  text as my_text
FROM table
```

Note that the python function returned a string `'text'`, but the rendered query uses `text` as a column name. That is due to the function's returned text being parsed as SQL code by SQLGlot and integrated into the query's semantic representation.

The rendered query will treat `text` as a string if we double-quote the single-quoted value in the function definition as `"'text'"`:

```python linenums="1"
from vulcan import macro

@macro()
def print_text(evaluator):
    return "'text'"
```

When run in the same model query as before, this will render to:

```sql linenums="1"
SELECT
  'text' as my_text
FROM table
```

#### Argument data types

Most macro functions provide arguments so users can supply custom values when the function is called. The data type of the argument plays a key role in how the macro code processes its value, and providing type annotations in the macro definition ensures that the macro code receives the data type it expects. This section provides a brief description of Vulcan macro type annotation - find additional information [below](#typed-macros).

As [mentioned above](#inputs-and-outputs), argument values passed to the macro call are parsed by SQLGlot before they become available to the function code. If an argument does not have a type annotation in the macro function definition, its value will always be a SQLGlot expression in the function body. Therefore, the macro function code must operate directly on the expression (and may need to extract information from it before usage).

If an argument does have a type annotation in the macro function definition, the value passed to the macro call will be coerced to that type after parsing by SQLGlot and before the values are used in the function body. Essentially, Vulcan will extract the relevant information of the annotated data type from the expression for you (if possible).

For example, this macro function determines whether an argument's value is any of the integers 1, 2, or 3:

```python linenums="1"
from vulcan import macro

@macro()
def arg_in_123(evaluator, my_arg):
    return my_arg in [1,2,3]
```

When this macro is called, it will return `FALSE` even if an integer was passed in the call. Consider this macro call:

``` sql linenums="1"
SELECT
  @arg_in_123(1)
```

It returns `SELECT FALSE` because:

1. The passed value `1` is parsed by SQLGlot into a SQLGlot expression before the function code executes and
2. There is no matching SQLGlot expression in `[1,2,3]`

However, the macro will treat the argument like a normal Python function does if we annotate `my_arg` with the integer `int` type in the function definition:

```python linenums="1"
from vulcan import macro

@macro()
def arg_in_123(evaluator, my_arg: int): # Type annotation `my_arg: int`
    return my_arg in [1,2,3]
```

Now the macro call will return `SELECT TRUE` because the value is coerced to a Python integer before the function code executes and `1` is in `[1,2,3]`.

If an argument has a default value, the value is not parsed by SQLGlot before the function code executes. Therefore, take care to ensure that the default's data type matches that of a user-supplied argument by adding a type annotation, making the default value a SQLGlot expression, or making the default value `None`.

#### Positional and keyword arguments

In a macro call, the arguments may be provided by position if none are skipped.

For example, consider the `add_args()` function - it has three arguments with default values provided in the function definition:

```python linenums="1"
from vulcan import macro

@macro()
def add_args(
    evaluator,
    argument_1: int = 1,
    argument_2: int = 2,
    argument_3: int = 3
):
    return argument_1 + argument_2 + argument_3
```

An `@add_args` call providing values for all arguments accepts positional arguments like this: `@add_args(5, 6, 7)` (which returns 5 + 6 + 7 = `18`). A call omitting and using the default value for the the final `argument_3` can also use positional arguments: `@add_args(5, 6)` (which returns 5 + 6 + 3 = `14`).

However, skipping an argument requires specifying the names of subsequent arguments (i.e., using "keyword arguments"). For example, skipping the second argument above by just omitting it - `@add_args(5, , 7)` - results in an error.

Unlike Python, Vulcan keyword arguments must use the special operator `:=`. To skip and use the default value for the second argument above, the call must name the third argument: `@add_args(5, argument_3 := 8)` (which returns 5 + 2 + 8 = `15`).

#### Variable-length arguments

The `add_args()` macro defined in the [previous section](#positional-and-keyword-arguments) accepts only three arguments and requires that all three have a value. This greatly limits the macro's flexibility because users may want to add any number of values together.

The macro can be improved by allowing users to provide any number of arguments at call time. We use Python's "variable-length arguments" to accomplish this:

```python linenums="1"
from vulcan import macro

@macro()
def add_args(evaluator, *args: int): # Variable-length arguments of integer type `*args: int`
    return sum(args)
```

This macro can be called with one or more arguments. For example:

- `@add_args(1)` returns 1
- `@add_args(1, 2)` returns 3
- `@add_args(1, 2, 3)` returns 6

#### Returning more than one value

Macro functions are a convenient way to tidy model code by creating multiple outputs from one function call. Python macro functions do this by returning a list of strings or SQLGlot expressions.

For example, we might want to create indicator variables from the values in a string column. We can do that by passing in the name of column and a list of values for which it should create indicators, which we then interpolate into `CASE WHEN` statements.

Because Vulcan parses the input objects, they become SQLGLot expressions in the function body. Therefore, the function code **cannot** treat the input list as a regular Python list.

Two things will happen to the input Python list before the function code is executed:

1. Each of its entries will be parsed by SQLGlot. Different inputs are parsed into different SQLGlot expressions:
    - Numbers are parsed into [`Literal` expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)
    - Quoted strings are parsed into [`Literal` expressions](https://sqlglot.com/sqlglot/expressions.html#Literal)
    - Unquoted strings are parsed into [`Column` expressions](https://sqlglot.com/sqlglot/expressions.html#Column)

2. The parsed entries will be contained in a SQLGlot [`Array` expression](https://sqlglot.com/sqlglot/expressions.html#Array), the SQL entity analogous to a Python list

Because the input  `Array` expression named `values` is not a Python list, we cannot iterate over it directly - instead, we iterate over its `expressions` attribute with `values.expressions`:

```python linenums="1"
from vulcan import macro

@macro()
def make_indicators(evaluator, string_column, values):
    cases = []

    for value in values.expressions: # Iterate over `values.expressions`
        cases.append(f"CASE WHEN {string_column} = '{value}' THEN '{value}' ELSE NULL END AS {string_column}_{value}")

    return cases
```

We call this function in a model query to create `CASE WHEN` statements for the `vehicle` column values `truck` and `bus` like this:

```sql linenums="1"
SELECT
  @make_indicators(vehicle, [truck, bus])
FROM table
```

Which renders to:

```sql linenums="1"
SELECT
  CASE WHEN vehicle = 'truck' THEN 'truck' ELSE NULL END AS vehicle_truck,
  CASE WHEN vehicle = 'bus' THEN 'bus' ELSE NULL END AS vehicle_bus,
FROM table
```

Note that in the call `@make_indicators(vehicle, [truck, bus])` none of the three values is quoted.

Because they are unquoted, SQLGlot will parse them all as `Column` expressions. In the places we used single quotes when building the string (`'{value}'`), they will be single-quoted in the output. In the places we did not quote them (`{string_column} = ` and `{string_column}_{value}`), they will not.

#### Accessing predefined and local variable values

[Pre-defined variables](../../components/advanced-features/macros/variables.md#predefined-variables) and [user-defined local variables](#local-variables) can be accessed within the macro's body via the `evaluator.locals` attribute.

The first argument to every macro function, the macro evaluation context `evaluator`, contains macro variable values in its `locals` attribute. `evaluator.locals` is a dictionary whose key:value pairs are macro variables names and the associated values.

For example, a function could access the predefined `execution_epoch` variable containing the epoch timestamp of when the execution started.

```python linenums="1"
from vulcan import macro

@macro()
def get_execution_epoch(evaluator):
    return evaluator.locals['execution_epoch']
```

The function would return the `execution_epoch` value when called in a model query:

```sql linenums="1"
SELECT
  @get_execution_epoch() as execution_epoch
FROM table
```

The same approach works for user-defined local macro variables, where the key `"execution_epoch"` would be replaced with the name of the user-defined variable to be accessed.

One downside of that approach to accessing user-defined local variables is that the name of the variable is hard-coded into the function. A more flexible approach is to pass the name of the local macro variable as a function argument:

```python linenums="1"
from vulcan import macro

@macro()
def get_macro_var(evaluator, macro_var):
    return evaluator.locals[macro_var]
```

We could define a local macro variable `my_macro_var` with a value of 1 and pass it to the `get_macro_var` function like this:

```sql linenums="1"
MODEL (...);

@DEF(my_macro_var, 1); -- Define local macro variable 'my_macro_var'

SELECT
  @get_macro_var('my_macro_var') as macro_var_value -- Access my_macro_var value from Python macro function
FROM table
```

The model query would render to:

```sql linenums="1"
SELECT
  1 as macro_var_value
FROM table
```

#### Accessing global variable values

[User-defined global variables](#global-variables) can be accessed within the macro's body using the `evaluator.var` method.

If a global variable is not defined, the method will return a Python `None` value. You may provide a different default value as the method's second argument.

For example:

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    var_value = evaluator.var("<var_name>") # Default value is `None`
    another_var_value = evaluator.var("<another_var_name>", "default_value") # Default value is `"default_value"`
    ...
```

#### Accessing model, physical table, and virtual layer view names

All Vulcan models have a name in their `MODEL` specification. We refer to that as the model's "unresolved" name because it may not correspond to any specific object in the SQL engine.

When Vulcan renders and executes a model, it converts the model name into three forms at different stages:

1. The *fully qualified* name

    - If the model name is of the form `schema.table`, Vulcan determines the correct catalog and adds it, like `catalog.schema.table`
    - Vulcan quotes each component of the name using the SQL engine's quoting and case-sensitivity rules, like `"catalog"."schema"."table"`

2. The *resolved* physical table name

    - The qualified name of the model's underlying physical table

3. The *resolved* virtual layer view name

    - The qualified name of the model's virtual layer view in the environment where the model is being executed

You can access any of these three forms in a Python macro through properties of the `evaluation` context object.

Access the unresolved, fully-qualified name through the `this_model_fqn` property.

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    # Example:
    # Name in model definition: landing.customers
    # Value returned here: '"datalake"."landing"."customers"'
    unresolved_model_fqn = evaluator.this_model_fqn
    ...
```

Access the resolved physical table and virtual layer view names through the `this_model` property.

The `this_model` property returns different names depending on the runtime stage:

- `promoting` runtime stage: `this_model` resolves to the virtual layer view name

    - Example
        - Model name is `db.test_model`
        - `plan` is running in the `dev` environment
        - `this_model` resolves to `"catalog"."db__dev"."test_model"` (note the `__dev` suffix in the schema name)

- All other runtime stages: `this_model` resolves to the physical table name

    - Example
        - Model name is `db.test_model`
        - `plan` is running in any environment
        - `this_model` resolves to `"catalog"."vulcan__project"."project__test_model__684351896"`

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    if evaluator.runtime_stage == "promoting":
        # virtual layer view name '"catalog"."db__dev"."test_model"'
        resolved_name = evaluator.this_model
    else:
        # physical table name '"catalog"."vulcan__project"."project__test_model__684351896"'
        resolved_name = evaluator.this_model
    ...
```

#### Accessing model schemas

Model schemas can be accessed within a Python macro function through its evaluation context's `column_to_types()` method, if the column types can be statically determined. For instance, a schema of an [external model](../../components/model/types/external_models.md) can be accessed only after the `vulcan create_external_models` command has been executed.

This macro function renames the columns of an upstream model by adding a prefix to them:

```python linenums="1"
from sqlglot import exp
from vulcan.core.macros import macro

@macro()
def prefix_columns(evaluator, model_name, prefix: str):
    renamed_projections = []

    # The following converts `model_name`, which is a SQLGlot expression, into a lookup key,
    # assuming that it does not contain quotes. If it did, we would have to generate SQL for
    # each part of `model_name` separately and then concatenate these parts, because in that
    # case `model_name.sql()` would produce an invalid lookup key.
    model_name_sql = model_name.sql()

    for name in evaluator.columns_to_types(model_name_sql):
        new_name = prefix + name
        renamed_projections.append(exp.column(name).as_(new_name))

    return renamed_projections
```

This can then be used in a SQL model like this:

```sql linenums="1"
MODEL (
  name schema.child,
  kind FULL
);

SELECT
  @prefix_columns(schema.parent, 'stg_')
FROM
  schema.parent
```

Note that `columns_to_types` expects an _unquoted model name_, such as `schema.parent`. Since macro arguments without type annotations are SQLGlot expressions, the macro code must extract meaningful information from them. For instance, the lookup key in the above macro definition is extracted by generating the SQL code for `model_name` using the `sql()` method.

Accessing the schema of an upstream model can be useful for various reasons. For example:

- Renaming columns so that downstream consumers are not tightly coupled to external or source tables
- Selecting only a subset of columns that satisfy some criteria (e.g. columns whose names start with a specific prefix)
- Applying transformations to columns, such as masking PII or computing various statistics based on the column types

Thus, leveraging `columns_to_types` can also enable one to write code according to the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principle, as a single macro function can implement the transformations instead of creating a different macro for each model of interest.

Note: there may be models whose schema is not available when the project is being loaded, in which case a special placeholder column will be returned, aptly named: `__schema_unavailable_at_load__`. In some cases, the macro's implementation will need to account for this placeholder in order to avoid issues due to the schema being unavailable.

#### Accessing snapshots

After a Vulcan project has been successfully loaded, its snapshots can be accessed in Python macro functions and Python models that generate SQL through the `get_snapshot` method of `MacroEvaluator`.

This enables the inspection of physical table names or the processed intervals for certain snapshots at runtime, as shown in the example below:

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def some_macro(evaluator):
    if evaluator.runtime_stage == "evaluating":
        # Check the intervals a snapshot has data for and alter the behavior of the macro accordingly
        intervals = evaluator.get_snapshot("some_model_name").intervals
        ...
    ...
```

#### Using SQLGlot expressions

Vulcan automatically parses strings returned by Python macro functions into [SQLGlot](https://github.com/tobymao/sqlglot) expressions so they can be incorporated into the model query's semantic representation. Functions can also return SQLGlot expressions directly.

For example, consider a macro function that uses the `BETWEEN` operator in the predicate of a `WHERE` clause. A function returning the predicate as a string might look like this, where the function arguments are substituted into a Python f-string:

```python linenums="1"
from vulcan import macro, SQL

@macro()
def between_where(evaluator, column_name: SQL, low_val: SQL, high_val: SQL):
    return f"{column_name} BETWEEN {low_val} AND {high_val}"
```

The function could then be called in a query:

```sql linenums="1"
SELECT
  a
FROM table
WHERE @between_where(a, 1, 3)
```

And it would render to:

```sql linenums="1"
SELECT
  a
FROM table
WHERE a BETWEEN 1 and 3
```

Alternatively, the function could return a [SQLGLot expression](https://github.com/tobymao/sqlglot/blob/main/sqlglot/expressions.py) equivalent to that string by using SQLGlot's expression methods for building semantic representations:

```python linenums="1"
from vulcan import macro

@macro()
def between_where(evaluator, column, low_val, high_val):
    return column.between(low_val, high_val)
```

The methods are available because the `column` argument is parsed as a SQLGlot [Column expression](https://sqlglot.com/sqlglot/expressions.html#Column) when the macro function is executed.

Column expressions are sub-classes of the [Condition class](https://sqlglot.com/sqlglot/expressions.html#Condition), so they have builder methods like [`between`](https://sqlglot.com/sqlglot/expressions.html#Condition.between) and [`like`](https://sqlglot.com/sqlglot/expressions.html#Condition.like).

#### Macro pre/post-statements

Macro functions may be used to generate pre/post-statements in a model.

By default, when you first add the pre/post-statement macro functions to a model, Vulcan will treat those models as directly modified and require a backfill in the next plan. Vulcan will also treat edits to or removals of pre/post-statement macros as a breaking change.

If your macro does not affect the data returned by a model and you do not want its addition/editing/removal to trigger a backfill, you can specify in the macro definition that it only affects the model's metadata. Vulcan will still detect changes and create new snapshots for a model when you add/edit/remove the macro, but it will not view the change as breaking and require a backfill.

Specify that a macro only affects a model's metadata by setting the `@macro()` decorator's `metadata_only` argument to `True`. For example:

```python linenums="1" hl_lines="3"
from vulcan import macro

@macro(metadata_only=True)
def print_message(evaluator, message):
  print(message)
```

### Typed Macros

Typed macros in Vulcan bring the power of type hints from Python, enhancing readability, maintainability, and usability of your SQL macros. These macros enable developers to specify expected types for arguments, making the macros more intuitive and less error-prone.

#### Benefits of Typed Macros

1. **Improved Readability**: By specifying types, the intent of the macro is clearer to other developers or future you.
2. **Reduced Boilerplate**: No need for manual type conversion within the macro function, allowing you to focus on the core logic.
3. **Enhanced Autocompletion**: IDEs can provide better autocompletion and documentation based on the specified types.

#### Defining a Typed Macro

Typed macros in Vulcan use Python's type hints. Here's a simple example of a typed macro that repeats a string a given number of times:

```python linenums="1"
from vulcan import macro

@macro()
def repeat_string(evaluator, text: str, count: int):
    return text * count
```

This macro takes two arguments: `text` of type `str` and `count` of type `int`, and it returns a string.

Without type hints, the inputs are two SQLGlot `exp.Literal` objects you would need to manually convert to Python `str` and `int` types. With type hints, you can work with them as string and integer types directly.

Let's try to use the macro in a Vulcan model:

```sql linenums="1"
SELECT
  @repeat_string('Vulcan ', 3) as repeated_string
FROM some_table;
```

Unfortunately, this model generates an error when rendered:

```
Error: Invalid expression / Unexpected token. Line 1, Col: 23.
  Vulcan Vulcan Vulcan
```

Why? The macro returned `Vulcan Vulcan Vulcan` as expected, but that string is not valid SQL in the rendered query:

```sql linenums="1" hl_lines="2"
SELECT
  Vulcan Vulcan Vulcan as repeated_string ### invalid SQL code
FROM some_table;
```

The problem is a mismatch between our macro's Python return type `str` and the type expected by the parsed SQL query.

Recall that Vulcan macros work by modifying the query's semantic representation. In that representation, a SQLGlot string literal type is expected. Vulcan will do its best to return the type expected by the query's semantic representation, but that is not possible in all scenarios.

Therefore, we must explicitly convert the output with SQLGlot's `exp.Literal.string()` method:

```python linenums="1" hl_lines="5"
from vulcan import macro

@macro()
def repeat_string(evaluator, text: str, count: int):
    return exp.Literal.string(text * count)
```

Now the query will render with a valid single-quoted string literal:

```sql linenums="1"
SELECT
  'Vulcan Vulcan Vulcan ' AS "repeated_string"
FROM "some_table" AS "some_table"
```

Typed macros coerce the **inputs** to a macro function, but the macro code is responsible for coercing the **output** to the type expected by the query's semantic representation.

#### Supported Types

Vulcan supports common Python types for typed macros including:

- `str` -- This handles string literals and basic identifiers, but won't coerce anything more complicated.
- `int`
- `float`
- `bool`
- `datetime.datetime`
- `datetime.date`
- `SQL` -- When you want the SQL string representation of the argument that's passed in
- `list[T]` - where `T` is any supported type including sqlglot expressions
- `tuple[T]` - where `T` is any supported type including sqlglot expressions
- `T1 | T2 | ...` - where `T1`, `T2`, etc. are any supported types including sqlglot expressions

We also support SQLGlot expressions as type hints, allowing you to ensure inputs are coerced to the desired SQL AST node your intending on working with. Some useful examples include:

- `exp.Table`
- `exp.Column`
- `exp.Literal`
- `exp.Identifier`

While these might be obvious examples, you can effectively coerce an input into _any_ SQLGlot expression type, which can be useful for more complex macros. When coercing to more complex types, you will almost certainly need to pass a string literal since expression to expression coercion is limited. When a string literal is passed to a macro that hints at a SQLGlot expression, the string will be parsed using SQLGlot and coerced to the correct type. Failure to coerce to the correct type will result in the original expression being passed to the macro and a warning being logged for the user to address as-needed.

```python linenums="1"
@macro()
def stamped(evaluator, query: exp.Select) -> exp.Subquery:
    return query.select(exp.Literal.string(str(datetime.now())).as_("stamp")).subquery()

# Coercing to a complex node like `exp.Select` works as expected given a string literal input
# SELECT * FROM @stamped('SELECT a, b, c')
```

When coercion fails, there will always be a warning logged but we will not crash. We believe the macro system should be flexible by default, meaning the default behavior is preserved if we cannot coerce. Given that, the user can express whatever level of additional checks they want. For example, if you would like to raise an error when the coercion fails, you can use an `assert` statement. For example:

```python linenums="1"
@macro()
def my_macro(evaluator, table: exp.Table) -> exp.Column:
    assert isinstance(table, exp.Table)
    table.set("catalog", "dev")
    return table

# Works
# SELECT * FROM @my_macro('some.table')
# SELECT * FROM @my_macro(some.table)

# Raises an error thanks to the users inclusion of the assert, otherwise would pass through the string literal and log a warning
# SELECT * FROM @my_macro('SELECT 1 + 1')
```

In using assert this way, you still get the benefits of reducing/removing the boilerplate needed to coerce types; but you **also** get guarantees about the type of the input. This is a useful pattern and is user-defined, so you can use it as you see fit. It ultimately allows you to keep the macro definition clean and focused on the core business logic.

#### Advanced Typed Macros

You can create more complex macros using advanced Python features like generics. For example, a macro that accepts a list of integers and returns their sum:

```python linenums="1"
from typing import List
from vulcan import macro

@macro()
def sum_integers(evaluator, numbers: List[int]) -> int:
    return sum(numbers)
```

Usage in Vulcan:

```sql linenums="1"
SELECT
  @sum_integers([1, 2, 3, 4, 5]) as total
FROM some_table;
```

Generics can be nested and are resolved recursively allowing for fairly robust type hinting.

See examples of the coercion function in action in the test suite [here](https://github.com/TobikoData/vulcan/blob/main/tests/core/test_macros.py).

#### Conclusion

Typed macros in Vulcan not only enhance the development experience by making macros more readable and easier to use but also contribute to more robust and maintainable code. By leveraging Python's type hinting system, developers can create powerful and intuitive macros for their SQL queries, further bridging the gap between SQL and Python.

## Mixing macro systems

Vulcan supports both Vulcan and [Jinja](../../components/advanced-features/macros/jinja.md) macro systems. We strongly recommend using only one system in a model - if both are present, they may fail or behave in unintuitive ways.



# Overview

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/overview/

---

# Overview

This page provides a conceptual overview of what Vulcan does and how its components fit together.

## What Vulcan is
Vulcan is a Python framework that automates everything needed to run a scalable data transformation platform. Vulcan works with a variety of execution engines.

It was created with a focus on both data and organizational scale and works regardless of your data warehouse or SQL engine's capabilities.

You can use Vulcan with the [CLI](getting_started/cli.md).

## How Vulcan works
### Create models
You begin by writing your business logic in SQL or Python. A model consists of code that populates a single table or view, along with metadata properties such as the model's name.

### Make a plan
Creating new models or changing existing models can have dramatic downstream effects in large data systems. Complex interdependencies between models make it challenging to determine the implications of changes to even a single model.

Beyond understanding the logical implications of a change, you also need to understand the computations required to implement the change *before* you expend the time and resources to actually perform the computations.

Vulcan automatically identifies all affected models and the computations a change entails by creating a "Vulcan plan." When you execute the [`plan` command](getting_started/cli.md#plan), Vulcan generates the plan *for the environment specified in the command* (e.g., dev, test, prod).

The plan conveys the full scope of a change's effects in the environment by automatically identifying both directly and indirectly-impacted models. This gives a holistic view of all impacts a change will have.

Learn more about [plans](../guides/plan.md).

#### Apply the plan
After using [`plan`](getting_started/cli.md#plan) to understand the impacts of a change in an environment, Vulcan offers to execute the computations by [`apply`](../guides/plan.md#plan-application)ing the plan. However, you must provide additional information that determines the scope of what computations are executed.

The computations needed to apply a Vulcan plan are determined by both the code changes reflected in the plan and the backfill parameters you specify.

"Backfilling" is the process of updating existing data to align with your changed models. For example, if your model change alters a calculation, then all existing data based on the old calculation method will be inaccurate once the new model is deployed. Backfilling entails re-calculating the existing fields whose calculation method has now changed.

Most business data is temporal &mdash; each data fact was collected at a specific moment in time. The scale of backfill computations is directly tied to how much historical data must be re-calculated.

The Vulcan plan automatically determines which models and dates require backfill due to your changes. Based on this information, you specify the dates for which backfills will occur before you apply the plan.

#### Build a Virtual Environment
Development activities for complex data systems should occur in a non-production environment so that errors can be detected before being deployed in production systems.

One challenge with using multiple data environments is that backfill and other computations must happen twice &mdash; once for the non-production, and again for the production environment. This process consumes time and computing resources, resulting in delays and extra costs.

Vulcan solves this problem by maintaining a record of all model versions and their changes. It uses this record to determine when computations executed in a non-production environment generate outputs identical to what they would generate in the production environment.

Vulcan uses its knowledge of equivalent outputs to create a **Virtual Environment**. It does this by replacing references to outdated tables in the production environment with references to newly computed tables in the non-production environment. It effectively promotes views and tables from non-production to production, but *without computation or data movement*.

Because Vulcan uses virtual environments instead of re-computing everything in the production environment, promoting changes to production is quick and has no downtime.

## Test your code and data
Bad data is worse than no data. The best way to keep bad data out of your system is by testing your transformation code and results.

### [Tests](../components/tests/tests.md)
Vulcan "tests" are similar to unit tests in software development, where the unit is a single model. Vulcan tests validate model *code* &mdash; you specify the input data and expected output, then Vulcan runs the test and compares the expected and actual output.

Vulcan automatically runs tests when you apply a `plan`, or you can run them on demand with the [`test` command](getting_started/cli.md#test).

### [Audits](../components/audits/audits.md)
In contrast to tests, Vulcan "audits" validate the results of model code applied to your actual data.

You create audits by writing SQL queries that should return 0 rows. For example, an audit query to ensure `your_field` has no `NULL` values would include `WHERE your_field IS NULL`. If any NULLs are detected, the query will return at least one row and the audit will fail.

Audits are flexible &mdash; they can be tied to a specific model's contents, or you can use [macros](../components/advanced-features/macros/overview.md) to create audits that are usable by multiple models. Vulcan also includes pre-made audits for common use cases, such as detecting NULL or duplicated values.

You specify which audits should run for a model by including them in the model's metadata properties. To apply them globally across your project, include them in the model defaults configuration.

Vulcan automatically runs audits when you apply a `plan` to an environment, or you can run them on demand with the [`audit` command](getting_started/cli.md#audit).

## Infrastructure and orchestration
Every company's data infrastructure is different. Vulcan is flexible with regard to which engines and orchestration frameworks you use &mdash; its only requirement is access to the target SQL/analytics engine.

Vulcan keeps track of model versions and processed data intervals using your existing infrastructure. Vulcan it automatically creates a `vulcan` schema in your data warehouse for its internal metadata.



# Plans

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/plans/

---

# Plans

A plan is a set of changes that summarizes the difference between the local state of a project and the state of a target [environment](environments.md). In order for any model changes to take effect in a target environment, a plan needs to be created and applied.

## Plan Architecture Overview

The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:

```mermaid
flowchart TD
    subgraph "1️⃣ Local Development"
        A[👨‍💻 Developer modifies model files<br/>📝 Edit SQL/Python models]
        B[📁 Local project state<br/>✨ Your changes ready]
    end

    subgraph "2️⃣ Plan Creation"
        C[⚡ vulcan plan<br/>🚀 Command execution]
        D[🔎 Compare local vs environment<br/>📊 State comparison]
        E{🔍 Changes detected?}
        F[📋 Generate plan summary<br/>✨ Plan ready for review]
        G[🏷️ Change categorization<br/>🔴 Breaking / 🟢 Non-breaking / 🟡 Forward-only]
    end

    subgraph "3️⃣ Plan Review"
        H[👀 Review plan output<br/>📊 Check changes & impacts]
        I{✅ Apply plan?}
        J[❌ Cancel<br/>🚫 No changes applied]
    end

    subgraph "4️⃣ Plan Application"
        K[🔷 Create model variants<br/>🔑 With unique fingerprints]
        L[🗄️ Create physical tables<br/>💾 In data warehouse]
        M[🔄 Backfill data<br/>📈 Process historical data]
        N[👁️ Update virtual layer<br/>🔍 Create/update views]
        O[🌍 Update environment references<br/>🔗 Point to new variants]
    end

    subgraph "5️⃣ Result"
        P[✅ Environment updated<br/>🎉 Changes deployed]
        Q[🔍 Models accessible via views<br/>📊 Ready for queries]
    end

    A -->|"📤"| B
    B -->|"➡️"| C
    C -->|"🔍"| D
    D -->|"🔎"| E
    E -->|"✅ Yes"| F
    E -->|"❌ No"| P
    F -->|"🏷️"| G
    G -->|"📋"| H
    H -->|"👀"| I
    I -->|"✅ Yes"| K
    I -->|"❌ No"| J
    K -->|"🔷"| L
    L -->|"💾"| M
    M -->|"🔄"| N
    N -->|"👁️"| O
    O -->|"🔗"| P
    P -->|"✨"| Q

    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000
    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
```

### Plan Components

```mermaid
graph LR
    subgraph "📋 Plan Contents"
        PC1[➕ Added Models<br/>✨ New models to create]
        PC2[➖ Removed Models<br/>🗑️ Models to delete]
        PC3[✏️ Modified Models<br/>📝 With diffs]
        PC4[🔗 Indirectly Affected<br/>📊 Downstream models]
        PC5[📅 Backfill Requirements<br/>📆 Date ranges]
    end

    subgraph "🏷️ Change Types"
        CT1[🔴 Breaking Change<br/>⚠️ Requires downstream backfill<br/>💥 Cascading impact]
        CT2[🟢 Non-Breaking Change<br/>✅ Only direct model backfill<br/>🎯 Isolated impact]
        CT3[🟡 Forward-Only<br/>♻️ Reuses existing tables<br/>💰 Cost-effective]
    end

    PC3 -->|"🔴"| CT1
    PC3 -->|"🟢"| CT2
    PC3 -->|"🟡"| CT3
    PC4 -->|"🔴"| CT1

    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000
```

During plan creation:

* The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.
* Users may be prompted to [categorize changes](#change-categories) to existing models so Vulcan can determine what actions to take for indirectly affected models (the downstream models that depend on the updated models). By default, Vulcan attempts to categorize changes automatically, but this behavior can be changed through [configuration](../configurations-old/configuration.md#plan).
* Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.

The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:

* A list of added models
* A list of removed models
* A list of directly modified models and a text diff of changes that have been made
* A list of indirectly modified models
* Missing data intervals for affected models
* A date range that will be affected by the plan application

To create a new plan, run the following command:
```bash
vulcan plan [environment name]
```

If no environment name is specified, the plan is generated for the `prod` environment.

## Change categories
Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.

If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.

### Change Propagation Flow

The following diagram illustrates how changes propagate through the dependency graph:

```mermaid
graph TD
    subgraph "📊 Model Dependencies"
        A[📥 raw.raw_orders<br/>⬆️ Upstream]
        B[📊 sales.daily_sales<br/>🔄 Midstream]
        C[📈 sales.weekly_sales<br/>⬇️ Downstream]
        D[📉 analytics.revenue_report<br/>⬇️ Downstream]
    end

    subgraph "🟢 Scenario 1: Non-Breaking Change"
        NB1[➕ Add column to daily_sales<br/>✨ New column added]
        NB2[✅ Only daily_sales backfilled<br/>🔄 Single model update]
        NB3[⏭️ weekly_sales NOT affected<br/>✅ No cascade]
        NB4[⏭️ revenue_report NOT affected<br/>✅ No cascade]
    end

    subgraph "🔴 Scenario 2: Breaking Change"
        BC1[🔍 Add WHERE clause to daily_sales<br/>⚠️ Filter logic changed]
        BC2[🔄 daily_sales backfilled<br/>📊 Data reprocessed]
        BC3[🔄 weekly_sales backfilled<br/>🔴 Indirect Breaking<br/>💥 Cascading impact]
        BC4[🔄 revenue_report backfilled<br/>🔴 Indirect Breaking<br/>💥 Cascading impact]
    end

    A -->|"📤"| B
    B -->|"📤"| C
    B -->|"📤"| D

    NB1 -->|"✏️"| B
    B -->|"✅"| NB2
    NB2 -.->|"⏭️ No cascade"| C
    NB2 -.->|"⏭️ No cascade"| D

    BC1 -->|"⚠️"| B
    B -->|"🔄"| BC2
    BC2 -->|"💥 Cascade"| BC3
    BC2 -->|"💥 Cascade"| BC4
    BC3 -->|"🔄"| C
    BC4 -->|"🔄"| D

    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
```

### Breaking change
If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.

In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to [backfilling](#backfilling)).

Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's `WHERE` clause is a breaking change because downstream models contain rows that would now be filtered out.

### Non-breaking change
A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.

This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.

If any downstream models contain a `select *` from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.

### Summary

| Change Category                      | Change Type                                                                                | Behaviour                                          |
|--------------------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------|
| [Breaking](#breaking-change)         | [Direct](glossary.md#direct-modification) or [Indirect](glossary.md#indirect-modification) | [Backfill](glossary.md#backfill)                   |
| [Non-breaking](#non-breaking-change) | [Direct](glossary.md#direct-modification)                                                  | [Backfill](glossary.md#backfill)                   |
| [Non-breaking](#non-breaking-change) | [Indirect](glossary.md#indirect-modification)                                              | [No Backfill](glossary.md#backfill)                |

## Forward-only change
In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.

A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the `prod` environment). This means that no backfill will take place.

While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.

In either case the data produced this way in the development environment can only be used for preview and will **not** be reused once the change is deployed to production. See [Forward-only Plans](#forward-only-plans) for more details.

This category is assigned by Vulcan automatically either when a user opts into using a [forward-only plan](#forward-only-plans) or when a model is explicitly configured to be forward-only.

## Plan application
Once a plan has been created and reviewed, it is then applied to the target [environment](environments.md) in order for its changes to take effect.

Every time a model is changed as part of a plan, a new variant of this model gets created behind the scenes (a [snapshot](architecture/snapshots.md) with a unique [fingerprint](architecture/snapshots.md#fingerprinting) is assigned to it). In turn, each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for [forward-only](#forward-only-plans) plans.

When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.

### Model Versioning Architecture

The following diagram shows how model variants, physical tables, and environments relate:

```mermaid
graph TB
    subgraph "📝 Model Definitions"
        M1[📊 Model: sales.daily_sales<br/>🔢 Version 1<br/>✨ Original]
        M2[📊 Model: sales.daily_sales<br/>🔢 Version 2 - Modified<br/>✏️ Updated]
    end

    subgraph "🔷 Model Variants & Snapshots"
        V1[🔷 Variant 1<br/>🔑 Fingerprint: abc123<br/>📸 Unique snapshot]
        V2[🔷 Variant 2<br/>🔑 Fingerprint: def456<br/>📸 Unique snapshot]
        S1[📸 Snapshot 1<br/>🔐 Immutable state]
        S2[📸 Snapshot 2<br/>🔐 Immutable state]
    end

    subgraph "💾 Physical Tables"
        T1[🗄️ Physical Table 1<br/>📦 db.vulcan__sales.daily_sales__abc123<br/>💾 Actual data storage]
        T2[🗄️ Physical Table 2<br/>📦 db.vulcan__sales.daily_sales__def456<br/>💾 Actual data storage]
    end

    subgraph "👁️ Virtual Layer Views"
        VL1[🔍 View: sales.daily_sales<br/>👁️ Points to Variant 1<br/>🔗 Reference mapping]
        VL2[🔍 View: sales.daily_sales<br/>👁️ Points to Variant 2<br/>🔗 Reference mapping]
    end

    subgraph "🌍 Environments"
        PROD[🚀 Production Environment<br/>✅ References Variant 1<br/>🌐 Live production data]
        DEV[🧪 Dev Environment<br/>🔬 References Variant 2<br/>🧪 Testing environment]
    end

    M1 -->|"✨"| V1
    M2 -->|"✏️"| V2
    V1 -->|"📸"| S1
    V2 -->|"📸"| S2
    S1 -->|"💾"| T1
    S2 -->|"💾"| T2
    T1 -->|"👁️"| VL1
    T2 -->|"👁️"| VL2
    PROD -->|"🔗"| V1
    DEV -->|"🔗"| V2

    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000
    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000
    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000
```

![Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)

*Each model variant gets its own physical table while environments only contain references to these tables.*

This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.

Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to [Virtual Update](#virtual-update)) as long as its physical table hasn't been garbage collected by the janitor process.

Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.

### Backfilling
Despite all the benefits, the approach described above is not without trade-offs.

When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.

We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:

* When a VIEW model is created
* When a FULL model is built
* When an INCREMENTAL model is built for the first time
* When an INCREMENTAL model has recent data appended to it
* When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)

Note for incremental models: despite the fact that backfilling can happen incrementally (see `batch_size` parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a [forward-only plan](#forward-only-plans) instead.

### Virtual Update
A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.

With this approach, the process of promoting a change to production is reduced to reference swapping.

If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.

### Start and end dates

The `plan` command provides two temporal options: `--start` and `--end`. These options are only applicable to plans for non-prod environments.

For context, every model has a start date. The start can be specified in [the model definition](../components/model/overview.md#start), in the [project configuration's `model_defaults`](guides-old/configuration.md#model-defaults), or by Vulcan's default value of yesterday.

Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the `plan` command's `--start` and `--end` options are not supported for regular plans against prod. The options are supported for [restatement plans](#restatement-plans) against prod to allow re-processing a subset of existing data.

Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the `--start` and `--end` options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.

#### Model kind limitations

Some model kinds do not support backfilling a limited date range.

For context, Vulcan strives to make models _idempotent_, meaning that if we ran them multiple times we would get the same correct result every time.

However, some model kinds are inherently non-idempotent:

- [INCREMENTAL_BY_UNIQUE_KEY](components/model/model_kinds.md#incremental_by_unique_key)
- [INCREMENTAL_BY_PARTITION](components/model/model_kinds.md#incremental_by_partition)
- [SCD_TYPE_2_BY_TIME](components/model/model_kinds.md#scd-type-2-by-time-recommended)
- [SCD_TYPE_2_BY_COLUMN](components/model/model_kinds.md#scd-type-2-by-column)
- Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)

Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:

- If the `--start` option date is the same as or before the model's start date, the model is fully refreshed for all of time
- If the `--start` option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production

#### Example

Consider a Vulcan project with a default start date of 2024-09-20.

It contains the following `INCREMENTAL_BY_UNIQUE_KEY` model that specifies an explicit start date of 2024-09-23:

```sql linenums="1" hl_lines="6"
MODEL (
  name vulcan_example.start_end_model,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key item_id
  ),
  start '2024-09-23'
);

SELECT
  item_id,
  num_orders
FROM
  vulcan_example.full_model
```

When we run the project's first plan, we see that Vulcan correctly detected a different start date for our `start_end_model` than the other models (which have the project default start of 2024-09-20):

```bash linenums="1" hl_lines="17"
❯ vulcan plan
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
`prod` environment will be initialized

Models:
└── Added:
    ├── vulcan_example.full_model
    ├── vulcan_example.incremental_model
    ├── vulcan_example.seed_model
    └── vulcan_example.start_end_model
Models needing backfill (missing dates):
├── vulcan_example.full_model: 2024-09-20 - 2024-09-26
├── vulcan_example.incremental_model: 2024-09-20 - 2024-09-26
├── vulcan_example.seed_model: 2024-09-20 - 2024-09-26
└── vulcan_example.start_end_model: 2024-09-23 - 2024-09-26
Apply - Backfill Tables [y/n]:
```

After executing that plan, we add columns to both the `incremental_model` and `start_end_model` queries.

We then execute `vulcan plan dev` to create the new `dev` environment:

```bash linenums="1" hl_lines="23-26"

❯ vulcan plan dev
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
New environment `dev` will be created from `prod`

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── vulcan_example__dev.start_end_model
│   └── vulcan_example__dev.incremental_model
└── Indirectly Modified:
    └── vulcan_example__dev.full_model

[...model diff omitted...]

Directly Modified: vulcan_example__dev.incremental_model (Non-breaking)
└── Indirectly Modified Children:
    └── vulcan_example__dev.full_model (Indirect Non-breaking)

[...model diff omitted...]

Directly Modified: vulcan_example__dev.start_end_model (Non-breaking)
Models needing backfill (missing dates):
├── vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26
└── vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26
Enter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:
```

Note two things about the output:

1. As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for `incremental_model` and 2024-09-23 for `start_end_model`
2. Vulcan prompted us for a backfill start date because we didn't pass the `--start` option to the `vulcan plan dev` command

Let's cancel that plan and start a new one, passing a start date of 2024-09-24.

The `start_end_model` is of kind `INCREMENTAL_BY_UNIQUE_KEY`, which is non-idempotent and cannot be backfilled for a limited time range.

Because the command's `--start` of 2024-09-24 is after `start_end_model`'s start date 2024-09-23, `start_end_model` is marked as preview:

``` bash linenums="1" hl_lines="12-13 20-21"
❯ vulcan plan dev --start 2024-09-24
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
New environment `dev` will be created from `prod`

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── vulcan_example__dev.start_end_model
│   └── vulcan_example__dev.incremental_model
└── Indirectly Modified:
    └── vulcan_example__dev.full_model

[...model diff omitted...]

Directly Modified: vulcan_example__dev.start_end_model (Non-breaking)
Models needing backfill (missing dates):
├── vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26
└── vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)
Enter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':
```

#### Minimum intervals

When you run a plan with a fixed `--start` or `--end` date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.

For example, if you have a model like so:

```sql
MODEL(
    name vulcan_example.monthly_model,
    kind INCREMENTAL_BY_TIME_RANGE (
        time_column month
    ),
    cron '@monthly'
);

SELECT SUM(a) AS sum_a, MONTH(day) AS month
FROM vulcan_example.upstream_model
WHERE day BETWEEN @start_ds AND @end_ds
```

make a change to it and run the following:

```bash linenums="1" hl_lines="8"
$ vulcan plan dev --start '1 day ago' 

Models:
└── Added:
    └── vulcan_example__dev.monthly_model
Apply - Virtual Update [y/n]: y

SKIP: No model batches to execute
```

No data will be backfilled because `1 day ago` does not contain a complete month. However, you can use the `--min-intervals` option to override this behaviour like so:

```bash linenums="1" hl_lines="11"
$ vulcan plan dev --start '1 day ago' --min-intervals 1

Models:
└── Added:
    └── vulcan_example__dev.monthly_model
Apply - Virtual Update [y/n]: y

[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   
Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:00                                                             
                                                                                                                                                    
✔ Model batches executed
```

This will ensure that regardless of the plan `--start` date, all added or modified models will have at least `--min-intervals` intervals considered for backfill.

!!! info

    If you are running plans manually you can just adjust the `--start` date to be wide enough to cover the models in question.

    The `--min-intervals` option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) "2 weeks worth of data" in the target environment.

### Data preview for forward-only changes
As mentioned earlier, the data output produced by [forward-only changes](#forward-only-change) in a development environment can only be used for preview and will not be reused in production.

The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.

Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with `(preview)` in the plan summary:
```bash
Models needing backfill (missing dates):
├── sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)
├── sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28
├── sushi__dev.top_waiters: 2023-12-22 - 2023-12-28
└── sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)
```

## Forward-only plans
Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.

When a forward-only plan is applied to the `prod` environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.

The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and [restatements](#restatement-plans).

Note that once a forward-only change is applied to `prod`, all development environments that referred to the previous versions of the updated models will be impacted.

A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, `vulcan plan [environment name]` evaluates code in non-`prod` environments while targeting shallow (a.k.a. "zero-copy") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.

This means that only a limited preview of changes is available in the development environment before the change is promoted to `prod`. The date range of the preview is provided as part of plan creation command.

Engines for which table cloning is supported include:

* `BigQuery`
* `Databricks`
* `Snowflake`

Note that all changes made as part of a forward-only plan automatically get a **forward-only** category assigned to them. These types of changes can't be mixed together with [breaking and non-breaking changes](#change-categories) within the same plan.

To create a forward-only plan, add the `--forward-only` option to the `plan` command:
```bash
vulcan plan [environment name] --forward-only
```

!!! note
    The `--forward-only` flag is not required when applying changes to models that have been explicitly configured as [forward-only](components/model/overview.md#forward_only).

    Use it only if you need to provide a time range for the preview window or the [effective date](#effective-date).

### Destructive changes

Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to [forward-only models](guides/incremental_by_time.md#forward-only-models) - learn more [here](guides/incremental_by_time.md#destructive-changes).

Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.

Vulcan determines what to do for each model based on this setting hierarchy: 

- **For destructive changes**: the [model's `on_destructive_change` value](guides/incremental_by_time.md#schema-changes) (if present), the `on_destructive_change` [model defaults](../configurations-old/configuration.md#model-defaults) value (if present), and the Vulcan global default of `error`
- **For additive changes**: the [model's `on_additive_change` value](guides/incremental_by_time.md#schema-changes) (if present), the `on_additive_change` [model defaults](../configurations-old/configuration.md#model-defaults) value (if present), and the Vulcan global default of `allow`

If you want to temporarily allow destructive changes to models that don't allow them, use the `plan` command's `--allow-destructive-model` selector to specify which models. 
Similarly, if you want to temporarily allow additive changes to models configured with `on_additive_change=error`, use the `--allow-additive-model` selector. 

For example, to allow destructive changes to all models in the `analytics` schema:
```bash
vulcan plan --forward-only --allow-destructive-model "analytics.*"
```

Or to allow destructive changes to multiple specific models:
```bash
vulcan plan --forward-only --allow-destructive-model "sales.revenue_model" --allow-destructive-model "marketing.campaign_model"
```

Learn more about model selectors [here](../guides/model_selection.md).

### Effective date
Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:

```bash
vulcan plan --forward-only --effective-from 2023-01-01
```

This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.

## Restatement plans

Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.

For example, these scenarios all require re-evaluating model data that already exists:

- Correcting an upstream data issue by reprocessing some of a model's existing data
- Retroactively applying a [forward-only plan](#forward-only-plans) change to some historical data
- Fully refreshing a model

In Vulcan, reprocessing existing data is called a "restatement."

Restate one or more models' data with the `plan` command's `--restate-model` selector. The [selector](../guides/model_selection.md) lets you specify which models to restate by name, wildcard, or tag (syntax [below](#restatement-examples)).

!!! warning "No changes allowed"

    Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.

    You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running `vulcan plan` without the `--restate-model` option.

Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.

You may restate external models. An [external model](../components/model/types/external_models.md) is just metadata about an external table, so the model does not actually reprocess anything. Instead, it triggers a cascading backfill of all downstream models.

The plan's `--start` and `--end` date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more [below](#model-kind-limitations).

!!! info "Just catching up"

    Restatement plans "catch models up" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.

    If you pass an `--end` date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.

To prevent models from ever being restated, set the [disable_restatement](components/model/overview.md#disable_restatement) attribute to `true`.

<a name="restatement-examples"></a>
These examples demonstrate how to select which models to restate based on model names or model tags.

=== "Names Only"

    ```bash
    vulcan plan --restate-model "db.model_a" --restate-model "tag:expensive"
    ```

=== "Upstream"

    ```bash
    # All selected models (including upstream models) will also include their downstream models
    vulcan plan --restate-model "+db.model_a" --restate-model "+tag:expensive"
    ```

=== "Wildcards"

    ```bash
    vulcan plan --restate-model "db*" --restate-model "tag:exp*"
    ```

=== "Upstream + Wildcards"

    ```bash
    vulcan plan --restate-model "+db*" --restate-model "+tag:exp*"
    ```

=== "Specific Date Range"

    ```bash
    vulcan plan --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
    ```

### Restating production vs development

Restatement plans behave differently depending on if you're targeting the `prod` environment or a [development environment](concepts-old/environments.md#how-to-use-environments).

If you target a development environment by including an environment name like `dev`:

```bash
vulcan plan dev --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
```

the restatement plan will restate the requested intervals for the specified model in the `dev` environment. In other environments, the model will be unaffected.

However, if you target the `prod` environment by omitting an environment name:

```bash
vulcan plan --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
```

the restatement plan will restate the intervals in the `prod` table *and clear the model's time intervals from state in every other environment*.

The next time you do a run in `dev`, the intervals already reprocessed in `prod` are reprocessed in `dev` as well. This is to prevent old data from getting promoted to `prod` in the future.

This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:

 - Table `A` exists in `prod`
 - A virtual environment `dev` is created with new tables `B` and `C` downstream of `A`
    - the DAG in `prod` looks like `A`
    - the DAG in `dev` looks like `A <- B <- C`
 - A restatement plan is executed against table `A` in `prod`
 - Vulcan will clear the affected intervals for `B` and `C` in `dev` even though those tables do not exist in `prod`

!!! info "Bringing development environments up to date"

    A restatement plan against `prod` clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.

    Execute `vulcan run <environment name>` to trigger reprocessing in the development environment.

    This is necessary because a `prod` restatement plan only does work in the `prod` environment for speed and efficiency.


# State

Source: https://tmdc-io.github.io/vulcan-book/concepts-old/state/

---

# State

Vulcan stores information about your project in a state database that is usually separate from your main warehouse.

The Vulcan state database contains:

- Information about every [Model Version](../components/model/overview.md) in your project (query, loaded intervals, dependencies)
- A list of every [Virtual Data Environment](concepts-old/environments.md) in the project
- Which model versions are [promoted](../guides/plan.md#plan-application) into each [Virtual Data Environment](concepts-old/environments.md)
- Information about any [auto restatements](../components/model/overview.md#auto_restatement_cron) present in your project
- Other metadata about your project such as current Vulcan / SQLGlot version

The state database is how Vulcan "remembers" what it's done before so it can compute a minimum set of operations to apply changes instead of rebuilding everything every time. It's also how Vulcan tracks what historical data has already been backfilled for [incremental models](../components/model/model_kinds.md#incremental_by_time_range) so you dont need to add branching logic into the model query to handle this.

!!! info "State database performance"

    The workload against the state database is an OLTP workload that requires transaction support in order to work correctly.

    For the best experience, we recommend databases designed for OLTP workloads such as [PostgreSQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md).

    Using your warehouse OLAP database to store state is supported for proof-of-concept projects but is not suitable for production and **will** lead to poor performance and consistency.

    For more information on engines suitable for the Vulcan state database, see the [configuration guide](guides-old/configuration.md#state-connection).

## Exporting / Importing State

Vulcan supports exporting the state database to a `.json` file. From there, you can inspect the file with any tool that can read text files. You can also pass the file around and import it back in to a Vulcan project running elsewhere.

### Exporting state

Vulcan can export the state database to a file like so:

```bash
$ vulcan state export -o state.json
Exporting state to 'state.json' from the following connection:

Gateway: dev
State Connection:
├── Type: postgres
├── Catalog: sushi_dev
└── Dialect: postgres

Continue? [y/n]: y

    Exporting versions ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3   • 0:00:00
   Exporting snapshots ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 17/17 • 0:00:00
Exporting environments ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1   • 0:00:00

State exported successfully to 'state.json'
```

This will produce a file `state.json` in the current directory containing the Vulcan state.

The state file is a simple `json` file that looks like:

```json
{
    /* State export metadata */
    "metadata": {
        "timestamp": "2025-03-16 23:09:00+00:00", /* UTC timestamp of when the file was produced */
        "file_version": 1, /* state export file format version */
        "importable": true /* whether or not this file can be imported with `vulcan state import` */
    },
    /* Library versions used to produce this state export file */
    "versions": {
        "schema_version": 76 /* vulcan state database schema version */,
        "sqlglot_version": "26.10.1" /* version of SQLGlot used to produce the state file */,
        "vulcan_version": "0.165.1" /* version of Vulcan used to produce the state file */,
    },
    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */
    "snapshots": [
        { "name": "..." }
    ],
    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */
    "environments": {
        "prod": {
            /* information about the environment itself */
            "environment": {
                "..."
            },
            /* information about any before_all / after_all statements for this environment */
            "statements": [
                "..."
            ]
        }
    }
}
```

#### Specific environments

You can export a specific environment like so:

```sh
$ vulcan state export --environment my_dev -o my_dev_state.json
```

Note that every snapshot that is part of the environment will be exported, not just the differences from `prod`. The reason for this is so that the environment can be fully imported elsewhere without any assumptions about which snapshots are already present in state.

#### Local state

You can export local state like so:

```bash
$ vulcan state export --local -o local_state.json
```

This essentially just exports the state of the local context which includes local changes that have not been applied to any virtual data environments.

Therefore, a local state export will only have `snapshots` populated. `environments` will be empty because virtual data environments are only present in the warehouse / remote state. In addition, the file is marked as **not importable** so it cannot be used with a subsequent `vulcan state import` command.

### Importing state

!!! warning "Back up your state database first!"

    Please ensure you have created an independent backup of your state database in case something goes wrong during the state import.

    Vulcan tries to wrap the state import in a transaction but some database engines do not support transactions against DDL which means
    a import error has the potential to leave the state database in an inconsistent state.

Vulcan can import a state file into the state database like so:

```bash
$ vulcan state import -i state.json --replace
Loading state from 'state.json' into the following connection:

Gateway: dev
State Connection:
├── Type: postgres
├── Catalog: sushi_dev
└── Dialect: postgres

[WARNING] This destructive operation will delete all existing state against the 'dev' gateway
and replace it with what\'s in the 'state.json' file.

Are you sure? [y/n]: y

State File Information:
├── Creation Timestamp: 2025-03-31 02:15:00+00:00
├── File Version: 1
├── Vulcan version: 0.170.1.dev0
├── Vulcan migration version: 76
└── SQLGlot version: 26.12.0

    Importing versions ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3   • 0:00:00
   Importing snapshots ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 17/17 • 0:00:00
Importing environments ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1   • 0:00:00

State imported successfully from 'state.json'
```

Note that the state database structure needs to be present and up to date, so run `vulcan migrate` before running `vulcan state import` if you get a version mismatch error.

If you have a partial state export, perhaps for a single environment - you can merge it in by omitting the `--replace` parameter:

```bash
$ vulcan state import -i state.json
...

[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.
Matching snapshots or environments will be replaced.
Non-matching snapshots or environments will be ignored.

Are you sure? [y/n]: y

...
State imported successfully from 'state.json'
```


### Specific gateways

If your project has [multiple gateways](guides-old/configuration.md#gateways) with different state connections per gateway, you can target the [state_connection](guides-old/configuration.md#state-connection) of a specific gateway like so:

```bash
# state export
$ vulcan --gateway <gateway> state export -o state.json

# state import
$ vulcan --gateway <gateway> state import -i state.json
```

## Version Compatibility

When importing state, the state file must have been produced with the same major and minor version of Vulcan that is being used to import it.

If you attempt to import state with an incompatible version, you will get the following error:

```bash
$ vulcan state import -i state.json
...SNIP...

State import failed!
Error: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.
Please upgrade/downgrade your Vulcan version to match the state file before performing the import.
```

### Upgrading a state file

You can upgrade a state file produced by an old Vulcan version to be compatible with a newer Vulcan version by:

- Loading it into a local database using the older Vulcan version
- Installing the newer Vulcan version
- Running `vulcan migrate` to upgrade the state within the local database
- Running `vulcan state export` to export it back out again. The new export is now compatible with the newer version of Vulcan.

Below is an example of how to upgrade a state file created with Vulcan `0.164.1` to be compatible with Vulcan `0.165.1`.

First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:

```bash
$ python -m venv migration-env

$ . ./migration-env/bin/activate

(migration-env)$
```

Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg `the state file was created with '0.164.1'` means you need to install Vulcan `0.164.1`:

```bash
(migration-env)$ pip install "vulcan==0.164.1"
```

Add a gateway to your `config.yaml` like so:

```yaml
gateways:
  migration:
    connection:
      type: duckdb
      database: ./state-migration.duckdb
```

The goal here is to define just enough config for Vulcan to be able to use a local database to run the state export/import commands. Vulcan still needs to inherit things like the `model_defaults` from your project in order to migrate state correctly which is why we have not used an isolated directory.

!!! warning

    From here on, be sure to specify `--gateway migration` to all Vulcan commands or you run the risk of accidentally clobbering any state on your main gateway

You can now import your state export using the same version of Vulcan it was created with:

```bash
(migration-env)$ vulcan --gateway migration migrate

(migration-env)$ vulcan --gateway migration state import -i state.json
...
State imported successfully from 'state.json'
```

Now we have the state imported, we can upgrade Vulcan and export the state from the new version.
The new version was printed in the original error message, eg `You are running '0.165.1'`

To upgrade Vulcan, simply install the new version:

```bash
(migration-env)$ pip install --upgrade "vulcan==0.165.1"
```

Migrate the state to the new version:

```bash
(migration-env)$ vulcan --gateway migration migrate
```

And finally, create a new state file which is now compatible with the new Vulcan version:

```bash
 (migration-env)$ vulcan --gateway migration state export -o state-migrated.json
```

The `state-migrated.json` file is now compatible with the newer version of Vulcan.
You can then transfer it to the place you originally needed it and import it in:

```bash
$ vulcan state import -i state-migrated.json
...
State imported successfully from 'state-migrated.json'
```


# CI/CD

Source: https://tmdc-io.github.io/vulcan-book/configurations/ci_cd/

---

# CI/CD

We're working on comprehensive CI/CD documentation to help you integrate Vulcan into your deployment pipelines. Check back soon for guides on automated testing, deployment workflows, and best practices for continuous integration and delivery with Vulcan.




# BigQuery

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/bigquery/bigquery/

---

# BigQuery

Google BigQuery is a serverless, highly scalable data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It's ideal for large-scale analytics, machine learning workloads, and real-time data analysis. Vulcan integrates seamlessly with BigQuery to manage your data transformations with version control and safe deployments.

## Local/Built-in Scheduler
**Engine Adapter Type**: `bigquery`

### Prerequisites

1. A Google Cloud Platform (GCP) project with BigQuery enabled
2. A service account with appropriate permissions (or OAuth credentials)
3. A service account key file (JSON format) for authentication

### Permissions

Vulcan requires the following BigQuery permissions:

- `bigquery.datasets.create` - to create datasets (schemas)
- `bigquery.tables.create` - to create tables and views
- `bigquery.tables.getData` - to read data from tables
- `bigquery.tables.updateData` - to insert, update, and delete data
- `bigquery.jobs.create` - to run queries

The `BigQuery Data Editor` and `BigQuery Job User` roles provide these permissions.

### Connection Options

Here are all the connection parameters you can use when setting up a BigQuery gateway:

| Option    | Description                                                                                | Type   | Required |
|-----------|--------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`    | Engine type name - must be `bigquery`                                                      | string | Y        |
| `method`  | Authentication method (`service-account`, `oauth`, `oauth-secrets`, `application-default`) | string | Y        |
| `project` | The GCP project ID where BigQuery resources are located                                    | string | Y        |
| `keyfile` | Path to the service account JSON key file (required for `service-account` method)          | string | N        |

### Service Account Key File

The `keyfile` is a JSON key file downloaded from the Google Cloud Console:

1. Go to **IAM & Admin → Service Accounts**
2. Select your service account (or create a new one)
3. Go to the **Keys** tab
4. Click **Add Key → Create new key**
5. Select **JSON** and click **Create**

The downloaded file will have the following structure:

```json
{
  "type": "service_account",
  "project_id": "<your-project-id>",
  "private_key_id": "<key-id>",
  "private_key": "-----BEGIN PRIVATE KEY-----\n<private-key-content>\n-----END PRIVATE KEY-----\n",
  "client_email": "<service-account-name>@<project-id>.iam.gserviceaccount.com",
  "client_id": "<client-id>",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/<service-account-name>",
  "universe_domain": "googleapis.com"
}
```

!!! note
    The `BigQuery Data Editor` and `BigQuery Job User` roles together provide the minimum permissions required for Vulcan to operate.

!!! warning
    Never commit your keyfile to version control. Add it to `.gitignore` and store it securely.



# Databricks

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/databricks/databricks/

---

# Databricks

Databricks is a unified analytics platform built on Apache Spark that provides collaborative notebooks, automated cluster management, and a powerful SQL engine. It's ideal for large-scale data engineering, machine learning, and collaborative analytics. Vulcan integrates with Databricks to manage your data transformations using Unity Catalog and Delta Lake.

## Local/Built-in Scheduler
**Engine Adapter Type**: `databricks`

### Prerequisites

1. A Databricks workspace with SQL warehouse or cluster access
2. A personal access token or service principal credentials
3. The HTTP path to your SQL warehouse or cluster

### Permissions

Vulcan requires the following Databricks permissions:

- `USE CATALOG` on the target catalog
- `USE SCHEMA` and `CREATE SCHEMA` on the target schemas
- `CREATE TABLE` and `CREATE VIEW` on schemas
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables

### Connection Options

Here are all the connection parameters you can use when setting up a Databricks gateway:

| Option            | Description                                                                     | Type   | Required |
|-------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`            | Engine type name - must be `databricks`                                         | string | Y        |
| `server_hostname` | The Databricks workspace hostname (e.g., `adb-xxxxx.azuredatabricks.net`)       | string | Y        |
| `http_path`       | The HTTP path to the SQL warehouse or cluster                                   | string | Y        |
| `access_token`    | Personal access token or service principal token for authentication             | string | Y        |
| `catalog`         | The Unity Catalog name to use as the default catalog                            | string | Y        |

!!! note
    The `http_path` can be found in your Databricks workspace under **SQL Warehouses → [Your Warehouse] → Connection Details**.

!!! warning
    Never commit your access token to version control. Use environment variables: `access_token: {{ env_var('DATABRICKS_TOKEN') }}`


# Microsoft Fabric

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/fabric/fabric/

---

# Microsoft Fabric

Microsoft Fabric is an all-in-one analytics solution that brings together data engineering, data science, real-time analytics, and business intelligence. Built on a lakehouse architecture, it provides a unified experience across your data estate. Vulcan integrates with Fabric to manage your data transformations using SQL endpoints.

## Local/Built-in Scheduler
**Engine Adapter Type**: `fabric`

### Prerequisites

1. A Microsoft Fabric workspace with a SQL endpoint
2. Azure Active Directory service principal or user credentials
3. ODBC Driver 18 for SQL Server installed on your system

### Permissions

Vulcan requires the following Microsoft Fabric permissions:

- `CREATE SCHEMA` on the target database
- `CREATE TABLE` and `CREATE VIEW` on schemas
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- Workspace Contributor or Admin role for full access

### Connection Options

Here are all the connection parameters you can use when setting up a Fabric gateway:

| Option            | Description                                                                     | Type   | Required |
|-------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`            | Engine type name - must be `fabric`                                             | string | Y        |
| `host`            | The Fabric SQL endpoint hostname                                                | string | Y        |
| `workspace_id`    | The Microsoft Fabric workspace ID (GUID)                                        | string | Y        |
| `tenant_id`       | The Azure Active Directory tenant ID                                            | string | Y        |
| `port`            | The port number for the SQL endpoint (typically `1433`)                         | int    | Y        |
| `user`            | The service principal client ID or username                                     | string | Y        |
| `password`        | The service principal client secret or password                                 | string | Y        |
| `database`        | The name of the database (lakehouse or warehouse) to connect to                 | string | Y        |
| `driver_name`     | The ODBC driver name (default: `ODBC Driver 18 for SQL Server`)                 | string | N        |
| `odbc_properties` | Additional ODBC connection properties as key-value pairs                        | dict   | N        |

### Obtaining Credentials

#### Tenant ID

1. Go to **Azure Portal → Azure Active Directory**
2. Click **Overview**
3. Copy the **Tenant ID** (GUID format)

#### Workspace ID

1. Open your workspace in the **Microsoft Fabric Portal**
2. Look at the URL: `https://app.fabric.microsoft.com/groups/<workspace-id>/...`
3. Copy the **workspace-id** (GUID format)

#### Service Principal (Client ID & Secret)

1. Go to **Azure Portal → Azure Active Directory → App registrations**
2. Select your app (or click **New registration** to create one)
3. Copy the **Application (client) ID** — use this as `user`
4. Go to **Certificates & secrets → New client secret**
5. Copy the **Secret value** — use this as `password`

#### Host (SQL Endpoint)

1. Open your workspace in the **Microsoft Fabric Portal**
2. Go to **Workspace settings**
3. Find the **SQL connection string** or **SQL endpoint**
4. Copy the hostname (e.g., `your-workspace.datawarehouse.fabric.microsoft.com`)

!!! note
    Ensure the ODBC Driver 18 for SQL Server is installed on your system. You can download it from the [Microsoft website](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server).

!!! warning
    Never commit your client secret to version control. Use environment variables to store sensitive credentials.



# Microsoft SQL Server

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/mssql/mssql/

---

# Microsoft SQL Server

Microsoft SQL Server is a relational database management system known for its robust performance, enterprise-grade security, and comprehensive tooling. It's widely used for transactional workloads, data warehousing, and business intelligence. Vulcan integrates with SQL Server to manage your data transformations with version control and safe deployments.

## Local/Built-in Scheduler
**Engine Adapter Type**: `mssql`

### Prerequisites

1. A SQL Server instance (on-premises, Azure SQL, or SQL Server in a container)
2. A database user with appropriate permissions
3. Network connectivity to the SQL Server instance

### Permissions

Vulcan requires the following SQL Server permissions:

- `CREATE SCHEMA` on the target database
- `CREATE TABLE` and `CREATE VIEW` on schemas
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- `ALTER` on schemas for schema modifications

### Connection Options

Here are all the connection parameters you can use when setting up a SQL Server gateway:

| Option                       | Description                                                                     | Type   | Required |
|------------------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`                       | Engine type name - must be `mssql`                                              | string | Y        |
| `host`                       | The hostname or IP address of the SQL Server instance                           | string | Y        |
| `port`                       | The port number of the SQL Server instance (default: `1433`)                    | int    | Y        |
| `user`                       | The username for SQL Server authentication                                      | string | Y        |
| `password`                   | The password for SQL Server authentication                                      | string | Y        |
| `database`                   | The name of the database to connect to                                          | string | Y        |
| `concurrent_tasks`           | Maximum number of concurrent tasks (default: `4`)                               | int    | N        |
| `trust_server_certificate`   | Whether to trust the server certificate without validation (default: `false`)   | bool   | N        |

!!! note
    The `dialect` for SQL Server models should be set to `tsql` (Transact-SQL), not `mssql`.

!!! warning
    Only set `trust_server_certificate: true` in development environments. In production, configure proper SSL certificates.



# MySQL

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/mysql/mysql/

---

# MySQL

MySQL is one of the world's most popular open-source relational databases, known for its reliability, ease of use, and strong community support. It's widely used for web applications, content management systems, and data warehousing. Vulcan integrates with MySQL to manage your data transformations with version control and safe deployments.

## Local/Built-in Scheduler
**Engine Adapter Type**: `mysql`

### Prerequisites

1. A MySQL server instance (version 5.7 or higher recommended)
2. A database user with appropriate permissions
3. Network connectivity to the MySQL server

### Permissions

Vulcan requires the following MySQL permissions:

- `CREATE` on the target database for creating schemas and tables
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- `ALTER` for schema modifications
- `DROP` for table cleanup during development

### Connection Options

Here are all the connection parameters you can use when setting up a MySQL gateway:

| Option     | Description                                                     | Type   | Required |
|------------|-----------------------------------------------------------------|:------:|:--------:|
| `type`     | Engine type name - must be `mysql`                              | string | Y        |
| `host`     | The hostname or IP address of the MySQL server                  | string | Y        |
| `port`     | The port number of the MySQL server (default: `3306`)           | int    | Y        |
| `user`     | The username for MySQL authentication                           | string | Y        |
| `password` | The password for MySQL authentication                           | string | Y        |
| `database` | The name of the database to connect to                          | string | Y        |
| `charset`  | The character set for the connection (default: `utf8mb4`)       | string | N        |
| `ssl`      | SSL configuration options for secure connections                | dict   | N        |

!!! note
    MySQL 5.7 or higher is recommended. Use the `ssl` option for secure connections in production environments.

!!! warning
    Always use environment variables for passwords: `password: {{ env_var('MYSQL_PASSWORD') }}`



# Postgres

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/postgres/postgres/

---

# Postgres

PostgreSQL is a powerful, open-source relational database that works great with Vulcan. It's perfect for smaller projects, development environments, or when you want full control over your database infrastructure.

## Local/Built-in Scheduler
**Engine Adapter Type**: `postgres`

### Prerequisites

1. A PostgreSQL server instance (version 12 or higher recommended)
2. A database user with appropriate permissions
3. Network connectivity to the PostgreSQL server

### Permissions

Vulcan requires the following PostgreSQL permissions:

- `CREATE` on the target database for creating schemas
- `CREATE` on schemas for creating tables and views
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- `USAGE` on schemas

### Connection Options

Here are all the connection parameters you can use when setting up a PostgreSQL gateway:

| Option             | Description                                                                     | Type   | Required |
|--------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`             | Engine type name - must be `postgres`                                           | string | Y        |
| `host`             | The hostname of the Postgres server                                             | string | Y        |
| `user`             | The username to use for authentication with the Postgres server                 | string | Y        |
| `password`         | The password to use for authentication with the Postgres server                 | string | Y        |
| `port`             | The port number of the Postgres server                                          | int    | Y        |
| `database`         | The name of the database instance to connect to                                 | string | Y        |
| `keepalives_idle`  | The number of seconds between each keepalive packet sent to the server.         | int    | N        |
| `connect_timeout`  | The number of seconds to wait for the connection to the server. (Default: `10`) | int    | N        |
| `role`             | The role to use for authentication with the Postgres server                     | string | N        |
| `sslmode`          | The security of the connection to the Postgres server                           | string | N        |
| `application_name` | The name of the application to use for the connection                           | string | N        |

### Docker Images

The following Docker images are available for running Vulcan with PostgreSQL:

| Image | Description |
|-------|-------------|
| `tmdcio/vulcan-postgres:0.228.1` | Main Vulcan API service for PostgreSQL |
| `tmdcio/vulcan-transpiler:0.228.1` | SQL transpiler service |
| `tmdcio/vulcan-graphql:0.228.1` | GraphQL API service |

Pull the images:

```bash
docker pull tmdcio/vulcan-postgres:0.228.1
docker pull tmdcio/vulcan-transpiler:0.228.1
docker pull tmdcio/vulcan-graphql:0.228.1
```

!!! note
    Use `sslmode: require` for secure connections in production environments.

!!! warning
    Always use environment variables for passwords: `password: {{ env_var('POSTGRES_PASSWORD') }}`



# Amazon Redshift

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/redshift/redshift/

---

# Amazon Redshift

Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. Built for high-performance analytics and business intelligence workloads, it uses columnar storage and massively parallel processing (MPP) to deliver fast query performance. Vulcan integrates seamlessly with Redshift to manage your data transformations with version control and safe deployments.

## Local/Built-in Scheduler
**Engine Adapter Type**: `redshift`

### Prerequisites

1. An Amazon Redshift cluster or Redshift Serverless endpoint
2. A database user with appropriate permissions
3. Network connectivity to the Redshift cluster (VPC configuration may be required)

### Permissions

Vulcan requires the following Redshift permissions:

- `CREATE` on the target database for creating schemas
- `CREATE` on schemas for creating tables and views
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- `USAGE` on schemas

### Connection Options

Here are all the connection parameters you can use when setting up a Redshift gateway:

| Option     | Description                                                          | Type   | Required |
|------------|----------------------------------------------------------------------|:------:|:--------:|
| `type`     | Engine type name - must be `redshift`                                | string | Y        |
| `host`     | The Redshift cluster endpoint hostname                               | string | Y        |
| `port`     | The port number of the Redshift cluster (default: `5439`)            | int    | Y        |
| `user`     | The username for Redshift authentication                             | string | Y        |
| `password` | The password for Redshift authentication                             | string | Y        |
| `database` | The name of the database to connect to                               | string | Y        |
| `sslmode`  | SSL mode for the connection (`require`, `verify-ca`, `verify-full`)  | string | N        |
| `timeout`  | Connection timeout in seconds                                        | int    | N        |

!!! note
    Use `sslmode: require` or higher for secure connections in production environments.

!!! warning
    Always use environment variables for passwords: `password: {{ env_var('REDSHIFT_PASSWORD') }}`



# Snowflake

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/snowflake/snowflake/

---

# Snowflake

Snowflake is a cloud-based data warehouse that provides scalable storage and compute resources. It's ideal for enterprise workloads, large-scale analytics, and data sharing across organizations. Vulcan integrates seamlessly with Snowflake to manage your data transformations with version control and safe deployments.

## Local/Built-in Scheduler
**Engine Adapter Type**: `snowflake`

### Prerequisites

1. A Snowflake account with valid credentials
2. A warehouse available for running computations

### Permissions

Vulcan requires the following Snowflake permissions:

- `USAGE` on a warehouse to execute computations
- `CREATE SCHEMA` on the target database
- `CREATE TABLE` and `CREATE VIEW` on schemas
- `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `TRUNCATE` on tables

### Connection Options

Here are all the connection parameters you can use when setting up a Snowflake gateway:

| Option                   | Description                                                                     | Type   | Required |
|--------------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`                   | Engine type name - must be `snowflake`                                          | string | Y        |
| `account`                | The Snowflake account identifier (e.g., `org-name-account-name`)                | string | Y        |
| `user`                   | The username to use for authentication with the Snowflake server                | string | Y        |
| `password`               | The password to use for authentication with the Snowflake server                | string | Y        |
| `warehouse`              | The name of the Snowflake warehouse to use for running computations             | string | Y        |
| `database`               | The name of the Snowflake database instance to connect to                       | string | Y        |
| `role`                   | The role to use for authentication with the Snowflake server                    | string | N        |
| `authenticator`          | The Snowflake authenticator method (e.g., `externalbrowser`, `oauth`)           | string | N        |
| `token`                  | The Snowflake OAuth 2.0 access token for authentication                         | string | N        |
| `private_key_path`       | The path to the private key file to use for authentication                      | string | N        |
| `private_key_passphrase` | The passphrase to decrypt the private key (if encrypted)                        | string | N        |

!!! note
    The `account` identifier format is `<org-name>-<account-name>` (e.g., `myorg-myaccount`). Find it in your Snowflake URL.

!!! warning
    Always use environment variables for sensitive credentials: `password: {{ env_var('SNOWFLAKE_PASSWORD') }}`


# Trino

Source: https://tmdc-io.github.io/vulcan-book/configurations/engines/trino/trino/

---

# Trino

Trino (formerly PrestoSQL) is a distributed SQL query engine designed for fast, interactive analytics across large datasets. It excels at querying data from multiple sources including data lakes, databases, and object storage. Vulcan integrates with Trino to manage your data transformations using catalogs like Iceberg, Hive, and Delta Lake.

## Local/Built-in Scheduler
**Engine Adapter Type**: `trino`

### Prerequisites

1. A Trino cluster with coordinator and worker nodes
2. A catalog configured (e.g., Iceberg, Hive, or Delta Lake)
3. Network connectivity to the Trino coordinator

### Permissions

Vulcan requires the following Trino permissions (depending on your security configuration):

- `CREATE SCHEMA` on the target catalog
- `CREATE TABLE` and `CREATE VIEW` on schemas
- `SELECT`, `INSERT`, `UPDATE`, `DELETE` on tables
- `DROP TABLE` for table cleanup during development

### Connection Options

Here are all the connection parameters you can use when setting up a Trino gateway:

| Option        | Description                                                              | Type   | Required |
|---------------|--------------------------------------------------------------------------|:------:|:--------:|
| `type`        | Engine type name - must be `trino`                                       | string | Y        |
| `host`        | The hostname of the Trino coordinator                                    | string | Y        |
| `port`        | The port number of the Trino coordinator (default: `8080`)               | int    | Y        |
| `user`        | The username for Trino authentication                                    | string | Y        |
| `catalog`     | The default catalog to use for queries                                   | string | Y        |
| `http_scheme` | The HTTP scheme (`http` or `https`)                                      | string | N        |
| `password`    | The password for Trino authentication (if password authentication is enabled) | string | N        |
| `roles`       | Role to use for queries (if role-based access control is enabled)        | dict   | N        |

!!! note
    Use `http_scheme: https` for secure connections in production environments.

!!! warning
    Always use environment variables for passwords: `password: {{ env_var('TRINO_PASSWORD') }}`



# Execution Hooks

Source: https://tmdc-io.github.io/vulcan-book/configurations/options/execution_hooks/

---

# Execution Hooks

Run SQL statements or macros automatically at the start and end of `vulcan plan` and `vulcan run` commands. Automate setup and cleanup tasks: create temporary tables, grant permissions, log pipeline runs, clean up after execution.

## Overview

Two hooks run at different times:

| Hook | When it Runs | Common Use Cases |
|------|--------------|------------------|
| `before_all` | Before any model is processed | Setup tables, initialize logging, validate prerequisites |
| `after_all` | After all models are processed | Grant privileges, cleanup, send notifications, update metadata |

The `before_all` hook runs once at the beginning, before Vulcan processes any models. Use it for setup tasks. The `after_all` hook runs once at the end, after all models are processed. Use it for cleanup or post-processing.

## Basic Configuration

=== "YAML"

    ```yaml linenums="1"
    before_all:
      - CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)

      - INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)
    
    after_all:
      - "@grant_select_privileges()"

      - UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config

    config = Config(
        before_all=[
            "CREATE TABLE IF NOT EXISTS audit_log (model VARCHAR, started_at TIMESTAMP)",
            "INSERT INTO audit_log VALUES ('pipeline', CURRENT_TIMESTAMP)"
        ],
        after_all=[
            "@grant_select_privileges()",
            "UPDATE audit_log SET completed_at = CURRENT_TIMESTAMP WHERE model = 'pipeline'"
        ],
    )
    ```

## Using Macros in Hooks

Hooks execute Vulcan macros using the `@macro_name()` syntax. Macros have access to runtime context, so hooks can be dynamic. They see what views were created, what schemas are used, and what environment you're running in. Write hooks that adapt to your pipeline state.

### Available Context Variables

Macros invoked in hooks have access to:

| Property | Type | Description |
|----------|------|-------------|
| `evaluator.views` | `list[str]` | All view names created in the virtual layer |
| `evaluator.schemas` | `list[str]` | All schema names used by models |
| `evaluator.this_env` | `str` | Current environment name (e.g., `prod`, `dev`) |
| `evaluator.gateway` | `str` | Current gateway name |

---

## Use Cases

### 1. Granting Privileges on Views

Creating many views and granting permissions model-by-model gets tedious. Use `after_all` to grant access to all views at once:

```python linenums="1" title="macros/privileges.py"
from vulcan.core.macros import macro

@macro()
def grant_select_privileges(evaluator):
    """Grant SELECT on all views to the analytics role."""
    if not evaluator.views:
        return []
    
    return [
        f"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE analytics_role;"
        for view_name in evaluator.views
    ]
```

```yaml title="config.yaml"
after_all:
  - "@grant_select_privileges()"
```

!!! tip "Preventing Name Replacement"
    The `/* sqlglot.meta replace=false */` comment tells Vulcan not to replace the view name with the physical table name during SQL rendering. Without it, Vulcan might swap in the underlying table name, which breaks your GRANT statement.

### 2. Environment-Specific Execution

Use different behavior in different environments. Grant certain permissions only in production. Run cleanup tasks only in development. The `@IF` macro conditionally executes statements based on the current environment:

```yaml linenums="1" title="config.yaml"
after_all:
  # Only grant schema usage in production
  - "@IF(@this_env = 'prod', @grant_schema_usage())"
  
  # Only run cleanup in development
  - "@IF(@this_env != 'prod', @cleanup_dev_tables())"
```

```python linenums="1" title="macros/privileges.py"
from vulcan.core.macros import macro

@macro()
def grant_schema_usage(evaluator):
    """Grant USAGE on all schemas to admin role (production only)."""
    if evaluator.this_env != "prod" or not evaluator.schemas:
        return []
    
    return [
        f"GRANT USAGE ON SCHEMA {schema} TO ROLE admin_role;"
        for schema in evaluator.schemas
    ]

@macro()
def cleanup_dev_tables(evaluator):
    """Clean up temporary tables in development environments."""
    return [
        "DROP TABLE IF EXISTS temp_debug_output;",
        "DROP TABLE IF EXISTS temp_test_data;"
    ]
```

### 3. Audit Logging

Track when your pipeline runs and how long it takes. Log the start time in `before_all` and the completion time in `after_all`:

```yaml linenums="1" title="config.yaml"
before_all:
  - |
    CREATE TABLE IF NOT EXISTS pipeline_audit (
      run_id VARCHAR,
      environment VARCHAR,
      started_at TIMESTAMP,
      completed_at TIMESTAMP,
      status VARCHAR
    )
  - "@log_pipeline_start()"

after_all:
  - "@log_pipeline_end()"
```

```python linenums="1" title="macros/audit.py"
from vulcan.core.macros import macro
import uuid

@macro()
def log_pipeline_start(evaluator):
    run_id = str(uuid.uuid4())[:8]
    return [
        f"""
        INSERT INTO pipeline_audit (run_id, environment, started_at, status)
        VALUES ('{run_id}', '{evaluator.this_env}', CURRENT_TIMESTAMP, 'running')
        """
    ]

@macro()
def log_pipeline_end(evaluator):
    return [
        f"""
        UPDATE pipeline_audit 
        SET completed_at = CURRENT_TIMESTAMP, status = 'completed'
        WHERE environment = '{evaluator.this_env}' 
          AND status = 'running'
        """
    ]
```

### 4. Schema and Database Setup

Before models run, ensure all schemas they depend on exist. Instead of creating them manually or remembering the order, let `before_all` handle it:

```yaml linenums="1" title="config.yaml"
before_all:
  - CREATE SCHEMA IF NOT EXISTS staging

  - CREATE SCHEMA IF NOT EXISTS analytics

  - CREATE SCHEMA IF NOT EXISTS reporting

  - "@setup_external_tables()"
```

```python linenums="1" title="macros/setup.py"
from vulcan.core.macros import macro

@macro()
def setup_external_tables(evaluator):
    """Create external tables for data ingestion."""
    return [
        """
        CREATE EXTERNAL TABLE IF NOT EXISTS staging.raw_events (
            event_id VARCHAR,
            event_type VARCHAR,
            event_data VARCHAR,
            created_at TIMESTAMP
        )
        LOCATION 's3://data-lake/events/'
        FILE_FORMAT = (TYPE = 'PARQUET')
        """
    ]
```

### 5. Data Quality Gates

Validate source data before processing. Use `before_all` to run validation checks. If checks fail, the pipeline stops before processing bad data:

```yaml linenums="1" title="config.yaml"
before_all:
  - "@validate_source_data()"
```

```python linenums="1" title="macros/validation.py"
from vulcan.core.macros import macro

@macro()
def validate_source_data(evaluator):
    """Validate that source data meets quality requirements."""
    return [
        """
        DO $$
        DECLARE
            row_count INTEGER;
        BEGIN
            SELECT COUNT(*) INTO row_count FROM raw_data.events WHERE created_at >= CURRENT_DATE;
            IF row_count = 0 THEN
                RAISE EXCEPTION 'No data found for today in raw_data.events';
            END IF;
        END $$;
        """
    ]
```

### 6. Refresh Materialized Views

If materialized views depend on your Vulcan models, refresh them after models update. Let `after_all` handle it:

```yaml linenums="1" title="config.yaml"
after_all:
  - "@refresh_materialized_views()"
```

```python linenums="1" title="macros/refresh.py"
from vulcan.core.macros import macro

@macro()
def refresh_materialized_views(evaluator):
    """Refresh all materialized views that depend on our models."""
    materialized_views = [
        "reporting.daily_summary_mv",
        "reporting.weekly_trends_mv",
        "analytics.user_metrics_mv"
    ]
    
    return [
        f"REFRESH MATERIALIZED VIEW {mv};"
        for mv in materialized_views
    ]
```

### 7. Notification Integration

Notify your team when the pipeline finishes. Use `after_all` to send notifications. This example logs to a table. Extend it to call an external API or send emails:

```yaml linenums="1" title="config.yaml"
after_all:
  - "@notify_completion()"
```

```python linenums="1" title="macros/notify.py"
from vulcan.core.macros import macro
import os

@macro()
def notify_completion(evaluator):
    """Log completion status (integrate with your notification system)."""
    # This example logs to a table; you could also call an external API
    view_count = len(evaluator.views) if evaluator.views else 0
    schema_count = len(evaluator.schemas) if evaluator.schemas else 0
    
    return [
        f"""
        INSERT INTO notifications_log (
            environment, 
            message, 
            view_count, 
            schema_count, 
            created_at
        )
        VALUES (
            '{evaluator.this_env}',
            'Pipeline completed successfully',
            {view_count},
            {schema_count},
            CURRENT_TIMESTAMP
        )
        """
    ]
```

---

## Execution Order

```mermaid
graph LR
    A[Start] --> B[before_all]
    B --> C[Process Models]
    C --> D[after_all]
    D --> E[Complete]
    
    style B fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style E fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px
```

## Best Practices

Use macros for complex logic. If hook logic gets complicated, move it to a Python macro. Keeps your YAML config clean and makes the logic easier to test and maintain.

Make hooks idempotent. Hooks might run multiple times if a plan fails and gets retried. Make sure they're safe to run repeatedly. Use `IF NOT EXISTS`, `ON CONFLICT`, or similar patterns.

Use environment checks. Not everything should run in every environment. Use `@IF(@this_env = 'prod', ...)` to gate production-only operations so you don't accidentally run them in development.

Handle failures gracefully. Think about what happens if a hook fails. Will it break your entire pipeline? Use transactions where appropriate, and handle failures appropriately.

Document your hooks. Add comments explaining why each hook exists and what it does.

Test in development first. Always test hooks in a development environment before running them in production. Hooks run automatically, so mistakes can be costly.

## Comparison with Model-Level Hooks

You might be wondering: when should I use execution hooks versus model-level hooks? Here's the difference:

| Feature | `before_all` / `after_all` | Model `pre_statements` / `post_statements` |
|---------|---------------------------|-------------------------------------------|
| Scope | Entire pipeline | Single model |
| Runs | Once per plan/run | Once per model execution |
| Access to | All views, schemas, environment | Model-specific context |
| Use for | Global setup, cleanup, privileges | Model-specific operations |

Use execution hooks (`before_all`/`after_all`) for operations that apply to your entire pipeline: setting up audit tables, granting permissions on all views, sending completion notifications.

Use model-level hooks (`pre_statements`/`post_statements`) for operations specific to individual models: creating temporary tables that only one model needs, running model-specific validations.



# Linter

Source: https://tmdc-io.github.io/vulcan-book/configurations/options/linter/

---

# Linter

Linting automatically checks your model definitions against your team's best practices and catches common mistakes before they cause problems.

When you create a Vulcan plan, each model's code gets checked against the linting rules you've configured. If any rules are violated, Vulcan tells you so you can fix the issues before deploying.

Vulcan includes built-in rules that catch common SQL mistakes and enforce good practices. You can also write custom rules that match your team's specific requirements. This maintains code quality and catches issues early, when they're easier to fix.

## Rules

Linting rules are pattern detectors. Each rule looks for a specific pattern (or lack of a pattern) in your model code.

Some rules check that a pattern isn't present, like the `NoSelectStar` rule that prevents `SELECT *` in your outermost query. Other rules check that a pattern is present, like making sure every model has an `owner` field specified. Both types keep your code consistent and maintainable.

Rules are written in Python. Each rule is a Python class that inherits from Vulcan's `Rule` base class. You define the logic for detecting the pattern, and Vulcan handles the rest.

When you create a custom rule, implement four things:

1. Name: The class name becomes the rule's name (converted to lowercase with underscores).

2. Description: A docstring that explains what the rule checks and why it matters.

3. Pattern validation logic: The `check_model()` method that checks your model code. You can access any attribute of the `Model` object.

4. Rule violation logic: If the pattern isn't valid, return a `RuleViolation` object with a message that tells the user what's wrong and how to fix it.

``` python linenums="1"
# Class name used as rule's name
class Rule:
    # Docstring provides rule's description
    """The base class for a rule."""

    # Pattern validation logic goes in `check_model()` method
    @abc.abstractmethod
    def check_model(self, model: Model) -> t.Optional[RuleViolation]:
        """The evaluation function that checks for a violation of this rule."""

    # Rule violation object returned by `violation()` method
    def violation(self, violation_msg: t.Optional[str] = None) -> RuleViolation:
        """Return a RuleViolation instance if this rule is violated"""
        return RuleViolation(rule=self, violation_msg=violation_msg or self.summary)
```

### Built-in Rules

Vulcan includes built-in rules that catch common SQL mistakes and enforce good coding practices. These rules catch real issues seen in production.

For example, the `NoSelectStar` rule prevents using `SELECT *` in your outermost query. `SELECT *` makes it unclear what columns your model produces, which can break downstream models and make debugging harder.

Here's what the `NoSelectStar` rule looks like, with annotations showing how it's structured:

``` python linenums="1"
# Rule's name is the class name `NoSelectStar`
class NoSelectStar(Rule):
    # Docstring explaining rule
    """Query should not contain SELECT * on its outer most projections, even if it can be expanded."""

    def check_model(self, model: Model) -> t.Optional[RuleViolation]:
        # If this model does not contain a SQL query, there is nothing to validate
        if not isinstance(model, SqlModel):
            return None

        # Use the query's `is_star` property to detect the `SELECT *` pattern.
        # If present, call the `violation()` method to return a `RuleViolation` object.
        return self.violation() if model.query.is_star else None
```

Here are all of Vulcan's built-in linting rules:

| Name                       | Check type  | Explanation                                                                                                              |
| -------------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |
| `ambiguousorinvalidcolumn`  | Correctness | Vulcan found duplicate columns or was unable to determine whether a column is duplicated or not                         |
| `invalidselectstarexpansion` | Correctness | The query's top-level selection may be `SELECT *`, but only if Vulcan can expand the `SELECT *` into individual columns |
| `noselectstar`               | Stylistic   | The query's top-level selection may not be `SELECT *`, even if Vulcan can expand the `SELECT *` into individual columns |
| `nomissingaudits`             | Governance  | Vulcan did not find any `audits` in the model's configuration to test data quality.                                                 |

### User-Defined Rules

Built-in rules are good, but every team has different standards. Write custom rules that enforce your team's specific best practices.

For example, make sure every model has an `owner` field so you know who's responsible for it:

``` python linenums="1" title="linter/user.py"
import typing as t

from vulcan.core.linter.rule import Rule, RuleViolation
from vulcan.core.model import Model

class NoMissingOwner(Rule):
    """Model owner should always be specified."""

    def check_model(self, model: Model) -> t.Optional[RuleViolation]:
        # Rule violated if the model's owner field (`model.owner`) is not specified
        return self.violation() if not model.owner else None

```

Put your custom rules in the `linter/` directory of your project. Vulcan will automatically find and load any classes that inherit from `Rule` in that directory.

Once you've added a rule to your [configuration file](#applying-linting-rules), Vulcan will run it automatically when:
- You create a plan with `vulcan plan`

- You run the `vulcan lint` command

If a model violates a rule, Vulcan will stop and tell you exactly which model(s) have problems. Here's what that looks like, in this example, `full_model.sql` is missing an owner, so the plan stops:

``` bash
$ vulcan plan

Linter errors for .../models/full_model.sql:
 - nomissingowner: Model owner should always be specified.

Error: Linter detected errors in the code. Please fix them before proceeding.
```

You can also run linting on its own for faster iteration during development:

``` bash
$ vulcan lint

Linter errors for .../models/full_model.sql:
 - nomissingowner: Model owner should always be specified.

Error: Linter detected errors in the code. Please fix them before proceeding.
```

Use `vulcan lint --help` for more information.


## Applying Linting Rules

Specify which linting rules a project should apply in the project's [configuration file](../overview.md).

List which rules to run under the `linter` key. Globally enable or disable linting with the `enabled` key (defaults to `false`, so you need to turn it on).

!!! important
    Don't forget to set `enabled: true`! If you don't, Vulcan won't run any linting rules, even if you've specified them.

### Specific Linting Rules

Use just a few specific rules. List them in the `rules` array. Example that enables two built-in rules:

=== "YAML"

    ```yaml linenums="1"
    linter:
      enabled: true
      rules: ["ambiguousorinvalidcolumn", "invalidselectstarexpansion"]
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, LinterConfig

    config = Config(
        linter=LinterConfig(
            enabled=True,
            rules=["ambiguousorinvalidcolumn", "invalidselectstarexpansion"]
        )
    )
    ```

### All Linting Rules

Enable all rules. Use `"ALL"` instead of listing them individually. This runs every built-in rule plus any custom rules you've defined:

=== "YAML"

    ```yaml linenums="1"
    linter:
      enabled: True
      rules: "ALL"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, LinterConfig

    config = Config(
        linter=LinterConfig(
            enabled=True,
            rules="all",
        )
    )
    ```

Sometimes you want almost everything, but one or two rules don't fit your workflow. Use `"ALL"` and exclude specific rules with `ignored_rules`:

=== "YAML"

    ```yaml linenums="1"
    linter:
      enabled: True
      rules: "ALL" # apply all built-in and user-defined rules and error if violated
      ignored_rules: ["noselectstar"] # but don't run the `noselectstar` rule
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, LinterConfig

    config = Config(
        linter=LinterConfig(
            enabled=True,
            # apply all built-in and user-defined linting rules and error if violated
            rules="all",
             # but don't run the `noselectstar` rule
            ignored_rules=["noselectstar"]
        )
    )
    ```

### Exclude a Model from Linting

Sometimes a model legitimately needs to violate a rule. Maybe it's a legacy model you're migrating, or there's a special case. Exclude specific models from specific rules (or all rules) by adding `ignored_rules` to the model's `MODEL` block.

Here's an example where we exclude one model from one rule:

```sql linenums="1"
MODEL(
  name docs_example.full_model,
  ignored_rules ["invalidselectstarexpansion"] # or "ALL" to turn off linting completely
);
```

### Rule Violation Behavior

By default, when a rule is violated, Vulcan treats it as an error and stops execution. This ensures you fix issues before they make it to production.

Sometimes you want a rule to be a suggestion rather than a hard requirement. Maybe it's a style preference that's nice to have but not critical. Put the rule in `warn_rules` instead of `rules`. Violations are still reported, but they won't stop execution:

=== "YAML"

    ```yaml linenums="1"
    linter:
      enabled: True
      # error if `ambiguousorinvalidcolumn` rule violated
      rules: ["ambiguousorinvalidcolumn"]
      # but only warn if "invalidselectstarexpansion" is violated
      warn_rules: ["invalidselectstarexpansion"]
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, LinterConfig

    config = Config(
        linter=LinterConfig(
            enabled=True,
            # error if `ambiguousorinvalidcolumn` rule violated
            rules=["ambiguousorinvalidcolumn"],
            # but only warn if "invalidselectstarexpansion" is violated
            warn_rules=["invalidselectstarexpansion"],
        )
    )
    ```

Vulcan will raise an error if the same rule is included in more than one of the `rules`, `warn_rules`, and `ignored_rules` keys since they should be mutually exclusive.



# Model Defaults

Source: https://tmdc-io.github.io/vulcan-book/configurations/options/model_defaults/

---

# Model Defaults

The `model_defaults` section is required. You must specify a value for the `dialect` key.

All supported `model_defaults` keys are listed in the [models configuration reference](../../references/model_configuration.md#model-defaults).

## Basic Configuration

Example:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      owner: jen
      start: 2022-01-01
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
        model_defaults=ModelDefaultsConfig(
            dialect="snowflake",
            owner="jen",
            start="2022-01-01",
        ),
    )
    ```

The default model kind is `VIEW` unless you override it with the `kind` key. See [model kinds](../../components/model/model_kinds.md) for more information.

## Identifier Resolution

When a SQL engine receives a query like `SELECT id FROM "some_table"`, it needs to understand what database objects the identifiers `id` and `"some_table"` correspond to. This is identifier resolution.

Different SQL dialects resolve identifiers differently. Some identifiers are case-sensitive if quoted. Case-insensitive identifiers are usually lowercased or uppercased before the engine looks up the object.

Vulcan analyzes model queries to extract information like column-level lineage. To do this, it normalizes and quotes all identifiers in queries, [respecting each dialect's resolution rules](https://sqlglot.com/sqlglot/dialects/dialect.html#Dialect.normalize_identifier).

The normalization strategy determines whether case-insensitive identifiers are lowercased or uppercased. You can configure this per dialect. To treat all identifiers as case-sensitive in a BigQuery project:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: "bigquery,normalization_strategy=case_sensitive"
    ```

This is useful when you need to preserve name casing, since Vulcan won't normalize them.

See [normalization strategies](https://sqlglot.com/sqlglot/dialects/dialect.html#NormalizationStrategy) for all supported options.

## Gateway-Specific Model Defaults

Define gateway-specific `model_defaults` in the `gateways` section. These override the global defaults for that gateway.

```yaml linenums="1" hl_lines="6 14"
gateways:
  redshift:
    connection:
      type: redshift
    model_defaults:
      dialect: "snowflake,normalization_strategy=case_insensitive"
  snowflake:
    connection:
      type: snowflake

default_gateway: snowflake

model_defaults:
  dialect: snowflake
  start: 2025-02-05
```

This lets you customize model behavior for each gateway without affecting global `model_defaults`.

Some SQL engines treat table and column names as case-sensitive. Others treat them as case-insensitive. If your project uses both types of engines, models need to align with each engine's normalization behavior, which makes maintenance and debugging harder.

Gateway-specific `model_defaults` let you change how Vulcan performs identifier normalization per engine to align their behavior.

In the example above, the project's default dialect is `snowflake` (line 14). The `redshift` gateway overrides that with `"snowflake,normalization_strategy=case_insensitive"` (line 6).

This tells Vulcan that the `redshift` gateway's models are written in Snowflake SQL dialect (so they need to be transpiled from Snowflake to Redshift), but the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.




# Notifications

Source: https://tmdc-io.github.io/vulcan-book/configurations/options/notifications/

---

# Notifications

Vulcan can send notifications when certain events occur. Configure notifications and specify recipients in your configuration file.

## Notification Targets

Configure notifications with notification targets. Specify targets in a project's [configuration](../../references/configuration.md) file (`config.yml` or `config.py`). You can specify multiple targets for a project.

A project can specify both global and user-specific notifications. Each target's notifications are sent for all instances of each [event type](#vulcan-event-types) (for example, notifications for `run` are sent for all of the project's environments), with exceptions for audit failures and when an [override is configured for development](#notifications-during-development).

[Audit](../../components/audits/audits.md) failure notifications can be sent for specific models if five conditions are met:

1. A model's `owner` field is populated
2. The model executes one or more audits
3. The owner has a user-specific notification target configured
4. The owner's notification target `notify_on` key includes audit failure events
5. The audit fails in the `prod` environment

When those conditions are met, the audit owner will be notified if their audit failed in the `prod` environment.

There are three types of notification targets, corresponding to the two [Slack notification methods](#slack-notifications) and [email notification](#email-notifications). Specify them in either a specific user's `notification_targets` key or the top-level `notification_targets` configuration key.

This example shows the location of both user-specific and global notification targets:

=== "YAML"

    ```yaml linenums="1"
    # User notification targets
    users:
      - username: User1
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...
      - username: User2
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...

    # Global notification targets
    notification_targets:
      - notification_target_1
        ...
      - notification_target_2
        ...
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        ...,
        # User notification targets
        users=[
            User(
                username="User1",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            ),
            User(
                username="User2",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            )
        ],

        # Global notification targets
        notification_targets=[
            notification_target_1(...),
            notification_target_2(...),
        ],
        ...
    )
    ```

### Notifications During Development

Events triggering notifications may execute repeatedly during code development. To prevent excessive notifications, Vulcan can stop all but one user's notification targets.

Specify the top-level `username` configuration key with a value also present in a user-specific notification target's `username` key to only notify that user. Specify this key in either the project configuration file or a machine-specific configuration file located in `~/.vulcan`. The latter is useful if a specific machine is always used for development.

This example stops all notifications other than those for `User1`:

=== "YAML"

    ```yaml linenums="1" hl_lines="1-2"
    # Top-level `username` key: only notify User1
    username: User1
    # User1 notification targets
    users:
      - username: User1
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...
    ```

=== "Python"

    ```python linenums="1" hl_lines="3-4"
    config = Config(
        ...,
        # Top-level `username` key: only notify User1
        username="User1",
        users=[
            User(
                # User1 notification targets
                username="User1",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            ),
        ]
    )
    ```

## Vulcan Event Types

Vulcan notifications are triggered by events. Specify which events should trigger a notification in the notification target's `notify_on` field.

Notifications are supported for [`plan` application](../../guides/plan.md) start/end/failure, [`run`](../../getting_started/cli.md#run) start/end/failure, and [`audit`](../../components/audits/audits.md) failures.

For `plan` and `run` start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.

This table lists each event, its associated `notify_on` value, and its notification message:

| Event                         | `notify_on` Key Value  | Notification message                                     |
| ----------------------------- | ---------------------- | -------------------------------------------------------- |
| Plan application start        | apply_start            | "Plan apply started for environment `{environment}`."    |
| Plan application end          | apply_end              | "Plan apply finished for environment `{environment}`."   |
| Plan application failure      | apply_failure          | "Failed to apply plan.\n{exception}"                     |
| Vulcan run start             | run_start              | "Vulcan run started for environment `{environment}`."   |
| Vulcan run end               | run_end                | "Vulcan run finished for environment `{environment}`."  |
| Vulcan run failure           | run_failure            | "Failed to run Vulcan.\n{exception}"                    |
| Audit failure                 | audit_failure          | "{audit_error}"                                          |

Any combination of these events can be specified in a notification target's `notify_on` field.

## Slack Notifications

Vulcan supports two types of Slack notifications. Slack webhooks notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.

### Webhook Configuration

Vulcan uses Slack's "Incoming Webhooks" for webhook notifications. When you [create an incoming webhook](https://api.slack.com/messaging/webhooks) in Slack, you receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.

This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable `SLACK_WEBHOOK_URL` instead of hard-coding the URL:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: slack_webhook
        notify_on:
          - apply_start

          - apply_failure

          - run_start
        url: "{{ env_var('SLACK_WEBHOOK_URL') }}"
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        SlackWebhookNotificationTarget(
            notify_on=["apply_start", "apply_failure", "run_start"],
            url=os.getenv("SLACK_WEBHOOK_URL"),
        )
    ]
    ```

### API Configuration

To notify users, use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See [Slack's official documentation](https://api.slack.com/tutorials/tracks/getting-a-token) for information on getting an API token.

This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable `SLACK_API_TOKEN` instead of hard-coding the token:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: slack_api
        notify_on:
          - apply_start

          - apply_end

          - audit_failure
        token: "{{ env_var('SLACK_API_TOKEN') }}"
        channel: "UXXXXXXXXX"  # Channel or a user's Slack member ID
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        SlackApiNotificationTarget(
            notify_on=["apply_start", "apply_end", "audit_failure"],
            token=os.getenv("SLACK_API_TOKEN"),
            channel="UXXXXXXXXX",  # Channel or a user's Slack member ID
        )
    ]
    ```

## Email Notifications

Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target can notify multiple recipient email addresses.

This example shows an email notification target, where `sushi@example.com` emails `data-team@example.com` on Vulcan run failure. The specification uses environment variables `SMTP_HOST`, `SMTP_USER`, and `SMTP_PASSWORD` instead of hard-coding the values:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: smtp
        notify_on:
          - run_failure
        host: "{{ env_var('SMTP_HOST') }}"
        user: "{{ env_var('SMTP_USER') }}"
        password: "{{ env_var('SMTP_PASSWORD') }}"
        sender: sushi@example.com
        recipients:
          - data-team@example.com
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        BasicSMTPNotificationTarget(
            notify_on=["run_failure"],
            host=os.getenv("SMTP_HOST"),
            user=os.getenv("SMTP_USER"),
            password=os.getenv("SMTP_PASSWORD"),
            sender="notifications@example.com",
            recipients=[
                "data-team@example.com",
            ],
        )
    ]
    ```

## Advanced Usage

### Overriding Notification Targets

In Python configuration files, configure new notification targets to send custom messages.

To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (`SlackWebhookNotificationTarget`, `SlackApiNotificationTarget`, or `BasicSMTPNotificationTarget`). See the definitions of these classes on [Github](https://github.com/TobikoData/vulcan/blob/main/vulcan/core/notification_target.py).

Each of those notification target classes is a subclass of `BaseNotificationTarget`, which contains a `notify` function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):

| Function name        | Contextual information           |
| -------------------- | -------------------------------- |
| notify_apply_start   | Environment name: `env`          |
| notify_apply_end     | Environment name: `env`          |
| notify_apply_failure | Exception stack trace: `exc`     |
| notify_run_start     | Environment name: `env`          |
| notify_run_end       | Environment name: `env`          |
| notify_run_failure   | Exception stack trace: `exc`     |
| notify_audit_failure | Audit error trace: `audit_error` |

This example creates a new notification target class `CustomSMTPNotificationTarget`.

It overrides the default `notify_run_failure` function to read a log file `"/home/vulcan/vulcan.log"` and append its contents to the exception stack trace `exc`:

=== "Python"

```python
from vulcan.core.notification_target import BasicSMTPNotificationTarget

class CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):
    def notify_run_failure(self, exc: str) -> None:
        with open("/home/vulcan/vulcan.log", "r", encoding="utf-8") as f:
            msg = f"{exc}\n\nLogs:\n{f.read()}"
        super().notify_run_failure(msg)
```

Use this new class by specifying it as a notification target in the configuration file:

=== "Python"

    ```python linenums="1" hl_lines="2"
    notification_targets=[
        CustomSMTPNotificationTarget(
            notify_on=["run_failure"],
            host=os.getenv("SMTP_HOST"),
            user=os.getenv("SMTP_USER"),
            password=os.getenv("SMTP_PASSWORD"),
            sender="notifications@example.com",
            recipients=[
                "data-team@example.com",
            ],
        )
    ]
    ```



# Variables

Source: https://tmdc-io.github.io/vulcan-book/configurations/options/variables/

---

# Variables

Store sensitive information like passwords and API keys without hardcoding them in your configuration files. Use environment variables, `.env` files, or configuration overrides to change settings dynamically.

## Environment Variables

Vulcan accesses environment variables during configuration. Store secrets outside configuration files and change settings based on who's running Vulcan.

### Using .env Files

Vulcan automatically loads environment variables from a `.env` file in your project directory:

```bash
# .env file
SNOWFLAKE_PW=my_secret_password
S3_BUCKET=s3://my-data-bucket/warehouse
DATABASE_URL=postgresql://user:pass@localhost/db

# Override Vulcan configuration values
VULCAN__DEFAULT_GATEWAY=production
VULCAN__MODEL_DEFAULTS__DIALECT=snowflake
```

!!! warning "Security"
    Add `.env` to your `.gitignore` file to avoid committing sensitive information.

### Custom .env File Location

Specify a custom path using the `--dotenv` CLI flag:

```bash
vulcan --dotenv /path/to/custom/.env plan
```

Or set the `VULCAN_DOTENV_PATH` environment variable:

```bash
export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env
vulcan plan
```

!!! note
    The `--dotenv` flag must be placed **before** the subcommand (e.g., `plan`, `run`).

### Accessing Variables in Configuration

=== "YAML"

    Use `{{ env_var('VARIABLE_NAME') }}` syntax:

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: snowflake
          user: admin
          password: "{{ env_var('SNOWFLAKE_PW') }}"
          account: my_account
    ```

=== "Python"

    Use `os.environ`:

    ```python linenums="1"
    import os
    from vulcan.core.config import Config, GatewayConfig, SnowflakeConnectionConfig

    config = Config(
        gateways={
            "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="admin",
                    password=os.environ['SNOWFLAKE_PW'],
                    account="my_account",
                ),
            ),
        }
    )
    ```

## Configuration Overrides

Environment variables have the highest precedence. They override configuration file values if they follow the `VULCAN__` naming convention.

### Override Naming Structure

Use double underscores `__` to navigate the configuration hierarchy:

```
VULCAN__<ROOT_KEY>__<NESTED_KEY>__<FIELD>=value
```

Example: Override a gateway connection password:

```yaml linenums="1"
# config.yaml
gateways:
  my_gateway:
    connection:
      type: snowflake
      password: dummy_pw  # This will be overridden
```

```bash
# Override with environment variable
export VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD="real_pw"
```

## Dynamic Configuration

### User-based Target Environment

Use the `{{ user() }}` function to dynamically set configuration based on the current user:

=== "YAML"

    ```yaml
    # Each user gets their own dev environment
    default_target_environment: dev_{{ user() }}
    ```

=== "Python"

    ```python linenums="1"
    import getpass
    from vulcan.core.config import Config

    config = Config(
        default_target_environment=f"dev_{getpass.getuser()}",
    )
    ```

This allows running `vulcan plan` instead of `vulcan plan dev_username`.



# Overview

Source: https://tmdc-io.github.io/vulcan-book/configurations/overview/

---

# Overview

Your Vulcan project needs a configuration file. It tells Vulcan how to connect to your data warehouse, where to store state, and what defaults to use for your models. Without it, Vulcan doesn't know where your data lives or how to run your transformations.

## Configuration File

Create a configuration file in your project root. Choose one:

- `config.yaml`: YAML format. Use this for most projects. Simple and readable.

- `config.py`: Python format. Use this if you need dynamic configuration or want to generate settings programmatically.

## Example Configuration

Here's what a typical configuration file looks like:

```yaml linenums="1"
# Project metadata
name: orders360
tenant: sales
description: Daily sales analytics pipeline

# Gateway Connection
gateways:
  default:
    connection:
      type: postgres
      host: warehouse
      port: 5432
      database: warehouse
      user: vulcan
      password: "{{ env_var('DB_PASSWORD') }}"
    state_connection:
      type: postgres
      host: statestore
      port: 5432
      database: statestore
      user: vulcan
      password: "{{ env_var('STATE_DB_PASSWORD') }}"

default_gateway: default

# Model Defaults (required)
model_defaults:
  dialect: postgres
  start: 2024-01-01
  cron: '@daily'

# Linting Rules
linter:
  enabled: true
  rules:
    - ambiguousorinvalidcolumn

    - invalidselectstarexpansion
```

## Configuration Structure

```mermaid
graph TB
    Config[config.yaml]
    Config --> Project[Project Settings]
    Config --> Gateways[Gateways]
    Config --> ModelDefaults[Model Defaults]
    Config --> Options[Optional Features]
    Gateways --> Connection[connection]
    Gateways --> StateConn[state_connection]
    Gateways --> TestConn[test_connection]
    Options --> Linter[linter]
    Options --> Notifications[notifications]
    Options --> Variables[variables]
    Options --> Format[format]
```

## Configuration Sections

### Project Settings

Metadata fields that identify your project. They don't affect how Vulcan runs, but they're useful for organization.

| Option | Description | Type |
|--------|-------------|:----:|
| `name` | Project name | string |
| `tenant` | Tenant or organization name | string |
| `description` | Project description | string |

### Gateways

Gateways define how Vulcan connects to your data warehouse and state backend. Define multiple gateways for different environments: dev, staging, prod. Each gateway has its own connection settings.

| Component | Description | Default |
|-----------|-------------|---------|
| `connection` | Primary data warehouse connection | Required |
| `state_connection` | Where Vulcan stores internal state | Uses `connection` |
| `test_connection` | Connection for running tests | Uses `connection` |
| `scheduler` | Scheduler configuration | `builtin` |
| `state_schema` | Schema name for state tables | `vulcan` |

See [Configuration Reference](../references/configuration.md#gateways) for detailed gateway options.

### Model Defaults

The `model_defaults` section is required. At minimum, specify `dialect` to tell Vulcan what SQL dialect your models use. Other defaults are optional but apply to all models automatically, so you don't repeat the same settings in every model file.

```yaml
model_defaults:
  dialect: postgres     # Required
  owner: data-team
  start: 2024-01-01
  cron: '@daily'
```

See [Model Defaults](./options/model_defaults.md) for all available options.

### Variables

Store sensitive information like passwords and API keys without hardcoding them. Use environment variables, `.env` files, or configuration overrides. Variables also let you override configuration values dynamically.

See [Variables](./options/variables.md) for details.

### Execution Hooks

Run SQL statements automatically at the start and end of `vulcan plan` and `vulcan run` commands. Use `before_all` for setup tasks like creating temporary tables or granting permissions. Use `after_all` for cleanup or post-processing.

See [Execution Hooks](./options/execution_hooks.md) for detailed examples and use cases.

### Linter

Automatic code quality checks that run when you create a plan or run the lint command. Catches common mistakes and enforces coding standards. Use built-in rules or create custom ones.

See [Linter](./options/linter.md) for rules and custom linter configuration.

### Notifications

Set up alerts via Slack or email. Get notified when plans start or finish, when runs complete, or when audits fail.

See [Notifications](./options/notifications.md) for Slack webhooks, API, and email setup.

## Supported Engines

Vulcan works with these data warehouses:

- [PostgreSQL](../references/integrations/engines/postgres.md)

- [Snowflake](../references/integrations/engines/snowflake.md)

## Configuration Reference

| Topic | Description |
|-------|-------------|
| [Configuration Reference](../references/configuration.md) | Complete list of all configuration parameters |
| [Variables](./options/variables.md) | Environment variables and `.env` files |
| [Model Defaults](./options/model_defaults.md) | Default settings for all models |
| [Execution Hooks](./options/execution_hooks.md) | `before_all` and `after_all` statements |
| [Linter](./options/linter.md) | Code quality rules and custom linters |
| [Notifications](./options/notifications.md) | Slack and email notification setup |

## Best Practices

Use environment variables for sensitive data like passwords and API keys. Keeps secrets out of your config files and makes it easier to manage different environments.

Set meaningful defaults in `model_defaults` to reduce boilerplate. If most of your models use the same dialect, start date, or cron schedule, set it once here instead of repeating it everywhere.

Enable linting to catch common errors early in development. Fix issues before they make it to production.

Separate state connection from your data warehouse for better isolation. Prevents state operations from interfering with your data processing.

Use multiple gateways for different environments: dev, staging, prod. Test changes safely before deploying to production. Use different database configurations for each environment.



# Cookbook

Source: https://tmdc-io.github.io/vulcan-book/cookbook/

---

# Cookbook

Coming soon...



# B2B SaaS Example - Split Docker Compose Setup

Source: https://tmdc-io.github.io/vulcan-book/examples/exhaustive_model/README/

---

# B2B SaaS Example - Split Docker Compose Setup

This project uses split Docker Compose files for better service management and scalability. Services are organized into separate compose files that share a common external Docker network.

## Setup

You can use `make` commands for convenience, or run the docker compose commands directly.


make setup 


make vulcan-up


alias vulcan="docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev vulcan"
 
vulcan plan 



make vulcan-down 

make all-down

vulcan transpile --format sql "select measure(total_order_lines) from orders"

vulcan create_test ANALYTICS.ORDERS --query DEMO.RAW_DATA.ORDERS "select ORDER_ID ,ORDER_DATE ,CUSTOMER_ID ,PRODUCT_ID ,QUANTITY ,UNIT_PRICE ,DISCOUNT ,TAX ,SHIPPING_COST ,TOTAL_AMOUNT FROM DEMO.RAW_DATA.ORDERS
WHERE ORDER_DATE  BETWEEN '2025-01-01' and '2025-01-15'"





# Orders360 - Postgres Mini Example

Source: https://tmdc-io.github.io/vulcan-book/examples/overview/

---

# Orders360 - Postgres Mini Example

A minimal e-commerce data product demonstrating Vulcan's core capabilities with a simple orders, customers, and products domain.

## Objective

Build a lightweight sales analytics pipeline that:
- Ingests raw transactional data (customers, orders, products)
- Transforms into daily sales aggregations
- Exposes semantic layer for BI tools and analytics

## User Story

**As a** Sales Operations Manager,  
**I want** a daily view of order volumes and revenue,  
**So that** I can track sales performance and identify trends.

### Business Context

The marketing team runs campaigns across multiple channels and needs to measure their impact on daily sales. Currently, they rely on manual spreadsheet exports that are error-prone and outdated by the time they're reviewed.

### Key Questions This Data Product Answers

1. **How many orders did we process yesterday?**  
    Query `total_orders` from `daily_sales`

2. **What was our revenue for last week?**  
    Use weekly granularity on `order_date` with `total_daily_revenue` measure

3. **Which days exceeded our $100 daily target?**  
    Apply the `high_revenue_days` segment

4. **Are there any data quality issues in incoming orders?**  
    Automated checks validate completeness, uniqueness, and value ranges

### Stakeholders

| Role | Usage |
|------|-------|
| Sales Manager | Daily dashboard for revenue tracking |
| Marketing Analyst | Campaign performance correlation |
| Finance Team | Monthly revenue reconciliation |
| Data Engineer | Pipeline health monitoring |

## Project Structure

```
postgres-mini/
 seeds/                  # Raw CSV data files
 models/
 seeds/              # Seed model definitions (raw data ingestion)
 daily_sales.sql     # Aggregated daily sales model
 semantics/              # Semantic layer definitions
 checks/                 # Data quality checks (Soda-style)
 tests/                  # Unit tests for models
 audits/                 # Custom audit definitions
 config.yaml             # Project configuration
```

## Data Models

### Seed Models (Raw Layer)

| Model | Description | Grain |
|-------|-------------|-------|
| `raw.raw_customers` | Customer master data | `customer_id` |
| `raw.raw_orders` | Order transactions | `order_id` |
| `raw.raw_products` | Product catalog | `product_id` |

### Transformation Models

| Model | Description | Grain | Schedule |
|-------|-------------|-------|----------|
| `sales.daily_sales` | Daily aggregated sales metrics | `order_date` | `@daily` |

**Daily Sales** aggregates orders by date with:
- `total_orders` - Count of orders per day
- `total_revenue` - Sum of order amounts per day
- `last_order_id` - Latest order ID processed

## Semantic Layer

The semantic layer (`semantics/daily_sales.yml`) provides:

**Dimensions** with time granularities:
- `order_date` (weekly, monthly, quarterly rollups)

**Measures**:
- `total_daily_orders` - Sum of orders across date range
- `total_daily_revenue` - Sum of revenue across date range

**Segments**:
- `high_revenue_days` - Days with $100+ revenue

## Data Quality

### Assertions (Model-level)
- Unique grain validation
- Not-null constraints on required columns
- Positive value checks on amounts

### Checks (Soda-style)
- Completeness checks (missing counts, row counts)
- Validity checks (negative values, data consistency)
- Anomaly detection on row counts

## Quick Start

```bash
# Start infrastructure
make setup

# plan to see changes
vulcan plan
```

## Configuration

- **Dialect**: PostgreSQL
- **State Store**: PostgreSQL




# Frequently Asked Questions

Source: https://tmdc-io.github.io/vulcan-book/faq/

---

# Frequently Asked Questions

Coming soon...



# CLI

Source: https://tmdc-io.github.io/vulcan-book/getting_started/cli/

---

# CLI

In this quickstart, you'll use the Vulcan command line interface (CLI) to get up and running with Vulcan's scaffold generator.

It will create an example project that runs locally on your computer using [DuckDB](https://duckdb.org/) as an embedded SQL engine.

Before beginning, ensure that you meet all the [prerequisites](./prerequisites.md) for using Vulcan.

!!! note "Using Docker?"
    If you're using the Docker installation method (recommended), you'll need to run the `vulcan` commands inside the Docker shell. 

    Start the Docker shell with:
    ```bash
    make vulcan-shell
    ```
    or
    ```bash
    docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell
    ```

    Alternatively, you can create a temporary alias:
    ```bash
    alias vulcan="docker compose -f docker/docker-compose.vulcan.yml run --rm vulcan-shell vulcan"
    ```

    For a complete Docker setup guide, see the [Docker Quickstart](guides/get-started/docker.md).

??? info "Learn more about the quickstart project structure"
    This project demonstrates key Vulcan features by walking through the Vulcan workflow on a simple data pipeline. This section describes the project structure and the Vulcan concepts you will encounter as you work through it.

    The project contains three models with a CSV file as the only data source:

    ```
    ┌─────────────┐
    │seed_data.csv│
    └────────────┬┘
                 │
                ┌▼─────────────┐
                │seed_model.sql│
                └─────────────┬┘
                              │
                             ┌▼────────────────────┐
                             │incremental_model.sql│
                             └────────────────────┬┘
                                                  │
                                                 ┌▼─────────────┐
                                                 │full_model.sql│
                                                 └──────────────┘
    ```

    Although the project is simple, it touches on all the primary concepts needed to use Vulcan productively.

## 1. Create the Vulcan project
First, create a project directory and navigate to it:

```bash
mkdir vulcan-example
```
```bash
cd vulcan-example
```

!!! note "Docker Users"
    If you're using Docker, make sure your Docker infrastructure is set up and running. See the [Docker Quickstart](guides/get-started/docker.md) for setup instructions. Then access the Vulcan shell before running the commands below.

!!! note "Python Library Users (Coming Soon)"
    If using a Python virtual environment (when available), ensure it's activated first by running the `source .venv/bin/activate` command.

### 1.1 Initialize the project

Vulcan includes a scaffold generator to initialize a new Vulcan project.

The scaffold generator will ask you some questions and create a Vulcan configuration file based on your responses.

Depending on your answers, it will also create multiple files for the Vulcan example project used in this quickstart.

Start the scaffold generator by executing the `vulcan init` command:

```bash
vulcan init
```

??? info "Skip the questions"

    If you don't want to use the interactive scaffold generator, you can initialize your project with arguments to the [`vulcan init` command](getting_started/cli.md#init).

    The only required argument is `engine`, which specifies the SQL engine your project will use. Specify one of the engine `type`s from the supported execution engines.

    In this example, we specify the `duckdb` engine:

    ```bash
    vulcan init duckdb
    ```

    The scaffold will include a Vulcan configuration file and example project directories and files. You're now ready to continue the quickstart [below](#2-create-a-prod-environment).

#### Project type

The first question asks about the type of project you want to create. Enter the number corresponding to the type of project you want to create and press `Enter`.

``` bash
──────────────────────────────
Welcome to Vulcan!
──────────────────────────────

What type of project do you want to set up?

    [1] DEFAULT - Create Vulcan example project models and files
    [2] dbt     - You have an existing dbt project and want to run it with Vulcan
    [3] EMPTY   - Create a Vulcan configuration file and project directories only

Enter a number: 1
```

For this quickstart, choose the `DEFAULT` option `1` so the example project files are included in the project directories.

#### SQL engine

The second question asks which SQL engine your project will use. Vulcan will include that engine's connection settings in the configuration file, which you will fill in later to connect your project to the engine.

For this quickstart, choose the `DuckDB` option `1` so we can run the example project with the built-in DuckDB engine that doesn't need additional configuration.

``` bash
Choose your SQL engine:

    [1]  DuckDB
    [2]  Snowflake
    [3]  Databricks
    [4]  BigQuery
    [5]  MotherDuck
    [6]  ClickHouse
    [7]  Redshift
    [8]  Spark
    [9]  Trino
    [10] Azure SQL
    [11] MSSQL
    [12] Postgres
    [13] GCP Postgres
    [14] MySQL
    [15] Athena
    [16] RisingWave

Enter a number: 1
```

#### CLI mode

Vulcan's core commands have multiple options that alter their behavior. Some of those options streamline the Vulcan `plan` workflow and CLI output.

If you prefer a streamlined workflow (no prompts, no file diff previews, auto-apply changes), choose the `FLOW` CLI mode to automatically include those options in your project configuration file.

If you prefer to see all the output Vulcan provides, choose `DEFAULT` mode, which we will use in this quickstart:

``` bash
Choose your Vulcan CLI experience:

    [1] DEFAULT - See and control every detail
    [2] FLOW    - Automatically run changes and show summary output

Enter a number: 1
```

#### Ready to go

Your project is now ready to go, and Vulcan displays a message with some good next steps.

If you chose the DuckDB engine, you're ready to move forward and run the example project with DuckDB.

If you chose a different engine, add your engine's connection information to the `config.yaml` file before you run any additional Vulcan commands.

``` bash
Your Vulcan project is ready!

Next steps:
- Update your gateway connection settings (e.g., username/password) in the project configuration file:
    /vulcan-example/config.yaml
- Run command in CLI: vulcan plan
- (Optional) Explain a plan: vulcan plan --explain

Quickstart guide:
https://vulcan.readthedocs.io/en/stable/quickstart/cli/

Need help?
- Docs:   https://vulcan.readthedocs.io
- Slack:  https://www.tobikodata.com/slack
- GitHub: https://github.com/TobikoData/vulcan/issues
```

??? info "Learn more about the project's configuration: `config.yaml`"
    Vulcan project-level configuration parameters are specified in the `config.yaml` file in the project directory.

    This example project uses the embedded DuckDB SQL engine, so its configuration specifies `duckdb` as the gateway's connection type. All available configuration settings are included in the file, with optional settings set to their default value and commented out.

    Vulcan requires a default model SQL dialect. Vulcan automatically specifies the SQL dialect for your project's SQL engine, which it places in the config `model_defaults` `dialect` key. In this example, we specified the DuckDB engine, so `duckdb` is the default SQL dialect:

    ```yaml linenums="1"
    # --- Gateway Connection ---
    gateways:
      duckdb:
        connection:
          # For more information on configuring the connection to your execution engine, visit:
          # https://vulcan.readthedocs.io/en/stable/reference/configuration/#connection
          # https://vulcan.readthedocs.io/en/stable/integrations/engines/duckdb/#connection-options
          #
          type: duckdb               # <-- DuckDB engine
          database: db.db
          # concurrent_tasks: 1
          # register_comments: True  # <-- Optional setting `register_comments` has a default value of True
          # pre_ping: False
          # pretty_sql: False
          # catalogs:                # <-- Optional setting `catalogs` has no default value
          # extensions:
          # connector_config:
          # secrets:
          # token:

    default_gateway: duckdb

    # --- Model Defaults ---
    # https://vulcan.readthedocs.io/en/stable/reference/model_configuration/#model-defaults

    model_defaults:
      dialect: duckdb                # <-- Models written in DuckDB SQL dialect by default
      start: 2025-06-12 # Start date for backfill history
      cron: '@daily'    # Run models daily at 12am UTC (can override per model)

    # --- Linting Rules ---
    # Enforce standards for your team
    # https://vulcan.readthedocs.io/en/stable/guides/linter/

    linter:
      enabled: true
      rules:
        - ambiguousorinvalidcolumn
        - invalidselectstarexpansion
    ```

    Learn more about Vulcan project configuration [here](../configurations-old/configuration.md).

The scaffold generator creates multiple directories where Vulcan project files are stored and multiple files that constitute the example project (e.g., SQL models).

??? info "Learn more about the project directories and files"
    Vulcan uses a scaffold generator to initiate a new project. The generator will create multiple sub-directories and files for organizing your Vulcan project code.

    The scaffold generator will create the following configuration file and directories:

    - config.yaml
        - The file for project configuration. More info about configuration [here](guides-old/configuration.md).
    - ./models
        - SQL and Python models. More info about models [here](components/model/overview.md).
    - ./seeds
        - Seed files. More info about seeds [here](components/model/model_kinds.md#seed).
    - ./audits
        - Shared audit files. More info about audits [here](components/audits/audits.md).
    - ./tests
        - Unit test files. More info about tests [here](components/tests/tests.md).
    - ./macros
        - Macro files. More info about macros [here](../components/advanced-features/macros/overview.md).

    It will also create the files needed for this quickstart example:

    - ./models
        - full_model.sql
        - incremental_model.sql
        - seed_model.sql
    - ./seeds
        - seed_data.csv
    - ./audits
        - assert_positive_order_ids.sql
    - ./tests
        - test_full_model.yaml

Finally, the scaffold generator creates data for the example project to use.

??? info "Learn more about the project's data"
    The data used in this example project is contained in the `seed_data.csv` file in the `/seeds` project directory. The data reflects sales of 3 items over 7 days in January 2020.

    The file contains three columns, `id`, `item_id`, and `event_date`, which correspond to each row's unique ID, the sold item's ID number, and the date the item was sold, respectively.

    This is the complete dataset:

    | id | item_id | event_date |
    | -- | ------- | ---------- |
    | 1  | 2       | 2020-01-01 |
    | 2  | 1       | 2020-01-01 |
    | 3  | 3       | 2020-01-03 |
    | 4  | 1       | 2020-01-04 |
    | 5  | 1       | 2020-01-05 |
    | 6  | 1       | 2020-01-06 |
    | 7  | 1       | 2020-01-07 |

## 2. Create a prod environment

Vulcan's key actions are creating and applying *plans* to *environments*. At this point, the only environment is the empty `prod` environment.

??? info "Learn more about Vulcan plans and environments"

    Vulcan's key actions are creating and applying *plans* to *environments*.

    A [Vulcan environment](concepts-old/environments.md) is an isolated namespace containing models and the data they generated.

    The most important environment is `prod` ("production"), which consists of the databases behind the applications your business uses to operate each day. Environments other than `prod` provide a place where you can test and preview changes to model code before they go live and affect business operations.

    A [Vulcan plan](guides/plan.md) contains a comparison of one environment to another and the set of changes needed to bring them into alignment.

    For example, if a new SQL model was added, tested, and run in the `dev` environment, it would need to be added and run in the `prod` environment to bring them into alignment. Vulcan identifies all such changes and classifies them as either breaking or non-breaking.

    Breaking changes are those that invalidate data already existing in an environment. For example, if a `WHERE` clause was added to a model in the `dev` environment, existing data created by that model in the `prod` environment are now invalid because they may contain rows that would be filtered out by the new `WHERE` clause.

    Other changes, like adding a new column to a model in `dev`, are non-breaking because all the existing data in `prod` are still valid to use - only new data must be added to align the environments.

    After Vulcan creates a plan, it summarizes the breaking and non-breaking changes so you can understand what will happen if you apply the plan. It will prompt you to "backfill" data to apply the plan. (In this context, backfill is a generic term for updating or adding to a table's data, including an initial load or full refresh.)

??? info "Learn more about a plan's actions: `vulcan plan --explain`"

    Before applying a plan, you can view a detailed description of the actions it will take by passing the explain flag in your `vulcan plan` command:

    ```bash
    vulcan plan --explain
    ```

    Passing the explain flag for the quickstart example project above adds the following information to the output:

    ```bash
    Explained plan
    ├── Validate SQL and create physical layer tables and views if they do not exist
    │   ├── vulcan_example.seed_model -> db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172
    │   │   ├── Dry run model query without inserting results
    │   │   └── Create table if it doesn't exist
    │   ├── vulcan_example.full_model -> db.vulcan__vulcan_example.vulcan_example__full_model__2278521865
    │   │   ├── Dry run model query without inserting results
    │   │   └── Create table if it doesn't exist
    │   └── vulcan_example.incremental_model -> db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781
    │       ├── Dry run model query without inserting results
    │       └── Create table if it doesn't exist
    ├── Backfill models by running their queries and run standalone audits
    │   ├── vulcan_example.seed_model -> db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172
    │   │   └── Fully refresh table
    │   ├── vulcan_example.full_model -> db.vulcan__vulcan_example.vulcan_example__full_model__2278521865
    │   │   ├── Fully refresh table
    │   │   └── Run 'assert_positive_order_ids' audit
    │   └── vulcan_example.incremental_model -> db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781
    │       └── Fully refresh table
    └── Update the virtual layer for environment 'prod'
        └── Create or update views in the virtual layer to point at new physical tables and views
            ├── vulcan_example.full_model -> db.vulcan__vulcan_example.vulcan_example__full_model__2278521865
            ├── vulcan_example.seed_model -> db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172
            └── vulcan_example.incremental_model -> db.vulcan__vulcan_example.vulcan_example__incremental_model__1880815781
    ```

    The explanation has three top-level sections, corresponding to the three types of actions a plan takes:

      - Validate SQL and create physical layer tables and views if they do not exist
      - Backfill models by running their queries and run standalone audits
      - Update the virtual layer for environment 'prod'

    Each section lists the affected models and provides more information about what will occur. For example, the first model in the first section is:

    ```bash
    ├── vulcan_example.seed_model -> db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172
    │   ├── Dry run model query without inserting results
    │   └── Create table if it doesn't exist
    ```

    The first line shows the model name `vulcan_example.seed_model` and the physical layer table Vulcan will create to store its data: `db.vulcan__vulcan_example.vulcan_example__seed_model__2185867172`. The second and third lines tell us that in this step Vulcan will dry-run the model query and create the physical layer table if it doesn't exist.

    The second section describes what will occur during the backfill step. The second model in this section is:

    ```bash
    ├── vulcan_example.full_model -> db.vulcan__vulcan_example.vulcan_example__full_model__2278521865
    │   ├── Fully refresh table
    │   └── Run 'assert_positive_order_ids' audit
    ```

    The first line shows the model name `vulcan_example.full_model` and the physical layer table Vulcan will insert the model's data into: `db.vulcan__vulcan_example.vulcan_example__full_model__2278521865`. The second and third lines tell us that the backfill action will fully refresh the model's physical table and run the `assert_positive_order_ids` audit.

    The final section describes Vulcan's action during the virtual layer update step. The first model in this section is:

    ```bash
    └── Create or update views in the virtual layer to point at new physical tables and views
        ├── vulcan_example.full_model -> db.vulcan__vulcan_example.vulcan_example__full_model__2278521865
    ```

    The virtual layer step will update the `vulcan_example.full_model` virtual layer view to `SELECT * FROM` the physical table `db.vulcan__vulcan_example.vulcan_example__full_model__2278521865`.

The first Vulcan plan must execute every model to populate the production environment. Running `vulcan plan` will generate the plan and the following output:

```bash linenums="1"
$ vulcan plan
======================================================================
Successfully Ran 1 tests against duckdb in 0.1 seconds.
----------------------------------------------------------------------

`prod` environment will be initialized

Models:
└── Added:
    ├── vulcan_example.full_model
    ├── vulcan_example.incremental_model
    └── vulcan_example.seed_model
Models needing backfill:
├── vulcan_example.full_model: [full refresh]
├── vulcan_example.incremental_model: [2020-01-01 - 2025-06-22]
└── vulcan_example.seed_model: [full refresh]
Apply - Backfill Tables [y/n]:
```

Line 3 of the output notes that `vulcan plan` successfully executed the project's test `tests/test_full_model.yaml` with duckdb.

Line 6 describes what environments the plan will affect when applied - a new `prod` environment in this case.

Lines 8-12 of the output show that Vulcan detected three new models relative to the current empty environment.

Lines 13-16 list each model that will be executed by the plan, along with the date intervals or refresh types. For both `full_model` and `seed_model`, it shows `[full refresh]`, while for `incremental_model` it shows a specific date range `[2020-01-01 - 2025-06-22]`. The incremental model date range begins from 2020-01-01 because its definition specifies a model start date of `2020-01-01`.

??? info "Learn more about the project's models"

    A plan's actions are determined by the [kinds](components/model/model_kinds.md) of models the project uses. This example project uses three model kinds:

    1. [`SEED` models](components/model/model_kinds.md#seed) read data from CSV files stored in the Vulcan project directory.
    2. [`FULL` models](components/model/model_kinds.md#full) fully refresh (rewrite) the data associated with the model every time the model is run.
    3. [`INCREMENTAL_BY_TIME_RANGE` models](components/model/model_kinds.md#incremental_by_time_range) use a date/time data column to track which time intervals are affected by a plan and process only the affected intervals when a model is run.

    We now briefly review each model in the project.

    The first model is a `SEED` model that imports `seed_data.csv`. This model consists of only a `MODEL` statement because `SEED` models do not query a database.

    In addition to specifying the model name and CSV path relative to the model file, it includes the column names and data types of the columns in the CSV. It also sets the `grain` of the model to the columns that collectively form the model's unique identifier, `id` and `event_date`.

    ```sql linenums="1"
    MODEL (
      name vulcan_example.seed_model,
      kind SEED (
        path '../seeds/seed_data.csv'
      ),
      columns (
        id INTEGER,
        item_id INTEGER,
        event_date DATE
      ),
      grain (id, event_date)
    );
    ```

    The second model is an `INCREMENTAL_BY_TIME_RANGE` model that includes both a `MODEL` statement and a SQL query selecting from the first seed model.

    The `MODEL` statement's `kind` property includes the required specification of the data column containing each record's timestamp. It also includes the optional `start` property specifying the earliest date/time for which the model should process data and the `cron` property specifying that the model should run daily. It sets the model's grain to columns `id` and `event_date`.

    The SQL query includes a `WHERE` clause that Vulcan uses to filter the data to a specific date/time interval when loading data incrementally:

    ```sql linenums="1"
    MODEL (
      name vulcan_example.incremental_model,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column event_date
      ),
      start '2020-01-01',
      cron '@daily',
      grain (id, event_date)
    );

    SELECT
      id,
      item_id,
      event_date,
    FROM
      vulcan_example.seed_model
    WHERE
      event_date between @start_date and @end_date
    ```

    The final model in the project is a `FULL` model. In addition to properties used in the other models, its `MODEL` statement includes the [`audits`](components/audits/audits.md) property. The project includes a custom `assert_positive_order_ids` audit in the project `audits` directory; it verifies that all `item_id` values are positive numbers. It will be run every time the model is executed.

    ```sql linenums="1"
    MODEL (
      name vulcan_example.full_model,
      kind FULL,
      cron '@daily',
      grain item_id,
      audits (assert_positive_order_ids),
    );

    SELECT
      item_id,
      count(distinct id) AS num_orders,
    FROM
      vulcan_example.incremental_model
    GROUP BY item_id
    ```

Line 18 asks you whether to proceed with executing the model backfills described in lines 13-16. Enter `y` and press `Enter`, and Vulcan will execute the models and return this output:

```bash linenums="1"
Apply - Backfill Tables [y/n]: y

Updating physical layer ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3 • 0:00:00

✔ Physical layer updated

[1/1] vulcan_example.seed_model          [insert seed file]                 0.01s
[1/1] vulcan_example.incremental_model   [insert 2020-01-01 - 2025-06-22]   0.01s
[1/1] vulcan_example.full_model          [full refresh, audits ✔1]          0.01s
Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3 • 0:00:00

✔ Model batches executed

Updating virtual layer  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3 • 0:00:00

✔ Virtual layer updated
```

Vulcan performs three actions when applying the plan:

- Creating and storing new versions of the models
- Evaluating/running the models
- Virtually updating the plan's target environment

Lines 2-4 show the progress and completion of the first step - updating the physical layer (creating new model versions).

Lines 6-11 show the execution of each model with their specific operations and timing. Line 6 shows the seed model being inserted, line 8 shows the incremental model being inserted for the specified date range, and line 10 shows the full model being processed with its audit check passing.

Lines 12-14 show the progress and completion of the second step - executing model batches.

Lines 16-18 show the progress and completion of the final step - virtually updating the plan's target environment, which makes the data available for querying.

Let's take a quick look at the project's DuckDB database file to see the objects Vulcan created. First, we open the built-in DuckDB CLI tool with the `duckdb db.db` command, then run our two queries.

Our first query shows the three physical tables Vulcan created in the `vulcan__vulcan_example` schema (one table for each model):

![Example project physical layer tables in the DuckDB CLI](./cli/cli-quickstart_duckdb-tables.png)

Our second query shows that in the `vulcan` schema Vulcan created three virtual layer views that read from the three physical tables:

![Example project virtual layer views in the DuckDB CLI](./cli/cli-quickstart_duckdb-views.png)

You've now created a new production environment with all of history backfilled!

## 3. Update a model

Now that we have populated the `prod` environment, let's modify one of the SQL models.

We modify the incremental SQL model by adding a new column to the query. Open the `models/incremental_model.sql` file and add `#!sql 'z' AS new_column` below `item_id` as follows:

```sql linenums="1" hl_lines="14"
MODEL (
  name vulcan_example.incremental_model,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column event_date
  ),
  start '2020-01-01',
  cron '@daily',
  grain (id, event_date)
);

SELECT
  id,
  item_id,
  'z' AS new_column, -- Added column
  event_date,
FROM
  vulcan_example.seed_model
WHERE
  event_date between @start_date and @end_date
```

## 4. Work with a development environment

### 4.1 Create a dev environment
Now that you've modified a model, it's time to create a development environment so that you can validate the model change without affecting production.

Run `vulcan plan dev` to create a development environment called `dev`:

```bash linenums="1"
$ vulcan plan dev
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------

New environment `dev` will be created from `prod`


Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── vulcan_example__dev.incremental_model
└── Indirectly Modified:
    └── vulcan_example__dev.full_model

---

+++

@@ -14,6 +14,7 @@

 SELECT
   id,
   item_id,
+  'z' AS new_column,
   event_date
 FROM vulcan_example.seed_model
 WHERE

Directly Modified: vulcan_example__dev.incremental_model
(Non-breaking)
└── Indirectly Modified Children:
    └── vulcan_example__dev.full_model (Indirect Non-breaking)
Models needing backfill:
└── vulcan_example__dev.incremental_model: [2020-01-01 - 2025-04-17]
Apply - Backfill Tables [y/n]:
```

Line 6 of the output states that a new environment `dev` will be created from the existing `prod` environment.

Lines 10-15 summarize the differences between the modified model and the `prod` environment, detecting that we directly modified `incremental_model` and that `full_model` was indirectly modified because it selects from the incremental model. Note that the model schemas are `vulcan_example__dev`, indicating that they are being created in the `dev` environment.

On line 31, we see that Vulcan automatically classified the change as `Non-breaking` because it understood that the change was additive (added a column not used by `full_model`) and did not invalidate any data already in `prod`.

Enter `y` at the prompt and press `Enter` to apply the plan and execute the backfill:

```bash linenums="1"
Apply - Backfill Tables [y/n]: y

Updating physical layer ━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:00

✔ Physical layer updated

[1/1] vulcan_example__dev.incremental_model  [insert 2020-01-01 - 2025-04-17] 0.03s
Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:00

✔ Model batches executed

Updating virtual layer  ━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:00

✔ Virtual layer updated
```

Lines 3-5 show the progress and completion of updating the physical layer.

Line 7 shows that Vulcan applied the change and evaluated `vulcan_example__dev.incremental_model` for the date range from 2020-01-01 to 2025-04-17.

Lines 9-11 show the progress and completion of executing model batches.

Lines 13-15 show the progress and completion of updating the virtual layer.

Vulcan did not need to backfill anything for the `full_model` since the change was `Non-breaking`.

### 4.2 Validate updates in dev
You can now view this change by querying data from `incremental_model` with `vulcan fetchdf "select * from vulcan_example__dev.incremental_model"`.

Note that the environment name `__dev` is appended to the schema namespace `vulcan_example` in the query:

```bash
$ vulcan fetchdf "select * from vulcan_example__dev.incremental_model"

   id  item_id new_column  event_date
0   1        2          z  2020-01-01
1   2        1          z  2020-01-01
2   3        3          z  2020-01-03
3   4        1          z  2020-01-04
4   5        1          z  2020-01-05
5   6        1          z  2020-01-06
6   7        1          z  2020-01-07
```

You can see that `new_column` was added to the dataset. The production table was not modified; you can validate this by querying the production table using `vulcan fetchdf "select * from vulcan_example.incremental_model"`.

Note that nothing has been appended to the schema namespace `vulcan_example` in this query because `prod` is the default environment.

```bash
$ vulcan fetchdf "select * from vulcan_example.incremental_model"

   id  item_id   event_date
0   1        2   2020-01-01
1   2        1   2020-01-01
2   3        3   2020-01-03
3   4        1   2020-01-04
4   5        1   2020-01-05
5   6        1   2020-01-06
6   7        1   2020-01-07
```

The production table does not have `new_column` because the changes to `dev` have not yet been applied to `prod`.

## 5. Update the prod environment

### 5.1 Apply updates to prod
Now that we've tested the changes in dev, it's time to move them to production. Run `vulcan plan` to plan and apply your changes to the `prod` environment.

Enter `y` and press `Enter` at the `Apply - Virtual Update [y/n]:` prompt to apply the plan and execute the backfill:

```bash
$ vulcan plan
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── vulcan_example.incremental_model
└── Indirectly Modified:
    └── vulcan_example.full_model

---

+++

@@ -14,6 +14,7 @@

 SELECT
   id,
   item_id,
+  'z' AS new_column,
   event_date
 FROM vulcan_example.seed_model
 WHERE

Directly Modified: vulcan_example.incremental_model (Non-breaking)
└── Indirectly Modified Children:
    └── vulcan_example.full_model (Indirect Non-breaking)
Apply - Virtual Update [y/n]: y

SKIP: No physical layer updates to perform

SKIP: No model batches to execute

Updating virtual layer  ━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:00

✔ Virtual layer updated
```

Note that a backfill was not necessary and only a Virtual Update occurred, as indicated by the "SKIP: No physical layer updates to perform" and "SKIP: No model batches to execute" messages. This is because the changes were already calculated and executed in the `dev` environment, and Vulcan is smart enough to recognize that it only needs to update the virtual references to the existing tables rather than recomputing everything.

### 5.2 Validate updates in prod
Double-check that the data updated in `prod` by running `vulcan fetchdf "select * from vulcan_example.incremental_model"`:

```bash
$ vulcan fetchdf "select * from vulcan_example.incremental_model"

   id  item_id new_column  event_date
0   1        2          z  2020-01-01
1   2        1          z  2020-01-01
2   3        3          z  2020-01-03
3   4        1          z  2020-01-04
4   5        1          z  2020-01-05
5   6        1          z  2020-01-06
6   7        1          z  2020-01-07
```

## 6. Next steps

Congratulations, you've now conquered the basics of using Vulcan!

From here, you can:

* [Learn more about Vulcan CLI commands](getting_started/cli.md)
* [Set up a connection to a database or SQL engine](guides-old/connections.md)
* [Learn more about Vulcan concepts](concepts-old/overview.md)
* [Join our Slack community](https://tobikodata.com/slack)


# Quickstart

Source: https://tmdc-io.github.io/vulcan-book/getting_started/

---

# Quickstart

Welcome to the Vulcan quickstart! This guide will help you get up and running with Vulcan quickly.

## Choose Your Path

Vulcan can be run in different ways. Choose the option that works best for you:

### Docker (Recommended)

**Best for:** Getting started quickly, consistent environments, production-like setup

The Docker approach uses Docker Compose to set up all necessary infrastructure and services. This is the fastest way to start working with Vulcan and provides a production-like environment.

👉 **[Start with Docker Quickstart](guides/get-started/docker.md)**

## Prerequisites

Before you begin, ensure your system meets the [prerequisites](./prerequisites.md) for using Vulcan.



# Prerequisites

Source: https://tmdc-io.github.io/vulcan-book/getting_started/prerequisites/

---

# Prerequisites

This page describes the system prerequisites needed to run Vulcan and provides instructions for meeting them.

## Docker Prerequisites

The recommended way to run Vulcan is using Docker. This provides a consistent, production-like environment.

### Docker Desktop

You'll need Docker Desktop installed and running on your machine:

- **macOS**: Download from [Docker Desktop for Mac](https://www.docker.com/products/docker-desktop/)
- **Windows**: Download from [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop/)
- **Linux**: Install Docker Engine and Docker Compose following the [official Docker installation guide](https://docs.docker.com/engine/install/)

### Verify Docker Installation

Verify that Docker is installed and running:

```bash
docker --version
docker compose version
```

You should see version numbers for both commands. If Docker Desktop is running, you should also be able to run:

```bash
docker ps
```

### System Requirements

- **RAM**: At least 4GB of available RAM (8GB recommended)
- **Disk Space**: At least 5GB of free disk space
- **CPU**: Modern multi-core processor

### Docker Compose

Docker Compose is included with Docker Desktop. If you're on Linux and need to install it separately, follow the [Docker Compose installation guide](https://docs.docker.com/compose/install/).

## Python Prerequisites

!!! info "Python Library Installation"
    Vulcan will be available as a Python library in the future. When available, you'll need Python 3.8 or higher.

    For now, use the Docker installation method above.



# Data Quality

Source: https://tmdc-io.github.io/vulcan-book/guides/data_quality/

---

# Data Quality

This guide explains how to use **Audits**, **Checks**, and **Tests** together to ensure data quality in your Orders360 project. You'll learn when to use each tool and see complex examples where they work together.

These three tools work as layers of protection for your data. Each serves a different purpose. Together they provide comprehensive coverage.

---

## The Three-Layer Quality Strategy

```mermaid
flowchart TB
    subgraph "Layer 1: Audits - Critical Blocking"
        AUDIT[Audits<br/>Block invalid data<br/>Run with model]
        EXAMPLES1["• Primary keys unique<br/>• Revenue non-negative<br/>• Foreign keys valid"]
    end
    
    subgraph "Layer 2: Checks - Monitoring"
        CHECK[Checks<br/>Track quality trends<br/>Non-blocking]
        EXAMPLES2["• Row count anomalies<br/>• Completeness trends<br/>• Cross-model validation"]
    end
    
    subgraph "Layer 3: Tests - Logic Validation"
        TEST[Tests<br/>Validate transformations<br/>Unit testing]
        EXAMPLES3["• SQL logic correct<br/>• Expected outputs<br/>• Edge cases"]
    end
    
    AUDIT --> EXAMPLES1
    CHECK --> EXAMPLES2
    TEST --> EXAMPLES3
    
    style AUDIT fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style TEST fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
```

**When to Use Each:**

Here's a quick guide to help you choose:

| Tool | Purpose | Blocks Pipeline? | Best For |
|------|---------|------------------|----------|
| **Audits** | Critical validation | Yes (always) | Business rules, data integrity |
| **Checks** | Quality monitoring | No | Trends, anomalies, monitoring |
| **Tests** | Logic validation | No | SQL correctness, edge cases |

The key difference: Audits stop everything if they fail. Checks and tests warn you, so you can investigate without blocking production.

*[Screenshot: Visual comparison of the three quality tools]*

---

## Quick Reference

### Audits: Critical Blocking Validation

**Use audits when:** Data must be correct or the pipeline should stop. These are your "must never fail" rules, if an audit fails, something is seriously wrong and you don't want that bad data flowing downstream.

```sql
MODEL (
  name sales.daily_sales,
  assertions (
    -- Primary key validation
    not_null(columns := (order_date)),
    unique_values(columns := (order_date)),
    
    -- Business rules
    positive_values(column := total_revenue),
    accepted_range(column := total_orders, min_v := 0, max_v := 10000)
  )
);
```

**Why audits are useful:**

- Always blocking - if they fail, execution stops immediately. No bad data gets through

- Run automatically with model execution - you don't have to remember to run them

- Fast feedback during development - catch issues before they hit production

- Use for critical business rules like "revenue must be positive" or "primary keys must be unique"

Think of audits as your bouncer, they check IDs at the door and don't let anyone sketchy in.

*[Screenshot: Audit failure blocking plan execution]*

### Checks: Quality Monitoring

**Use checks when:** You want to monitor trends and detect anomalies over time. Unlike audits, checks don't block your pipeline, they just keep an eye on things and warn you if something looks off.

```yaml
# checks/daily_sales.yml
checks:
  sales.daily_sales:
    completeness:
      - row_count > 0:
          name: daily_records_exist
          attributes:
            description: "At least one record per day"
    
    accuracy:
      - anomaly detection for total_revenue:
          name: revenue_anomaly
          attributes:
            description: "Detect unusual revenue patterns"
```

**Why checks are useful:**

- Non-blocking - warnings, not failures. Your pipeline keeps running even if a check flags something

- Historical tracking - see trends over time. You can spot patterns like "revenue always drops on weekends" or "row counts are trending down"

- Anomaly detection - statistical analysis. Checks can detect when something is statistically unusual, even if it's not technically wrong

- Use for monitoring and alerting. Set up alerts for when checks fail, so you know to investigate.

Checks monitor everything and alert you if something suspicious happens, but they don't stop execution.

*[Screenshot: Check results showing trends over time]*

### Tests: Logic Validation

**Use tests when:** You need to validate SQL transformations and edge cases. These are your unit tests for SQL, they make sure your logic is correct before you deploy it.

```yaml
# tests/test_daily_sales.yaml
tests:
  - name: test_daily_sales_aggregation
    model: sales.daily_sales
    inputs:
      raw.raw_orders:
        - order_id: ORD-001
          order_date: 2025-01-15
          total_amount: 100.50
    outputs:
      - order_date: 2025-01-15
        total_orders: 1
        total_revenue: 100.50
```

**Why tests are useful:**

- Unit testing for SQL logic - test your transformations in isolation

- Validates expected outputs - make sure you're getting the results you expect

- Tests edge cases - what happens with empty data? Null values? Boundary conditions?

- Use for development. Catch bugs before they make it to production.

Tests run in a controlled environment before production.

*[Screenshot: Test execution showing pass/fail results]*

---

## Complex Example: Orders360 Daily Sales

Let's see how all three tools work together for the `sales.daily_sales` model. This is a real-world example that shows you how to layer these tools for maximum protection.

### The Model

```sql
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date,
  assertions (
    -- Audit: Critical validations
    not_null(columns := (order_date, total_orders, total_revenue)),
    unique_values(columns := (order_date)),
    positive_values(column := total_orders),
    positive_values(column := total_revenue),
    accepted_range(column := total_revenue, min_v := 0, max_v := 1000000)
  )
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

### Layer 1: Audits (Critical Blocking)

**Why:** These rules must never fail. Invalid data should not flow downstream. If revenue is negative or primary keys aren't unique, that's a critical problem that needs to stop everything immediately.

```sql
-- audits/revenue_consistency.sql
AUDIT (name assert_revenue_consistency);
-- Ensure revenue matches sum of individual orders
SELECT 
  ds.order_date,
  ds.total_revenue,
  SUM(o.total_amount) as calculated_revenue
FROM @this_model ds
JOIN raw.raw_orders o ON DATE(o.order_date) = ds.order_date
GROUP BY ds.order_date, ds.total_revenue
HAVING ABS(ds.total_revenue - SUM(o.total_amount)) > 0.01;
```

**Attach to model:**
```sql
MODEL (
  name sales.daily_sales,
  assertions (
    -- ... other audits ...
    assert_revenue_consistency  -- Custom audit
  )
);
```

*[Screenshot: Audit failure showing revenue mismatch]*

### Layer 2: Checks (Monitoring)

**Why:** Monitor trends and detect anomalies without blocking the pipeline. You want to know if revenue spikes unexpectedly or if row counts drop, but these might be legitimate business events, so you investigate rather than blocking.

```yaml
# checks/daily_sales.yml
checks:
  sales.daily_sales:
    # Completeness: Ensure data exists
    completeness:
      - row_count > 0:
          name: daily_records_exist
          attributes:
            description: "At least one record per day"
            severity: error
    
      - missing_count(order_date) = 0:
          name: no_missing_dates
          attributes:
            description: "All dates must be present"
    
    # Validity: Check data ranges
    validity:
      - failed rows:
          name: revenue_outliers
          fail query: |
            SELECT order_date, total_revenue
            FROM sales.daily_sales
            WHERE total_revenue > 500000 OR total_revenue < 0
          samples limit: 10
          attributes:
            description: "Revenue outside expected range"
            severity: warning
    
    # Accuracy: Anomaly detection
    accuracy:
      - anomaly detection for total_revenue:
          name: revenue_anomaly
          attributes:
            description: "Detect unusual revenue patterns"
            severity: warning
      
      - anomaly detection for total_orders:
          name: order_count_anomaly
          attributes:
            description: "Detect unusual order volume"
    
    # Consistency: Cross-model validation
    consistency:
      - failed rows:
          name: revenue_mismatch_with_raw
          fail query: |
            SELECT 
              ds.order_date,
              ds.total_revenue as daily_revenue,
              SUM(o.total_amount) as raw_revenue
            FROM sales.daily_sales ds
            LEFT JOIN raw.raw_orders o 
              ON DATE(o.order_date) = ds.order_date
            GROUP BY ds.order_date, ds.total_revenue
            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) > 1.0
          samples limit: 5
          attributes:
            description: "Daily revenue should match sum of raw orders"
            severity: error
    
    # Timeliness: Check data freshness
    timeliness:
      - change for row_count >= -20%:
          name: row_count_drop_alert
          attributes:
            description: "Alert if daily records drop more than 20%"
            severity: warning
```

*[Screenshot: Check dashboard showing trends and anomalies]*

### Layer 3: Tests (Logic Validation)

**Why:** Validate SQL logic and edge cases during development. Before you even deploy, you want to make sure your SQL is doing what you think it's doing. Tests catch logic errors early.

```yaml
# tests/test_daily_sales.yaml
tests:
  - name: test_daily_sales_single_order
    model: sales.daily_sales
    inputs:
      raw.raw_orders:
        - order_id: ORD-001
          order_date: 2025-01-15
          customer_id: CUST-001
          product_id: PROD-001
          total_amount: 100.50
    outputs:
      - order_date: 2025-01-15
        total_orders: 1
        total_revenue: 100.50
        last_order_id: ORD-001
  
  - name: test_daily_sales_multiple_orders
    model: sales.daily_sales
    inputs:
      raw.raw_orders:
        - order_id: ORD-001
          order_date: 2025-01-15
          total_amount: 100.00
        - order_id: ORD-002
          order_date: 2025-01-15
          total_amount: 200.00
        - order_id: ORD-003
          order_date: 2025-01-15
          total_amount: 50.00
    outputs:
      - order_date: 2025-01-15
        total_orders: 3
        total_revenue: 350.00
        last_order_id: ORD-003
  
  - name: test_daily_sales_empty_day
    model: sales.daily_sales
    inputs:
      raw.raw_orders: []
    outputs: []
  
  - name: test_daily_sales_date_grouping
    model: sales.daily_sales
    inputs:
      raw.raw_orders:
        - order_id: ORD-001
          order_date: 2025-01-15 10:00:00
          total_amount: 100.00
        - order_id: ORD-002
          order_date: 2025-01-15 15:30:00
          total_amount: 200.00
        - order_id: ORD-003
          order_date: 2025-01-16 09:00:00
          total_amount: 150.00
    outputs:
      - order_date: 2025-01-15
        total_orders: 2
        total_revenue: 300.00
      - order_date: 2025-01-16
        total_orders: 1
        total_revenue: 150.00
```

*[Screenshot: Test execution showing all tests passing]*

---

## How They Work Together

```mermaid
flowchart TB
    subgraph "Development Workflow"
        DEV[Developer writes model]
        TEST[Run Tests<br/>Validate logic]
        PLAN[Run Plan<br/>Apply changes]
    end
    
    subgraph "Execution Flow"
        EXEC[Model Executes]
        AUDIT_RUN[Audits Run<br/>Block if fail]
        CHECK_RUN[Checks Run<br/>Track trends]
    end
    
    subgraph "Results"
        PASS[Pass<br/>Data flows]
        FAIL[Fail<br/>Pipeline stops]
        TREND[Trends<br/>Monitor quality]
    end
    
    DEV --> TEST
    TEST --> PLAN
    PLAN --> EXEC
    EXEC --> AUDIT_RUN
    EXEC --> CHECK_RUN
    
    AUDIT_RUN -->|Pass| PASS
    AUDIT_RUN -->|Fail| FAIL
    CHECK_RUN --> TREND
    
    style DEV fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style TEST fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style AUDIT_RUN fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style CHECK_RUN fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style PASS fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style FAIL fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
```

**Execution Order:**
1. **Tests** run during development (validate logic) - catch bugs before deployment
2. **Plan** applies changes to environment - your changes go live
3. **Model** executes transformation - data gets processed
4. **Audits** run immediately (block if fail) - critical validation happens right away
5. **Checks** run (track trends, don't block) - monitoring happens in the background

Tests happen first, then audits catch critical issues, and checks monitor everything. This layered approach provides comprehensive coverage.

*[Screenshot: Complete workflow showing all three layers]*

---

## Complex Scenario: Revenue Validation

Here's a complex example where audits and checks work together to validate revenue data. This shows you how to use both tools for the same concern, critical blocking vs. monitoring.

### The Problem

We need to ensure:

1. **Critical:** Revenue is always positive (audit - blocks)
2. **Critical:** Daily totals match raw order sums (audit - blocks)
3. **Monitoring:** Revenue trends are normal (check - warns)
4. **Monitoring:** Detect unusual spikes/drops (check - warns)

### Solution: Combined Approach

**Audits (Critical - Blocking):**
```sql
MODEL (
  name sales.daily_sales,
  assertions (
    -- Basic validation
    positive_values(column := total_revenue),
    not_null(columns := (order_date, total_revenue)),
    
    -- Complex validation: Revenue consistency
    assert_revenue_matches_raw_orders
  )
);

-- audits/revenue_matches_raw.sql
AUDIT (name assert_revenue_matches_raw_orders);
SELECT 
  ds.order_date,
  ds.total_revenue as daily_total,
  COALESCE(SUM(o.total_amount), 0) as raw_total,
  ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) as difference
FROM @this_model ds
LEFT JOIN raw.raw_orders o 
  ON DATE(o.order_date) = ds.order_date
GROUP BY ds.order_date, ds.total_revenue
HAVING ABS(ds.total_revenue - COALESCE(SUM(o.total_amount), 0)) > 0.01;
```

**Checks (Monitoring - Non-Blocking):**
```yaml
# checks/revenue_monitoring.yml
checks:
  sales.daily_sales:
    accuracy:
      # Anomaly detection for revenue
      - anomaly detection for total_revenue:
          name: revenue_anomaly_detection
          attributes:
            description: "Detect statistically unusual revenue"
            severity: warning
      
      # Trend monitoring
      - change for total_revenue >= 50%:
          name: revenue_spike_alert
          attributes:
            description: "Alert if revenue increases >50% day-over-day"
            severity: warning
      
      - change for total_revenue <= -30%:
          name: revenue_drop_alert
          attributes:
            description: "Alert if revenue drops >30% day-over-day"
            severity: error
    
    consistency:
      # Cross-model validation (non-blocking)
      - failed rows:
          name: revenue_vs_raw_check
          fail query: |
            SELECT 
              ds.order_date,
              ds.total_revenue,
              SUM(o.total_amount) as raw_sum,
              ABS(ds.total_revenue - SUM(o.total_amount)) as diff
            FROM sales.daily_sales ds
            LEFT JOIN raw.raw_orders o 
              ON DATE(o.order_date) = ds.order_date
            GROUP BY ds.order_date, ds.total_revenue
            HAVING ABS(ds.total_revenue - SUM(o.total_amount)) > 10.0
          samples limit: 5
          attributes:
            description: "Monitor revenue consistency (wider tolerance than audit)"
            severity: warning
```

**Why Both?**

You might wonder why you need both an audit and a check for revenue. Here's the thing:

- **Audit:** Stops pipeline if revenue is wrong (critical). If daily totals don't match raw orders, that's a data integrity issue and everything stops.

- **Check:** Warns about trends and anomalies (monitoring) - if revenue spikes 50% day-over-day, that might be legitimate (big sale!) or it might be a problem, but you want to investigate, not block

- **Together:** Critical issues blocked, trends monitored - you get both immediate protection and ongoing visibility

The audit has a tight tolerance (0.01) because it's checking for correctness. The check has a wider tolerance (10.0) because it's looking for trends, not exact matches. Pretty clever, right?

*[Screenshot: Dashboard showing audit blocks vs check warnings]*

---

## Running Quality Tools

### Run Tests
```bash
# Run all tests
vulcan test

# Run specific test
vulcan test tests/test_daily_sales.yaml::test_daily_sales_single_order

# Run tests matching pattern
vulcan test tests/test_daily*
```

*[Screenshot: Test execution output]*

### Run Audits
```bash
# Run all audits
vulcan audit

# Audits also run automatically with plan
vulcan plan dev
```

*[Screenshot: Audit execution output]*

### Run Checks
```bash
# Run all checks
vulcan check

# Run checks for specific model
vulcan check --select sales.daily_sales

# Checks also run automatically with plan/run
vulcan plan dev
```

*[Screenshot: Check execution output with trends]*

---

## Best Practices

### DO:

Here are some tips to help you use these tools effectively:

1. **Start with Audits** - Add critical blocking validations first. Get your safety net in place before worrying about trends.
2. **Add Checks Gradually** - Monitor trends, then add anomaly detection. Don't try to check everything at once, build up your monitoring over time.
3. **Test During Development** - Write tests before deploying. Catch logic errors before they hit production.
4. **Use Descriptive Names** - Makes debugging easier. Names like `revenue_mismatch_with_raw` are much better than `check_1`.
5. **Order Audits Efficiently** - Fast checks first, slow checks last. If you have multiple audits, put the quick ones first so you fail fast.

### DON'T:

And here's what to avoid:

1. **Don't use Checks for Critical Rules** - Use audits instead. If it's critical, it should block. Checks are for monitoring, not blocking.
2. **Don't Skip Audit Failures** - Fix the root cause. If an audit fails, something is wrong. Don't just disable it, fix the problem.
3. **Don't Over-Audit** - Focus on critical business rules. Too many audits can slow things down. Only audit what really matters.
4. **Don't Ignore Check Trends** - They indicate data quality issues. If checks are consistently failing, there's probably a real problem you need to address.

---

## Summary

**Three-Layer Strategy:**

- **Audits** = Critical blocking validation (must pass)

- **Checks** = Quality monitoring (trends, anomalies)

- **Tests** = Logic validation (development)

**Use Together:**

- Audits block invalid data

- Checks monitor quality trends

- Tests validate SQL logic

**Orders360 Example:**

- Audits ensure revenue is positive and matches raw data

- Checks detect anomalies and trends

- Tests validate aggregation logic

---

## Next Steps

- Learn about [Built-in Audits](../components/audits/audits.md#built-in-audits)

- Explore [Check Dimensions](../components/checks/checks.md#data-quality-dimensions)

- Read about [Testing](../components/tests/tests.md)




# Get Started

Source: https://tmdc-io.github.io/vulcan-book/guides/get-started/docker/

---

# Get Started

This guide shows you how to set up a complete Vulcan project on your local machine.

The example project runs locally using a Postgres SQL engine. Vulcan automatically generates all necessary project files and configurations.

To get started, ensure your system meets the [prerequisites](#prerequisites) below, then follow the step-by-step instructions for your operating system.

## Prerequisites

Before you begin, make sure you have Docker installed and configured on your system. Follow the instructions below for your operating system.

=== "Mac/Linux"
    
    **1. Verify Docker Installation**
    
    First, check if Docker Desktop (Mac) or Docker Engine (Linux) is installed and running:
    
    ```bash
    docker --version
    docker compose version
    ```
    
    If both commands return version numbers, Docker is installed. Make sure Docker Desktop is running (you should see the Docker icon in your menu bar or system tray).
    
    **2. Install Docker (if needed)**
    
    - **Mac**: Download and install [Docker Desktop for Mac](https://www.docker.com/products/docker-desktop/){:target="_blank"}

    - **Linux**: Install Docker Engine and Docker Compose following the [official Docker installation guide](https://docs.docker.com/engine/install/){:target="_blank"}
    
    **3. Configure Resources**
    
    Ensure Docker Desktop has at least **4GB of RAM** allocated. You can adjust this in Docker Desktop settings under Resources → Advanced.

=== "Windows"
    
    **1. Verify Docker Installation**
    
    Check if Docker Desktop for Windows is installed and running:
    
    ```bash
    docker --version
    docker compose version
    ```
    
    If both commands return version numbers, Docker is installed. Make sure Docker Desktop is running (you should see the Docker icon in your system tray).
    
    **2. Install Docker (if needed)**
    
    If Docker is not installed, download and install [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop/){:target="_blank"}
    
    **3. Configure Resources**
    
    Ensure Docker Desktop has at least **4GB of RAM** allocated. You can adjust this in Docker Desktop settings under Settings → Resources → Advanced.

## Vulcan Setup Locally

Follow these steps to set up Vulcan on your local machine. The setup process will create all necessary infrastructure services and prepare your environment for development.

=== "Mac/Linux"
      
    [:material-download: Download for Mac/Linux](zip-mac/vulcan-project.zip){ .md-button .md-button--primary .download-button target="_blank" }
    
    <small>The download includes: Docker Compose files, Makefile, and a comprehensive README</small>
        
    **Step 1: Extract and Navigate**
    
    Extract the downloaded zip file and open the `vulcan-project` folder in VS Code or your preferred IDE:
    
    ```bash
    cd vulcan-project
    ```
    
    **Step 2: Run Setup**
    
    **Important**: Before running setup, ensure Docker Desktop is running on your machine and that you are logged into RubikLabs.
    
    Execute the setup command:
    
    ```bash
    make setup
    ```
    
    This command creates and starts three essential services:
    
    - **statestore** (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, and execution history. This database persists your semantic model, plans, and tracks materialization state.
    
    - **minio** (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates. This service provides data retrieval and caching for your workflows.
    
    - **minio-init**: Initializes MinIO buckets and policies with the correct configuration. This service runs once to set up the storage infrastructure.
    
    **Note**: These services are essential for Vulcan's operation and must be running before you can use Vulcan. The setup process typically takes 1-2 minutes to complete.
    
    **Step 3: Configure Vulcan CLI Access**
    
    Create an alias to access the Vulcan CLI easily:
    
    ```bash
    alias vulcan="docker run -it --network=vulcan --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan"
    ```
    
    **Note**: This alias is temporary and will be lost when you close your shell session. To make it permanent, add this line to your shell configuration file (`~/.bashrc` for Bash or `~/.zshrc` for Zsh), then restart your terminal or run `source ~/.zshrc` (or `source ~/.bashrc`).
    
    **Step 4: Start API Services**
    
    Start the Vulcan API services:
    
    ```bash
    make vulcan-up
    ```
    
    This command starts two services:
    
    - **vulcan-api**: A REST API server for querying your semantic model (available at `http://localhost:8000`)

    - **vulcan-transpiler**: A service for transpiling semantic queries to SQL
    
    Once these services are running, you're ready to create your first project!

=== "Windows"
        
    [:material-download: Download for Windows](zip-window/vulcan-project.zip){ .md-button .md-button--primary .download-button target="_blank" }
    
    <small>The download includes: Docker Compose files, Windows batch scripts, and a comprehensive README</small>
        
    **Step 1: Extract and Navigate**
    
    Extract the downloaded zip file and navigate to the `vulcan-project` directory:
    
    ```cmd
    cd vulcan-project
    ```
    
    **Step 2: Run Setup**
    
    **Important**: Before running setup, ensure Docker Desktop for Windows is running and that you are logged into RubikLabs.
    
    Execute the setup script:
    
    ```cmd
    setup.bat
    ```
    
    This script creates and starts three essential services:
    
    - **statestore** (PostgreSQL): Stores Vulcan's internal state, including model definitions, plan information, and execution history. This database persists your semantic model, plans, and tracks materialization state.
    
    - **minio** (Object Storage): Stores query results, artifacts, and other data objects that Vulcan generates. This service provides data retrieval and caching for your workflows.
    
    - **minio-init**: Initializes MinIO buckets and policies with the correct configuration. This service runs once to set up the storage infrastructure.
    
    **Note**: These services are essential for Vulcan's operation and must be running before you can use Vulcan. The setup process typically takes 1-2 minutes to complete.
    
    **Step 3: Access Vulcan CLI**
    
    Use the provided batch script to access the Vulcan CLI:
    
    ```cmd
    vulcan.bat
    ```
    
    This script runs Vulcan commands in a Docker container with the correct network and volume settings.
    
    **Step 4: Start API Services**
    
    Start the Vulcan API services:
    
    ```cmd
    start-vulcan-api.bat
    ```
    
    This command starts two services:
    
    - **vulcan-api**: A REST API server for querying your semantic model (available at `http://localhost:8000`)

    - **vulcan-transpiler**: A service for transpiling semantic queries to SQL
    
    Once these services are running, you're ready to create your first project!

## Create Your First Project

Now that your environment is set up, let's create your first Vulcan project. This section walks you through initializing a project, verifying the setup, running your first plan, and querying your data.

=== "Mac/Linux"
    
    **Step 1: Initialize Your Project**
    
    Initialize a new Vulcan project: [*Learn more about init*](../../cli-command/cli.md#init){:target="_blank"}
    
    ```bash
    vulcan init
    ```
    
    When prompted:

    - Choose `DEFAULT` as the project type

    - Select `Postgres` as your SQL engine
    
    This command creates a complete project structure with 7 directories:

    - `models/` - Contains `.sql` and `.py` files for your data models

    - `seeds/` - CSV files for static datasets

    - `audits/` - Write logic to assert data quality and block downstream models if checks fail

    - `tests/` - Test files for validating your model logic

    - `macros/` - Write custom macros for reusable SQL patterns

    - `checks/` - Write data quality checks

    - `semantics/` - Semantic layer definitions (measures, dimensions, etc.)

    **Step 2: Verify Your Setup**
    
    Check your project configuration and connection status: [*Learn more about info*](../../cli-command/cli.md#info){:target="_blank"}
    
    ```bash
    vulcan info
    ```
    
    This command displays:

    - Connection status to your database

    - Number of models, macros, and other project components

    - Project configuration details
    
    **Important**: Verify that the setup is correct before proceeding to run plans. If you see any errors, check the Troubleshooting section below.

    **Step 3: Create and Apply Your First Plan**
    
    Generate a plan for your models: [*Learn more about plan*](../../cli-command/cli.md#plan){:target="_blank"}
    
    ```bash
    vulcan plan
    ```
    
    This command performs three key actions:
    
    1. **Validates** your models and creates the necessary database objects (tables, views, etc.)
    2. **Calculates** which data intervals need to be backfilled based on your model's `start` date and `cron` schedule
    3. **Prompts** you to apply the plan
    
    When prompted, enter `y` to apply the plan and backfill your models with historical data.
    
    **Note**: The backfill process may take a few minutes depending on the amount of historical data to process.
        
    **Step 4: Query Your Models**
    
    Execute SQL queries against your models: [*Learn more about fetchdf*](../../cli-command/cli.md#fetchdf){:target="_blank"}
    
    ```bash
    vulcan fetchdf "select * from schema.model_name"
    ```
    
    This command executes a SQL query and returns the results as a pandas DataFrame.

    **Step 5: Query Using Semantic Layer**
    
    Use Vulcan's semantic layer to query your data: [*Learn more about transpile*](../../cli-command/cli.md#transpile){:target="_blank"}
    
    ```bash
    vulcan transpile --format sql "SELECT MEASURE(measure_name) FROM model"
    ```
    
    This command transpiles your semantic query into SQL that can be executed against your data warehouse. The semantic layer provides a business-friendly interface for querying your data models.

=== "Windows"
    
    **Step 1: Initialize Your Project**
    
    Initialize a new Vulcan project: [*Learn more about init*](../../cli-command/cli.md#init){:target="_blank"}
    
    ```cmd
    vulcan init
    ```
    
    When prompted:

    - Choose `DEFAULT` as the project type

    - Select `Postgres` as your SQL engine
    
    This command creates a complete project structure with 7 directories:

    - `models/` - Contains `.sql` and `.py` files for your data models

    - `seeds/` - CSV files for static datasets

    - `audits/` - Write logic to assert data quality and block downstream models if checks fail

    - `tests/` - Test files for validating your model logic

    - `macros/` - Write custom macros for reusable SQL patterns

    - `checks/` - Write data quality checks

    - `semantics/` - Semantic layer definitions (measures, dimensions, etc.)

    **Step 2: Verify Your Setup**
    
    Check your project configuration and connection status: [*Learn more about info*](../../cli-command/cli.md#info){:target="_blank"}
    
    ```cmd
    vulcan info
    ```
    
    This command displays:

    - Connection status to your database

    - Number of models, macros, and other project components

    - Project configuration details
    
    **Important**: Verify that the setup is correct before proceeding to run plans. If you see any errors, check the Troubleshooting section below.

    **Step 3: Create and Apply Your First Plan**
    
    Generate a plan for your models: [*Learn more about plan*](../../cli-command/cli.md#plan){:target="_blank"}
    
    ```cmd
    vulcan plan
    ```
    
    This command performs three key actions:
    
    1. **Validates** your models and creates the necessary database objects (tables, views, etc.)
    2. **Calculates** which data intervals need to be backfilled based on your model's `start` date and `cron` schedule
    3. **Prompts** you to apply the plan
    
    When prompted, enter `y` to apply the plan and backfill your models with historical data.
    
    **Note**: The backfill process may take a few minutes depending on the amount of historical data to process.
        
    **Step 4: Query Your Models**
    
    Execute SQL queries against your models: [*Learn more about fetchdf*](../../cli-command/cli.md#fetchdf){:target="_blank"}
    
    ```cmd
    vulcan fetchdf "select * from schema.model_name"
    ```
    
    This command executes a SQL query and returns the results as a pandas DataFrame.

    **Step 5: Query Using Semantic Layer**
    
    Use Vulcan's semantic layer to query your data: [*Learn more about transpile*](../../cli-command/cli.md#transpile){:target="_blank"}
    
    ```cmd
    vulcan transpile --format sql "SELECT MEASURE(measure_name) FROM model"
    ```
    
    This command transpiles your semantic query into SQL that can be executed against your data warehouse. The semantic layer provides a business-friendly interface for querying your data models.

## Stopping Services

When you're done working with Vulcan, you can stop the services to free up system resources. Use the commands below based on your operating system.

=== "Mac/Linux"
    
    **Stop All Services**
    
    To stop all running services:
    
    ```bash
    make all-down       # Stop all services
    ```
    
    **Stop and Clean Up (Warning: This deletes all data)**
    
    To stop all services and remove volumes (this will delete all data):
    
    ```bash
    make all-clean      # Stop and remove volumes (this will delete all data)
    ```
    
    **Stop Individual Service Groups**
    
    You can also stop specific service groups:
    
    ```bash
    make vulcan-down     # Stop only Vulcan API services
    make infra-down      # Stop infrastructure services (statestore, minio)
    make warehouse-down  # Stop warehouse services
    ```

=== "Windows"
    
    **Stop All Services**
    
    To stop all running services:
    
    ```cmd
    stop-all.bat           # Stop all services
    ```
    
    **Stop and Clean Up (Warning: This deletes all data)**
    
    To stop all services and remove volumes (this will delete all data):
    
    ```cmd
    clean.bat              # Stop and remove volumes (this will delete all data)
    ```
    
    **Stop Individual Services**
    
    To stop only the Vulcan API services:
    
    ```cmd
    vulcan-down.bat        # Stop only Vulcan API services
    ```

## Troubleshooting

If you encounter any issues during setup or while using Vulcan, refer to the solutions below.

??? note "Common Issues and Solutions"
    
    **Services Won't Start**
    
    If services fail to start, ensure Docker Desktop is running with at least 4GB RAM allocated. You can check and adjust this in Docker Desktop settings:

    - **Mac**: Docker Desktop → Settings → Resources → Advanced

    - **Windows**: Docker Desktop → Settings → Resources → Advanced
    
    **Network Errors**
    
    If you encounter network-related errors, ensure the `vulcan` Docker network exists:
    
    === "Mac/Linux"
        Check if the network exists:
        ```bash
        docker network ls | grep vulcan
        ```
        If it doesn't exist, create it:
        ```bash
        docker network create vulcan
        ```
    === "Windows"
        Check if the network exists:
        ```cmd
        docker network ls | grep vulcan
        ```
        If it doesn't exist, create it:
        ```cmd
        docker network create vulcan
        ```

    **Port Conflicts**
    
    If you see errors about ports already being in use, one of the required ports (5431, 5433, 9000, 9001, or 8000) is likely occupied by another application. You have two options:
    
    1. **Stop the conflicting application** using that port
    2. **Modify the port mappings** in the Docker Compose files (`docker/docker-compose.infra.yml` and `docker/docker-compose.warehouse.yml`)

    **Can't Connect to Services**
    
    If you're unable to connect to Vulcan services, verify that all required services are running:
    
    ```bash
    docker compose -f docker/docker-compose.infra.yml ps
    docker compose -f docker/docker-compose.warehouse.yml ps
    ```
    
    All services should show as "Up" or "running". If any service shows as "Exited" or "Stopped", check the logs:
    
    ```bash
    docker compose -f docker/docker-compose.infra.yml logs
    ```

    **Access MinIO Console**
    
    You can access the MinIO console to manage your object storage:

    - **URL**: `http://localhost:9001`

    - **Username**: `admin`

    - **Password**: `password`
    
    The MinIO console allows you to browse buckets, upload files, and manage storage policies.


## Next Steps

You've set up Vulcan and created your first project. Here are recommended next steps:

- **[Learn more about Vulcan CLI commands](../../cli-command/cli.md){:target="_blank"}** - Explore all available commands and their options

- **[Explore Vulcan concepts](../../components/model/overview.md){:target="_blank"}** - Deep dive into how models work and how to structure your data pipeline

- **[Read the model kinds documentation](../../components/model/model_kinds.md){:target="_blank"}** - Understand different model types and when to use them



# Vulcan Docker Quickstart

Source: https://tmdc-io.github.io/vulcan-book/guides/get-started/zip-mac/vulcan-project/README/

---

# Vulcan Docker Quickstart

This package contains all the Docker Compose files and configuration needed to get started with Vulcan using Docker.

## Contents

- `docker/docker-compose.infra.yml` - Infrastructure services (statestore, MinIO)

- `docker/docker-compose.warehouse.yml` - Warehouse database (PostgreSQL)

- `docker/docker-compose.vulcan.yml` - Vulcan API and transpiler services

- `Makefile` - Convenient commands for managing services

## Prerequisites

- Docker Desktop installed and running

- Docker Compose (included with Docker Desktop)

- At least 4GB of available RAM

- A terminal/command line interface

## Quick Start

1. **Start all infrastructure:**
   ```bash
   make setup
   ```
   This will:
   - Create the Docker network

   - Start statestore (PostgreSQL) on port 5431

   - Start MinIO object storage on ports 9000 and 9001

   - Start warehouse database (PostgreSQL) on port 5433

2. **Access Vulcan:**
   ```bash
   alias vulcan="docker run -it --network=vulcan  --rm -v .:/workspace tmdcio/vulcan:0.225.0-dev-02 vulcan"
   ```

3. **Initialize your project:**
   ```bash
   vulcan init
   ```

4. **Update your `config.yaml`** to match the Docker setup:
   ```yaml
   # Project metadata
   name: orders360
   tenant: sales
   description: Daily sales analytics pipeline

   # Gateway Connection
   gateways:
   default:
      connection:
         type: postgres
         host: warehouse
         port: 5432
         database: warehouse
         user: vulcan
         password: vulcan
      state_connection:
         type: postgres
         host: statestore
         port: 5432
         database: statestore
         user: vulcan
         password: vulcan

   default_gateway: default

   # Model Defaults (required)
   model_defaults:
   dialect: postgres
   start: 2024-01-01
   cron: '@daily'

   # Linting Rules
   linter:
   enabled: true
   rules:
      - ambiguousorinvalidcolumn

      - invalidselectstarexpansion
   ```

5. **Create and apply your first plan:**
   ```bash
   vulcan plan
   ```

## Available Make Commands
- `make setup` - Run all setup steps

- `make vulcan-up` - Start Vulcan API services

- `make network` - Create the Docker network

- `make infra` - Start infrastructure services

- `make warehouse` - Start warehouse database

- `make all-down` - Stop all services

- `make all-clean` - Stop all services and remove volumes

## Service Ports

- **Statestore**: 5431

- **Warehouse**: 5433

- **MinIO API**: 9000

- **MinIO Console**: 9001 (admin/password)

- **Vulcan API**: 8000






# Vulcan Docker Quickstart - Windows

Source: https://tmdc-io.github.io/vulcan-book/guides/get-started/zip-window/vulcan-project/README/

---

# Vulcan Docker Quickstart - Windows

This package contains all the Docker Compose files and Windows batch scripts needed to get started with Vulcan using Docker on Windows.

## Contents

- `docker/docker-compose.infra.yml` - Infrastructure services (statestore, MinIO)

- `docker/docker-compose.warehouse.yml` - Warehouse database (PostgreSQL)

- `docker/docker-compose.vulcan.yml` - Vulcan API and transpiler services

- `setup.bat` - Setup script to start all infrastructure

- `vulcan.bat` - Wrapper script to run Vulcan CLI commands

- `start-vulcan-api.bat` - Start Vulcan API services

- `stop-all.bat` - Stop all services

- `clean.bat` - Stop all services and remove volumes

## Prerequisites

- Docker Desktop for Windows installed and running

- At least 4GB of available RAM

## Quick Start

1. **Run the setup script:**
   ```cmd
   setup.bat
   ```
   This will:
   - Create the Docker network

   - Start statestore (PostgreSQL) on port 5431

   - Start MinIO object storage on ports 9000 and 9001

   - Start warehouse database (PostgreSQL) on port 5433

2. **Access Vulcan:**
   ```cmd
   vulcan.bat
   ```
3. **Initialize your project:**
   ```cmd
   vulcan.bat init
   ```

4. **Update your `config.yaml`** to match the Docker setup:
   ```yaml
   # Project metadata
   name: orders360
   tenant: sales
   description: Daily sales analytics pipeline

   # Gateway Connection
   gateways:
   default:
      connection:
         type: postgres
         host: warehouse
         port: 5432
         database: warehouse
         user: vulcan
         password: vulcan
      state_connection:
         type: postgres
         host: statestore
         port: 5432
         database: statestore
         user: vulcan
         password: vulcan

   default_gateway: default

   # Model Defaults (required)
   model_defaults:
   dialect: postgres
   start: 2024-01-01
   cron: '@daily'

   # Linting Rules
   linter:
   enabled: true
   rules:
      - ambiguousorinvalidcolumn

      - invalidselectstarexpansion
   ```

5. **Create and apply your first plan:**
   ```cmd
   vulcan.bat plan
   ```

## Available Scripts

- `setup.bat` - Create network and start all infrastructure

- `start-vulcan-api.bat` - Start Vulcan API services

- `stop-all.bat` - Stop all services

- `clean.bat` - Stop all services and remove volumes


## Service Ports

- **Statestore**: 5431

- **Warehouse**: 5433

- **MinIO API**: 9000

- **MinIO Console**: 9001 (admin/password)

- **Vulcan API**: 8000
```





# Incremental by Time

Source: https://tmdc-io.github.io/vulcan-book/guides/incremental_by_time/

---

# Incremental by Time

This guide explains how incremental by time models work in Vulcan using the Orders360 example project. You'll learn why they're efficient, how they process data, and how to create them.

See the [models guide](./models.md) for general model information or the [model kinds page](../components/model/model_kinds.md) for all model types.

---

## Why Use Incremental Models?

### The Problem: Full Refreshes Are Expensive

If you have a table with sales data from the last year (365 days), every time you run a `FULL` model, it processes all 365 days. This is inefficient when you only need to process today's data.

```mermaid
flowchart LR
    subgraph "FULL Model - Every Run"
        FULL[FULL Model Run]
        PROCESS[Process ALL 365 Days]
        DAY1[Day 1]
        DAY2[Day 2]
        DAY3[Day 3]
        DOTS[...]
        DAY365[Day 365]
    end
    
    subgraph "Results"
        TIME1[Time: 10 minutes]
        COST1[Cost: $10]
        DATA1[All 365 days]
    end
    
    FULL --> PROCESS
    PROCESS --> DAY1
    PROCESS --> DAY2
    PROCESS --> DAY3
    PROCESS --> DOTS
    PROCESS --> DAY365
    
    DAY365 --> TIME1
    DAY365 --> COST1
    DAY365 --> DATA1
    
    style FULL fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style PROCESS fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style TIME1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style COST1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style DATA1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
```

*[Screenshot: Visual showing FULL model processing all 365 days]*

### The Solution: Only Process What's New

With incremental models, Vulcan only processes **new or missing** days:

```mermaid
flowchart LR
    subgraph "INCREMENTAL Model - Every Run"
        INCR[INCREMENTAL Model Run]
        CHECK[Check State Database]
        SKIP[Skip Days 1-364]
        PROCESS_NEW[Process Day 365 Only]
    end
    
    subgraph "Results"
        TIME2[Time: 30 seconds]
        COST2[Cost: $0.20]
        DATA2[Only Day 365]
    end
    
    INCR --> CHECK
    CHECK --> SKIP
    CHECK --> PROCESS_NEW
    
    PROCESS_NEW --> TIME2
    PROCESS_NEW --> COST2
    PROCESS_NEW --> DATA2
    
    style INCR fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style PROCESS_NEW fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style TIME2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style COST2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style DATA2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
```

*[Screenshot: Visual showing incremental model processing only Day 365]*

**Result:** 50x faster and 50x cheaper! 

Incremental models process only what you need, when you need it.

---

## How Incremental Models Work

Incremental models use **time intervals** to track what's been processed. Think of it like a calendar where Vulcan checks off each day. It knows what's done and what still needs work.

```mermaid
flowchart TB
    subgraph "Incremental Processing Flow"
        START[vulcan run]
        CHECK[Check State Database<br/>What's already processed?]
        
        subgraph "State Database"
            PROCESSED1[Jan 1-7: Processed]
            PROCESSED2[Jan 8-14: Processed]
            MISSING[Jan 15-21: Missing]
        end
        
        CALC[Calculate Missing Intervals<br/>Jan 15-21 needs processing]
        SET_MACROS[Set Macros<br/>@start_ds = '2025-01-15'<br/>@end_ds = '2025-01-21']
        QUERY[Run Query<br/>WHERE order_date BETWEEN @start_ds AND @end_ds]
        INSERT[Insert Results<br/>Into weekly_sales table]
        UPDATE[Update State<br/>Mark Jan 15-21 as processed]
    end
    
    START --> CHECK
    CHECK --> PROCESSED1
    CHECK --> PROCESSED2
    CHECK --> MISSING
    
    MISSING --> CALC
    CALC --> SET_MACROS
    SET_MACROS --> QUERY
    QUERY --> INSERT
    INSERT --> UPDATE
    
    UPDATE --> PROCESSED1
    UPDATE --> PROCESSED2
    
    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style CHECK fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PROCESSED1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style PROCESSED2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style MISSING fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style CALC fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style SET_MACROS fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style QUERY fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style INSERT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style UPDATE fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

### Step 1: Vulcan Checks What's Already Done

When you run `vulcan run`, Vulcan looks at your state database and asks:

- "What dates have I already processed?" - These are done, skip them

- "What dates are missing?" - These need work, process them

It's like checking your to-do list, you only work on what's not done yet.

```
State Database Check:
Jan 1-7:   Already processed
Jan 8-14:  Already processed  
Jan 15-21: Missing - needs processing
```

*[Screenshot: Visual diagram showing state database check with processed vs missing intervals]*

### Step 2: Vulcan Processes Only Missing Intervals

Vulcan then processes only the missing dates. It sets up the macros (`@start_ds` and `@end_ds`) and runs your query for just that time range:

```
Processing Jan 15-21:
@start_ds = '2025-01-15'
@end_ds   = '2025-01-21'

Query runs:
SELECT ... FROM daily_sales
WHERE order_date BETWEEN '2025-01-15' AND '2025-01-21'
```

*[Screenshot: Visual showing how @start_ds and @end_ds are used in the query]*

### Step 3: Results Are Inserted

The processed data is inserted into your table, and Vulcan records that these dates are now complete. Next time you run, it'll know these dates are done and skip them. Efficient!

```
Jan 15-21: Now processed and recorded
```

*[Screenshot: Visual showing data insertion and state update]*

---

## Understanding Time Intervals

Vulcan divides time into **intervals** based on your model's schedule. Each interval is a chunk of time that gets processed together, like a day, a week, or an hour, depending on your model's `cron` schedule.

### Daily Intervals Example

For a daily model (`cron '@daily'`), each day is one interval:

```mermaid
gantt
    title Daily Intervals
    dateFormat YYYY-MM-DD
    section Intervals
    Jan 1 Complete     :done, interval1, 2025-01-01, 1d
    Jan 2 Complete     :done, interval2, 2025-01-02, 1d
    Jan 3 In Progress :active, interval3, 2025-01-03, 1d
```

```
Model Start: Jan 1, 2025
Today: Jan 3, 2025 at 2pm

Intervals:
- Jan 1: Complete (full day passed)

- Jan 2: Complete (full day passed)

- Jan 3: In progress (day not finished yet)
```

*[Screenshot: Calendar view showing daily intervals with Jan 1-2 complete, Jan 3 in progress]*

### Weekly Intervals Example

For a weekly model (`cron '@weekly'`), each week is one interval:

```mermaid
gantt
    title Weekly Intervals
    dateFormat YYYY-MM-DD
    section Intervals
    Week 1 Jan 1-7     :done, week1, 2025-01-01, 7d
    Week 2 Jan 8-14    :done, week2, 2025-01-08, 7d
    Week 3 Jan 15-21   :active, week3, 2025-01-15, 7d
```

```
Model Start: Jan 1, 2025
Today: Jan 15, 2025

Intervals:
- Week 1 (Jan 1-7):   Complete

- Week 2 (Jan 8-14):  Complete

- Week 3 (Jan 15-21): In progress
```

*[Screenshot: Calendar view showing weekly intervals]*

### How Vulcan Tracks Intervals

When you first run `vulcan plan` on an incremental model, Vulcan:

```mermaid
flowchart TB
    subgraph "First Plan - Jan 15, 2025"
        PLAN1[vulcan plan dev]
        CALC1[Calculate Intervals<br/>From start to now<br/>3 weeks total]
        PROCESS1[Process All Intervals<br/>Backfill everything]
        RECORD1[Record in State DB<br/>Weeks 1-3 processed]
        
        subgraph "State Database After Plan"
            W1[Week 1: Jan 1-7]
            W2[Week 2: Jan 8-14]
            W3[Week 3: Jan 15-21]
        end
    end
    
    PLAN1 --> CALC1
    CALC1 --> PROCESS1
    PROCESS1 --> RECORD1
    RECORD1 --> W1
    RECORD1 --> W2
    RECORD1 --> W3
    
    style PLAN1 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style CALC1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PROCESS1 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style RECORD1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style W1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style W2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style W3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

1. **Calculates all intervals** from the start date to now
2. **Processes all missing intervals** (backfill)
3. **Records what was processed** in the state database

```
First Plan (Jan 15, 2025):
- Calculates: 3 weeks of intervals

- Processes: All 3 weeks

- Records: "Weeks 1-3 processed"

State Database:
Week 1 (Jan 1-7)
Week 2 (Jan 8-14)
Week 3 (Jan 15-21)
```

*[Screenshot: Visual showing first plan calculating and processing all intervals]*

When you run `vulcan run` later, Vulcan:

```mermaid
flowchart TB
    subgraph "Second Run - Jan 22, 2025"
        RUN2[vulcan run]
        CALC2[Calculate Intervals<br/>From start to now<br/>4 weeks total]
        CHECK[Check State DB<br/>What's already processed?]
        
        subgraph "Current State"
            W1_EXIST[Week 1: Jan 1-7]
            W2_EXIST[Week 2: Jan 8-14]
            W3_EXIST[Week 3: Jan 15-21]
            W4_MISS[Week 4: Jan 22-28]
        end
        
        PROCESS2[Process Only Week 4<br/>Skip Weeks 1-3]
        RECORD2[Update State DB<br/>Week 4 now processed]
        
        subgraph "Updated State"
            W1_NEW[Week 1: Jan 1-7]
            W2_NEW[Week 2: Jan 8-14]
            W3_NEW[Week 3: Jan 15-21]
            W4_NEW[Week 4: Jan 22-28]
        end
    end
    
    RUN2 --> CALC2
    CALC2 --> CHECK
    CHECK --> W1_EXIST
    CHECK --> W2_EXIST
    CHECK --> W3_EXIST
    CHECK --> W4_MISS
    
    W4_MISS --> PROCESS2
    PROCESS2 --> RECORD2
    
    RECORD2 --> W1_NEW
    RECORD2 --> W2_NEW
    RECORD2 --> W3_NEW
    RECORD2 --> W4_NEW
    
    style RUN2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style CALC2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style W4_MISS fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style PROCESS2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style RECORD2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

1. **Calculates intervals** from start to now
2. **Compares** with what's already processed
3. **Processes only new intervals**

```
Second Run (Jan 22, 2025):
- Calculates: 4 weeks total

- Already processed: Weeks 1-3

- Missing: Week 4 (Jan 22-28)

- Processes: Only Week 4

State Database:
Week 1 (Jan 1-7)
Week 2 (Jan 8-14)
Week 3 (Jan 15-21)
Week 4 (Jan 22-28) ← NEW
```

*[Screenshot: Visual showing second run processing only new Week 4]*

---

## Creating an Incremental Model

Let's create a weekly sales aggregation model for Orders360.

### Step 1: Create the Model File

```bash
touch models/sales/weekly_sales.sql
```

*[Screenshot: File explorer showing new weekly_sales.sql file]*

### Step 2: Define the Model

Edit `models/sales/weekly_sales.sql`:

```sql
MODEL (
  name sales.weekly_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,  -- ⏰ This column contains the date
    batch_size 1             -- Process 1 week at a time
  ),
  start '2025-01-01',       -- Start processing from this date
  cron '@weekly',            -- Run weekly
  grain [order_date],        -- One row per week
  description 'Weekly aggregated sales metrics'
);

SELECT
  DATE_TRUNC('week', order_date) AS order_date,
  COUNT(DISTINCT order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  AVG(total_amount)::FLOAT AS avg_order_value
FROM sales.daily_sales
WHERE order_date BETWEEN @start_ds AND @end_ds  -- Filter by time range
GROUP BY DATE_TRUNC('week', order_date)
ORDER BY order_date
```

*[Screenshot: Code editor showing complete weekly_sales.sql model]*

### Key Components Explained

#### 1. Time Column Declaration

```sql
kind INCREMENTAL_BY_TIME_RANGE (
  time_column order_date  -- Tell Vulcan which column has dates
)
```

**What it does:** Tells Vulcan which column contains the timestamp/date for each row. This is how Vulcan knows how to filter and group your data by time.

*[Screenshot: Code highlighting time_column declaration]*

#### 2. WHERE Clause with Macros

```sql
WHERE order_date BETWEEN @start_ds AND @end_ds
```

**What it does:** Filters data to only the time range being processed. Without it, you'd process all your data every time.

- `@start_ds` = Start date of the interval (e.g., '2025-01-15')

- `@end_ds` = End date of the interval (e.g., '2025-01-21')

Vulcan automatically replaces these with the correct dates! You don't have to figure out what dates to process, Vulcan does that for you.

*[Screenshot: Code highlighting WHERE clause with macros, showing how they're replaced]*

#### 3. Start Date

```sql
start '2025-01-01'
```

**What it does:** Tells Vulcan when your data begins. Vulcan will backfill from this date when you first create the model, processing all historical data up to today.

*[Screenshot: Code highlighting start date]*

### Step 3: Apply the Model

Run `vulcan plan` to apply your new model:

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Added:
    └── sales.weekly_sales

Models needing backfill (missing dates):
└── sales.weekly_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]: y
```

*[Screenshot: Plan output showing weekly_sales model to be added]*

Vulcan will process each week incrementally:

```
[1/3] sales.weekly_sales  [insert 2025-01-01 - 2025-01-07]  1.2s
[2/3] sales.weekly_sales  [insert 2025-01-08 - 2025-01-14]  1.1s
[3/3] sales.weekly_sales  [insert 2025-01-15 - 2025-01-21]  1.3s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3 • 0:00:03

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Backfill progress showing each week being processed]*

---

## Real Example: Daily Sales from Orders360

Here's the actual `daily_sales` model from Orders360 (currently FULL, but could be incremental):

```sql
MODEL (
  name sales.daily_sales,
  kind FULL,  -- Could be INCREMENTAL_BY_TIME_RANGE
  cron '@daily',
  grain order_date,
  description 'Daily sales summary with order counts and revenue',
  column_descriptions (
    order_date = 'Date of the sales',
    total_orders = 'Total number of orders for the day',
    total_revenue = 'Total revenue for the day',
    last_order_id = 'Last order ID processed for the day'
  ),
  assertions (
    unique_values(columns := (order_date)),
    not_null(columns := (order_date, total_orders, total_revenue)),
    positive_values(column := total_orders),
    positive_values(column := total_revenue)
  )
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql file showing complete model]*

**To make this incremental**, you would:

1. Change `kind FULL` to `kind INCREMENTAL_BY_TIME_RANGE`
2. Add `time_column order_date`
3. Add `WHERE order_date BETWEEN @start_ds AND @end_ds`

```sql
MODEL (
  name sales.daily_sales,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date
  ),
  start '2025-01-01',
  cron '@daily',
  -- ... rest stays the same
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
WHERE order_date BETWEEN @start_ds AND @end_ds  -- ADD THIS
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: Comparison showing FULL vs INCREMENTAL changes]*

---

## Understanding the WHERE Clause

You might wonder: "Why do I need a WHERE clause if Vulcan adds one automatically?" They serve different purposes.

### Two WHERE Clauses, Two Purposes

Vulcan actually uses **two** WHERE clauses:

#### 1. Your Model's WHERE Clause

```sql
WHERE order_date BETWEEN @start_ds AND @end_ds
```

**Purpose:** Filters data **read into** the model

- Only reads necessary data from upstream tables - this is a performance optimization

- Saves processing time and resources - why read data you're not going to use?

- You control this in your SQL - you write this WHERE clause

This is your optimization. Without it, you'd read all the data from upstream tables, even though you only need a small date range.

*[Screenshot: Visual showing model WHERE clause filtering input data]*

#### 2. Vulcan's Automatic WHERE Clause

Vulcan automatically adds another filter on the output:

```sql
-- Vulcan adds this automatically:
WHERE order_date BETWEEN @start_ds AND @end_ds
```

**Purpose:** Filters data **output by** the model

- Prevents data leakage (ensures no rows outside the time range) - this is a safety check

- Safety mechanism - even if your SQL has a bug, Vulcan makes sure you don't output wrong dates

- Vulcan controls this automatically - you don't write this one, Vulcan adds it

This is Vulcan's safety net. It makes sure that even if your WHERE clause has a bug, you can't accidentally output data for the wrong time period.

*[Screenshot: Visual showing Vulcan's automatic WHERE clause filtering output]*

### Why Both Are Needed

Here's why you need both:

- **Your WHERE clause:** Optimizes performance by reading less data - makes your queries faster

- **Vulcan's WHERE clause:** Ensures correctness by preventing data leakage - makes sure your data is right

**Always include the WHERE clause in your model SQL!** It's not optional, Vulcan needs it to know what time range to process, and it makes your queries way more efficient.

*[Screenshot: Side-by-side comparison showing both WHERE clauses and their purposes]*

---

## Running Incremental Models

Vulcan has two commands for processing models:

### `vulcan plan` - For Model Changes

Use when you've **changed a model**:

```bash
vulcan plan dev
```

**What it does:**

- Detects model changes

- Shows what will be affected

- Backfills missing intervals

- Applies changes to the environment

*[Screenshot: Plan command output showing model changes]*

### `vulcan run` - For Scheduled Execution

Use when **no models have changed**:

```bash
vulcan run
```

**What it does:**

- Checks each model's `cron` schedule

- Processes only models that are due

- Processes only missing intervals

- Fast and efficient

*[Screenshot: Run command output showing scheduled execution]*

### How Cron Schedules Work

Each model has a `cron` parameter that determines how often it should run:

```mermaid
flowchart LR
    subgraph "Cron Schedules"
        DAILY[@daily<br/>Every 24 hours]
        WEEKLY[@weekly<br/>Every 7 days]
        HOURLY[@hourly<br/>Every 1 hour]
    end
    
    subgraph "Example Models"
        M1[sales.daily_sales<br/>cron: @daily]
        M2[sales.weekly_sales<br/>cron: @weekly]
    end
    
    DAILY --> M1
    WEEKLY --> M2
    
    style DAILY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style WEEKLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style HOURLY fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style M1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style M2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

```sql
cron '@daily'   -- Run once per day
cron '@weekly'  -- Run once per week
cron '@hourly'  -- Run once per hour
```

**Example from Orders360:**

```sql
-- Daily model
MODEL (
  name sales.daily_sales,
  cron '@daily'  -- Runs every day
);

-- Weekly model
MODEL (
  name sales.weekly_sales,
  cron '@weekly'  -- Runs once per week
);
```

*[Screenshot: Visual showing cron schedules for daily vs weekly models]*

When you run `vulcan run`:

```mermaid
flowchart TB
    subgraph "vulcan run Execution"
        RUN[vulcan run<br/>at 2pm on Jan 15]
        
        subgraph "Model Evaluation"
            CHECK1[Check daily_sales<br/>cron: @daily<br/>Last run: 24h ago]
            CHECK2[Check weekly_sales<br/>cron: @weekly<br/>Last run: 2 days ago]
        end
        
        subgraph "Decision"
            DUE1[Due!<br/>Process daily_sales]
            SKIP[Not due<br/>Skip weekly_sales]
        end
        
        EXEC1[Execute daily_sales<br/>Process missing intervals]
    end
    
    RUN --> CHECK1
    RUN --> CHECK2
    
    CHECK1 --> DUE1
    CHECK2 --> SKIP
    
    DUE1 --> EXEC1
    
    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style CHECK1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style CHECK2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style DUE1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style SKIP fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

1. Vulcan checks each model's `cron`
2. Determines if enough time has passed since last run
3. Processes only models that are due

```
vulcan run at 2pm on Jan 15:

daily_sales (@daily):   Last run 24h ago → Due, process!
weekly_sales (@weekly): Last run 2 days ago → Not due, skip
```

*[Screenshot: Visual showing cron evaluation logic]*

---

## Batch Processing

For large datasets, you can process intervals in batches using `batch_size`:

```mermaid
flowchart TB
    subgraph "Without batch_size Default"
        ALL[12 Weeks Missing]
        SINGLE["Single Job<br/>Process all 12 weeks"]
        RESULT1["All done in 1 job<br/>30 minutes"]
    end
    
    subgraph "With batch_size = 4"
        ALL2[12 Weeks Missing]
        BATCH1["Batch 1<br/>Weeks 1-4"]
        BATCH2["Batch 2<br/>Weeks 5-8"]
        BATCH3["Batch 3<br/>Weeks 9-12"]
        RESULT2["All done in 3 jobs<br/>10 min each"]
    end
    
    ALL --> SINGLE
    SINGLE --> RESULT1
    
    ALL2 --> BATCH1
    ALL2 --> BATCH2
    ALL2 --> BATCH3
    BATCH1 --> RESULT2
    BATCH2 --> RESULT2
    BATCH3 --> RESULT2
    
    style ALL fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style SINGLE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style ALL2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style BATCH1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style BATCH2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style BATCH3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style RESULT2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

```sql
MODEL (
  name sales.weekly_sales,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 4  -- Process 4 weeks at a time
  )
);
```

**Without batch_size (default):**

- Processes all missing intervals in one job

- Example: 12 weeks = 1 job

**With batch_size:**

- Divides intervals into batches

- Example: 12 weeks ÷ 4 = 3 jobs

*[Screenshot: Visual comparison showing batch processing vs single job]*

**When to use batches:**

- Large datasets that might timeout

- Need better progress tracking

- Want to parallelize processing

**When not to use batches:**

- Small datasets (< 1GB)

- Fast queries (< 1 minute)

- Simple transformations

---

## Forward-Only Models

Sometimes you have models so large that rebuilding them is impossible. Forward-only models solve this. Use them for massive tables where a full backfill would take days or cost too much.

### What Are Forward-Only Models?

Forward-only models never rebuild historical data. Changes are only applied going forward in time. Once historical data is processed, you can't go back and change it. You can only change what happens going forward.

```mermaid
flowchart TB
    subgraph "Regular Model Change"
        REG_CHANGE["Breaking Change Detected"]
        REG_REBUILD["Rebuild Entire Table<br/>All dates: Jan 1 - Dec 31"]
        REG_RESULT["All data updated"]
    end
    
    subgraph "Forward-Only Model Change"
        FWD_CHANGE["Breaking Change Detected<br/>forward_only: true"]
        FWD_CHECK["Check Existing Data<br/>Jan 1 - Dec 15: Keep as-is"]
        FWD_APPLY["Apply Change Forward<br/>Dec 16 - Dec 31: New data"]
        FWD_RESULT["Historical preserved<br/>Future updated"]
    end
    
    REG_CHANGE --> REG_REBUILD
    REG_REBUILD --> REG_RESULT
    
    FWD_CHANGE --> FWD_CHECK
    FWD_CHECK --> FWD_APPLY
    FWD_APPLY --> FWD_RESULT
    
    style REG_CHANGE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style REG_REBUILD fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style REG_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style FWD_CHANGE fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style FWD_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style FWD_APPLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style FWD_RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

**Regular Model Change:**
```
Breaking change → Rebuild entire table (all dates)
```

**Forward-Only Model Change:**
```
Breaking change → Only apply to new dates going forward
```

*[Screenshot: Visual comparison showing regular rebuild vs forward-only]*

### When to Use Forward-Only

**Use forward-only when:**

- Tables are too large to rebuild - if a full backfill would take forever or cost too much

- Historical data can't be reprocessed - maybe the source data is gone or too expensive to reprocess

- You only care about future data - if historical data is "good enough" and you just need new data to be correct

**Don't use forward-only when:**

- You need to fix historical data - forward-only won't help you fix the past

- Schema changes affect past data - if your change affects how historical data should look, you need a full rebuild

- You want full data consistency - forward-only means historical and new data might have different schemas

It's a trade-off: you get speed and cost savings, but you lose the ability to fix historical data. Make sure that's a trade-off you're okay with!

### Making a Model Forward-Only

Add `forward_only true` to your model:

```sql
MODEL (
  name sales.weekly_sales,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    forward_only true  -- All changes are forward-only
  )
);
```

*[Screenshot: Code showing forward_only configuration]*

### Forward-Only Plans

You can also make a specific plan forward-only:

```bash
vulcan plan dev --forward-only
```

This treats **all changes in the plan** as forward-only, even if models aren't configured that way.

*[Screenshot: Plan command with --forward-only flag]*

---

## Schema Changes in Forward-Only Models

When you change a forward-only model, Vulcan checks for schema changes that could cause problems.

### Types of Schema Changes

#### Destructive Changes

Changes that **remove or modify** existing data:

```mermaid
flowchart LR
    subgraph "Destructive Changes"
        DROP["Dropping Column<br/>total_amount removed"]
        RENAME["Renaming Column<br/>order_id to id"]
        TYPE["Changing Type<br/>INT to STRING<br/>may cause loss"]
    end
    
    subgraph "Example"
        BEFORE1["Before:<br/>order_id, total_amount"]
        AFTER1["After:<br/>order_id only"]
    end
    
    DROP --> BEFORE1
    BEFORE1 --> AFTER1
    
    style DROP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style RENAME fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style TYPE fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style BEFORE1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style AFTER1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
```

- Dropping a column

- Renaming a column

- Changing data type (could cause data loss)

**Example:**
```sql
-- Before
SELECT order_id, total_amount FROM orders

-- After (destructive - drops total_amount)
SELECT order_id FROM orders
```

*[Screenshot: Visual showing destructive change example]*

#### Additive Changes

Changes that **add** new data without removing existing:

```mermaid
flowchart LR
    subgraph "Additive Changes"
        ADD["Adding Column<br/>customer_name added"]
        TYPE2["Compatible Type Change<br/>INT to STRING<br/>no data loss"]
    end
    
    subgraph "Example"
        BEFORE2["Before:<br/>order_id, total_amount"]
        AFTER2["After:<br/>order_id, total_amount,<br/>customer_name"]
    end
    
    ADD --> BEFORE2
    BEFORE2 --> AFTER2
    
    style ADD fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style TYPE2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style BEFORE2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style AFTER2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

- Adding a new column

- Changing data type (compatible, e.g., INT → STRING)

**Example:**
```sql
-- Before
SELECT order_id, total_amount FROM orders

-- After (additive - adds customer_name)
SELECT order_id, total_amount, customer_name FROM orders
```

*[Screenshot: Visual showing additive change example]*

### Controlling Schema Change Behavior

You can control how Vulcan handles schema changes:

```sql
MODEL (
  name sales.weekly_sales,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
        forward_only true,
        on_destructive_change error,  -- Block destructive changes
    on_additive_change allow      -- Allow new columns
  )
);
```

**Options:**

- `error` - Stop and raise an error (default for destructive)

- `warn` - Log a warning but continue

- `allow` - Silently proceed (default for additive)

- `ignore` - Skip the check entirely (dangerous!)

*[Screenshot: Code showing schema change configuration options]*

### Common Patterns

#### Strict Schema Control

Prevent any schema changes:

```sql
MODEL (
  name sales.production_model,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
        forward_only true,
    on_destructive_change error,  -- Block destructive
    on_additive_change error       -- Block even new columns
  )
);
```

*[Screenshot: Strict schema control example]*

#### Development Model

Allow all changes for rapid iteration:

```sql
MODEL (
  name sales.dev_model,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
        forward_only true,
        on_destructive_change allow,  -- Allow dropping columns
    on_additive_change allow      -- Allow new columns
  )
);
```

*[Screenshot: Development model example]*

#### Production Safety

Allow safe changes, warn about risky ones:

```sql
MODEL (
  name sales.production_model,
    kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
        forward_only true,
    on_destructive_change warn,   -- Warn but allow
    on_additive_change allow      -- Allow new columns
  )
);
```

*[Screenshot: Production safety example]*

---

## Important Notes

### Time Column Must Be UTC

Always use UTC timezone for your `time_column`:

```sql
-- Good: UTC timezone
time_column order_date_utc

-- Bad: Local timezone
time_column order_date_local
```

**Why?** Ensures correct interval calculations and proper interaction with Vulcan's scheduler. If you use local timezones, you'll run into issues with daylight saving time changes, timezone differences, and interval calculations. UTC is consistent everywhere.

*[Screenshot: Visual warning about UTC requirement]*

### Always Include WHERE Clause

Your model SQL **must** include a WHERE clause with `@start_ds` and `@end_ds`:

```sql
-- Required - This tells Vulcan what time range to process
WHERE order_date BETWEEN @start_ds AND @end_ds

-- Missing WHERE clause - Don't do this

-- WHERE clause is required
```

Without this WHERE clause, Vulcan won't know what time range to process, and your queries will be inefficient (or fail entirely). Always include it!

*[Screenshot: Code showing required WHERE clause]*

### Set a Start Date

Always specify when your data begins:

```sql
start '2025-01-01'  -- Start processing from this date
```

This tells Vulcan where to start backfilling. If your data goes back to 2020 but you only want to process from 2025, set the start date to 2025-01-01. Vulcan will backfill from this date when you first create the model.

*[Screenshot: Code showing start date configuration]*

### Choose Appropriate batch_size

- Start with `batch_size 1` for small datasets - process one interval at a time

- Increase for larger datasets that might timeout - if processing all intervals at once times out, batch them

- Monitor performance to find the right balance. Too small and you have too many jobs. Too large and jobs timeout.

The default is to process all missing intervals in one job. If that doesn't work for you, use `batch_size` to break it up.

*[Screenshot: Visual guide for choosing batch_size]*

---

## Summary

**Incremental by time models:**

- Only process new or missing time intervals

- Faster and cheaper than full refreshes

- Use for time-based data (orders, events, transactions)

- Require a time column and WHERE clause

- Use cron schedules to control execution frequency

**Key concepts:**

- **Intervals:** Time periods (days, weeks, hours) that Vulcan tracks

- **Backfill:** Processing historical intervals when first creating a model

- **Cron:** Schedule that determines how often a model runs

- **Forward-only:** Models that never rebuild historical data

---

## Next Steps

- Learn about [Model Kinds](../components/model/model_kinds.md) for all model types




# Model Selection

Source: https://tmdc-io.github.io/vulcan-book/guides/model_selection/

---

# Model Selection

This guide explains how to select specific models to include in a Vulcan plan using the Orders360 example project. Use this when you want to test or apply changes to a subset of your models without processing everything.

In large projects, model selection saves time. Instead of waiting for all models to process, focus on what you're working on.

**Note:** The selector syntax described below is also used for the Vulcan `plan` [`--allow-destructive-model` and `--allow-additive-model` selectors](../references/plans.md#destructive-changes).

---

## Background

A Vulcan [plan](../references/plans.md) automatically detects changes between your local project and the deployed environment. When applied, it backfills directly modified models and their downstream dependencies.

In large projects, a single model change can impact many downstream models, making plans take a long time. Model selection lets you filter which changes to include, so you can test specific models without processing everything.

**Key Concept:**

- **Directly Modified**: Models you changed in your code - these are the ones you actually edited

- **Indirectly Modified**: Downstream models affected by your changes - these depend on what you changed, so they need to be reprocessed too

Understanding this distinction helps you understand what model selection is doing. You're filtering which directly modified models to include, and Vulcan automatically figures out the indirect ones.

*[Screenshot: Visual showing directly vs indirectly modified models]*

---

## Understanding Model Dependencies

Before we dive into selection, let's understand how models relate to each other in Orders360. This will help you understand why selecting one model might include others.

```mermaid
flowchart TD
    subgraph "Orders360 Model DAG"
        RAW_CUSTOMERS[raw.raw_customers<br/>Seed Model]
        RAW_ORDERS[raw.raw_orders<br/>Seed Model]
        RAW_PRODUCTS[raw.raw_products<br/>Seed Model]
        
        DAILY_SALES[sales.daily_sales<br/>Daily Aggregation]
        WEEKLY_SALES[sales.weekly_sales<br/>Weekly Aggregation]
    end
    
    RAW_CUSTOMERS --> DAILY_SALES
    RAW_ORDERS --> DAILY_SALES
    RAW_PRODUCTS --> DAILY_SALES
    
    DAILY_SALES --> WEEKLY_SALES
    
    style RAW_CUSTOMERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style RAW_ORDERS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style RAW_PRODUCTS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style DAILY_SALES fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style WEEKLY_SALES fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

**Dependency Flow:**

- `raw.raw_orders` → `sales.daily_sales` → `sales.weekly_sales`

- Changing `raw.raw_orders` affects `daily_sales` (indirectly modified) - because daily_sales reads from raw_orders

- Changing `daily_sales` affects `weekly_sales` (indirectly modified) - because weekly_sales reads from daily_sales

This is important! When you select a model, Vulcan automatically includes its downstream dependencies. You can't process `weekly_sales` without processing `daily_sales` first, because `weekly_sales` depends on it.

*[Screenshot: Orders360 project structure showing model files]*

---

## Syntax

Model selections use the `--select-model` argument in `vulcan plan`. You can select models in several ways, by name, pattern, tags, git changes, and more. Let's explore all the options!

### Basic Selection

Select a single model by name:

```bash
vulcan plan dev --select-model "sales.daily_sales"
```

*[Screenshot: Plan output showing only daily_sales selected]*

Select multiple models:

```bash
vulcan plan dev --select-model "sales.daily_sales" --select-model "raw.raw_orders"
```

*[Screenshot: Plan output showing multiple models selected]*

### Wildcard Selection

Use `*` to match multiple models:

```bash
# Select all models starting with "raw."
vulcan plan dev --select-model "raw.*"

# Select all models ending with "_sales"
vulcan plan dev --select-model "sales.*_sales"

# Select all models containing "daily"
vulcan plan dev --select-model "*daily*"
```

**Examples:**

- `"raw.*"` matches `raw.raw_customers`, `raw.raw_orders`, `raw.raw_products` - all models in the raw schema

- `"sales.*_sales"` matches `sales.daily_sales`, `sales.weekly_sales` - all models ending with _sales in the sales schema

- `"*.daily_sales"` matches `sales.daily_sales` - matches daily_sales in any schema

Wildcards let you select a group of related models without listing them all individually.

*[Screenshot: Plan output showing wildcard selection results]*

### Tag Selection

Select models by tags using `tag:tag_name`:

```bash
# Select all models with "seed" tag
vulcan plan dev --select-model "tag:seed"

# Select all models with tags starting with "reporting"
vulcan plan dev --select-model "tag:reporting*"
```

**Example:** If `raw.raw_orders` and `raw.raw_customers` have the `seed` tag:

```bash
vulcan plan dev --select-model "tag:seed"
# Selects: raw.raw_orders, raw.raw_customers
```

*[Screenshot: Plan output showing tag-based selection]*

### Upstream/Downstream Selection

Use `+` to include upstream or downstream models:

- `+model_name` = Include upstream models (dependencies)

- `model_name+` = Include downstream models (dependents)

```mermaid
flowchart LR
    subgraph "Model Dependencies"
        RAW[raw.raw_orders]
        DAILY[sales.daily_sales]
        WEEKLY[sales.weekly_sales]
    end
    
    RAW -->|upstream| DAILY
    DAILY -->|downstream| WEEKLY
    
    style RAW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style DAILY fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000
    style WEEKLY fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

**Examples:**

```bash
# Select daily_sales only
vulcan plan dev --select-model "sales.daily_sales"
# Result: daily_sales (directly modified)

# Select daily_sales + upstream (raw.raw_orders)
vulcan plan dev --select-model "+sales.daily_sales"
# Result: raw.raw_orders, daily_sales

# Select daily_sales + downstream (weekly_sales)
vulcan plan dev --select-model "sales.daily_sales+"
# Result: daily_sales, weekly_sales

# Select daily_sales + both upstream and downstream
vulcan plan dev --select-model "+sales.daily_sales+"
# Result: raw.raw_orders, daily_sales, weekly_sales
```

*[Screenshot: Plan outputs showing different selection results]*

### Git-Based Selection

Select models changed in a git branch:

```bash
# Select models changed in feature branch
vulcan plan dev --select-model "git:feature"

# Select changed models + downstream
vulcan plan dev --select-model "git:feature+"

# Select changed models + upstream
vulcan plan dev --select-model "+git:feature"
```

**What it includes:**

- Untracked files (new models) - models you've created but haven't committed yet

- Uncommitted changes - models you've modified but haven't committed

- Committed changes different from target branch - models that differ between your branch and the target (like `main`)

Use this for feature branches. Select all models you've changed in your feature branch without listing them manually.

*[Screenshot: Plan output showing git-based selection]*

### Complex Selections

Combine conditions with logical operators:

- `&` (AND): Both conditions must be true

- `|` (OR): Either condition must be true

- `^` (NOT): Negates a condition

```bash
# Models with finance tag that don't have deprecated tag
vulcan plan dev --select-model "(tag:finance & ^tag:deprecated)"

# daily_sales + upstream OR weekly_sales + downstream
vulcan plan dev --select-model "(+sales.daily_sales | sales.weekly_sales+)"

# Changed models that also have finance tag
vulcan plan dev --select-model "(tag:finance & git:main)"

# Models in sales schema without test tag
vulcan plan dev --select-model "^(tag:test) & sales.*"
```

*[Screenshot: Plan output showing complex selection results]*

---

## Examples with Orders360

Let's see how model selection works with the Orders360 project. We'll modify `raw.raw_orders` and `sales.daily_sales` to demonstrate different selection scenarios.

### Example Setup

We've modified two models:

- `raw.raw_orders` (directly modified)

- `sales.daily_sales` (directly modified)

The dependency chain:
```
raw.raw_orders → sales.daily_sales → sales.weekly_sales
```

*[Screenshot: Orders360 project showing modified files]*

### No Selection (Default)

Without selection, Vulcan includes all directly modified models and their downstream dependencies:

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── sales.daily_sales
│   └── raw.raw_orders
└── Indirectly Modified:
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing all modified models]*

**What Happened:**

- Both directly modified models are included - you changed both, so both are in the plan

- `weekly_sales` is indirectly modified (depends on `daily_sales`) - even though you didn't change weekly_sales, it depends on daily_sales, so it needs to be reprocessed

This is the default behavior, Vulcan includes everything that's affected. Model selection lets you narrow this down.

### Select Single Model

Select only `sales.daily_sales`:

```bash
vulcan plan dev --select-model "sales.daily_sales"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing only daily_sales selected]*

**What Happened:**

- `raw.raw_orders` is excluded (not selected) - you changed it, but you didn't select it, so it's not in the plan

- `daily_sales` is included (directly modified) - you selected it, so it's in the plan

- `weekly_sales` is included (indirectly modified, downstream of `daily_sales`) - it depends on daily_sales, so Vulcan automatically includes it

Notice how Vulcan automatically includes downstream models. You can't process daily_sales without processing weekly_sales, because weekly_sales depends on it.

### Select with Upstream Indicator

Select `daily_sales` and include its upstream dependencies:

```bash
vulcan plan dev --select-model "+sales.daily_sales"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── raw.raw_orders
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing upstream selection]*

**What Happened:**

- `raw.raw_orders` is included (upstream of `daily_sales`)

- `daily_sales` is included (selected)

- `weekly_sales` is included (downstream of `daily_sales`)

### Select with Downstream Indicator

Select `daily_sales` and include its downstream dependencies:

```bash
vulcan plan dev --select-model "sales.daily_sales+"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── sales.daily_sales
│   └── sales.weekly_sales
└── Indirectly Modified:
    (none)
```

*[Screenshot: Plan output showing downstream selection]*

**What Happened:**

- `daily_sales` is included (selected)

- `weekly_sales` is included (downstream, now directly modified)

- `raw.raw_orders` is excluded (not selected)

### Select with Wildcard

Select all models matching a pattern:

```bash
vulcan plan dev --select-model "sales.*_sales"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing wildcard selection]*

**What Happened:**

- `sales.daily_sales` matches the pattern (selected)

- `sales.weekly_sales` matches the pattern but is indirectly modified

- `raw.raw_orders` doesn't match (excluded)

### Select with Tags

If models have tags, select by tag:

```bash
vulcan plan dev --select-model "tag:seed"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── raw.raw_orders
└── Indirectly Modified:
    ├── sales.daily_sales
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing tag-based selection]*

**What Happened:**

- `raw.raw_orders` has `seed` tag (selected)

- Downstream models are indirectly modified

### Select with Git Changes

Select models changed in a git branch:

```bash
vulcan plan dev --select-model "git:feature"
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales  # Changed in feature branch
└── Indirectly Modified:
    └── sales.weekly_sales
```

*[Screenshot: Plan output showing git-based selection]*

**What Happened:**

- Only models changed in `feature` branch are selected

- Downstream models are included automatically

---

## Backfill Selection

By default, Vulcan backfills all models in a plan. You can limit which models are backfilled using `--backfill-model`.

**Important:** `--backfill-model` only works in development environments (not `prod`).

### How Backfill Selection Works

```mermaid
flowchart TB
    subgraph "Backfill Selection Flow"
        PLAN[vulcan plan dev]
        SELECT[--select-model<br/>Which models in plan?]
        BACKFILL[--backfill-model<br/>Which models to backfill?]
        
        subgraph "Plan Includes"
            IN_PLAN[Models in Plan<br/>daily_sales, weekly_sales]
        end
        
        subgraph "Backfill Includes"
            BACKFILL_LIST[Models to Backfill<br/>Only daily_sales]
        end
        
        RESULT[Result:<br/>Plan shows all models<br/>Only selected models backfilled]
    end
    
    PLAN --> SELECT
    PLAN --> BACKFILL
    SELECT --> IN_PLAN
    BACKFILL --> BACKFILL_LIST
    IN_PLAN --> RESULT
    BACKFILL_LIST --> RESULT
    
    style PLAN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style SELECT fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style BACKFILL fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style RESULT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
```

**Key Points:**

- `--select-model` determines which models appear in the plan - this is about what's in the plan

- `--backfill-model` determines which models are actually backfilled - this is about what gets processed

- Upstream models are always backfilled (required for downstream models) - if you backfill weekly_sales, you need daily_sales first

This separation lets you see what would be affected (select-model) but only process what you need (backfill-model). Useful for testing.

*[Screenshot: Visual diagram explaining backfill selection]*

### Backfill Examples

#### No Backfill Selection (Default)

All models in the plan are backfilled:

```bash
vulcan plan dev
```

**Expected Output:**
```
Models needing backfill (missing dates):
├── sales__dev.daily_sales: 2025-01-01 - 2025-01-15
└── sales__dev.weekly_sales: 2025-01-01 - 2025-01-15
```

*[Screenshot: Plan output showing all models needing backfill]*

#### Backfill Specific Model

Only backfill `daily_sales`:

```bash
vulcan plan dev --backfill-model "sales.daily_sales"
```

**Expected Output:**
```
Models needing backfill (missing dates):
└── sales__dev.daily_sales: 2025-01-01 - 2025-01-15
```

*[Screenshot: Plan output showing only daily_sales needs backfill]*

**What Happened:**

- `weekly_sales` is excluded from backfill - it's in the plan, but it won't be processed

- Only `daily_sales` will be processed - just what you selected

Use this in development. See what would be affected, but only process what you're actually testing. Saves time and compute costs.

#### Backfill with Upstream

When you backfill a model, its upstream dependencies are automatically included:

```bash
vulcan plan dev --backfill-model "sales.weekly_sales"
```

**Expected Output:**
```
Models needing backfill (missing dates):
├── raw__dev.raw_orders: 2025-01-01 - 2025-01-15
└── sales__dev.weekly_sales: 2025-01-01 - 2025-01-15
```

*[Screenshot: Plan output showing upstream models included in backfill]*

**What Happened:**

- `weekly_sales` is selected for backfill - you want to process this one

- `raw.raw_orders` is automatically included (upstream dependency) - weekly_sales depends on daily_sales, which depends on raw_orders, so Vulcan includes it

- `daily_sales` is excluded (not upstream of `weekly_sales`) - wait, that doesn't seem right...

Actually, this example might be incorrect. If weekly_sales depends on daily_sales, then daily_sales should be included as an upstream dependency. The key point is: Vulcan automatically includes upstream dependencies when you backfill a model.

---

## Visual Selection Guide

Here's a quick reference for common selection patterns:

```mermaid
flowchart LR
    subgraph "Selection Patterns"
        PAT1["sales.daily_sales<br/>Select only this model"]
        PAT2["+sales.daily_sales<br/>Select + upstream"]
        PAT3["sales.daily_sales+<br/>Select + downstream"]
        PAT4["+sales.daily_sales+<br/>Select + both"]
        PAT5["sales.*_sales<br/>Wildcard match"]
        PAT6["tag:seed<br/>Tag selection"]
    end
    
    subgraph "Results"
        RES1[daily_sales only]
        RES2[raw_orders + daily_sales]
        RES3[daily_sales + weekly_sales]
        RES4[All connected]
        RES5[All matching]
        RES6[All tagged]
    end
    
    PAT1 --> RES1
    PAT2 --> RES2
    PAT3 --> RES3
    PAT4 --> RES4
    PAT5 --> RES5
    PAT6 --> RES6
    
    style PAT1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PAT2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PAT3 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PAT4 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PAT5 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style PAT6 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
```

*[Screenshot: Visual cheat sheet for selection patterns]*

---

## Best Practices

Here are some tips to help you use model selection effectively:

1. **Start Small**: Select only the models you're testing
   ```bash
   vulcan plan dev --select-model "sales.daily_sales"
   ```
   Don't process everything if you're only testing one model. Start small, then expand if needed.

2. **Use Wildcards**: When selecting multiple related models
   ```bash
   vulcan plan dev --select-model "sales.*"
   ```
   Wildcards are your friend! They let you select groups of models without listing them all.

3. **Include Dependencies**: Use `+` when you need upstream/downstream models
   ```bash
   vulcan plan dev --select-model "+sales.daily_sales+"
   ```
   The `+` syntax includes the full dependency chain. Use it when you need all dependencies.

4. **Limit Backfill**: Use `--backfill-model` to save time in development
   ```bash
   vulcan plan dev --backfill-model "sales.daily_sales"
   ```
   In dev environments, you often don't need to backfill everything. Use this to save time and money.

5. **Use Tags**: Organize models with tags for easier selection
   ```bash
   vulcan plan dev --select-model "tag:reporting"
   ```
   Tags organize models. If you tag related models, you can select them all at once.

---

## Summary

**Model Selection:**

- Filter which models appear in a plan

- Use wildcards, tags, and git changes

- Include upstream/downstream with `+`

- Combine with logical operators

**Backfill Selection:**

- Limit which models are actually backfilled

- Upstream models are always included

- Only works in development environments

- Saves time when testing specific models

---

## Next Steps

- Learn about [Plans](../references/plans.md) for understanding plan behavior




# Models

Source: https://tmdc-io.github.io/vulcan-book/guides/models/

---

# Models

This guide shows you how to work with models in Vulcan using the Orders360 example project. You'll learn how to add, edit, evaluate, and manage models with practical examples.

Models define your data transformations. They contain the SQL or Python code that transforms your data.

## Prerequisites

Before adding a model, ensure that you have:

- [Created your project](./get-started/docker.md) 

- [Applied your first plan](./plan.md#scenario-1-first-plan-initializing-production)

- Working in a [dev environment](../references/environments.md) for testing changes

---

## Understanding Models

Models in Vulcan consist of two core components:

1. **DDL (Data Definition Language)**: The `MODEL` block that defines structure, metadata, and behavior - this is where you configure how the model works
2. **DML (Data Manipulation Language)**: The `SELECT` query that contains transformation logic - this is where you write your SQL

Think of the MODEL block as the configuration and the SELECT as the actual work. Together, they define what your model does and how it does it.

### Example: Daily Sales Model

Here's a real example from Orders360:

```sql
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date,
  description 'Daily sales summary with order counts and revenue',
  column_descriptions (
    order_date = 'Date of the sales',
    total_orders = 'Total number of orders for the day',
    total_revenue = 'Total revenue for the day',
    last_order_id = 'Last order ID processed for the day'
  ),
  assertions (
    unique_values(columns := (order_date)),
    not_null(columns := (order_date, total_orders, total_revenue)),
    positive_values(column := total_orders),
    positive_values(column := total_revenue)
  )
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql file in editor showing the complete model definition]*

---

## Adding a Model

To add a new model to your Orders360 project:

### Step 1: Create Model File

Create a new file in your `models` directory. For example, let's add a weekly sales aggregation:

```bash
touch models/sales/weekly_sales.sql
```

*[Screenshot: File explorer showing models/sales directory structure]*

### Step 2: Define the Model

Edit the file and add your model definition:

```sql
MODEL (
  name sales.weekly_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 1
  ),
  start '2025-01-01',
  cron '@weekly',
  grain [order_date],
  description 'Weekly aggregated sales metrics'
);

SELECT
  DATE_TRUNC('week', order_date) AS order_date,
  COUNT(DISTINCT order_id) AS total_orders,
  SUM(total_amount) AS total_revenue,
  AVG(total_amount) AS avg_order_value
FROM sales.daily_sales
WHERE order_date BETWEEN @start_ds AND @end_ds
GROUP BY DATE_TRUNC('week', order_date)
```

*[Screenshot: weekly_sales.sql file in editor with model definition]*

### Step 3: Check Model Status

Verify your model is detected:

```bash
vulcan info
```

**Expected Output:**
```
Connection: Connected
Models: 5
  - raw.raw_customers

  - raw.raw_orders

  - raw.raw_products

  - sales.daily_sales

  - sales.weekly_sales  ← NEW MODEL
...
```

*[Screenshot: `vulcan info` output showing the new weekly_sales model]*

### Step 4: Apply the Model

Use `vulcan plan` to apply your new model:

```bash
vulcan plan
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Added:
    └── sales.weekly_sales

Models needing backfill (missing dates):
└── sales.weekly_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan output showing new weekly_sales model to be added]*

Type `y` to apply and backfill the model.

---

## Editing an Existing Model

To edit an existing model, modify the model file and use Vulcan's tools to preview and apply changes.

### Step 1: Edit the Model File

Let's modify `sales.daily_sales` to add a new column. Open `models/sales/daily_sales.sql`:

```sql
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date,
  description 'Daily sales summary with order counts and revenue',
  column_descriptions (
    order_date = 'Date of the sales',
    total_orders = 'Total number of orders for the day',
    total_revenue = 'Total revenue for the day',
    last_order_id = 'Last order ID processed for the day',
    avg_order_value = 'Average order value for the day'  -- NEW COLUMN DESCRIPTION
  ),
  assertions (
    unique_values(columns := (order_date)),
    not_null(columns := (order_date, total_orders, total_revenue)),
    positive_values(column := total_orders),
    positive_values(column := total_revenue)
  )
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  AVG(total_amount)::FLOAT AS avg_order_value,  -- NEW COLUMN
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql file showing the added avg_order_value column]*

### Step 2: Evaluate the Model (Optional)

Preview the model output without materializing it:

```bash
vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15
```

**Expected Output:**
```
order_date          total_orders  total_revenue  avg_order_value  last_order_id
2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142
```

*[Screenshot: Evaluate command output showing the new avg_order_value column]*

**What Happened?**

- The `evaluate` command runs the model query without creating tables. It's a dry run.

- Shows you the output with the new column. You can see what the data will look like.

- Useful for testing changes before applying them. Catch issues before they hit production.

Use `evaluate` to test changes quickly without waiting for full materialization.

### Step 3: Preview Changes with Plan

See what will change and how it affects downstream models:

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Directly Modified:
    └── sales.daily_sales

Directly Modified: sales.daily_sales (Non-breaking)
└── Diff:
    @@ -22,6 +22,7 @@
      SELECT
        CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
        COUNT(order_id)::INTEGER AS total_orders,
        SUM(total_amount)::FLOAT AS total_revenue,
    +   AVG(total_amount)::FLOAT AS avg_order_value,
        MAX(order_id)::VARCHAR AS last_order_id
      FROM raw.raw_orders

Models needing backfill (missing dates):
└── sales.daily_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan output showing non-breaking change with diff highlighting the new column]*

**Understanding the Output:**

- **Non-breaking**: Vulcan detected this as non-breaking (adding a column). Adding columns is safe. Existing queries still work.

- **Diff**: Shows exactly what changed. The green `+` indicates an added line. You can see exactly what you modified.

- **No downstream impact**: `sales.weekly_sales` isn't listed because it doesn't use this column yet. Downstream models don't need to be reprocessed.

Non-breaking changes don't cascade. You can add columns without forcing downstream models to reprocess.

### Step 4: Apply the Changes

Type `y` to apply the plan:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:05

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Plan application showing daily_sales being backfilled]*

---

## Making a Breaking Change

Breaking changes affect downstream models. Let's see how Vulcan handles this.

### Step 1: Add a Filter to Daily Sales

Edit `models/sales/daily_sales.sql` to add a WHERE clause:

```sql
SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  AVG(total_amount)::FLOAT AS avg_order_value,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
WHERE total_amount > 10  -- NEW FILTER: Only orders > $10
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql showing the WHERE clause filter]*

### Step 2: Create Plan

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales

Directly Modified: sales.daily_sales (Breaking)
└── Diff:
    @@ -26,6 +26,7 @@
      FROM raw.raw_orders
    + WHERE total_amount > 10
      GROUP BY order_date

└── Indirectly Modified Children:
    └── sales.weekly_sales (Indirect Breaking)

Models needing backfill (missing dates):
├── sales.daily_sales: 2025-01-01 - 2025-01-15
└── sales.weekly_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan output showing breaking change with downstream impact on weekly_sales]*

**Understanding Breaking Changes:**

- **Breaking**: Adding a WHERE clause filters data, making existing data invalid. Rows that should be filtered out might still be in the table.

- **Indirectly Modified**: `sales.weekly_sales` depends on `daily_sales`, so it's affected. It needs to be reprocessed with the new filtered data.

- **Cascading backfill**: Both models need to be reprocessed. Vulcan handles this automatically, processing upstream first.

Breaking changes are more expensive because they cascade. Make sure you need to make a breaking change before you do it.

---

## Evaluating a Model

The `evaluate` command tests models without materializing data. Use it for iteration and debugging. It shows what your model will produce without creating tables.

### Basic Evaluation

```bash
vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15
```

**Expected Output:**
```
order_date          total_orders  total_revenue  avg_order_value  last_order_id
2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142
```

*[Screenshot: Evaluate output showing single day results]*

### Evaluate Multiple Days

```bash
vulcan evaluate sales.daily_sales --start=2025-01-10 --end=2025-01-15
```

**Expected Output:**
```
order_date          total_orders  total_revenue  avg_order_value  last_order_id
2025-01-10 00:00:00           38         1120.25           29.48        ORD-00110
2025-01-11 00:00:00           45         1350.75           30.02        ORD-00111
2025-01-12 00:00:00           41         1225.50           29.89        ORD-00112
2025-01-13 00:00:00           39         1180.00           30.26        ORD-00113
2025-01-14 00:00:00           44         1320.50           30.01        ORD-00114
2025-01-15 00:00:00           42         1250.50           29.77        ORD-00142
```

*[Screenshot: Evaluate output showing multiple days of data]*

### Evaluate with Filters

Test your model logic with different conditions:

```bash
vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15 --where "total_amount > 50"
```

*[Screenshot: Evaluate command with WHERE clause filter]*

**Use Cases for Evaluate:**

- Test model logic before applying changes. Make sure your SQL does what you think it does.

- Debug query issues. See what's actually happening with your data.

- Verify data transformations. Check that aggregations, joins, etc. are working correctly.

- Check data quality. Spot issues before they make it to production.

- Iterate quickly without materialization costs. Test changes fast without waiting for full backfills.

---

## Reverting a Change

Vulcan lets you revert model changes using Virtual Updates. You can revert quickly without reprocessing all your data.

### Step 1: Revert the Change

Edit `models/sales/daily_sales.sql` to remove the WHERE clause we added:

```sql
SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  AVG(total_amount)::FLOAT AS avg_order_value,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
-- WHERE total_amount > 10  -- REMOVED FILTER
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql with WHERE clause removed/commented out]*

### Step 2: Apply Reverted Plan

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `dev` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales

Directly Modified: sales.daily_sales (Breaking)
└── Diff:
    @@ -26,7 +26,6 @@
      FROM raw.raw_orders
    - WHERE total_amount > 10
      GROUP BY order_date

Apply - Virtual Update [y/n]: y
```

*[Screenshot: Plan showing reverted change with diff]*

**Virtual Update:**

- No backfill required. Just updates references. Vulcan changes which physical table the view points to.

- Fast operation. Completes in seconds. Faster than a full backfill.

- Previous data remains available. The old data is still there. You're just not using it anymore.

Virtual updates work well for reverting changes. They're fast and don't require reprocessing data.

---

## Validating Models

Vulcan provides multiple ways to validate your models. Vulcan checks them automatically.

### Automatic Validation

Vulcan automatically validates models when you run `plan`:

1. **Unit Tests**: Run automatically to validate logic
2. **Audits**: Execute when data is loaded to tables
3. **Assertions**: Check data quality constraints

**Example Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------
```

*[Screenshot: Plan output showing tests passed]*

### Manual Validation Options

1. **Evaluate**: Test model output without materialization
   ```bash
   vulcan evaluate sales.daily_sales --start=2025-01-15 --end=2025-01-15
   ```

2. **Unit Tests**: Write tests in `tests/` directory
   ```bash
   vulcan test
   ```

3. **Plan Preview**: See changes before applying
   ```bash
   vulcan plan dev
   ```

*[Screenshot: Test execution showing all tests passing]*

---

## Deleting a Model

To remove a model from your project:

### Step 1: Delete Model File

```bash
rm models/sales/weekly_sales.sql
```

*[Screenshot: File explorer showing weekly_sales.sql deleted]*

### Step 2: Delete Associated Tests (if any)

```bash
rm tests/test_weekly_sales.yaml
```

### Step 3: Apply Deletion Plan

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 1 tests against postgres
----------------------------------------------------------------------

Differences from the `dev` environment:

Models:
└── Removed Models:
    └── sales.weekly_sales

Apply - Virtual Update [y/n]: y
```

*[Screenshot: Plan output showing weekly_sales as removed]*

Type `y` to apply the deletion.

**Expected Output:**
```
Virtually Updating 'dev' ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 0:00:00

The target environment has been updated successfully
Virtual Update executed successfully
```

*[Screenshot: Virtual update completing successfully]*

### Step 4: Apply to Production

```bash
vulcan plan
```

**Expected Output:**
```
Differences from the `prod` environment:

Models:
└── Removed Models:
    └── sales.weekly_sales

Apply - Virtual Update [y/n]: y
```

*[Screenshot: Production plan showing model removal]*

---

## Model Examples from Orders360

### Seed Model: Raw Orders

```sql
MODEL (
  name raw.raw_orders,
  kind SEED (
    path '../../seeds/raw_orders.csv'
  ),
  description 'Seed model loading raw order data from CSV file',
  columns (
    order_id VARCHAR,
    order_date DATE,
    customer_id VARCHAR,
    product_id VARCHAR,
    total_amount FLOAT
  ),
  column_descriptions (
    order_id = 'Unique identifier for each order',
    order_date = 'Date when the order was placed',
    customer_id = 'Reference to customer who placed the order',
    product_id = 'Reference to product that was ordered',
    total_amount = 'Total order amount in dollars'
  ),
  assertions (
    unique_values(columns := (order_id)),
    not_null(columns := (order_id, order_date, customer_id, product_id)),
    positive_values(column := total_amount)
  ),
  grain order_id
);
```

*[Screenshot: raw_orders.sql seed model file]*

### Transformation Model: Daily Sales

```sql
MODEL (
  name sales.daily_sales,
  kind FULL,
  cron '@daily',
  grain order_date,
  description 'Daily sales summary with order counts and revenue',
  column_descriptions (
    order_date = 'Date of the sales',
    total_orders = 'Total number of orders for the day',
    total_revenue = 'Total revenue for the day',
    last_order_id = 'Last order ID processed for the day'
  ),
  assertions (
    unique_values(columns := (order_date)),
    not_null(columns := (order_date, total_orders, total_revenue)),
    positive_values(column := total_orders),
    positive_values(column := total_revenue)
  )
);

SELECT
  CAST(order_date AS TIMESTAMP)::TIMESTAMP AS order_date,
  COUNT(order_id)::INTEGER AS total_orders,
  SUM(total_amount)::FLOAT AS total_revenue,
  MAX(order_id)::VARCHAR AS last_order_id
FROM raw.raw_orders
GROUP BY order_date
ORDER BY order_date
```

*[Screenshot: daily_sales.sql transformation model file]*

---

## Best Practices

Here are some tips to help you work effectively with models:

1. **Use descriptive names**: `sales.daily_sales` is clearer than `sales.ds`.

2. **Add column descriptions**: Document what each column represents. Helps others understand the data.

3. **Use assertions**: Validate data quality at the model level. Catch issues automatically.

4. **Test before applying**: Use `evaluate` to preview changes. Catch bugs before they hit production.

5. **Review plans carefully**: Check diffs and downstream impacts. Make sure you understand what will change.

6. **Use dev environments**: Test changes before production. Don't test in prod.

---

## Next Steps

- Learn about [Model Kinds](../components/model/model_kinds.md) for different model types

- Explore [Model Properties](../components/model/properties.md) for advanced configuration

- Read about [Plans](./plan.md) for applying model changes



# Plan

Source: https://tmdc-io.github.io/vulcan-book/guides/plan/

---

# Plan

This guide walks you through Vulcan's plan functionality with practical scenarios using the Orders360 example project. You'll learn how plans work, how to interpret them, and how to apply changes to your data warehouse.

## Plan Architecture

The following diagram illustrates how Vulcan's plan system works, showing the relationship between local project state, plans, model variants, physical tables, and environments:

```mermaid
graph TB
    subgraph "Local Project"
        LP[Local Project Files<br/>Your SQL/Python Models]
        M1[Model: daily_sales v1]
        M2[Model: weekly_sales v1]
    end

    subgraph "Plan Creation"
        PC[vulcan plan]
        COMP[Compare Local vs Environment]
        CAT[Categorize Changes<br/>Breaking / Non-breaking]
        PLAN[Plan Generated<br/>Ready for Review]
    end

    subgraph "Model Variants & Snapshots"
        MV1[Model Variant 1<br/>daily_sales__hash1]
        MV2[Model Variant 2<br/>daily_sales__hash2]
        MV3[Model Variant 3<br/>weekly_sales__hash1]
        SNAP1[Snapshot 1<br/>Fingerprint: hash1]
        SNAP2[Snapshot 2<br/>Fingerprint: hash2]
        SNAP3[Snapshot 3<br/>Fingerprint: hash3]
    end

    subgraph "Physical Layer"
        PT1[Physical Table 1<br/>db.vulcan__sales.daily_sales__hash1]
        PT2[Physical Table 2<br/>db.vulcan__sales.daily_sales__hash2]
        PT3[Physical Table 3<br/>db.vulcan__sales.weekly_sales__hash1]
    end

    subgraph "Virtual Layer"
        VL1[View: sales.daily_sales]
        VL2[View: sales.weekly_sales]
    end

    subgraph "Environments"
        PROD[Production Environment<br/>References Variant 1 & 3]
        DEV[Dev Environment<br/>References Variant 2 & 3]
    end

    subgraph "Backfill Process"
        BF[Backfill Execution]
        INC[Incremental Backfill]
        FULL[Full Refresh]
    end

    LP -->|"to"| M1
    LP -->|"to"| M2
    M1 -->|"to"| PC
    M2 -->|"to"| PC
    PC -->|"to"| COMP
    COMP -->|"to"| CAT
    CAT -->|"to"| PLAN
    PLAN -->|"to"| MV1
    PLAN -->|"to"| MV2
    PLAN -->|"to"| MV3
    
    MV1 -->|"to"| SNAP1
    MV2 -->|"to"| SNAP2
    MV3 -->|"to"| SNAP3
    
    SNAP1 -->|"to"| PT1
    SNAP2 -->|"to"| PT2
    SNAP3 -->|"to"| PT3
    
    PT1 -->|"to"| VL1
    PT2 -->|"to"| VL1
    PT3 -->|"to"| VL2
    
    PROD -->|"to"| MV1
    PROD -->|"to"| MV3
    DEV -->|"to"| MV2
    DEV -->|"to"| MV3
    
    PLAN -->|"to"| BF
    BF -->|"to"| INC
    BF -->|"to"| FULL
    INC -->|"to"| PT1
    INC -->|"to"| PT2
    FULL -->|"to"| PT3

    style LP fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style PC fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style PLAN fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000
    style PROD fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style DEV fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000
    style BF fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style MV1 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style MV2 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style MV3 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    style PT1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    style PT2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    style PT3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
```

### Key Concepts Illustrated

1. **Local Project**: Your model files define the desired state
2. **Plan Creation**: Vulcan compares local state to environment state and generates a plan
3. **Model Variants**: Each model change creates a new variant with a unique fingerprint
4. **Physical Tables**: Each variant gets its own physical table in the warehouse
5. **Virtual Layer**: Views point to the appropriate physical tables based on environment
6. **Environments**: Collections of references to model variants (not the data itself)
7. **Backfill**: Process of populating physical tables with data

## What is a Plan?

A **plan** is Vulcan's way of comparing your local project state with a target environment and determining what changes need to be applied. Before any model changes take effect, Vulcan creates a plan that shows:

- **Added models** - New models to be created

- **Removed models** - Models to be deleted

- **Modified models** - Changes to existing models (with diffs)

- **Indirectly affected models** - Downstream models that depend on changed models

- **Backfill requirements** - Date ranges that need data reprocessing

Plans allow you to review and verify all changes before they're applied to your data warehouse.

## Prerequisites

Before following this guide, ensure you have:

1. **Orders360 example project** set up (see [Examples Overview](../../examples/overview.md))
2. **Docker environment** running (see [Docker Quickstart](./get-started/docker.md))
3. **Vulcan CLI** accessible via `vulcan` command or `vulcan.bat` (Windows)

## Scenario 1: First Plan - Initializing Production

When you first create a project, you need to initialize the production environment. This scenario shows how Vulcan detects all models and creates the initial plan.

### Step 1: Check Project Status

First, verify your project setup:

```bash
vulcan info
```

**Expected Output:**
```
Connection: Connected
Models: 4
Macros: 0
Tests: 2
...
```

*[Screenshot: `vulcan info` output showing project status]*

### Step 2: Create Your First Plan

Run the plan command to see what Vulcan will create:

```bash
vulcan plan
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

`prod` environment will be initialized

Models:
└── Added:
    ├── raw.raw_customers
    ├── raw.raw_orders
    ├── raw.raw_products
    └── sales.daily_sales

Models needing backfill (missing dates):
├── raw.raw_customers: 2025-01-01 - 2025-01-15
├── raw.raw_orders: 2025-01-01 - 2025-01-15
├── raw.raw_products: 2025-01-01 - 2025-01-15
└── sales.daily_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: First plan output showing all models to be added]*

### Understanding the Output

- **`prod` environment will be initialized**: This is your first plan, so Vulcan is creating the production environment

- **Added models**: All 4 models are new and will be created

- **Backfill dates**: Each model needs data from its start date (2025-01-01) to the current date

### Step 3: Apply the Plan

Type `y` and press Enter to apply the plan:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/4] raw.raw_customers          [full refresh]                   2.3s
[2/4] raw.raw_orders             [full refresh]                   1.8s
[3/4] raw.raw_products           [full refresh]                   0.5s
[4/4] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 4.2s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 4/4 • 0:00:09

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Plan application progress showing all models being backfilled]*

### What Happened?

1. **Seed models** (`raw.*`) were fully refreshed - all data was reloaded
2. **Daily sales model** was incrementally backfilled - data was inserted for each day from start date to today
3. **Physical tables** were created in your warehouse
4. **Virtual layer** was updated to point to the new tables

---

## Scenario 2: Adding a New Model

After your initial setup, you'll add new models. This scenario shows how Vulcan detects new models and determines their dependencies.

### Step 1: Add a New Model

Create a new model file `models/sales/weekly_sales.sql`:

```sql
MODEL (
  name sales.weekly_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 1
  ),
  start '2025-01-01',
  cron '@weekly',
  grain [order_date]
);

SELECT
  DATE_TRUNC('week', order_date) AS order_date,
  COUNT(DISTINCT order_id) AS total_orders,
  SUM(order_amount) AS total_revenue
FROM sales.daily_sales
WHERE order_date BETWEEN @start_ds AND @end_ds
GROUP BY DATE_TRUNC('week', order_date)
```

### Step 2: Create Plan for New Model

Run the plan command:

```bash
vulcan plan
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Added:
    └── sales.weekly_sales

Models needing backfill (missing dates):
└── sales.weekly_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan showing new weekly_sales model]*

### Step 3: Review Dependencies

Notice that:

- Only the new model appears in the plan

- The upstream model (`sales.daily_sales`) is not affected because adding a downstream model doesn't change upstream data

- Backfill is needed from the model's start date

### Step 4: Apply the Plan

Type `y` to apply:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/1] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.1s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:03

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Weekly sales model being backfilled]*

---

## Scenario 3: Modifying a Model - Non-Breaking Change

Non-breaking changes don't affect existing data validity. Adding a new column is a common non-breaking change.

### Step 1: Modify the Model

Edit `models/sales/daily_sales.sql` to add a new column:

```sql
MODEL (
  name sales.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 1
  ),
  start '2025-01-01',
  cron '@daily',
  grain [order_date]
);

SELECT
  order_date,
  COUNT(DISTINCT order_id) AS total_orders,
  SUM(order_amount) AS total_revenue,
  AVG(order_amount) AS avg_order_value,  -- NEW COLUMN
  MAX(order_id) AS last_order_id
FROM raw.raw_orders
WHERE order_date BETWEEN @start_ds AND @end_ds
GROUP BY order_date
```

### Step 2: Create Plan

```bash
vulcan plan
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Directly Modified:
    └── sales.daily_sales

Directly Modified: sales.daily_sales (Non-breaking)
└── Diff:
    @@ -5,6 +5,7 @@
      SELECT
        order_date,
        COUNT(DISTINCT order_id) AS total_orders,
        SUM(order_amount) AS total_revenue,
    +   AVG(order_amount) AS avg_order_value,
        MAX(order_id) AS last_order_id
      FROM raw.raw_orders
      WHERE order_date BETWEEN @start_ds AND @end_ds

Models needing backfill (missing dates):
└── sales.daily_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan showing non-breaking change with diff]*

### Understanding Non-Breaking Changes

- **Directly Modified**: The model you changed

- **Non-breaking**: Vulcan automatically detected this as non-breaking because:

  - You added a new column

  - Existing columns weren't modified

  - Downstream models aren't affected (they don't use this column yet)

- **Backfill required**: The modified model needs to be backfilled to populate the new column

### Step 3: Check Downstream Models

Notice that `sales.weekly_sales` (which depends on `daily_sales`) is **not** listed. This is because:

- The change is non-breaking

- Downstream models don't need to be reprocessed

- They'll automatically see the new column once `daily_sales` is backfilled

### Step 4: Apply the Plan

Type `y` to apply:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/1] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.2s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:05

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Daily sales model being backfilled with new column]*

---

## Scenario 4: Modifying a Model - Breaking Change

Breaking changes invalidate existing data and require downstream models to be reprocessed. Adding a WHERE clause is a common breaking change.

### Step 1: Modify the Model with a Filter

Edit `models/sales/daily_sales.sql` to add a WHERE clause:

```sql
MODEL (
  name sales.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 1
  ),
  start '2025-01-01',
  cron '@daily',
  grain [order_date]
);

SELECT
  order_date,
  COUNT(DISTINCT order_id) AS total_orders,
  SUM(order_amount) AS total_revenue,
  AVG(order_amount) AS avg_order_value,
  MAX(order_id) AS last_order_id
FROM raw.raw_orders
WHERE order_date BETWEEN @start_ds AND @end_ds
  AND order_amount > 10  -- NEW FILTER: Only orders > $10
GROUP BY order_date
```

### Step 2: Create Plan

```bash
vulcan plan
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales.daily_sales
└── Indirectly Modified:
    └── sales.weekly_sales

Directly Modified: sales.daily_sales (Breaking)
└── Diff:
    @@ -8,6 +8,7 @@
      FROM raw.raw_orders
      WHERE order_date BETWEEN @start_ds AND @end_ds
    +   AND order_amount > 10
      GROUP BY order_date

└── Indirectly Modified Children:
    └── sales.weekly_sales (Indirect Breaking)

Models needing backfill (missing dates):
├── sales.daily_sales: 2025-01-01 - 2025-01-15
└── sales.weekly_sales: 2025-01-01 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Plan showing breaking change with downstream impact]*

### Understanding Breaking Changes

- **Directly Modified**: `sales.daily_sales` - the model you changed

- **Breaking**: Vulcan detected this as breaking because:

  - You added a WHERE clause that filters data

  - Existing data may now be invalid (rows that should be filtered out)

- **Indirectly Modified**: `sales.weekly_sales` - downstream model affected

- **Cascading backfill**: Both models need to be reprocessed

### Step 3: Apply the Plan

Type `y` to apply:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/2] sales.daily_sales          [insert 2025-01-01 - 2025-01-15] 5.1s
[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 3.8s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:09

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Both models being backfilled due to breaking change]*

### What Happened?

1. **Daily sales** was backfilled first (upstream)
2. **Weekly sales** was backfilled second (downstream), using the updated daily sales data
3. Both models now reflect the filtered data (only orders > $10)

---

## Scenario 5: Creating a Development Environment

Development environments let you test changes without affecting production. This scenario shows how to create and use a dev environment.

### Step 1: Revert Your Changes

First, revert the breaking change from Scenario 4 to restore production:

```bash
# Revert daily_sales.sql to remove the WHERE clause filter
# (Edit the file to remove: AND order_amount > 10)
```

### Step 2: Apply Reverted Plan to Production

```bash
vulcan plan
# Type 'y' to apply
```

### Step 3: Create Development Environment

Now create a dev environment with your breaking change:

```bash
vulcan plan dev
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

New environment `dev` will be created from `prod`

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   └── sales__dev.daily_sales
└── Indirectly Modified:
    └── sales__dev.weekly_sales

Directly Modified: sales__dev.daily_sales (Breaking)
└── Diff:
    @@ -8,6 +8,7 @@
      FROM raw.raw_orders
      WHERE order_date BETWEEN @start_ds AND @end_ds
    +   AND order_amount > 10
      GROUP BY order_date

└── Indirectly Modified Children:
    └── sales__dev.weekly_sales (Indirect Breaking)

Enter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:
```

*[Screenshot: Creating dev environment with date prompt]*

### Step 4: Specify Date Range

For faster development, backfill only recent data:

```
Enter the backfill start date: 2025-01-10
```

**Expected Output:**
```
Enter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2025-01-15 00:00:00':
```

Press Enter to use the default end date (today).

**Expected Output:**
```
Models needing backfill (missing dates):
├── sales__dev.daily_sales: 2025-01-10 - 2025-01-15
└── sales__dev.weekly_sales: 2025-01-10 - 2025-01-15

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Dev environment plan with limited date range]*

### Step 5: Apply Dev Plan

Type `y` to apply:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/2] sales__dev.daily_sales     [insert 2025-01-10 - 2025-01-15] 2.1s
[2/2] sales__dev.weekly_sales    [insert 2025-01-13 - 2025-01-13] 1.5s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:04

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Dev environment models being backfilled]*

### Understanding Dev Environments

- **Isolated namespace**: Models are prefixed with `__dev` (e.g., `sales__dev.daily_sales`)

- **Separate tables**: Dev environment has its own physical tables

- **Limited backfill**: Only recent data was processed (2025-01-10 to 2025-01-15)

- **Production unaffected**: Production data remains unchanged

### Step 6: Query Dev Environment

You can query the dev environment to verify changes:

```bash
vulcan fetchdf "SELECT * FROM sales__dev.daily_sales LIMIT 5"
```

Compare with production:

```bash
vulcan fetchdf "SELECT * FROM sales.daily_sales LIMIT 5"
```

*[Screenshot: Comparing dev vs prod query results]*

---

## Scenario 6: Forward-Only Plans

Forward-only plans reuse existing tables instead of creating new ones, avoiding backfill costs. This is useful for expensive models.

### Step 1: Modify Model for Forward-Only

Edit `models/sales/daily_sales.sql` to add a comment (minimal change):

```sql
MODEL (
  name sales.daily_sales,
  kind INCREMENTAL_BY_TIME_RANGE (
    time_column order_date,
    batch_size 1
  ),
  start '2025-01-01',
  cron '@daily',
  grain [order_date]
);

-- Updated: Added comment for documentation
SELECT
  order_date,
  COUNT(DISTINCT order_id) AS total_orders,
  SUM(order_amount) AS total_revenue,
  AVG(order_amount) AS avg_order_value,
  MAX(order_id) AS last_order_id
FROM raw.raw_orders
WHERE order_date BETWEEN @start_ds AND @end_ds
GROUP BY order_date
```

### Step 2: Create Forward-Only Plan

```bash
vulcan plan --forward-only
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Differences from the `prod` environment:

Models:
└── Directly Modified:
    └── sales.daily_sales

Directly Modified: sales.daily_sales (Forward-only)
└── Diff:
    @@ -1,6 +1,7 @@
    MODEL (
      name sales.daily_sales,
    ...
    +-- Updated: Added comment for documentation
      SELECT
        order_date,

Models needing backfill (missing dates):
└── sales.daily_sales: 2025-01-15 - 2025-01-15 (preview)

Apply - Virtual Update [y/n]:
```

*[Screenshot: Forward-only plan showing preview backfill]*

### Understanding Forward-Only Plans

- **Forward-only category**: Automatically assigned

- **Preview backfill**: Only processes the latest interval for preview

- **Virtual Update**: No new physical table created

- **Reuses existing table**: Production will use the same physical table

### Step 3: Apply Forward-Only Plan

Type `y` to apply:

```
Apply - Virtual Update [y/n]: y
```

**Expected Output:**
```
[1/1] sales.daily_sales          [insert 2025-01-15 - 2025-01-15] 0.8s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:01

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Forward-only plan applied with minimal backfill]*

### Benefits of Forward-Only

- **Fast**: Only processes latest interval

- **Cost-effective**: No full backfill required

- **Safe for production**: Reuses existing tables

---

## Scenario 7: Restatement Plans

Restatement plans reprocess existing data without changing model definitions. Useful for fixing data issues or reprocessing after upstream corrections.

### Step 1: Check Current State

First, verify what data exists:

```bash
vulcan fetchdf "SELECT MIN(order_date), MAX(order_date), COUNT(*) FROM sales.daily_sales"
```

*[Screenshot: Current data range in daily_sales]*

### Step 2: Create Restatement Plan

Restate the `daily_sales` model for a specific date range:

```bash
vulcan plan --restate-model "sales.daily_sales" --start "2025-01-10" --end "2025-01-12"
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Restatement plan for `prod` environment

Models to restate:
└── sales.daily_sales

Models needing backfill (missing dates):
├── sales.daily_sales: 2025-01-10 - 2025-01-12
└── sales.weekly_sales: 2025-01-06 - 2025-01-13

Apply - Backfill Tables [y/n]:
```

*[Screenshot: Restatement plan showing date range]*

### Understanding Restatement

- **No model changes**: Model definition unchanged

- **Cascading restatement**: Downstream models (`weekly_sales`) also need restatement

- **Date range**: Only specified dates will be reprocessed

### Step 3: Apply Restatement Plan

Type `y` to apply:

```
Apply - Backfill Tables [y/n]: y
```

**Expected Output:**
```
[1/2] sales.daily_sales          [insert 2025-01-10 - 2025-01-12] 1.2s
[2/2] sales.weekly_sales         [insert 2025-01-06 - 2025-01-13] 2.3s

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:04

✔ Model batches executed
✔ Plan applied successfully
```

*[Screenshot: Restatement plan execution]*

### Use Cases for Restatement

- **Upstream data correction**: Raw data was fixed, need to reprocess

- **Bug fixes**: Found an issue in data processing logic (after fixing the model)

- **Data refresh**: Need to refresh specific date ranges

---

## Scenario 8: Plan with Explain Flag

The `--explain` flag provides detailed information about what a plan will do.

### Step 1: Create Plan with Explain

```bash
vulcan plan --explain
```

**Expected Output:**
```
======================================================================
Successfully Ran 2 tests against postgres
----------------------------------------------------------------------

Explained plan
├── Validate SQL and create physical layer tables and views if they do not exist
│   ├── raw.raw_customers -> db.vulcan__raw.vulcan__raw_customers__1234567890
│   │   ├── Dry run model query without inserting results
│   │   └── Create table if it doesn't exist
│   └── sales.daily_sales -> db.vulcan__sales.vulcan__sales__daily_sales__9876543210
│       ├── Dry run model query without inserting results
│       └── Create table if it doesn't exist
├── Backfill models by running their queries and run standalone audits
│   ├── raw.raw_customers -> db.vulcan__raw.vulcan__raw_customers__1234567890
│   │   └── Fully refresh table
│   └── sales.daily_sales -> db.vulcan__sales.vulcan__sales__daily_sales__9876543210
│       ├── Insert 2025-01-01 - 2025-01-15
│       └── Run 'assert_positive_order_ids' audit
└── Update the virtual layer for environment 'prod'
    └── Create or update views in the virtual layer to point at new physical tables
        ├── raw.raw_customers -> db.vulcan__raw.vulcan__raw_customers__1234567890
        └── sales.daily_sales -> db.vulcan__sales.vulcan__sales__daily_sales__9876543210
```

*[Screenshot: Explained plan output showing detailed actions]*

### Understanding Explained Plans

The explain output shows three main phases:

1. **Validation & Table Creation**: 
   - Dry runs each model query

   - Creates physical tables if needed

   - Shows mapping: `model_name -> physical_table_name`

2. **Backfill**:
   - Shows which models will be backfilled

   - Indicates backfill type (full refresh vs incremental)

   - Lists audits that will run

3. **Virtual Layer Update**:
   - Shows how views will be created/updated

   - Maps virtual layer names to physical tables

---

## Common Plan Scenarios Summary

| Scenario | Command | When to Use |
|----------|---------|-------------|
| **First Plan** | `vulcan plan` | Initializing production environment |
| **Add Model** | `vulcan plan` | Adding new models to existing project |
| **Non-Breaking Change** | `vulcan plan` | Adding columns, comments, or non-functional changes |
| **Breaking Change** | `vulcan plan` | Modifying logic that affects downstream models |
| **Dev Environment** | `vulcan plan dev` | Testing changes without affecting production |
| **Forward-Only** | `vulcan plan --forward-only` | Expensive models, avoiding backfill costs |
| **Restatement** | `vulcan plan --restate-model "model"` | Reprocessing existing data |
| **Explain** | `vulcan plan --explain` | Understanding detailed plan actions |

---

## Best Practices

1. **Always review plans** before applying them
2. **Use dev environments** for testing breaking changes
3. **Use forward-only** for expensive models when appropriate
4. **Check downstream impact** - breaking changes cascade
5. **Use `--explain`** when unsure about plan actions
6. **Restate carefully** - it reprocesses existing data

---

## Next Steps

- Learn about [Plans Concepts](../references/plans.md) for deeper understanding

- Explore [Environments](../references/environments.md) for managing multiple environments

- Read about [Model Kinds](../components/model/model_kinds.md) to understand different model types

- Set up [Notifications](../configurations/options/notifications.md) to monitor plan execution




# Plan

Source: https://tmdc-io.github.io/vulcan-book/guides/plan_guide/

---

# Plan

`vulcan plan` shows you what changed, what will be recomputed, and what will be promoted into an environment's virtual layer before changes take effect.

Vulcan plan tracks changes to:

- **Semantics and metrics** (keeps business definitions consistent)
- **Assertions, checks, and profiles** (keeps data quality visible and predictable)
- **Model SQL and Python** (transformation logic)

## Quick start

Create a plan:

```bash
vulcan plan [environment name]
```

If you omit the environment name, Vulcan plans against `prod`.

## What you'll see in a plan

A plan summary includes:

- **What changed**: added/removed/direct/indirect/metadata updates across models and related definitions
- **Impact classification**: breaking vs non-breaking (auto-identified)
- **Backfill window**: which models need intervals executed
- **Apply stages**: executing model batches, updating virtual layer, running non-blocking quality signals

For a walkthrough with screenshots and CLI snippets, see [Plan](./plan.md). For details on environments, snapshots, forward-only plans, and restatements, see [Plans](../references/plans.md).

## How plan works

Vulcan follows this flow:

1. **Diff scope**: Compare local project state to the target environment (models, standalone assertions, semantics, metrics, checks, metadata).

2. **Plan type**: Determine whether this is a backfill plan or a virtual update (and whether forward-only or restatement options apply).

3. **Categorize impact**: Auto-identify breaking vs non-breaking and propagate impact to indirectly impacted objects.

4. **Compute intervals**: Compute the time ranges that must be executed.

5. **Apply**:
   - Create model variants (snapshots)
   - Execute model batches (backfill) if needed
   - Update the virtual layer (promote)
   - Run non-blocking quality signals (checks and profiles)

## Change types

Vulcan reports changes using consistent categories. You'll see these for models, and often for semantics, metrics, checks, and assertions.

- **Added**: exists locally, not in the environment (new object will be created/registered).
- **Removed**: exists in the environment, removed from local project (will be removed from the environment).
- **Directly modified**: you edited the object itself (model SQL/Python, semantics YAML, check YAML, Assertion SQL).
- **Indirectly impacted**: You didn't edit the object, but something it depends on changed. Common for downstream models, and for semantics, metrics, checks that reference a changed model or column.
- **Metadata-only**: description/tags/ownership/config changes that don’t require historical recomputation.

!!! example

    You rename a column in `sales.daily_sales`.

    - `sales.daily_sales` is **directly modified**
    - Downstream models are **indirectly impacted**
    - A metric or check referencing the renamed column is **indirectly impacted**. It may fail validation until you update it.

## Change categories (breaking vs non-breaking)

Vulcan identifies breaking vs non-breaking automatically during plan creation (and may prompt only in ambiguous cases).

### Breaking

Breaking means the change can alter results in a way that downstream models and consumers must reflect.

- **For models**: Backfill cascades downstream. More expensive, but safest.
- **For semantics, metrics, checks**: Definitions may become invalid or meaningfully change for consumers.

!!! example

    Adding/modifying a filter changes the set of rows produced:

    ```sql
    WHERE order_date BETWEEN @start_ds AND @end_ds
      AND order_amount > 10
    ```

### Non-breaking

Non-breaking means the change is additive or non-functional in a way that doesn't invalidate downstream dependencies.

- **For models**: Only the directly modified model needs backfill.
- **For semantics, metrics, checks**: Additive updates (new metric, new check, updated description) don't break existing consumers.

!!! example

    Adding a new column is usually non-breaking:

    ```sql
    SELECT
      order_date,
      SUM(order_amount) AS total_revenue,
      AVG(order_amount) AS avg_order_value
    FROM raw.raw_orders
    GROUP BY order_date
    ```

## Snapshots (model variants) and fingerprinting

Whenever a **model definition** changes, Vulcan creates a **snapshot**: a record of that model at a point in time. A snapshot contains everything needed to evaluate and render the model query, including macro definitions and global variables at the time the snapshot was created, and it tracks which time intervals have data.

Snapshots have unique fingerprints derived from their models. Vulcan uses fingerprints to decide whether an existing physical table can be reused or whether the model must be backfilled. Vulcan understands SQL, so superficial edits like formatting won't generate a new fingerprint.

## Physical tables, virtual layer, and environments

How these components work together:

- **Physical layer**: Versioned physical tables hold the data for a specific snapshot.
- **Virtual layer**: Environment views point to the correct physical tables.
- **Environment**: A set of references to model snapshots (and therefore to physical tables).

![Virtual vs physical layers: how snapshots change when you update a model](../assets/images/vulcan_plan_snapshot_flow.png)



## Backfill vs virtual update

- **Backfill**: Vulcan executes intervals to populate the physical tables for new snapshots.
- **Virtual update**: Vulcan only swaps references in the virtual layer (no new execution needed).

## Start/end dates (non-prod) and limitations

In non-prod environments, you can control the backfill window:

```bash
vulcan plan dev --start "2024-01-01" --end "2024-01-05"
```

Some model kinds are inherently non-idempotent (for example `INCREMENTAL_BY_UNIQUE_KEY`, `INCREMENTAL_BY_PARTITION`, and SCD variants). In those cases, Vulcan may compute a **preview** for a limited range that can’t be reused when deploying to production.

If your specified window is smaller than a model’s interval size, the model may be skipped; use `--min-intervals` to force at least N intervals.

## Restatement plans (`--restate-model`)

Restatement is how you reprocess **existing data** for a time range even when the model definition hasn’t changed (for example, upstream data was corrected or you want to re-run a subset of history).

```bash
vulcan plan --restate-model "db.model_a"
```

!!! warning "No local changes allowed"

    Restatement plans ignore local file changes. They can only restate model versions already present in the target environment.

How it works:

- **Cascading backfill**: selected models and downstream models are reprocessed.
- **Selectors**: Select by name, wildcard, or tag. See [Model selection](./model_selection.md).
- **External models**: restating an external model triggers downstream backfills (the external model itself is metadata-only).
- **Disable restatement**: set `disable_restatement: true` to prevent restatement for a model.

Prod vs dev behavior differs. Restating `prod` can clear affected intervals in other environments' state to prevent stale data being promoted later. For details, see [Plans](../references/plans.md).




# Run and Scheduling

Source: https://tmdc-io.github.io/vulcan-book/guides/run_and_scheduling/

---

# Run and Scheduling

This guide covers Vulcan's run functionality and scheduling strategies. You'll learn how `vulcan run` processes new data intervals and how to automate it for production.

The `run` command is different from `plan`, it's for regular scheduled execution, not for applying changes. Once you understand the difference, you'll know when to use each one.

## Run and Scheduler Architecture

The following diagram illustrates how Vulcan's run system works with cron-based scheduling:

```mermaid
graph TB
    subgraph "Scheduler Triggers"
        CRON[Cron Job / CI/CD<br/>Runs periodically]
        MANUAL[Manual Execution<br/>vulcan run]
    end

    subgraph "Run Process"
        START[vulcan run<br/>Command starts]
        CHECK[Check for missing intervals<br/>Compare with state]
        CRON_CHECK[Check cron schedules<br/>Which models are due?]
        FILTER[Filter models<br/>Only process due intervals]
    end

    subgraph "Model Execution"
        M1[sales.daily_sales<br/>cron: @daily<br/>Due: Yes]
        M2[sales.weekly_sales<br/>cron: @weekly<br/>Due: No]
        M3[sales.monthly_sales<br/>cron: @monthly<br/>Due: No]
    end

    subgraph "State Management"
        STATE[State Database<br/>Tracks processed intervals]
        UPDATE[Update State<br/>Mark intervals as processed]
    end

    subgraph "Execution Flow"
        EXEC1[Execute daily_sales<br/>Process missing intervals]
        EXEC2[Skip weekly_sales<br/>Not due yet]
        EXEC3[Skip monthly_sales<br/>Not due yet]
    end

    subgraph "Results"
        SUCCESS[Run Complete<br/>Intervals processed]
        LOG[Log Results<br/>Execution summary]
    end

    CRON -->|"Scheduled"| START
    MANUAL -->|"Triggered"| START
    START -->|"to"| CHECK
    CHECK -->|"to"| CRON_CHECK
    CRON_CHECK -->|"to"| FILTER
    FILTER -->|"Due"| M1
    FILTER -->|"Not due"| M2
    FILTER -->|"Not due"| M3
    
    M1 -->|"to"| EXEC1
    M2 -->|"to"| EXEC2
    M3 -->|"to"| EXEC3
    
    EXEC1 -->|"to"| STATE
    EXEC2 -.->|"Skip"| STATE
    EXEC3 -.->|"Skip"| STATE
    
    STATE -->|"to"| UPDATE
    UPDATE -->|"to"| SUCCESS
    SUCCESS -->|"to"| LOG

    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style START fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style CRON_CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style M1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style M2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style M3 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
```

### Key Concepts Illustrated

1. **Scheduler Triggers**: Run can be triggered by cron jobs, CI/CD pipelines, or manually
2. **Interval Detection**: Vulcan checks for missing intervals by comparing current state with model schedules
3. **Cron-Based Filtering**: Only models whose cron schedules indicate they're due are executed
4. **State Tracking**: Processed intervals are tracked in the state database
5. **Efficient Execution**: Models not due are skipped, saving computational resources

---

## Cron Schedule Flow

The following diagram shows how different cron schedules determine model execution:

```mermaid
gantt
    title Model Execution Timeline (Example: Hourly, Daily, Weekly)
    dateFormat YYYY-MM-DD HH:mm
    axisFormat %H:%M

    section Hourly Model
    Run every hour    :active, hourly1, 2025-01-20 00:00, 1h
    Run every hour    :active, hourly2, 2025-01-20 01:00, 1h
    Run every hour    :active, hourly3, 2025-01-20 02:00, 1h
    Run every hour    :active, hourly4, 2025-01-20 03:00, 1h

    section Daily Model
    Run once daily    :active, daily1, 2025-01-20 00:00, 24h

    section Weekly Model
    Run once weekly   :active, weekly1, 2025-01-20 00:00, 168h
```

**Visual Explanation**: 
- **Hourly models** run every hour when `vulcan run` executes

- **Daily models** run once per day (at the scheduled time)

- **Weekly models** run once per week (at the scheduled time)

---

## Understanding Run vs Plan

| Aspect | `vulcan plan` | `vulcan run` |
|--------|---------------|--------------|
| **Purpose** | Apply model changes to environment | Execute existing models on schedule |
| **When to Use** | When models are modified/added/removed | When no changes, just process new data |
| **Change Detection** | Compares local files vs environment | No file comparison needed |
| **Backfill** | Backfills based on changes | Processes missing intervals only |
| **Cron Schedule** | Not used (processes all affected dates) | Uses model's cron to determine what runs |
| **User Interaction** | Prompts for change categorization | Runs automatically |
| **Output** | Shows diffs and change summary | Shows execution progress |

**Key Insight**: Use `plan` when you've changed code. Use `run` for regular scheduled execution. 

Think of it this way: `plan` is for deploying changes, `run` is for processing new data. They serve different purposes!

---

## How Run Works

The `vulcan run` command processes missing data intervals for models that haven't changed:

```mermaid
flowchart TD
    START[vulcan run<br/>Command starts] --> CHECK{Check model<br/>definitions}
    
    CHECK -->|"Changed"| ERROR[Error: Use 'vulcan plan'<br/>to apply changes first]
    CHECK -->|"No changes"| STATE[Query state database<br/>Get processed intervals]
    
    STATE --> CRON[Check cron schedules<br/>Which models are due?]
    
    CRON --> FILTER{Filter models<br/>by cron schedule}
    
    FILTER -->|"Due"| EXEC1[Execute Model 1<br/>Process missing intervals]
    FILTER -->|"Due"| EXEC2[Execute Model 2<br/>Process missing intervals]
    FILTER -->|"Not due"| SKIP1[Skip Model 3<br/>Not due yet]
    FILTER -->|"Not due"| SKIP2[Skip Model 4<br/>Not due yet]
    
    EXEC1 --> UPDATE[Update state database<br/>Mark intervals as processed]
    EXEC2 --> UPDATE
    SKIP1 -.->|"Skip"| UPDATE
    SKIP2 -.->|"Skip"| UPDATE
    
    UPDATE --> SUCCESS[Run complete<br/>Summary output]
    
    ERROR --> END[Exit with error]
    SUCCESS --> END

    style START fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style CHECK fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style ERROR fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style CRON fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style EXEC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style EXEC2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style SKIP1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style SKIP2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style SUCCESS fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
```

**Process Steps:**

1. **No Model Changes**: Assumes no model definitions have changed - if they have, you'll get an error telling you to use `plan` first
2. **Cron-Based Execution**: Each model's `cron` parameter determines if it should run - daily models run daily, weekly models run weekly, etc.
3. **Missing Intervals**: Only processes intervals that haven't been processed yet - efficient!
4. **Automatic**: No prompts or user interaction required. Works well for automation.

The `run` command works well for scheduled execution. It's fast, automatic, and only processes what's needed.

!!! tip "Interactive Diagrams"
    All diagrams in this guide are interactive! Double-click any diagram to zoom in and explore details. Use drag to pan, arrow keys to navigate, or the zoom controls.

---

## Scenario 1: First Run - Processing New Data

After applying your first plan, use `run` to process new data as it arrives.

```bash
vulcan run
```

**Expected Output:**
```
======================================================================
Checking for missing intervals...
----------------------------------------------------------------------

Models to execute:
└── sales.daily_sales: 2025-01-16 (1 interval)

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:02

[1/1] sales.daily_sales          [insert 2025-01-16 - 2025-01-16]   2.1s

✔ All model batches executed successfully
```

*[Screenshot: First run output showing new interval processing]*

**What Happened?**
- `sales.daily_sales` has `cron: '@daily'`, so it runs daily - Vulcan checks if enough time has passed

- Yesterday's plan processed up to 2025-01-15 - that's what's already done

- Today (2025-01-16) is a new interval that needs processing - this is what's missing

- `run` automatically processes this missing interval - no prompts, just works

This is the beauty of `run`, it automatically figures out what needs processing and does it. Set it up once, and it keeps running!

---

## Scenario 2: Cron-Based Execution

Different models can have different `cron` schedules. `run` respects each model's schedule.

### Daily Model Execution

```bash
vulcan run
```

**Expected Output (Day 2):**
```
Models to execute:
└── sales.daily_sales: 2025-01-17 (1 interval)
```

*[Screenshot: Daily run showing only daily model executed]*

### Weekly Model Execution

After 7 days, both daily and weekly models run:

```bash
vulcan run
```

**Expected Output:**
```
Models to execute:
├── sales.daily_sales: 2025-01-18 - 2025-01-24 (7 intervals)
└── sales.weekly_sales: 2025-01-20 - 2025-01-20 (1 interval)

Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 2/2 • 0:00:08

[1/2] sales.daily_sales          [insert 2025-01-18 - 2025-01-24]   5.2s
[2/2] sales.weekly_sales         [insert 2025-01-20 - 2025-01-20]   2.8s

✔ All model batches executed successfully
```

*[Screenshot: Weekly run showing both daily and weekly models]*

**Understanding Cron Schedules:**

- **Daily model (`@daily`)**: Processes missing daily intervals - runs every day when `run` executes

- **Weekly model (`@weekly`)**: Only processes when 7 days have elapsed - skips if not enough time has passed

- **Efficient**: Each model only processes what's due based on its schedule - no wasted compute

This is why cron schedules are important. They tell Vulcan when each model should run, so you don't process things unnecessarily.

---

## Scenario 3: Run with No Missing Intervals

When all intervals are up to date, `run` skips execution:

```bash
vulcan run
```

**Expected Output:**
```
======================================================================
Checking for missing intervals...
----------------------------------------------------------------------

No models to execute. All intervals are up to date.

✔ Run completed successfully
```

*[Screenshot: Run output showing no models to execute]*

This is normal when running frequently. Nothing to process means everything is up to date. Your automation is working and keeping things current.

---

## Scenario 4: Run After Model Changes (Error Case)

If models have changed, Vulcan detects this and requires a plan first:

```bash
vulcan run
```

**Expected Output:**
```
======================================================================
Error: Model definitions have changed. Use 'vulcan plan' to apply changes first.

Changed models:
└── sales.daily_sales

Please run 'vulcan plan' to apply these changes before using 'vulcan run'.
```

*[Screenshot: Error message when trying to run with model changes]*

**Workflow**: Always `plan` first to apply changes, then `run` for scheduled execution. 

This is the key workflow: use `plan` when you've changed code, then use `run` for regular data processing. Don't mix them up!

---

## Scheduling for Production

The `vulcan run` command doesn't run continuously - it executes once and exits. For production, you need to schedule it to run periodically. This is where automation comes in, you'll set up cron jobs, CI/CD pipelines, or Kubernetes CronJobs to trigger `run` on a schedule.

### Built-in Scheduler Architecture

```mermaid
graph TB
    subgraph "Automation Layer - Triggers"
        CRON[Cron Job<br/>Schedule: Every hour<br/>Example: 0 * * * *]
        CI[CI/CD Pipeline<br/>GitHub Actions / GitLab CI<br/>Scheduled workflows]
        K8S[Kubernetes CronJob<br/>Container orchestration<br/>K8s native scheduling]
        MANUAL[Manual Trigger<br/>Developer runs manually<br/>vulcan run]
    end

    subgraph "Vulcan Run Command"
        RUN[vulcan run<br/>Command starts]
        VALIDATE[Validate Models<br/>Check for changes<br/>Error if modified]
        QUERY[Query State Database<br/>Get execution history<br/>Read processed intervals]
    end

    subgraph "State Database"
        STATE[State Storage<br/>PostgreSQL / SQL Engine<br/>Transaction-safe storage]
        
        subgraph "State Tables"
            INTERVALS[Processed Intervals<br/>model_name, start_ds, end_ds<br/>status: completed]
            CRON_STATE[Cron Execution State<br/>model_name, last_run_time<br/>next_run_time]
            MODEL_STATE[Model State<br/>model_name, fingerprint<br/>environment, version]
        end
    end

    subgraph "Cron Evaluation Engine"
        CRON_CHECK[Evaluate Cron Schedules<br/>Compare current time<br/>with last execution]
        CALC[Calculate Missing Intervals<br/>Determine what's due<br/>Based on cron + state]
        FILTER[Filter Models<br/>Only select due models<br/>Skip not-due models]
    end

    subgraph "Model Execution Queue"
        QUEUE[Execution Queue<br/>Ordered by dependencies<br/>Upstream first]
        EXEC1[Execute Hourly Model<br/>@hourly - Due<br/>Process missing intervals]
        EXEC2[Execute Daily Model<br/>@daily - Due<br/>Process missing intervals]
        SKIP[Skip Weekly Model<br/>@weekly - Not due<br/>Wait for next week]
    end

    subgraph "Update State"
        UPDATE[Update State Database<br/>Mark intervals processed<br/>Update cron state]
        COMMIT[Commit Transaction<br/>Ensure consistency<br/>Rollback on error]
    end

    subgraph "Results & Logging"
        LOG[Log Execution<br/>Summary output<br/>Success/failure status]
        NOTIFY[Notifications<br/>Optional: Slack/Email<br/>On success/failure]
    end

    CRON -->|"Scheduled trigger"| RUN
    CI -->|"Pipeline trigger"| RUN
    K8S -->|"K8s trigger"| RUN
    MANUAL -->|"Manual trigger"| RUN
    
    RUN -->|"1. Validate"| VALIDATE
    VALIDATE -->|"2. Query state"| QUERY
    QUERY -->|"Read"| STATE
    
    STATE -->|"Intervals"| INTERVALS
    STATE -->|"Cron state"| CRON_STATE
    STATE -->|"Model state"| MODEL_STATE
    
    INTERVALS -->|"Compare"| CRON_CHECK
    CRON_STATE -->|"Check schedule"| CRON_CHECK
    MODEL_STATE -->|"Get models"| CRON_CHECK
    
    CRON_CHECK -->|"Evaluate"| CALC
    CALC -->|"Calculate"| FILTER
    
    FILTER -->|"Due models"| QUEUE
    FILTER -.->|"Skip"| SKIP
    
    QUEUE -->|"Execute"| EXEC1
    QUEUE -->|"Execute"| EXEC2
    
    EXEC1 -->|"Update"| UPDATE
    EXEC2 -->|"Update"| UPDATE
    SKIP -.->|"No update"| UPDATE
    
    UPDATE -->|"Commit"| COMMIT
    COMMIT -->|"Success"| LOG
    LOG -->|"Optional"| NOTIFY

    style CRON fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style CI fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style K8S fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style MANUAL fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style RUN fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style VALIDATE fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style STATE fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style INTERVALS fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style CRON_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style MODEL_STATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style CRON_CHECK fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style CALC fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style FILTER fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style QUEUE fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style EXEC1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style EXEC2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style SKIP fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style UPDATE fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style COMMIT fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style LOG fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style NOTIFY fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
```

### Built-in Scheduler Components

The built-in scheduler consists of several key components working together:

1. **Automation Layer**: External triggers (cron, CI/CD, Kubernetes) that periodically execute `vulcan run`
2. **State Database**: Stores execution history, processed intervals, and cron state
3. **Cron Evaluation Engine**: Determines which models are due based on their schedules
4. **Execution Queue**: Orders models by dependencies and executes them
5. **State Updates**: Records what was processed for future runs

**Key Features:**

- Stores state in your SQL engine (or separate state database)

- Automatically detects missing intervals

- Respects each model's `cron` schedule

- Processes only what's due

- Transaction-safe state updates

- Dependency-aware execution order

### Setting Up Automation

Run `vulcan run` periodically using one of these methods:

#### Option 1: Linux/Mac Cron Job

```bash
# Edit crontab
crontab -e

# Run every hour
0 * * * * cd /path/to/project && vulcan run >> /var/log/vulcan-run.log 2>&1

# Run every 15 minutes
*/15 * * * * cd /path/to/project && vulcan run >> /var/log/vulcan-run.log 2>&1
```

#### Option 2: CI/CD Pipeline

**GitHub Actions Example:**
```yaml
name: Vulcan Run
on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run Vulcan
        run: |
          docker run --network=vulcan --rm \
            -v $PWD:/workspace \
            tmdcio/vulcan:latest vulcan run
```

**GitLab CI Example:**
```yaml
vulcan_run:
  schedule:
    - cron: '0 * * * *'  # Every hour
  script:
    - docker run --network=vulcan --rm \

        -v $PWD:/workspace \
        tmdcio/vulcan:latest vulcan run
```

#### Option 3: Kubernetes CronJob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: vulcan-run
spec:
  schedule: "0 * * * *"  # Every hour
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: vulcan
            image: tmdcio/vulcan:latest
            command: ["vulcan", "run"]
          restartPolicy: OnFailure
```

### Determining Run Frequency

Set your automation frequency based on your most frequent model's `cron`:

```mermaid
graph TD
    subgraph "Model Cron Schedules"
        H[Hourly Model<br/>cron: @hourly]
        D[Daily Model<br/>cron: @daily]
        W[Weekly Model<br/>cron: @weekly]
    end

    subgraph "Automation Frequency"
        AUTO_H[Run every hour<br/>vulcan run]
        AUTO_D[Run daily<br/>vulcan run]
        AUTO_W[Run weekly<br/>vulcan run]
    end

    subgraph "Execution Result"
        RESULT1[Hourly: Runs every time<br/>Daily: Runs when due<br/>Weekly: Runs when due]
        RESULT2[Hourly: Skipped<br/>Daily: Runs when due<br/>Weekly: Runs when due]
        RESULT3[Hourly: Skipped<br/>Daily: Skipped<br/>Weekly: Runs when due]
    end

    H -->|"Requires"| AUTO_H
    D -->|"Can use"| AUTO_H
    W -->|"Can use"| AUTO_H
    
    AUTO_H -->|"Hour 1"| RESULT1
    AUTO_H -->|"Hour 2-23"| RESULT2
    AUTO_H -->|"Week 1"| RESULT3

    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style W fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style AUTO_H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style RESULT1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style RESULT2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style RESULT3 fill:#ffe082,stroke:#f9a825,stroke-width:2px,color:#000
```

**Rule**: Schedule `vulcan run` based on your **fastest model's cron**.

- **Hourly models** → Run automation every hour - if you have hourly models, you need to run at least hourly

- **Daily models** → Run automation daily - if your fastest model is daily, you can run daily

- **Weekly models** → Run automation weekly - if your fastest model is weekly, you can run weekly

**Example**: If your fastest model runs `@hourly`, schedule `vulcan run` to execute hourly. Models with slower schedules (daily, weekly) only process when their intervals are due. Vulcan won't process daily models every hour. It waits until they're actually due.

The key insight: you can run `vulcan run` more frequently than your slowest model's schedule. Vulcan will just skip models that aren't due yet.

---

## Advanced Run Options

### Run Specific Models

```bash
vulcan run --select-model "sales.daily_sales"
```

Processes only the specified model and its upstream dependencies.

### Ignore Cron Schedules

```bash
vulcan run --ignore-cron
```

Processes all missing intervals regardless of cron schedules. Use sparingly - typically for catching up after downtime. 

This is useful if your automation was down for a while and you need to catch up on missed intervals. But normally, you want Vulcan to respect cron schedules, that's the whole point!

### Custom Execution Time

```bash
vulcan run --execution-time "2025-01-20 10:00:00"
```

Simulates running at a specific time. Useful for testing cron schedules.

### Run in Different Environments

```bash
vulcan run dev
```

Runs models in the `dev` environment, maintaining separate execution state from production.

---

## State Database Considerations

By default, Vulcan stores scheduler state in your SQL engine. For production:

**Recommended**: Use a separate PostgreSQL database for state storage when:
- Your SQL engine is BigQuery (not optimized for frequent transactions)

- You observe performance degradation

- You need better isolation

See [Configuration Guide](../references/configuration.md#gateways) for configuring a separate state database.

---

## Best Practices

1. **Use `run` for scheduled execution** - Don't use `plan` for regular data processing
2. **Set up automation** - Schedule `vulcan run` based on your most frequent model's cron
3. **Monitor execution** - Check logs to ensure intervals are processing correctly
4. **Use `--ignore-cron` sparingly** - Only when catching up on missed intervals
5. **Separate state database** - Consider PostgreSQL for state storage in production
6. **Handle errors gracefully** - Set up [notifications](../configurations/options/notifications.md) for run failures

Here are some tips to help you use `run` effectively:

1. **Use `run` for scheduled execution** - Don't use `plan` for regular data processing. They serve different purposes!
2. **Set up automation** - Schedule `vulcan run` based on your most frequent model's cron. Set it and forget it.
3. **Monitor execution** - Check logs to ensure intervals are processing correctly. Make sure your automation is actually working.
4. **Use `--ignore-cron` sparingly** - Only when catching up on missed intervals. Normally, let Vulcan respect cron schedules.
5. **Separate state database** - Consider PostgreSQL for state storage in production. Some SQL engines aren't optimized for frequent transactions.
6. **Handle errors gracefully** - Set up [notifications](../configurations/options/notifications.md) for run failures.

Following these practices will help you build reliable, automated data pipelines.

---

## Quick Reference

| Scenario | Command | When to Use |
|----------|---------|-------------|
| **Regular Run** | `vulcan run` | Scheduled execution (cron jobs, CI/CD) |
| **Dev Environment** | `vulcan run dev` | Running models in dev environment |
| **Select Models** | `vulcan run --select-model "model"` | Running specific models only |
| **Ignore Cron** | `vulcan run --ignore-cron` | Catch up on all missing intervals |
| **Custom Time** | `vulcan run --execution-time "..."` | Testing/simulating runs |

---

## Next Steps

- Learn about [Plan Guide](../references/plans.md) for applying model changes



# Transpiling Semantics

Source: https://tmdc-io.github.io/vulcan-book/guides/transpiling_semantics/

---

# Transpiling Semantics

The `vulcan transpile` command converts semantic queries into executable SQL. Use it to preview, debug, and validate semantic logic before execution.

Transpilation converts semantic layer queries (business-friendly) into SQL that your database understands.

## What is Transpilation?

Transpilation transforms semantic layer queries into database-specific SQL. It converts "business language" (semantic queries) into "database language" (SQL).

- **Semantic SQL → Native SQL**: Converts semantic SQL queries with `MEASURE()` functions into standard SQL - takes your business-friendly queries and makes them database-ready

- **REST API Payload → Native SQL**: Converts JSON query payloads into executable SQL statements. Use for API-driven applications.

- **Validation**: Catches errors before query execution - find problems before they hit production

- **Debugging**: Inspect the generated SQL to understand query behavior - see exactly what your semantic queries are doing under the hood

Semantic queries are easier to write and understand, but databases need SQL. Transpilation bridges that gap.

## Basic Structure

### Semantic SQL Query Structure

Semantic SQL queries follow standard SQL syntax with semantic layer extensions:

```sql
SELECT 
  alias.dimension_name,           # Dimensions: attributes for grouping and filtering
  MEASURE(alias.measure_name)  # Measures: aggregated calculations (required wrapper)
FROM alias                        # Semantic model alias (business-friendly name)
CROSS JOIN other_alias            # Optional: join multiple models
WHERE 
  alias.dimension_name = 'value'  # Optional: filter on dimensions
  AND segment_name = true         # Optional: use segments (only = true supported)
GROUP BY alias.dimension_name     # Required: all non-aggregated columns
ORDER BY MEASURE(alias.measure_name)    # Optional: sort results
LIMIT 100                         # Optional: limit result set
OFFSET 0                          # Optional: pagination offset
```

**Key Components:**

- `alias.dimension_name`: Reference dimensions using semantic model alias

- `MEASURE(measure_name)`: Required wrapper for measures to apply aggregation

- `FROM alias`: Use semantic model alias, not physical model name

- `CROSS JOIN`: Join syntax (join conditions automatically inferred)

- `segment_name = true`: Segments only support `= true`, not `= false`

### REST API Payload Structure

REST API queries use JSON payloads with semantic query definitions:

```json
{
  "query": {
    "measures": ["alias.measure_name"],              # Required: array of measure names
    "dimensions": ["alias.dimension_name"],         # Optional: array of dimension names
    "segments": ["segment_name"],                    # Optional: array of segment names
    "timeDimensions": [{                             # Optional: array of time dimension objects
      "dimension": "alias.time_dimension",           # Required: time dimension member
      "dateRange": ["2024-01-01", "2024-12-31"],    # Optional: date range array or string
      "granularity": "month"                         # Optional: hour, day, week, month, quarter, year
    }],
    "filters": [{                                    # Optional: array of filter objects
      "member": "alias.dimension_name",              # Required: fully qualified member name
      "operator": "equals",                          # Required: filter operator
      "values": ["value1", "value2"]                 # Optional: array of filter values
    }],
    "order": {                                       # Optional: sort order object
      "alias.measure_name": "desc",                  # Member name: "asc" or "desc"
      "alias.dimension_name": "asc"
    },
    "limit": 100,                                    # Optional: maximum rows to return
    "offset": 0,                                     # Optional: rows to skip
    "timezone": "UTC",                               # Optional: timezone for date parsing
    "renewQuery": false                              # Optional: bypass cache if true
  },
  "ttl_minutes": 60                                  # Optional: cache duration in minutes
}
```

**Key Components:**

- `measures`: Array of fully qualified measure names: `"alias.measure_name"`

- `dimensions`: Array of fully qualified dimension names: `"alias.dimension_name"`

- `segments`: Array of segment names (no alias prefix needed)

- `timeDimensions`: Array of objects with `dimension`, `dateRange`, and `granularity`

- `filters`: Array of filter objects with `member`, `operator`, and `values`

- `order`: Object mapping member names to sort direction (`"asc"` or `"desc"`)

## Basic Usage

### Transpiling Semantic SQL Queries

Convert semantic SQL queries to native SQL:

```bash
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"
```

**Output:** Generated SQL that can be executed directly against your database.

### Transpiling REST API Payloads

Convert JSON query payloads to native SQL:

```bash
vulcan transpile --format json '{"query": {"measures": ["users.total_users"]}}'
```

**Output:** Generated SQL from the REST-style query definition.

## Command Syntax

### Basic Format

```bash
vulcan transpile --format <format> "<query>"
```

**Parameters:**

- `--format` (required): Output format: `sql` or `json`

- `"<query>"` (required): The semantic query to transpile

  - For SQL format: Semantic SQL query string

  - For JSON format: JSON query payload string

### Advanced Options

```bash
vulcan transpile --format sql "<query>" [--disable-post-processing]
```

**Options:**

- `--disable-post-processing`: Enable pushdown mode for CTE support and advanced SQL features

  - **Default**: Post-processing enabled (CTEs not supported)

  - **With flag**: Pushdown enabled (CTEs supported, no pre-aggregations)

## Transpiling Semantic SQL

### Basic Query

Transpile a simple semantic SQL query:

```bash
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"
```

**Generated SQL:**
```sql
SELECT sum("users".user_id) AS total_users
FROM analytics.users AS "users"
```

### Query with Dimensions

Transpile queries with dimensions and grouping:

```bash
vulcan transpile --format sql "SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type"
```

**Generated SQL:**
```sql
SELECT "users".plan_type, sum("users".user_id) AS total_users
FROM analytics.users AS "users"
GROUP BY "users".plan_type
```

### Query with Filters

Transpile queries with WHERE conditions:

```bash
vulcan transpile --format sql "SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'"
```

**Generated SQL:**
```sql
SELECT sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
WHERE "subscriptions".status = 'active'
```

### Query with Time Grouping

Transpile time-based queries:

```bash
vulcan transpile --format sql "SELECT DATE_TRUNC('month', subscriptions.start_date) as month, MEASURE(total_arr) FROM subscriptions GROUP BY month"
```

**Generated SQL:**
```sql
SELECT DATE_TRUNC('month', "subscriptions".start_date) AS month,
       sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
GROUP BY DATE_TRUNC('month', "subscriptions".start_date)
```

### Query with Joins

Transpile queries joining multiple models:

```bash
vulcan transpile --format sql "SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users GROUP BY users.industry"
```

**Generated SQL:**
```sql
SELECT "users".industry, sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
CROSS JOIN analytics.users AS "users"
WHERE "subscriptions".user_id = "users".user_id
GROUP BY "users".industry
```

## Transpiling REST API Payloads

### Minimal Query

Transpile a basic REST API query:

```bash
vulcan transpile --format json '{"query": {"measures": ["users.total_users"]}}'
```

**Generated SQL:**
```sql
SELECT sum("users".user_id) AS total_users
FROM analytics.users AS "users"
```

### Query with Dimensions

Transpile queries with dimensions:

```bash
vulcan transpile --format json '{"query": {"measures": ["subscriptions.total_arr"], "dimensions": ["subscriptions.plan_type"]}}'
```

**Generated SQL:**
```sql
SELECT "subscriptions".plan_type, sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
GROUP BY "subscriptions".plan_type
```

### Query with Time Dimensions

Transpile time-based queries:

```bash
vulcan transpile --format json '{"query": {"measures": ["orders.total_revenue"], "timeDimensions": [{"dimension": "orders.order_date", "dateRange": ["2024-01-01", "2024-12-31"], "granularity": "month"}]}}'
```

**Generated SQL:**
```sql
SELECT DATE_TRUNC('month', "orders".order_date) AS orders_order_date_month,
       sum("orders".amount) AS total_revenue
FROM analytics.orders AS "orders"
WHERE "orders".order_date >= '2024-01-01T00:00:00.000'
  AND "orders".order_date <= '2024-12-31T23:59:59.999'
GROUP BY DATE_TRUNC('month', "orders".order_date)
```

### Query with Filters

Transpile queries with filters:

```bash
vulcan transpile --format json '{"query": {"measures": ["subscriptions.total_arr"], "filters": [{"member": "subscriptions.status", "operator": "equals", "values": ["active"]}]}}'
```

**Generated SQL:**
```sql
SELECT sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
WHERE "subscriptions".status = 'active'
```

### Query with Segments

Transpile queries using segments:

```bash
vulcan transpile --format json '{"query": {"measures": ["subscriptions.total_arr"], "segments": ["subscriptions.active_subscriptions"]}}'
```

**Generated SQL:**
```sql
SELECT sum("subscriptions".arr) AS total_arr
FROM analytics.subscriptions AS "subscriptions"
WHERE "subscriptions".status = 'active'
  AND "subscriptions".end_date IS NULL
```

### Complex Query

Transpile complex queries with multiple components:

```bash
vulcan transpile --format json '{"query": {"measures": ["subscriptions.total_arr", "subscriptions.total_seats"], "dimensions": ["subscriptions.plan_type", "users.industry"], "filters": [{"member": "subscriptions.status", "operator": "equals", "values": ["active"]}], "timeDimensions": [{"dimension": "subscriptions.start_date", "dateRange": ["2024-01-01", "2024-12-31"], "granularity": "month"}], "order": {"subscriptions.total_arr": "desc"}, "limit": 100}}'
```

**Generated SQL:**
```sql
SELECT DATE_TRUNC('month', "subscriptions".start_date) AS subscriptions_start_date_month,
       "subscriptions".plan_type,
       "users".industry,
       sum("subscriptions".arr) AS total_arr,
       sum("subscriptions".seats) AS total_seats
FROM analytics.subscriptions AS "subscriptions"
CROSS JOIN analytics.users AS "users"
WHERE "subscriptions".status = 'active'
  AND "subscriptions".start_date >= '2024-01-01T00:00:00.000'
  AND "subscriptions".start_date <= '2024-12-31T23:59:59.999'
  AND "subscriptions".user_id = "users".user_id
GROUP BY DATE_TRUNC('month', "subscriptions".start_date),
         "subscriptions".plan_type,
         "users".industry
ORDER BY sum("subscriptions".arr) DESC
LIMIT 100
```

## Use Cases

### Query Validation

Validate semantic queries before execution:

```bash
# Check if query syntax is correct
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"
```

If the query is invalid, you'll get an error message indicating the issue. This is way better than finding out at runtime! Catch errors early, fix them, then execute.

### Debugging Query Behavior

Inspect generated SQL to understand how semantic queries are translated. When queries return unexpected results, transpile them to see what's actually happening:

```bash
# See how measures are aggregated
vulcan transpile --format sql "SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type"
```

This shows you exactly how Vulcan is interpreting your semantic query. Sometimes the generated SQL reveals issues you didn't expect!

### Performance Analysis

Review generated SQL to identify optimization opportunities. The generated SQL shows you exactly what the database will execute, so you can spot performance issues:

```bash
# Check join conditions and filter placement
vulcan transpile --format sql "SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry"
```

Look at the generated SQL, are joins efficient? Are filters in the right place? This is your chance to optimize before execution.

### Documentation

Generate SQL examples for documentation or training. Use transpilation to create SQL examples that show how semantic queries translate:

```bash
# Create SQL reference from semantic queries
vulcan transpile --format sql "SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'"
```

Use this for documentation. Show both the semantic query (easy to understand) and the generated SQL (what actually runs).

## Common Errors and Solutions

### Error: "Unknown member: X"

**Cause:** Member doesn't exist in semantic model or is misspelled.

**Solution:**

- Verify member exists in your semantic model - check your semantic model definitions

- Check spelling and casing (case-sensitive) - `users.plan_type` is different from `users.Plan_Type`

- Use fully qualified format: `alias.member_name` - always include the alias prefix

This error usually means you've made a typo or the member doesn't exist yet. Double-check your semantic model!

### Error: "Measure not found: X"

**Cause:** Measure referenced without proper qualification or doesn't exist.

**Solution:**

- Use `MEASURE(measure_name)` wrapper for SQL format - measures need the MEASURE() wrapper in SQL

- Use fully qualified format: `alias.measure_name` for JSON format - JSON format uses dot notation

- Verify measure is defined in semantic model - make sure the measure actually exists

The format differs between SQL and JSON, so make sure you're using the right syntax for each!

### Error: "Model not found: X"

**Cause:** Alias doesn't match any semantic model.

**Solution:**

- Check semantic model aliases in your `semantics/` directory

- Verify alias spelling and casing

- Ensure semantic models are properly defined


### Error: "Invalid JSON format"

**Cause:** JSON payload is malformed.

**Solution:**

- Validate JSON syntax

- Ensure proper quoting of strings

- Check array and object structure

### Error: "Projection references non-aggregate values"

**Cause:** Non-aggregated columns not in GROUP BY, or measures missing MEASURE() wrapper.

**Solution:**

- Add all non-aggregated columns to GROUP BY - if you select a column, it needs to be in GROUP BY (unless it's aggregated)

- Use MEASURE() wrapper for all measures in SQL format - measures must be wrapped in MEASURE()

This is a SQL rule, you can't mix aggregated and non-aggregated columns without GROUP BY. The error is telling you exactly what's wrong!

## Best Practices

### Validate Before Execution

Always transpile queries before running them in production. It's like checking your work before turning it in:

```bash
# Good: Validate first
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"
# Review output, then execute - make sure the SQL looks right

# Bad: Execute without validation
# Direct execution without checking generated SQL - don't do this
```

Transpilation catches errors early. Use it!

### Use Transpilation for Debugging

When queries return unexpected results, transpile to inspect generated SQL. The generated SQL often reveals what's actually happening:

```bash
# Debug query behavior
vulcan transpile --format sql "SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type"
# Compare generated SQL with expected behavior - does it match what you think should happen?
```

Sometimes the issue isn't with your semantic query, it's with how it's being translated. Transpilation shows you the translation.

### Document Query Patterns

Use transpilation output to document common query patterns:

```bash
# Generate SQL examples for documentation
vulcan transpile --format sql "SELECT MEASURE(total_arr) FROM subscriptions WHERE subscriptions.status = 'active'"
```

### Test Both Formats

When building applications, test both SQL and JSON formats:

```bash
# Test SQL format
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"

# Test equivalent JSON format
vulcan transpile --format json '{"query": {"measures": ["users.total_users"]}}'
```

### Choose Appropriate Mode

Select post-processing or pushdown mode based on needs:

- **Post-processing (default)**: Use for queries that benefit from pre-aggregations and caching

- **Pushdown (`--disable-post-processing`)**: Use when you need CTEs or complex SQL structures

## Integration with Development Workflow

### Pre-commit Validation

Add transpilation checks to your development workflow:

```bash
# Validate semantic queries in CI/CD
vulcan transpile --format sql "SELECT MEASURE(total_users) FROM users"
```

### Query Testing

Use transpilation to generate test SQL:

```bash
# Generate SQL for testing
vulcan transpile --format sql "SELECT users.plan_type, MEASURE(total_users) FROM users GROUP BY users.plan_type"
# Use output in test assertions
```

### Performance Tuning

Analyze generated SQL for optimization:

```bash
# Review join conditions and filter placement
vulcan transpile --format sql "SELECT users.industry, MEASURE(total_arr) FROM subscriptions CROSS JOIN users WHERE subscriptions.status = 'active' GROUP BY users.industry"
```

## Next Steps

- Learn about [Semantic Models](../components/semantics/models.md) that define the queryable members

- Explore [Business Metrics](../components/semantics/business_metrics.md) for time-series analysis

- See the [Semantics Overview](../components/semantics/overview.md) for the complete picture




# Configuration

Source: https://tmdc-io.github.io/vulcan-book/guides-old/configuration/

---

# Configuration

Vulcan's behavior is determined by three things: a project's files (e.g., models), user actions (e.g., creating a `plan`), and how Vulcan is configured.

This page describes how Vulcan configuration works and discusses the aspects of Vulcan behavior that can be modified via configuration.

The [configuration reference page](../configurations-old/configuration.md) contains concise lists of all configuration parameters and their default values.

## Configuration files

**NOTE:** Vulcan project configurations have the following two requirements:

1. A `config.yaml` or `config.py` file must be present in the project's folder.
2. That configuration file must contain a default SQL dialect for the project's models in the [`model_defaults` `dialect` key](#models).

Vulcan configuration parameters can be set as environment variables, in a configuration file in the `~/.vulcan` folder, and in the configuration file within a project folder.

The sources have the following order of precedence:

1. Environment variable (e.g., `VULCAN__MODEL_DEFAULTS__DIALECT`). [HIGHEST PRECEDENCE]
2. `config.yaml` or `config.py` in the `~/.vulcan` folder.
3. `config.yaml` or `config.py` in a project folder. [LOWEST PRECEDENCE]

!!! note
    To relocate the `.vulcan` folder, set the `VULCAN_HOME` environment variable to your preferred directory path.

### File type

You can specify a Vulcan configuration in either YAML or Python.

YAML configuration is simpler, and we recommend it for most projects. Python configuration is more complex, but it enables functionality that YAML does not support.

Because Python configuration files are evaluated by Python when Vulcan reads them, they support dynamic parameters based on the computational environment in which Vulcan is running.

For example, Python configuration files enable use of third-party secrets managers for storing passwords and other sensitive information. They also support user-specific parameters such as automatically setting project defaults based on which user account is running Vulcan.

#### YAML

YAML configuration files consist of configuration keys and values. Strings are not quoted, and some keys are "dictionaries" that contain one or more sub-keys.

For example, the `default_gateway` key specifies the default gateway Vulcan should use when executing commands. It takes a single, unquoted gateway name as its value:

```yaml linenums="1"
default_gateway: local
```

In contrast, the `gateways` key takes dictionaries as values, and each gateway dictionary contains one or more connection dictionaries. This example specifies the `my_gateway` gateway with a Snowflake `connection`:

```yaml linenums="1"
gateways:
  my_gateway:
    connection:
      type: snowflake
      user: <username>
      password: <password>
      account: <account>
```

Gateway dictionaries can contain multiple connection dictionaries if different Vulcan components should use different connections (e.g., Vulcan `test`s should run in a different database than Vulcan `plan`s). See the [gateways section](#gateways) for more information on gateway configuration.

#### Python

Python configuration files consist of statements that import Vulcan configuration classes and a configuration specification using those classes.

At minimum, a Python configuration file must:

1. Create an object of the Vulcan `Config` class named `config`
2. Specify that object's `model_defaults` argument with a `ModelDefaultsConfig()` object specifying the default SQL dialect for the project's models

For example, this minimal configuration specifies a default SQL dialect of `duckdb` and uses the default values for all other configuration parameters:

```python linenums="1"
from vulcan.core.config import Config, ModelDefaultsConfig

config = Config(
    model_defaults=ModelDefaultsConfig(dialect="duckdb"),
)
```

Python configuration files may optionally define additional configuration objects and switch between the configurations when issuing `vulcan` commands. For example, if a configuration file contained a second configuration object `my_second_config`, you could create a plan using that config with `vulcan --config my_second_config plan`.

Different `Config` arguments accept different object types. Some, such as `model_defaults`, take Vulcan configuration objects. Others, such as `default_gateway`, take strings or other Python object types like dictionaries.

Vulcan's Python configuration components are documented in the `vulcan.core.config` module's [API documentation](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config.html).

The `config` sub-module API documentation describes the individual classes used for the relevant `Config` arguments:

- [Model defaults configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/model.html): `ModelDefaultsConfig()`
- [Gateway configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/gateway.html): `GatewayConfig()`
    - [Connection configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/connection.html) (separate classes for each supported database/engine)
    - [Scheduler configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/scheduler.html) (separate classes for each supported scheduler)
- [Plan change categorization configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/config/categorizer.html#CategorizerConfig): `CategorizerConfig()`
- [User configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/user.html#User): `User()`
- [Notification configuration](https://vulcan.readthedocs.io/en/latest/_readthedocs/html/vulcan/core/notification_target.html) (separate classes for each notification target)

See the [notifications guide](guides-old/notifications.md) for more information about user and notification specification.

## Environment variables

All software runs within a system environment that stores information as "environment variables."

Vulcan can access environment variables during configuration, which enables approaches like storing passwords/secrets outside the configuration file and changing configuration parameters dynamically based on which user is running Vulcan.

You can specify environment variables in the configuration file or by storing them in a `.env` file.

### .env files

Vulcan automatically loads environment variables from a `.env` file in your project directory. This provides a convenient way to manage environment variables without having to set them in your shell.

Create a `.env` file in your project root with key-value pairs:

```bash
# .env file
SNOWFLAKE_PW=my_secret_password
S3_BUCKET=s3://my-data-bucket/warehouse
DATABASE_URL=postgresql://user:pass@localhost/db

# Override specific Vulcan configuration values
VULCAN__DEFAULT_GATEWAY=production
VULCAN__MODEL_DEFAULTS__DIALECT=snowflake
```

See the [overrides](#overrides) section for a detailed explanation of how these are defined.

The rest of the `.env` file variables can be used in your configuration files with `{{ env_var('VARIABLE_NAME') }}` syntax in YAML or accessed via `os.environ['VARIABLE_NAME']` in Python.

#### Custom dot env file location and name

By default, Vulcan loads `.env` files from each project directory. However, you can specify a custom path using the `--dotenv` CLI flag directly when running a command:

```bash
vulcan --dotenv /path/to/custom/.env plan
```

!!! note
    The `--dotenv` flag is a global option and must be placed **before** the subcommand (e.g. `plan`, `run`), not after.

Alternatively, you can export the `VULCAN_DOTENV_PATH` environment variable once, to persist a custom path across all subsequent commands in your shell session:

```bash
export VULCAN_DOTENV_PATH=/path/to/custom/.custom_env
vulcan plan
vulcan run
```

**Important considerations:**
- Add `.env` to your `.gitignore` file to avoid committing sensitive information
- Vulcan will only load the `.env` file if it exists in the project directory (unless a custom path is specified)
- When using a custom path, that specific file takes precedence over any `.env` file in the project directory.

### Configuration file

This section demonstrates using environment variables in YAML and Python configuration files.

The examples specify a Snowflake connection whose password is stored in an environment variable `SNOWFLAKE_PW`.

=== "YAML"

    Specify environment variables in a YAML configuration with the syntax `{{ env_var('<ENVIRONMENT VARIABLE NAME>') }}`. Note that the environment variable name is contained in single quotes.

    Access the `SNOWFLAKE_PW` environment variable in a Snowflake connection configuration like this:

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: snowflake
          user: <username>
          password: {{ env_var('SNOWFLAKE_PW') }}
          account: <account>
    ```

=== "Python"

    Python accesses environment variables via the `os` library's `environ` dictionary.

    Access the `SNOWFLAKE_PW` environment variable in a Snowflake connection configuration like this:

    ```python linenums="1"
    import os
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        SnowflakeConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user=<username>,
                    password=os.environ['SNOWFLAKE_PW'],
                    account=<account>,
                ),
            ),
        }
    )
    ```

#### Default target environment

The Vulcan `plan` command acts on the `prod` environment by default (i.e., `vulcan plan` is equivalent to `vulcan plan prod`).

In some organizations, users never run plans directly against `prod` - they do all Vulcan work in a development environment unique to them. In a standard Vulcan configuration, this means they need to include their development environment name every time they issue the `plan` command (e.g., `vulcan plan dev_tony`).

If your organization works like this, it may be convenient to change the `plan` command's default environment from `prod` to each user's development environment. That way people can issue `vulcan plan` without typing the environment name every time.

The Vulcan configuration `user()` function returns the name of the user currently logged in and running Vulcan. It retrieves the username from system environment variables like `USER` on MacOS/Linux or `USERNAME` on Windows.

Call `user()` inside Jinja curly braces with the syntax `{{ user() }}`, which allows you to combine the user name with a prefix or suffix.

The example configuration below constructs the environment name by appending the username to the end of the string `dev_`. If the user running Vulcan is `tony`, the default target environment when they run Vulcan will be `dev_tony`. In other words, `vulcan plan` will be equivalent to `vulcan plan dev_tony`.

=== "YAML"

    Default target environment is `dev_` combined with the username running Vulcan.

    ```yaml
    default_target_environment: dev_{{ user() }}
    ```

=== "Python"

    Default target environment is `dev_` combined with the username running Vulcan.

    Retrieve the username with the `getpass.getuser()` function, and combine it with `dev_` in a Python f-string.

    ```python linenums="1" hl_lines="1 17"
    import getpass
    import os
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        SnowflakeConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="duckdb"),
        gateways={
            "my_gateway": GatewayConfig(
                connection=DuckDBConnectionConfig(),
            ),
        },
        default_target_environment=f"dev_{getpass.getuser()}",
    )
    ```

### Overrides

Environment variables have the highest precedence among configuration methods, as [noted above](#configuration-files). They will automatically override configuration file specifications if they follow a specific naming structure.

The structure is based on the names of the configuration fields, with double underscores `__` between the field names. The environment variable name must begin with `VULCAN__`, followed by the YAML field names starting at the root and moving downward in the hierarchy.

For example, we can override the password specified in a Snowflake connection. This is the YAML specification contained in our configuration file, which specifies a password `dummy_pw`:

```yaml linenums="1"
gateways:
  my_gateway:
    connection:
      type: snowflake
      user: <username>
      password: dummy_pw
      account: <account>
```

We can override the `dummy_pw` value with the true password `real_pw` by creating the environment variable. This example demonstrates creating the variable with the bash `export` function:

```bash
$ export VULCAN__GATEWAYS__MY_GATEWAY__CONNECTION__PASSWORD="real_pw"
```

After the initial string `VULCAN__`, the environment variable name components move down the key hierarchy in the YAML specification: `GATEWAYS` --> `MY_GATEWAY` --> `CONNECTION` --> `PASSWORD`.

## Configuration types

A Vulcan project configuration is hierarchical and consists of root level parameters within which other parameters are defined.

Conceptually, we can group the root level parameters into the following types. Each type links to its table of parameters in the [Vulcan configuration reference page](../configurations-old/configuration.md):

1. [Project](../configurations-old/configuration.md#projects) - configuration options for Vulcan project directories.
2. [Environment](../configurations-old/configuration.md#environments-virtual-layer) - configuration options for Vulcan environment creation/promotion, physical table schemas, and view schemas.
3. [Gateways](../configurations-old/configuration.md#gateways) - configuration options for how Vulcan should connect to the data warehouse, state backend, and scheduler.
4. [Gateway/connection defaults](../configurations-old/configuration.md#gatewayconnection-defaults) - configuration options for what should happen when gateways or connections are not all explicitly specified.
5. [Model defaults](../configurations-old/configuration.md) - configuration options for what should happen when model-specific configurations are not explicitly specified in a model's file.
6. [Debug mode](../configurations-old/configuration.md#debug-mode) - configuration option for Vulcan to print and log actions and full backtraces.

## Configuration details

The rest of this page provides additional detail for some of the configuration options and provides brief examples. Comprehensive lists of configuration options are at the [configuration reference page](../configurations-old/configuration.md).

### Cache directory

By default, the Vulcan cache is stored in a `.cache` directory within your project folder. You can customize the cache location using the `cache_dir` configuration option:

=== "YAML"

    ```yaml linenums="1"
    # Relative path to project directory
    cache_dir: my_custom_cache

    # Absolute path
    cache_dir: /tmp/vulcan_cache

    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="duckdb"),
        cache_dir="/tmp/vulcan_cache",
    )
    ```

The cache directory is automatically created if it doesn't exist. You can clear the cache using the `vulcan clean` command.

### Table/view storage locations

Vulcan creates schemas, physical tables, and views in the data warehouse/engine. You can override *where* Vulcan creates physical tables and views with the `physical_schema_mapping`, `environment_suffix_target`, and `environment_catalog_mapping` configuration options.

You can also override *what* the physical tables are called by using the `physical_table_naming_convention` option.

These options are in the [environments](../configurations-old/configuration.md#environments-virtual-layer) section of the configuration reference page.

#### Physical table schemas
By default, Vulcan creates physical schemas for a model with a naming convention of `vulcan__[model schema]`.

This can be overridden on a per-schema basis using the `physical_schema_mapping` option, which removes the `vulcan__` prefix and uses the [regex pattern](https://docs.python.org/3/library/re.html#regular-expression-syntax) you provide to map the schemas defined in your model to their corresponding physical schemas.

This example configuration overrides the default physical schemas for the `my_schema` model schema and any model schemas starting with `dev`:

=== "YAML"

    ```yaml linenums="1"
    physical_schema_mapping:
      '^my_schema$': my_new_schema,
      '^dev.*': development
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        physical_schema_mapping={
            "^my_schema$": "my_new_schema",
            '^dev.*': "development"
        },
    )
    ```

This config causes the following mapping behaviour:

| Model name            | Default physical location                 | Resolved physical location
| --------------------- | ----------------------------------------- | ------------------------------------ |
| `my_schema.my_table`  | `vulcan__my_schema.table_<fingerprint>`  | `my_new_schema.table_<fingerprint>`  |
| `dev_schema.my_table` | `vulcan__dev_schema.table_<fingerprint>` | `development.table_<fingerprint>`    |
| `other.my_table`      | `vulcan__other.table_<fingerprint>`      | `vulcan__other.table_<fingerprint>` |


This only applies to the _physical tables_ that Vulcan creates - the views are still created in `my_schema` (prod) or `my_schema__<env>`.

#### Disable environment-specific schemas

Vulcan stores `prod` environment views in the schema in a model's name - for example, the `prod` views for a model `my_schema.users` will be located in `my_schema`.

By default, for non-prod environments Vulcan creates a new schema that appends the environment name to the model name's schema. For example, by default the view for a model `my_schema.users` in a Vulcan environment named `dev` will be located in the schema `my_schema__dev` as `my_schema__dev.users`.

##### Show at the table level instead

This behavior can be changed to append a suffix at the end of a _table/view_ name instead. Appending the suffix to a table/view name means that non-prod environment views will be created in the same schema as the `prod` environment. The prod and non-prod views are differentiated by non-prod view names ending with `__<env>`.

For example, if you created a `dev` environment for a project containing a model named `my_schema.users`, the model view would be created as `my_schema.users__dev` instead of the default behavior of `my_schema__dev.users`.

Config example:

=== "YAML"

    ```yaml linenums="1"
    environment_suffix_target: table
    ```

=== "Python"

    The Python `environment_suffix_target` argument takes an `EnvironmentSuffixTarget` enumeration with a value of `EnvironmentSuffixTarget.TABLE`, `EnvironmentSuffixTarget.CATALOG` or `EnvironmentSuffixTarget.SCHEMA` (default).

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        environment_suffix_target=EnvironmentSuffixTarget.TABLE,
    )
    ```

!!! info "Default behavior"
    The default behavior of appending the suffix to schemas is recommended because it leaves production with a single clean interface for accessing the views. However, if you are deploying Vulcan in an environment with tight restrictions on schema creation then this can be a useful way of reducing the number of schemas Vulcan uses.

##### Show at the catalog level instead

If neither the schema (default) nor the table level are sufficient for your use case, you can indicate the environment at the catalog level instead.

This can be useful if you have downstream BI reporting tools and you would like to point them at a development environment to test something out without renaming all the table / schema references within the report query.

In order to achieve this, you can configure [environment_suffix_target](../configurations-old/configuration.md#environments-virtual-layer) like so:

=== "YAML"

    ```yaml linenums="1"
    environment_suffix_target: catalog
    ```

=== "Python"

    The Python `environment_suffix_target` argument takes an `EnvironmentSuffixTarget` enumeration with a value of `EnvironmentSuffixTarget.TABLE`, `EnvironmentSuffixTarget.CATALOG` or `EnvironmentSuffixTarget.SCHEMA` (default).

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig, EnvironmentSuffixTarget

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        environment_suffix_target=EnvironmentSuffixTarget.CATALOG,
    )
    ```

Given the example of a model called `my_schema.users` with a default catalog of `warehouse` this will cause the following behavior:

- For the `prod` environment, the default catalog as configured in the gateway will be used. So the view will be created at `warehouse.my_schema.users`
- For any other environment, eg `dev`, the environment name will be appended to the default catalog. So the view will be created at `warehouse__dev.my_schema.users`
- If a model is fully qualified with a catalog already, eg `finance_mart.my_schema.users`, then the environment catalog will be based off the model catalog and not the default catalog. In this example, the view will be created at `finance_mart__dev.my_schema.users`


!!! warning "Caveats"
    - Using `environment_suffix_target: catalog` only works on engines that support querying across different catalogs. If your engine does not support cross-catalog queries then you will need to use `environment_suffix_target: schema` or `environment_suffix_target: table` instead.
    - Automatic catalog creation is not supported on all engines even if they support cross-catalog queries. For engines where it is not supported, the catalogs must be managed externally from Vulcan and exist prior to invoking Vulcan.

#### Physical table naming convention

Out of the box, Vulcan has the following defaults set:

 - `environment_suffix_target: schema`
 - `physical_table_naming_convention: schema_and_table`
 - no `physical_schema_mapping` overrides, so a `vulcan__<model schema>` physical schema will be created for each model schema

This means that given a catalog of `warehouse` and a model named `finance_mart.transaction_events_over_threshold`, Vulcan will create physical tables using the following convention:

```
# <catalog>.vulcan__<schema>.<schema>__<table>__<fingerprint>

warehouse.vulcan__finance_mart.finance_mart__transaction_events_over_threshold__<fingerprint>
```

This deliberately contains some redundancy with the *model* schema as it's repeated at the physical layer in both the physical schema name as well as the physical table name.

This default exists to make the physical table names portable between different configurations. If you were to define a `physical_schema_mapping` that maps all models to the same physical schema, since the model schema is included in the table name as well, there are no naming conflicts.

##### Table only

Some engines have object name length limitations which cause them to [silently truncate](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS) table and view names that exceed this limit. This behaviour breaks Vulcan, so we raise a runtime error if we detect the engine would silently truncate the name of the table we are trying to create.

Having redundancy in the physical table names does reduce the number of characters that can be utilised in model names. To increase the number of characters available to model names, you can use `physical_table_naming_convention` like so:

=== "YAML"

    ```yaml linenums="1"
    physical_table_naming_convention: table_only
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        physical_table_naming_convention=TableNamingConvention.TABLE_ONLY,
    )
    ```

This will cause Vulcan to omit the model schema from the table name and generate physical names that look like (using the above example):
```
# <catalog>.vulcan__<schema>.<table>__<fingerprint>

warehouse.vulcan__finance_mart.transaction_events_over_threshold__<fingerprint>
```

Notice that the model schema name is no longer part of the physical table name. This allows for slightly longer model names on engines with low identifier length limits, which may be useful for your project.

In this configuration, it is your responsibility to ensure that any schema overrides in `physical_schema_mapping` result in each model schema getting mapped to a unique physical schema.

For example, the following configuration will cause **data corruption**:

```yaml
physical_table_naming_convention: table_only
physical_schema_mapping:
  '.*': vulcan
```

This is because every model schema is mapped to the same physical schema but the model schema name is omitted from the physical table name.

##### MD5 hash

If you *still* need more characters, you can set `physical_table_naming_convention: hash_md5` like so:

=== "YAML"

    ```yaml linenums="1"
    physical_table_naming_convention: hash_md5
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig, TableNamingConvention

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        physical_table_naming_convention=TableNamingConvention.HASH_MD5,
    )
    ```

This will cause Vulcan generate physical names that are always 45-50 characters in length and look something like:

```
# vulcan_md5__<hash of what we would have generated using 'schema_and_table'>

vulcan_md5__d3b07384d113edec49eaa6238ad5ff00

# or, for a dev preview
vulcan_md5__d3b07384d113edec49eaa6238ad5ff00__dev
```

This has a downside that now it's much more difficult to determine which table corresponds to which model by just looking at the database with a SQL client. However, the table names have a predictable length so there are no longer any surprises with identfiers exceeding the max length at the physical layer.

#### Virtual Data Environment Modes

By default, Virtual Data Environments (VDE) are applied across both development and production environments. This allows Vulcan to reuse physical tables when appropriate, even when promoting from development to production.

However, users may prefer their production environment to be non-virtual. The non-exhaustive list of reasons may include:

- Integration with third-party tools and platforms, such as data catalogs, may not work well with the virtual view layer that Vulcan imposes by default
- A desire to rely on time travel features provided by cloud data warehouses such as BigQuery, Snowflake, and Databricks

To mitigate this, Vulcan offers an alternative 'dev-only' mode for using VDE. It can be enabled in the project configuration like so:

=== "YAML"

    ```yaml linenums="1"
    virtual_environment_mode: dev_only
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config

    config = Config(
        virtual_environment_mode="dev_only",
    )
    ```

'dev-only' mode means that VDE is applied only in development environments. While in production, model tables and views are updated directly and bypass the virtual layer. This also means that physical tables in production will be created using the original, **unversioned** model names. Users will still benefit from VDE and data reuse across development environments.

Please note the following tradeoffs when enabling this mode:

- All data inserted in development environments is used only for [preview](guides/plan.md#data-preview-for-forward-only-changes) and will **not** be reused in production
- Reverting a model to a previous version will be applied going forward and may require an explicit data restatement

!!! warning
    Switching the mode for an existing project will result in a **complete rebuild** of all models in the project. Refer to the [Table Migration Guide](./table_migration.md) to migrate existing tables without rebuilding them from scratch.


#### Environment view catalogs

By default, Vulcan creates an environment view in the same [catalog](concepts-old/glossary.md#catalog) as the physical table the view points to. The physical table's catalog is determined by either the catalog specified in the model name or the default catalog defined in the connection.

It can be desirable to create `prod` and non-prod virtual layer objects in separate catalogs instead. For example, there might be a "prod" catalog that contains all `prod` environment views and a separate "dev" catalog that contains all `dev` environment views.

Separate prod and non-prod catalogs can also be useful if you have a CI/CD pipeline that creates environments, like the Vulcan Github Actions CI/CD Bot. You might want to store the CI/CD environment objects in a dedicated catalog since there can be many of them.

!!! info "Virtual layer only"
    Note that the following setting only affects the [virtual layer](concepts-old/glossary.md#virtual-layer). If you need full segregation by catalog between environments in the [physical layer](concepts-old/glossary.md#physical-layer) as well, see the [Isolated Systems Guide](../guides/isolated_systems.md).

To configure separate catalogs, provide a mapping from [regex patterns](https://en.wikipedia.org/wiki/Regular_expression) to catalog names. Vulcan will compare the name of an environment to the regex patterns; when it finds a match it will store the environment's objects in the corresponding catalog.

Vulcan evaluates the regex patterns in the order defined in the configuration; it uses the catalog for the first matching pattern. If no match is found, the catalog defined in the model or the default catalog defined on the connection will be used.

Config example:

=== "YAML"

    ```yaml linenums="1"
    environment_catalog_mapping:
      '^prod$': prod
      '^dev.*': dev
      '^analytics_repo.*': cicd
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        environment_catalog_mapping={
            '^prod$': 'prod',
            '^dev.*': 'dev',
            '^analytics_repo.*': 'cicd',
        },
    )
    ```

With the example configuration above, Vulcan would evaluate environment names as follows:

* If the environment name is `prod`, the catalog will be `prod`.
* If the environment name starts with `dev`, the catalog will be `dev`.
* If the environment name starts with `analytics_repo`, the catalog will be `cicd`.

!!! warning
    This feature is mutually exclusive with `environment_suffix_target: catalog` in order to prevent ambiguous mappings from being defined. Attempting to specify both `environment_catalog_mapping` and `environment_suffix_target: catalog` will raise an error on project load

*Note:* This feature is only available for engines that support querying across catalogs. At the time of writing, the following engines are **NOT** supported:

* [MySQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mysql.md)
* [Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md)
* [GCP Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/gcp-postgres.md)

##### Regex Tips
* If you are less familiar with regex, you can use a tool like [regex101](https://regex101.com/) to help you build your regex patterns.
    * LLMs, like [ChatGPT](https://chat.openai.com), can help with generating regex patterns. Make sure to validate the suggestion in regex101.
* If you are wanting to do an exact word match then surround it with `^` and `$` like in the example above.
* If you want a catch-all at the end of your mapping, to avoid ever using the model catalog or default catalog, then use `.*` as the pattern. This will match any environment name that hasn't already been matched.


### Auto-categorize model changes

Vulcan compares the current state of project files to an environment when `vulcan plan` is run. It detects changes to models, which can be classified as breaking or non-breaking.

Vulcan can  attempt to automatically [categorize](guides/plan.md#change-categories) the changes it detects. The `plan.auto_categorize_changes` option determines whether Vulcan should attempt automatic change categorization. This option is in the [plan](../configurations-old/configuration.md#plan) section of the configuration reference page.

Supported values:

* `full`: Never prompt the user for input, instead fall back to the most conservative category ([breaking](guides/plan.md#breaking-change)) if the category can't be determined automatically.
* `semi`: Prompt the user for input only if the change category can't be determined automatically.
* `off`: Always prompt the user for input; automatic categorization will not be attempted.

Example showing default values:

=== "YAML"

    ```yaml linenums="1"
    plan:
      auto_categorize_changes:
        external: full
        python: off
        sql: full
        seed: full
    ```

=== "Python"

    The Python `auto_categorize_changes` argument takes `CategorizerConfig` object. That object's arguments take an `AutoCategorizationMode` enumeration with values of `AutoCategorizationMode.FULL`, `AutoCategorizationMode.SEMI`, or `AutoCategorizationMode.OFF`.

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        AutoCategorizationMode,
        CategorizerConfig,
        PlanConfig,
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        plan=PlanConfig(
            auto_categorize_changes=CategorizerConfig(
                external=AutoCategorizationMode.FULL,
                python=AutoCategorizationMode.OFF,
                sql=AutoCategorizationMode.FULL,
                seed=AutoCategorizationMode.FULL,
            )
        ),
    )
    ```


### Always comparing against production

By default, Vulcan compares the current state of project files to the target `<env>` environment when `vulcan plan <env>` is run. However, a common expectation is that local changes should always be compared to the production environment.

The `always_recreate_environment` boolean plan option can alter this behavior. When enabled, Vulcan will always attempt to compare against the production environment by recreating the target environment; If `prod` does not exist, Vulcan will fall back to comparing against the target environment.

**NOTE:**: Upon succesfull plan application, changes are still promoted to the target `<env>` environment.

=== "YAML"

    ```yaml linenums="1"
    plan:
        always_recreate_environment: True
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        PlanConfig,
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        plan=PlanConfig(
            always_recreate_environment=True,
        ),
    )
    ```

#### Change Categorization Example

Consider this scenario with `always_recreate_environment` enabled:

1. Initial state in `prod`:
```sql
MODEL (name vulcan_example.test_model, kind FULL);
SELECT 1 AS col
```

1. First (breaking) change in `dev`:
```sql
MODEL (name vulcan_example__dev.test_model, kind FULL);
SELECT 2 AS col
```

??? "Output plan example #1"

    ```bash
    New environment `dev` will be created from `prod`

    Differences from the `prod` environment:

    Models:
    └── Directly Modified:
        └── vulcan_example__dev.test_model

    ---
    +++


    kind FULL
    )
    SELECT
    -  1 AS col
    +  2 AS col
    ```

3. Second (metadata) change in `dev`:
```sql
MODEL (name vulcan_example__dev.test_model, kind FULL, owner 'John Doe');
SELECT 5 AS col
```

??? "Output plan example #2"

    ```bash
    New environment `dev` will be created from `prod`

    Differences from the `prod` environment:

    Models:
    └── Directly Modified:
        └── vulcan_example__dev.test_model

    ---

    +++

    @@ -1,8 +1,9 @@

    MODEL (
    name vulcan_example.test_model,
    +  owner "John Doe",
    kind FULL
    )
    SELECT
    -  1 AS col
    +  2 AS col

    Directly Modified: vulcan_example__dev.test_model (Breaking)
    Models needing backfill:
    └── vulcan_example__dev.test_model: [full refresh]
    ```

Even though the second change should have been a metadata change (thus not requiring a backfill), it will still be classified as a breaking change because the comparison is against production instead of the previous development state. This is intentional and may cause additional backfills as more changes are accumulated.


### Gateways

The `gateways` configuration defines how Vulcan should connect to the data warehouse, state backend, and scheduler. These options are in the [gateway](../configurations-old/configuration.md#gateway) section of the configuration reference page.

Each gateway key represents a unique gateway name and configures its connections. **Gateway names are case-insensitive** - Vulcan automatically normalizes gateway names to lowercase during configuration validation. This means you can use any case in your configuration files (e.g., `MyGateway`, `mygateway`, `MYGATEWAY`) and they will all work correctly.

For example, this configures the `my_gateway` gateway:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          ...
        state_connection:
          ...
        test_connection:
          ...
        scheduler:
          ...
    ```

=== "Python"

    The Python `gateways` argument takes a dictionary of gateway names and `GatewayConfig` objects. A `GatewayConfig`'s connection-related arguments take an [engine-specific connection config](#engine-connection-configuration) object, and the `scheduler` argument takes a [scheduler config](#scheduler) object.


    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        ...
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=...,
                state_connection=...,
                test_connection=...,
                scheduler=...,
            ),
        }
    )
    ```

Gateways do not need to specify all four components in the example above. The gateway defaults options control what happens if they are not all specified - find more information on [gateway defaults below](#gatewayconnection-defaults).

### Connections

The `connection` configuration controls the data warehouse connection. These options are in the [connection](../configurations-old/configuration.md#connection) section of the configuration reference page.

The allowed keys include:

- The optional `concurrent_tasks` key specifies the maximum number of concurrent tasks Vulcan will run. Default value is 4 for engines that support concurrent tasks.
- Most keys are specific to the connection engine `type` - see [below](#engine-connection-configuration). The default data warehouse connection type is an in-memory DuckDB database.

Example snowflake connection configuration:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: snowflake
          user: <username>
          password: <password>
          account: <account>
    ```

=== "Python"

    A Snowflake connection is specified with a `SnowflakeConnectionConfig` object.

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        SnowflakeConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user=<username>,
                    password=<password>,
                    account=<account>,
                ),
            ),
        }
    )
    ```

#### Engine connection configuration

These pages describe the connection configuration options for each execution engine.

* [Athena](../references/configurations-old/configurations-old/configurations-old/integrations/engines/athena.md)
* [BigQuery](../references/configurations-old/configurations-old/configurations-old/integrations/engines/bigquery.md)
* [Databricks](../references/configurations-old/configurations-old/configurations-old/integrations/engines/databricks.md)
* [DuckDB](../references/configurations-old/configurations-old/configurations-old/integrations/engines/duckdb.md)
* [Fabric](../references/configurations-old/configurations-old/configurations-old/integrations/engines/fabric.md)
* [MotherDuck](../references/configurations-old/configurations-old/configurations-old/integrations/engines/motherduck.md)
* [MySQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mysql.md)
* [MSSQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mssql.md)
* [Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md)
* [GCP Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/gcp-postgres.md)
* [Redshift](../references/configurations-old/configurations-old/configurations-old/integrations/engines/redshift.md)
* [Snowflake](../references/configurations-old/configurations-old/configurations-old/integrations/engines/snowflake.md)
* [Spark](../references/configurations-old/configurations-old/configurations-old/integrations/engines/spark.md)
* [Trino](../references/configurations-old/configurations-old/configurations-old/integrations/engines/trino.md)

#### State connection

Configuration for the state backend connection if different from the data warehouse connection.

The data warehouse connection is used to store Vulcan state if the `state_connection` key is not specified.

Unlike data transformations, storing state information requires database transactions. Data warehouses aren’t optimized for executing transactions, and storing state information in them can slow down your project or produce corrupted data due to simultaneous writes to the same table. Therefore, production Vulcan deployments should use a dedicated state connection.

!!! note
    Using the same connection for data warehouse and state is not recommended for production deployments of Vulcan.

The easiest and most reliable way to manage your state connection is for [Tobiko Cloud](https://tobikodata.com/product.html) to do it for you. If you'd rather handle it yourself, we list recommended and unsupported state engines below.

Recommended state engines for production deployments:

* [Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md)
* [GCP Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/gcp-postgres.md)

Other state engines with fast and reliable database transactions (less tested than the recommended engines):

* [DuckDB](../references/configurations-old/configurations-old/configurations-old/integrations/engines/duckdb.md)
    * With the caveat that it's a [single user](https://duckdb.org/docs/connect/concurrency.html#writing-to-duckdb-from-multiple-processes) database so will not scale to production usage
* [MySQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mysql.md)
* [MSSQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mssql.md)

Unsupported state engines, even for development:

* [ClickHouse](../references/configurations-old/configurations-old/configurations-old/integrations/engines/clickhouse.md)
* [Spark](../references/configurations-old/configurations-old/configurations-old/integrations/engines/spark.md)
* [Trino](../references/configurations-old/configurations-old/configurations-old/integrations/engines/trino.md)

This example gateway configuration uses Snowflake for the data warehouse connection and Postgres for the state backend connection:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          # snowflake credentials here
          type: snowflake
          user: <username>
          password: <password>
          account: <account>
        state_connection:
          # postgres credentials here
          type: postgres
          host: <host>
          port: <port>
          user: <username>
          password: <password>
          database: <database>
    ```

=== "Python"

    A Postgres connection is specified with a `PostgresConnectionConfig` object.

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        PostgresConnectionConfig,
        SnowflakeConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                # snowflake credentials here
                connection=SnowflakeConnectionConfig(
                    user=<username>,
                    password=<password>,
                    account=<account>,
                ),
                # postgres credentials here
                state_connection=PostgresConnectionConfig(
                    host=<host>,
                    port=<port>,
                    user=<username>,
                    password=<password>,
                    database=<database>,
                ),
            ),
        }
    )
    ```

#### State schema name

By default, the schema name used to store state tables is `vulcan`. This can be changed by providing the `state_schema` config key in the gateway configuration.

Example configuration to store state information in a postgres database's `custom_name` schema:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        state_connection:
          type: postgres
          host: <host>
          port: <port>
          user: <username>
          password: <password>
          database: <database>
        state_schema: custom_name
    ```

=== "Python"


    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        PostgresConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                state_connection=PostgresConnectionConfig(
                    host=<host>,
                    port=<port>,
                    user=<username>,
                    password=<password>,
                    database=<database>,
                ),
                state_schema="custom_name",
            ),
        }
    )
    ```

This would create all state tables in the schema `custom_name`.

#### Test connection

Configuration for a connection used to run unit tests. An in-memory DuckDB database is used if the `test_connection` key is not specified.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        test_connection:
          type: duckdb
    ```

=== "Python"

    A DuckDB connection is specified with a `DuckDBConnectionConfig` object. A `DuckDBConnectionConfig` with no arguments specified uses an in-memory DuckDB database.

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                test_connection=DuckDBConnectionConfig(),
            ),
        }
    )
    ```

### Scheduler

Identifies which scheduler backend to use. The scheduler backend is used both for storing metadata and for executing [plans](guides/plan.md). By default, the scheduler type is set to `builtin`, which uses the existing SQL engine to store metadata.

These options are in the [scheduler](../configurations-old/configuration.md#scheduler) section of the configuration reference page.

#### Builtin

Example configuration:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        scheduler:
          type: builtin
    ```

=== "Python"

    A built-in scheduler is specified with a `BuiltInSchedulerConfig` object.

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        BuiltInSchedulerConfig,
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                scheduler=BuiltInSchedulerConfig(),
            ),
        }
    )
    ```

No additional configuration options are supported by this scheduler type.


### Gateway/connection defaults

The default gateway and connection keys specify what should happen when gateways or connections are not explicitly specified. These options are in the [gateway/connection defaults](../configurations-old/configuration.md#gatewayconnection-defaults) section of the configuration reference page.

The gateway specified in `default_gateway` is used when a `vulcan` command does not explicitly specify a gateway. All Vulcan CLI commands [accept a gateway option](getting_started/cli.md#cli) after `vulcan` and before the command name; for example, `vulcan --gateway my_gateway plan`. If the option is not specified in a command call, the `default_gateway` is used.

The three default connection types are used when some gateways in the `gateways` configuration dictionaries do not specify every connection type.

#### Default gateway

If a configuration contains multiple gateways, Vulcan will use the first one in the `gateways` dictionary by default. The `default_gateway` key is used to specify a different gateway name as the Vulcan default.

Example configuration:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        <gateway specification>
    default_gateway: my_gateway
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                <gateway specification>
            ),
        },
        default_gateway="my_gateway",
    )
    ```

#### Default connections/scheduler

The `default_connection`, `default_test_connection`, and `default_scheduler` keys are used to specify shared defaults across multiple gateways.

For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the `default_test_connection` key.

Example configuration specifying a Postgres default connection, in-memory DuckDB default test connection, and builtin default scheduler:

=== "YAML"

    ```yaml linenums="1"
    default_connection:
      type: postgres
      host: <host>
      port: <port>
      user: <username>
      password: <password>
      database: <database>
    default_test_connection:
      type: duckdb
    default_scheduler:
      type: builtin
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        PostgresConnectionConfig,
        DuckDBConnectionConfig,
        BuiltInSchedulerConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        default_connection=PostgresConnectionConfig(
            host=<host>,
            port=<port>,
            user=<username>,
            password=<password>,
            database=<database>,
        ),
        default_test_connection=DuckDBConnectionConfig(),
        default_scheduler=BuiltInSchedulerConfig(),
    )
    ```

### Models

#### Model defaults

The `model_defaults` key is **required** and must contain a value for the `dialect` key. All SQL dialects [supported by the SQLGlot library](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/dialect.py) are allowed. Other values are set automatically unless explicitly overridden in the model definition.

All supported `model_defaults` keys are listed in the [models configuration reference page](../configurations-old/configuration.md#model-defaults).

Example configuration:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      owner: jen
      start: 2022-01-01
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
        model_defaults=ModelDefaultsConfig(
            dialect="snowflake",
            owner="jen",
            start="2022-01-01",
        ),
    )
    ```

The default model kind is `VIEW` unless overridden with the `kind` key. For more information on model kinds, refer to [model concepts page](components/model/model_kinds.md).

##### Identifier resolution

When a SQL engine receives a query such as `SELECT id FROM "some_table"`, it eventually needs to understand what database objects the identifiers `id` and `"some_table"` correspond to. This process is usually referred to as identifier (or name) resolution.

Different SQL dialects implement different rules when resolving identifiers in queries. For example, certain identifiers may be treated as case-sensitive (e.g. if they're quoted), and a case-insensitive identifier is usually either lowercased or uppercased, before the engine actually looks up what object it corresponds to.

Vulcan analyzes model queries so that it can extract useful information from them, such as computing Column-Level Lineage. To facilitate this analysis, it _normalizes_ and _quotes_ all identifiers in those queries, [respecting each dialect's resolution rules](https://sqlglot.com/sqlglot/dialects/dialect.html#Dialect.normalize_identifier).

The "normalization strategy", i.e. whether case-insensitive identifiers are lowercased or uppercased, is configurable per dialect. For example, to treat all identifiers as case-sensitive in a BigQuery project, one can do:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: "bigquery,normalization_strategy=case_sensitive"
    ```

This may be useful in cases where the name casing needs to be preserved, since then Vulcan won't be able to normalize them.

See [here](https://sqlglot.com/sqlglot/dialects/dialect.html#NormalizationStrategy) to learn more about the supported normalization strategies.

##### Gateway-specific model defaults

You can also define gateway specific `model_defaults` in the `gateways` section, which override the global defaults for that gateway.

```yaml linenums="1" hl_lines="6 14"
gateways:
  redshift:
    connection:
      type: redshift
    model_defaults:
      dialect: "snowflake,normalization_strategy=case_insensitive"
  snowflake:
    connection:
      type: snowflake

default_gateway: snowflake

model_defaults:
  dialect: snowflake
  start: 2025-02-05
```

This allows you to tailor the behavior of models for each gateway without affecting the global `model_defaults`.

For example, in some SQL engines identifiers like table and column names are case-sensitive, but they are case-insensitive in other engines. By default, a project that uses both types of engines would need to ensure the models for each engine aligned with the engine's normalization behavior, which makes project maintenance and debugging more challenging.

Gateway-specific `model_defaults` allow you to change how Vulcan performs identifier normalization *by engine* to align the different engines' behavior.

In the example above, the project's default dialect is `snowflake` (line 14). The `redshift` gateway configuration overrides that global default dialect with `"snowflake,normalization_strategy=case_insensitive"` (line 6).

That value tells Vulcan that the `redshift` gateway's models will be written in the Snowflake SQL dialect (so need to be transpiled from Snowflake to Redshift), but that the resulting Redshift SQL should treat identifiers as case-insensitive to match Snowflake's behavior.


#### Model Kinds

Model kinds are required in each model file's `MODEL` DDL statement. They may optionally be used to specify a default kind in the model defaults configuration key.

All model kind specification keys are listed in the [models configuration reference page](../configurations-old/configuration.md#model-kind-properties).

The `VIEW`, `FULL`, and `EMBEDDED` model kinds are specified by name only, while other models kinds require additional parameters and are provided with an array of parameters:

=== "YAML"

    `FULL` model only requires a name:

    ```sql linenums="1"
    MODEL(
      name docs_example.full_model,
      kind FULL
    );
    ```

    `INCREMENTAL_BY_TIME_RANGE` requires an array specifying the model's `time_column` (which should be in the UTC time zone):

    ```sql linenums="1"
    MODEL(
      name docs_example.incremental_model,
      kind INCREMENTAL_BY_TIME_RANGE (
        time_column model_time_column
      )
    );
    ```

Python model kinds are specified with model kind objects. Python model kind objects have the same arguments as their SQL counterparts, listed in the [models configuration reference page](../configurations-old/configuration.md#model-kind-properties).

This example demonstrates how to specify an incremental by time range model kind in Python:

=== "Python"

    ```python linenums="1"
    from vulcan import ExecutionContext, model
    from vulcan.core.model.kind import ModelKindName

    @model(
        "docs_example.incremental_model",
        kind=dict(
            name=ModelKindName.INCREMENTAL_BY_TIME_RANGE,
            time_column="ds"
        )
    )
    ```

Learn more about specifying Python models at the [Python models concepts page](components/model/types/python_models.md#model-specification).


#### Model Naming

The `model_naming` configuration controls if model names are inferred based on the project's directory structure. If `model_naming` is not defined or `infer_names` is set to false, the model names must be provided explicitly.

With `infer_names` set to true, model names are inferred based on their path. For example, a model located at `models/catalog/schema/model.sql` would be named `catalog.schema.model`. However, if a name is provided in the model definition, it will take precedence over the inferred name.

Example enabling name inference:

=== "YAML"

    ```yaml linenums="1"
    model_naming:
      infer_names: true
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, NameInferenceConfig

    config = Config(
        model_naming=NameInferenceConfig(
            infer_names=True
        )
    )
    ```

### Before_all and after_all Statements

The `before_all` and `after_all` statements are executed at the start and end, respectively, of the `vulcan plan` and `vulcan run` commands.

These statements can be defined in the configuration file under the `before_all` and `after_all` keys, either as a list of SQL statements or by using Vulcan macros:

=== "YAML"

    ```yaml linenums="1"
    before_all:
      - CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)
    after_all:
      - "@grant_select_privileges()"
      - "@IF(@this_env = 'prod', @grant_schema_usage())"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config

    config = Config(
        before_all = [
            "CREATE TABLE IF NOT EXISTS analytics (table VARCHAR, eval_time VARCHAR)"
        ],
        after_all = [
            "@grant_select_privileges()",
            "@IF(@this_env = 'prod', @grant_schema_usage())"
        ],
    )
    ```

#### Examples

These statements allow for actions to be executed before all individual model statements or after all have run, respectively. They can also simplify tasks such as granting privileges.

##### Example: Granting Select Privileges

For example, rather than using an `on_virtual_update` statement in each model to grant privileges on the views of the virtual layer, a single macro can be defined and used at the end of the plan:

```python linenums="1"
from vulcan.core.macros import macro

@macro()
def grant_select_privileges(evaluator):
    if evaluator.views:
        return [
            f"GRANT SELECT ON VIEW {view_name} /* sqlglot.meta replace=false */ TO ROLE admin_role;"
            for view_name in evaluator.views
        ]
```

By including the comment `/* sqlglot.meta replace=false */`, you further ensure that the evaluator does not replace the view name with the physical table name during rendering.

##### Example: Granting Schema Privileges

Similarly, you can define a macro to grant schema usage privileges and, as demonstrated in the configuration above, using `this_env` macro conditionally execute it only in the production environment.

```python linenums="1"
from vulcan import macro

@macro()
def grant_schema_usage(evaluator):
    if evaluator.this_env == "prod" and evaluator.schemas:
        return [
            f"GRANT USAGE ON SCHEMA {schema} TO admin_role;"
            for schema in evaluator.schemas
        ]
```

As demonstrated in these examples, the `schemas`  and `views` are available within the macro evaluator for macros invoked within the `before_all` and `after_all` statements. Additionally, the macro `this_env` provides access to the current environment name, which can be helpful for more advanced use cases that require fine-grained control over their behaviour.

### Linting

Vulcan provides a linter that checks for potential issues in your models' code. Enable it and specify which linting rules to apply in the configuration file's `linter` key.

Learn more about linting configuration in the [linting guide](./linter.md).

### Debug mode

To enable debug mode set the `VULCAN_DEBUG` environment variable to one of the following values: "1", "true", "t", "yes" or "y".

Enabling this mode ensures that full backtraces are printed when using CLI. The default log level is set to `DEBUG` when this mode is enabled.

Example enabling debug mode for the CLI command `vulcan plan`:

=== "Bash"

    ```bash
    $ VULCAN_DEBUG=1 vulcan plan
    ```

=== "MS Powershell"

    ```powershell
    PS> $env:VULCAN_DEBUG=1
    PS> vulcan plan
    ```

=== "MS CMD"

    ```cmd
    C:\> set VULCAN_DEBUG=1
    C:\> vulcan plan
    ```


### Python library dependencies
Vulcan enables you to write Python models and macros which depend on third-party libraries. To ensure each run / evaluation uses the same version, you can specify versions in a `vulcan-requirements.lock` file in the root of your project.

The vulcan.lock must be of the format `dep==version`. Only `==` is supported.

For example:

```
numpy==2.1.2
pandas==2.2.3
```

This feature is only available in [Tobiko Cloud](https://tobikodata.com/product.html).

#### Excluding dependencies

You can exclude dependencies by prefixing the dependency with a `^`. For example:

```
^numpy
pandas==2.2.3
```



# Connections

Source: https://tmdc-io.github.io/vulcan-book/guides-old/connections/

---

# Connections

## Overview

In order to deploy models and to apply changes to them, you must configure a connection to your Data Warehouse and, optionally, connection to the database where the Vulcan state is stored. This can be done in either the `config.yaml` file in your project folder, or the one in `~/.vulcan`.

Each connection is configured as part of a gateway which has a unique name associated with it. The gateway name can be used to select a specific combination of connection settings  when using the CLI. For example:

```yaml linenums="1"
gateways:
  local_db:
    connection:
      type: duckdb
```

Now the defined connection can be selected in the `vulcan plan` CLI command as follows:

```bash
vulcan --gateway local_db plan
```

## State connection

By default, the data warehouse connection is also used to store the Vulcan state.

The state connection can be changed by providing different connection settings in the `state_connection` key of the gateway configuration:

```yaml linenums="1"
gateways:
  local_db:
    state_connection:
      type: duckdb
      database: state.db
```

NOTE: Spark and Trino engines may not be used for the state connection.

## Default connection

Additionally, you can set a default connection by defining its configuration in the `default_connection` key:

```yaml linenums="1"
default_connection:
  type: duckdb
  database: local.db
```

This connection configuration will be used if one is not provided in the target gateway.

## Test connection

By default, when running [tests](components/tests/tests.md), Vulcan uses an in-memory DuckDB database connection. You can override this behavior by providing connection settings in the `test_connection` key of the gateway configuration:

```yaml linenums="1"
gateways:
  local_db:
    test_connection:
      type: duckdb
      database: test.db
```

### Default test connection

To configure a default test connection for all gateways use the `default_test_connection` key:

```yaml linenums="1"
default_test_connection:
  type: duckdb
  database: test.db
```

## Default gateway

To change the default gateway used by the CLI when no gateway name is provided, set the desired name in the `default_gateway` key:

```yaml linenums="1"
default_gateway: local_db
```

## Supported engines

* [BigQuery](../references/configurations-old/configurations-old/configurations-old/integrations/engines/bigquery.md)
* [Databricks](../references/configurations-old/configurations-old/configurations-old/integrations/engines/databricks.md)
* [DuckDB](../references/configurations-old/configurations-old/configurations-old/integrations/engines/duckdb.md)
* [MotherDuck](../references/configurations-old/configurations-old/configurations-old/integrations/engines/motherduck.md)
* [MySQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mysql.md)
* [MSSQL](../references/configurations-old/configurations-old/configurations-old/integrations/engines/mssql.md)
* [Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md)
* [GCP Postgres](../references/configurations-old/configurations-old/configurations-old/integrations/engines/gcp-postgres.md)
* [Redshift](../references/configurations-old/configurations-old/configurations-old/integrations/engines/redshift.md)
* [Snowflake](../references/configurations-old/configurations-old/configurations-old/integrations/engines/snowflake.md)
* [Spark](../references/configurations-old/configurations-old/configurations-old/integrations/engines/spark.md)
* [Trino](../references/configurations-old/configurations-old/configurations-old/integrations/engines/trino.md)



# Customizing Vulcan

Source: https://tmdc-io.github.io/vulcan-book/guides-old/customizing_vulcan/

---

# Customizing Vulcan

Vulcan supports the workflows used by the vast majority of data engineering teams. However, your company may have bespoke processes or tools that require special integration with Vulcan.

Fortunately, Vulcan is an open-source Python library, so you can view its underlying code and customize it for your needs.

Customization generally involves subclassing Vulcan classes to extend or modify their functionality.

!!! danger "Caution"

    Customize Vulcan with extreme caution. Errors may cause Vulcan to produce unexpected results.

## Custom loader

Loading is the process of reading project files and converting their contents into Vulcan's internal Python objects.

The loading stage is a convenient place to customize Vulcan behavior because you can access a project's objects after they've been ingested from file but before Vulcan uses them.

Vulcan's `VulcanLoader` class handles the loading process - customize it by subclassing it and overriding its methods.

!!! note "Python configuration only"

    Custom loaders require using the [Python configuration format](guides-old/configuration.md#python) (YAML is not supported).

### Modify every model

One reason to customize the loading process is to do something to every model. For example, you might want to add a post-statement to every model.

The loading process parses all model SQL statements, so new or modified SQL must be parsed by SQLGlot before being passed to a model object.

This custom loader example adds a post-statement to every model:

``` python linenums="1" title="config.py"
from vulcan.core.loader import VulcanLoader
from vulcan.utils import UniqueKeyDict
from vulcan.core.dialect import parse_one
from vulcan.core.config import Config

# New `CustomLoader` class subclasses `VulcanLoader`
class CustomLoader(VulcanLoader):
    # Override VulcanLoader's `_load_models` method to access every model
    def _load_models(
        self,
        macros: "MacroRegistry",
        jinja_macros: "JinjaMacroRegistry",
        gateway: str | None,
        audits: UniqueKeyDict[str, "ModelAudit"],
        signals: UniqueKeyDict[str, "signal"],
    ) -> UniqueKeyDict[str, "Model"]:
        # Call VulcanLoader's normal `_load_models` method to ingest models from file and parse model SQL
        models = super()._load_models(macros, jinja_macros, gateway, audits, signals)

        new_models = {}
        # Loop through the existing model names/objects
        for model_name, model in models.items():
            # Create list of existing and new post-statements
            new_post_statements = [
                # Existing post-statements from model object
                *model.post_statements,
                # New post-statement is raw SQL, so we parse it with SQLGlot's `parse_one` function.
                # Make sure to specify the SQL dialect if different from the project default.
                parse_one(f"VACUUM @this_model"),
            ]
            # Create a copy of the model with the `post_statements_` field updated
            new_models[model_name] = model.copy(update={"post_statements_": new_post_statements})

        return new_models

# Pass the CustomLoader class to the Vulcan configuration object
config = Config(
    # < your configuration parameters here >,
    loader=CustomLoader,
)
```


# Isolated systems

Source: https://tmdc-io.github.io/vulcan-book/guides-old/isolated_systems/

---

# Isolated systems

Vulcan is optimized for use in systems where developers have access to production data.

Writing code against partial or unrepresentative data can cause problems because you don't become aware of changes in production data until errors have already occurred.

Other data products, such as machine learning models, may depend on the distribution of values in the training data - building them on unrepresentative data may lead to different behavior in production than in development.

However, some companies store production and non-production data in different data warehouses that can't talk to one another ("isolated systems"). This is usually due to information security concerns, as the non-production warehouse may be accessible to more users and/or have looser security restrictions.

This guide explains how to use Vulcan with isolated systems and how isolating systems affects Vulcan's behavior.

## Terminology

Isolated systems are sometimes referred to as "isolated environments," but we avoid that term because "environments" has a specific meaning in Vulcan.

Instead, we will refer to them as isolated systems - the "production system" and "non-production system."

When we refer to "environments," we are always talking about [Vulcan environments](concepts-old/environments.md) - the isolated namespaces created and managed by Vulcan.

## Configuring Vulcan

### Separate state data

Vulcan maintains a record of every model version so it can identify changes when models are updated. Those records are called "state" data, as in "the state of the model at that point in time."

State data can be stored alongside other data in the primary data warehouse or in a [separate database](guides-old/configuration.md#state-connection). We recommend using a separate transactional database for projects running on cloud SQL engines.

Isolated systems must use a **separate** state database for each system. The state of models and other objects in the non-production system is not accurate for the production system, and sharing state data will prevent the project from running correctly.

### Multiple gateways

Vulcan database connections are configured with [gateways](guides-old/configuration.md#gateways) that contain [connections](guides-old/connections.md) and other configuration parameters.

A gateway must contain a connection to a SQL engine and may optionally contain a different connection to the database where Vulcan should store its state data.

Isolated systems should configure two separate gateways: one for the production system and one for the non-production system.

For example, this configuration creates gateways named `nonproduction` and `production`. You may omit the `state_connection` keys if state data will be stored in the gateway's primary connection.

```yaml linenums="1"
gateways:
  nonproduction:
    connection:
      ...[your non-production connection parameters]...
    state_connection:
      ...[your non-production state connection parameters]...
  production:
    connection:
      ...[your production connection parameters]...
    state_connection:
      ...[your production state connection parameters]...
```

Vulcan will use the first gateway in the configuration as the default when executing a command. For example, with the configuration above Vulcan would use the `nonproduction` gateway when executing the command `vulcan plan`.

Commands can override the default gateway with the `--gateway` option, such as `vulcan --gateway production plan`.

### Gateway-specific schemas

We recommend using identical schema and model names in both systems, but in some scenarios that is not possible.

Schema and model names may be parameterized by gateway using the predefined [`@gateway` macro variable](components/advanced-features/macros/variables.md#runtime-variables).

This example demonstrates conditioning the model schema name on the current gateway with the Vulcan [`@IF` macro operator](components/advanced-features/macros/built_in.md#if). If the gateway is named `production`, `my_model`'s schema is `prod_schema`; otherwise, it is `dev_schema`.

```sql linenums="1"
MODEL (
  name @IF(@gateway = 'production', prod_schema, dev_schema).my_model
)
```

To embed the gateway name directly in the schema name, use the curly brace `@{gateway}` syntax:

```sql linenums="1"
MODEL (
  name @{gateway}_schema.my_model
)
```

Learn more about the curly brace `@{}` syntax [here](components/advanced-features/macros/built_in.md#embedding-variables-in-strings).

## Workflow

### Linking systems

The point of isolating systems is to prevent sharing of data by limiting network communications between the systems. Given this, how can a Vulcan project be shared between them at all?

The Vulcan project files provide the link between the systems. The files should be stored in a mutually accessible location, such as a git repository.

![Vulcan project files link systems](./isolated_systems/isolated-systems_linkage.png)

### Workflow with one system

This section describes workflows for updating Vulcan projects with one system.

We assume that a version of the Vulcan project is currently running in production and serves as the starting point for code modifications.

#### Basic workflow

Use this workflow if your data system does not use CI/CD to implement changes:

- Make a change to a model
- Run `vulcan plan dev` (or another environment name) to preview the changes in a local environment
- Run `vulcan plan` to apply the changes to the `prod` environment

#### CI/CD workflow

Use this workflow with the Vulcan Github CI/CD bot:

- `git clone` the project repo
- Make a change to a model in a git branch
- Push the branch to the project repo and make a pull request. The bot will create a development environment for you to preview the changes if it is configured for synchronized deployments.
- Merge the branch into `main` to apply the changes to the `prod` environment

Learn more about synchronized and desynchronized deployments in the CI/CD bot documentation.

#### Reusing computations

Local environment previews are computed on the same data used by the `prod` environment in these workflows, so applying the changes to `prod` reuses the preview computations and only requires a virtual update.

### Workflow with isolated systems

This section describes the workflow with isolated systems.

This workflow combines the basic and CI/CD workflows above, where the basic workflow is used in the non-production system and the CI/CD workflow is used in the production system:

- `git clone` the project repo
- Make a change to a model in a git branch
- Run `vulcan plan dev` (or another environment name) to preview the changes in the nonproduction system. You may need to include the nonproduction `--gateway` option, depending on your project configuration.
- Push the branch to the project repo and make a pull request. The bot will create an environment to preview the changes in the production system if it is configured for synchronized deployments.
- Merge the branch into `main` to apply the changes to the `prod` environment

The breaking/non-breaking change classifications in the non-production system will not be available to the production system because the systems do not share Vulcan state data. Therefore, the classifications must occur again in the production system.

#### Reusing computations

In isolated systems, Vulcan's virtual data environments operate normally *within* each system, but not across systems.

In the non-production system, computations will be reused across preview environments. However, the system's data are not representative of the production data and will not be reused by the production system.

In the production system, the CI/CD bot will execute the necessary computations when a pull request is submitted if it is configured for synchronized deployment. Merging to main and applying the changes to `prod` reuses the preview computations and only requires a virtual update.

This approach enables true [blue-green deployment](https://en.m.wikipedia.org/wiki/Blue%E2%80%93green_deployment). Deploying to production occurs with no system downtime because virtual updates only require swapping views. If issues are identified after changes have been pushed to production, reverting is quick and painless because it just swaps the views back.


# Migrations

Source: https://tmdc-io.github.io/vulcan-book/guides-old/migrations/

---

# Migrations

New versions of Vulcan may be incompatible with the project's stored metadata format. Migrations provide a way to upgrade the project metadata format to operate with the new Vulcan version.

## Detecting incompatibility
When issuing a Vulcan command, Vulcan will automatically check for incompatibilities between the installed version of Vulcan and the project's metadata format, prompting what action is required. Vulcan commands will not execute until the action is complete.

### Installed version is newer than metadata format
In this scenario, the project's metadata format needs to be migrated.

```bash
> vulcan plan my_dev
Error: Vulcan (local) is using version '2' which is ahead of '1' (remote). Please run a migration ('vulcan migrate' command).
```

### Installed version is older than metadata format
Here, the installed version of Vulcan needs to be upgraded.

```bash
> vulcan plan my_dev
VulcanError: Vulcan (local) is using version '1' which is behind '2' (remote). Please upgrade Vulcan.
```

## How to migrate

### Built-in Scheduler Migrations

The project metadata can be migrated to the latest metadata format using Vulcan's migrate command.

```bash
> vulcan migrate
```

Migration should be issued manually by a single user and the migration will affect all users of the project. 
Migrations should ideally run when no one will be running plan/apply. 
Migrations should not be run in parallel. 
Due to these constraints, it is better for a person responsible for managing Vulcan to manually issue migrations. 
Therefore, it is not recommended to issue migrations from CI/CD pipelines.



# Notifications

Source: https://tmdc-io.github.io/vulcan-book/guides-old/notifications/

---

# Notifications

Vulcan can send notifications via Slack or email when certain events occur. This page describes how to configure notifications and specify recipients.

## Notification targets

Notifications are configured with `notification targets`. Targets are specified in a project's [configuration](https://vulcan.readthedocs.io/en/stable/reference/configuration/) file (`config.yml` or `config.py`), and multiple targets can be specified for a project.

A project may specify both global and user-specific notifications. Each target's notifications will be sent for all instances of each [event type](#vulcan-event-types) (e.g., notifications for `run` will be sent for *all* of the project's environments), with exceptions for audit failures and when an [override is configured for development](#notifications-during-development).

[Audit](components/audits/audits.md) failure notifications can be sent for specific models if five conditions are met:

1. A model's `owner` field is populated
2. The model executes one or more audits
3. The owner has a user-specific notification target configured
4. The owner's notification target `notify_on` key includes audit failure events
5. The audit fails in the `prod` environment

When those conditions are met, the audit owner will be notified if their audit failed in the `prod` environment.

There are three types of notification target, corresponding to the two [Slack notification methods](#slack-notifications) and [email notification](#email-notifications). They are specified in either a specific user's `notification_targets` key or the top-level `notification_targets` configuration key.

This example shows the location of both user-specific and global notification targets:

=== "YAML"

    ```yaml linenums="1"
    # User notification targets
    users:
      - username: User1
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...
      - username: User2
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...

    # Global notification targets
    notification_targets:
      - notification_target_1
        ...
      - notification_target_2
        ...
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        ...,
        # User notification targets
        users=[
            User(
                username="User1",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            ),
            User(
                username="User2",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            )
        ],

        # Global notification targets
        notification_targets=[
            notification_target_1(...),
            notification_target_2(...),
        ],
        ...
    )
    ```

### Notifications During Development

Events triggering notifications may be executed repeatedly during code development. To prevent excessive notification, Vulcan can stop all but one user's notification targets.

Specify the top-level `username` configuration key with a value also present in a user-specific notification target's `username` key to only notify that user. This key can be specified in either the project configuration file or a machine-specific configuration file located in `~/.vulcan`. The latter may be useful if a specific machine is always used for development.

This example stops all notifications other than those for `User1`:

=== "YAML"

    ```yaml linenums="1" hl_lines="1-2"
    # Top-level `username` key: only notify User1
    username: User1
    # User1 notification targets
    users:
      - username: User1
        ...
        notification_targets:
          - notification_target_1
            ...
          - notification_target_2
            ...
    ```

=== "Python"

    ```python linenums="1" hl_lines="3-4"
    config = Config(
        ...,
        # Top-level `username` key: only notify User1
        username="User1",
        users=[
            User(
                # User1 notification targets
                username="User1",
                notification_targets=[
                    notification_target_1(...),
                    notification_target_2(...),
                ],
            ),
        ]
    )
    ```

## Vulcan Event Types

Vulcan notifications are triggered by events. The events that should trigger a notification are specified in the notification target's `notify_on` field.

Notifications are supported for [`plan` application](guides/plan.md) start/end/failure, [`run`](getting_started/cli.md#run) start/end/failure, and [`audit`](components/audits/audits.md) failures.

For `plan` and `run` start/end, the target environment name is included in the notification message. For failures, the Python exception or error text is included in the notification message.

This table lists each event, its associated `notify_on` value, and its notification message:

| Event                         | `notify_on` Key Value  | Notification message                                     |
| ----------------------------- | ---------------------- | -------------------------------------------------------- |
| Plan application start        | apply_start            | "Plan apply started for environment `{environment}`."    |
| Plan application end          | apply_end              | "Plan apply finished for environment `{environment}`."   |
| Plan application failure      | apply_failure          | "Failed to apply plan.\n{exception}"                     |
| Vulcan run start             | run_start              | "Vulcan run started for environment `{environment}`."   |
| Vulcan run end               | run_end                | "Vulcan run finished for environment `{environment}`."  |
| Vulcan run failure           | run_failure            | "Failed to run Vulcan.\n{exception}"                    |
| Audit failure                 | audit_failure          | "{audit_error}"                                          |

Any combination of these events can be specified in a notification target's `notify_on` field.

## Slack Notifications

Vulcan supports two types of Slack notification. Slack webhooks can notify a Slack channel, but they cannot message specific users. The Slack Web API can notify channels or users.

### Webhook Configuration

Vulcan uses Slack's "Incoming Webhooks" for webhook notifications. When you [create an incoming webhook](https://api.slack.com/messaging/webhooks) in Slack, you will receive a unique URL associated with a specific Slack channel. Vulcan transmits the notification message by submitting a JSON payload to that URL.

This example shows a Slack webhook notification target. Notifications are triggered by plan application start, plan application failure, or Vulcan run start. The specification uses an environment variable `SLACK_WEBHOOK_URL` instead of hard-coding the URL directly into the configuration file:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: slack_webhook
        notify_on:
          - apply_start
          - apply_failure
          - run_start
        url: "{{ env_var('SLACK_WEBHOOK_URL') }}"
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        SlackWebhookNotificationTarget(
            notify_on=["apply_start", "apply_failure", "run_start"],
            url=os.getenv("SLACK_WEBHOOK_URL"),
        )
    ]
    ```

### API Configuration

If you want to notify users, you can use the Slack API notification target. This requires a Slack API token, which can be used for multiple notification targets with different channels or users. See [Slack's official documentation](https://api.slack.com/tutorials/tracks/getting-a-token) for information on getting an API token.

This example shows a Slack API notification target. Notifications are triggered by plan application start, plan application end, or audit failure. The specification uses an environment variable `SLACK_API_TOKEN` instead of hard-coding the token directly into the configuration file:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: slack_api
        notify_on:
          - apply_start
          - apply_end
          - audit_failure
        token: "{{ env_var('SLACK_API_TOKEN') }}"
        channel: "UXXXXXXXXX"  # Channel or a user's Slack member ID
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        SlackApiNotificationTarget(
            notify_on=["apply_start", "apply_end", "audit_failure"],
            token=os.getenv("SLACK_API_TOKEN"),
            channel="UXXXXXXXXX",  # Channel or a user's Slack member ID
        )
    ]
    ```

## Email Notifications

Vulcan supports notifications via email. The notification target specifies the SMTP host, user, password, and sender address. A target may notify multiple recipient email addresses.

This example shows an email notification target, where `sushi@example.com` emails `data-team@example.com` on Vulcan run failure. The specification uses environment variables `SMTP_HOST`, `SMTP_USER`, and `SMTP_PASSWORD` instead of hard-coding the values directly into the configuration file:

=== "YAML"

    ```yaml linenums="1"
    notification_targets:
      - type: smtp
        notify_on:
          - run_failure
        host: "{{ env_var('SMTP_HOST') }}"
        user: "{{ env_var('SMTP_USER') }}"
        password: "{{ env_var('SMTP_PASSWORD') }}"
        sender: sushi@example.com
        recipients:
          - data-team@example.com
    ```

=== "Python"

    ```python linenums="1"
    notification_targets=[
        BasicSMTPNotificationTarget(
            notify_on=["run_failure"],
            host=os.getenv("SMTP_HOST"),
            user=os.getenv("SMTP_USER"),
            password=os.getenv("SMTP_PASSWORD"),
            sender="notifications@example.com",
            recipients=[
                "data-team@example.com",
            ],
        )
    ]
    ```

## Advanced Usage

### Overriding Notification Targets

In Python configuration files, new notification targets can be configured to send custom messages.

To customize a notification, create a new notification target class as a subclass of one of the three target classes described above (`SlackWebhookNotificationTarget`, `SlackApiNotificationTarget`, or `BasicSMTPNotificationTarget`). See the definitions of these classes on Github [here](https://github.com/TobikoData/vulcan/blob/main/vulcan/core/notification_target.py).

Each of those notification target classes is a subclass of `BaseNotificationTarget`, which contains a `notify` function corresponding to each event type. This table lists the notification functions, along with the contextual information available to them at calling time (e.g., the environment name for start/end events):

| Function name        | Contextual information           |
| -------------------- | -------------------------------- |
| notify_apply_start   | Environment name: `env`          |
| notify_apply_end     | Environment name: `env`          |
| notify_apply_failure | Exception stack trace: `exc`     |
| notify_run_start     | Environment name: `env`          |
| notify_run_end       | Environment name: `env`          |
| notify_run_failure   | Exception stack trace: `exc`     |
| notify_audit_failure | Audit error trace: `audit_error` |

This example creates a new notification target class `CustomSMTPNotificationTarget`.

It overrides the default `notify_run_failure` function to read a log file `"/home/vulcan/vulcan.log"` and append its contents to the exception stack trace `exc`:

=== "Python"

```python
from vulcan.core.notification_target import BasicSMTPNotificationTarget

class CustomSMTPNotificationTarget(BasicSMTPNotificationTarget):
    def notify_run_failure(self, exc: str) -> None:
        with open("/home/vulcan/vulcan.log", "r", encoding="utf-8") as f:
            msg = f"{exc}\n\nLogs:\n{f.read()}"
        super().notify_run_failure(msg)
```

Use this new class by specifying it as a notification target in the configuration file:

=== "Python"

    ```python linenums="1" hl_lines="2"
    notification_targets=[
        CustomSMTPNotificationTarget(
            notify_on=["run_failure"],
            host=os.getenv("SMTP_HOST"),
            user=os.getenv("SMTP_USER"),
            password=os.getenv("SMTP_PASSWORD"),
            sender="notifications@example.com",
            recipients=[
                "data-team@example.com",
            ],
        )
    ]
    ```


# Projects

Source: https://tmdc-io.github.io/vulcan-book/guides-old/projects/

---

# Projects

## Creating a project

---

Before getting started, ensure that you meet the [prerequisites](../getting_started/prerequisites.md) for using Vulcan.

---

To create a project from the command line, follow these steps:

1. Create a directory for your project:

    ```bash
    mkdir my-project
    ```

2. Change directories into your new project:

    ```bash
    cd my-project
    ```

    From here, you can create your project structure from scratch, or Vulcan can scaffold one for you. For the purposes of this guide, we'll show you how to scaffold your project so that you can get up and running quickly.

1. To scaffold a project, it is recommended that you use a python virtual environment by running the following commands:

    ```bash
    python -m venv .venv
    ```

    ```bash
    source .venv/bin/activate
    ```

    ```bash
    pip install vulcan
    ```

    **Note:** When using a python virtual environment, you must ensure that it is activated first. You should see `(.venv)` in your command line; if you don't, run `source .venv/bin/activate` from your project directory to activate your environment.

1. Once you have activated your environment, run the following command and Vulcan will build out your project:

    ```bash
    vulcan init [SQL_DIALECT]
    ```

    In the command above, you can use any [SQL dialect supported by sqlglot](https://sqlglot.com/sqlglot/dialects.html), for example "duckdb".

    The following directories and files will be created that you can use to organize your Vulcan project:

    - config.py (database configuration file)
    - ./models (SQL and Python models)
    - ./audits (shared audits)
    - ./tests (unit tests)
    - ./macros

## Editing an existing project

To edit an existing project, open the project file you wish to edit in your preferred editor.

If using CLI, you can open a file in your project for editing by using the `vulcan` command with the `-p` variable, and pointing to your project's path as follows:

```bash
vulcan -p <your-project-path>
```

For more details, refer to [CLI](getting_started/cli.md)




# Table migration

Source: https://tmdc-io.github.io/vulcan-book/guides-old/table_migration/

---

# Table migration

Vulcan projects can read directly from tables not managed by Vulcan, but in some scenarios it may be useful to migrate an existing table into a Vulcan project.

This guide describes two methods for migrating existing tables into a Vulcan project.

## Do you need to migrate?

Vulcan does not assume it manages all data sources: SQL models can read from any data source accessible by the SQL engine, treating them as [external models](components/model/model_kinds.md#external) that include column-level lineage or as generic sources. This approach is preferred to migrating existing tables into a Vulcan project.

You should only migrate a table if both of the following are true:

1. The table is ingesting from an upstream source that will continue generating new data
2. The table is either too large to be rebuilt or cannot be rebuilt because the necessary historical data is unavailable

If the table's upstream source will not generate more data, there is no ongoing activity for Vulcan to manage. A Vulcan model or any other downstream consumer can select directly from the table under its current name.

If the table's upstream source is generating new data, we assume that the table is already being loaded incrementally, as there is no need for migration if the table can be fully rebuilt.

We describe two migration methods below. The stage and union method is preferred and should be used if feasible.

## Migration methods

This section describes two methods for migrating tables into Vulcan.

The method descriptions contain renaming steps that are only necessary if downstream consumers must select from the original table name (e.g., step 2 in the first example). If that is not the case, the original table can retain its name.

The table and model names in the examples below are arbitrary - you may name them whatever is appropriate for your project.

### Stage and union

The stage and union method works by treating new and historical data as separate sources.

It requires creating an incremental staging model to ingest new records and a `VIEW` model that unions those records with the existing table's static historical records.

#### Example

Consider an existing table named `my_schema.existing_table`. Migrating this table with the stage and union method consists of five steps:

1. Ensure `my_schema.existing_table` is up to date (has ingested all available source data)
2. Rename `my_schema.existing_table` to any other name, such as `my_schema.existing_table_historical`
    - Optionally, enable column-level lineage for the table by making it an [`EXTERNAL` model](components/model/model_kinds.md#external) and adding it to the project's `external_models.yaml` file
3. Create a new incremental staging model named `my_schema.existing_table_staging` (see below for code)
4. Create a new [`VIEW` model](components/model/model_kinds.md#view) named `my_schema.existing_table` (see below for code)
5. Run `vulcan plan` to create and backfill the models

The staging model would contain code similar to the following for an `INCREMENTAL_BY_TIME_RANGE` model. An `INCREMENTAL_BY_UNIQUE_KEY` model would have a different `kind` specification in the `MODEL` DDL and might not include the query's `WHERE` clause.

``` sql linenums="1"
MODEL(
  name my_schema.existing_table_staging,
  kind INCREMENTAL_BY_TIME_RANGE ( -- or INCREMENTAL_BY_UNIQUE_KEY
    time_column table_time_column
  )
);

SELECT
  col1,
  col2,
  col3
FROM
  [your model's ongoing data source]
WHERE
  table_time_column BETWEEN @start_ds and @end_ds;
```

The primary model would contain code similar to:

``` sql linenums="1"
MODEL(
  name my_schema.existing_table,
  kind VIEW
)

SELECT
  col1,
  col2,
  col3
FROM
  my_schema.existing_table_staging -- New data
UNION
SELECT
  col1,
  col2,
  col3
FROM
  my_schema.existing_table_historical; -- Historical data
```

Changes to columns in the source data or staging model may require modifying the code selecting from the historical data so the two tables can be safely unioned.

### Snapshot replacement

The snapshot replacement method works by renaming an existing table to a name that Vulcan recognizes as an existing Vulcan model.

#### Background

This section briefly describes how Vulcan's virtual data environments, forward-only models, and start times work. This information is not necessary for migrating tables but is necessary for understanding why each step in the migration process is required.

##### Virtual data environments

Conceptually, Vulcan divides the database into a "physical layer" where data is stored and a "virtual layer" where data is accessed by end users. The physical layer stores materialized objects like tables, and the virtual layer contains views that point to the physical layer objects.

Each time a Vulcan `plan` adds or modifies a model, Vulcan creates a physical layer "snapshot" object to which the virtual layer view points. The snapshot replacement method simply renames the migrating table to the name of the appropriate snapshot table.

##### Forward-only models

Sometimes a model's data may be so large that it is not feasible to rebuild either its own or its downstream models' physical tables. In those situations a  "forward only" model can be used. The name reflects that the change is only applied "going forward" in time.

Historical data already in the migrated table should not be overwritten, so we specify that the new model is forward-only in step 3a below.

##### Start time

Vulcan incremental by time models track the time periods whose data a model has loaded with the [interval approach](https://vulcan.readthedocs.io/en/stable/guides/incremental_time/#counting-time).

The interval approach requires specifying the earliest time interval Vulcan should track - when time "starts" for the model. For migrated tables, Vulcan should never load data for the time intervals the table ingested before migration, so interval tracking should start immediately after the time of the last ingested record.

In the example below, we set the model's start time in its `MODEL` DDL (step 3b) and pass it as an option to the `vulcan plan` command (step 3c). The same value must be used in both the `MODEL` DDL and the plan command. In this example, the existing table's data ingestion stopped on 2023-12-31, so the model and plan start date is the next day 2024-01-01.

#### Example

Consider an existing table named `my_schema.existing_table`. Migrating this table with the snapshot replacement method involves five steps:

1. Ensure `my_schema.existing_table` is up to date (has ingested all available source data)
2. Rename `my_schema.existing_table` to any other name, such as `my_schema.existing_table_temp`
3. Create and initialize an empty incremental model named `my_schema.existing_table`:

    a. Make the model [forward only](guides-old/incremental_time.md#forward-only-models) by setting the `MODEL` DDL `kind`'s `forward_only` key to `true`

    b. Specify the start of the first time interval Vulcan should track in the `MODEL` DDL `start` key (example uses "2024-01-01")

    c. Create the model in the Vulcan project without backfilling any data by running `vulcan plan [environment name] --empty-backfill --start 2024-01-01`, replacing "[environment name]" with an environment name other than `prod` and using the same start date from the `MODEL` DDL in step 3b.

4. Determine the name of the model's snapshot physical table by running `vulcan table_name --env [environment name] --prod my_schema.existing_table`. For example, it might return `vulcan__my_schema.existing_table_123456`.
5. Rename the original table `my_schema.existing_table_temp` to `vulcan__my_schema.existing_table_123456`

The model would have code similar to:

``` sql linenums="1" hl_lines="5 7-9"
MODEL(
  name my_schema.existing_table,
  kind INCREMENTAL_BY_TIME_RANGE( -- or INCREMENTAL_BY_UNIQUE_KEY
    time_column table_time_column,
    forward_only true -- Forward-only model
  ),
  -- Start of first time interval Vulcan should track, immediately
  --  after the last data point the table ingested. Must match
  --  the value passed to the `vulcan plan --start` option.
  start "2024-01-01"
)

SELECT
  col1,
  col2,
  col3
FROM
  [your model's ongoing data source]
WHERE
  table_time_column BETWEEN @start_ds and @end_ds;
```



# About

Source: https://tmdc-io.github.io/vulcan-book/

---

# About

Stop guessing why your production pipeline failed at 3 AM. Vulcan is a complete stack for building data products that gives you visibility into what's happening at every step, with built-in validation that blocks bad data before it hits production.

You write SQL or Python models in your Vulcan project. Most teams start with SQL for transformations, then add Python models when they hit logic that's painful to express in SQL: calling external APIs, running machine learning models, or handling complex business rules. Mix both in the same project. Vulcan handles CI/CD, testing, data quality, and API generation. Same stack whether you're processing thousands or billions of rows.

```mermaid
graph LR
    subgraph VulcanStack [THE VULCAN STACK]
        Start[Write SQL or Python Models] --> Linter[Linter: Code Safety]
        
        Linter --> ModelCore((Model Definition))
        
        ModelCore --> Tests[Automated Tests<br/>validate if desired output<br/>can be achieved]
        Tests --> Signals[Signals<br/>validate if dependencies<br/>are met]
        Signals --> ModelCore
        
        ModelCore --> AC[Assertions & Checks<br/>validate data quality]
        AC --> Semantics[Semantics: <br> Dimensions & Metrics]
    end

    Semantics --> APIs[Automatic APIs: <br> REST & Graph]
    Semantics --> Warehouse[(Postgres or Snowflake)]
    
    APIs --> Products[Production Data Products]
    Warehouse --> Products

    style VulcanStack fill:#f0f4f8,stroke:#01579b,stroke-width:2px
    style Products fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px
    style Semantics fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
    style ModelCore fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
```
<details>
<summary>How to read this diagram</summary>

Read the diagram left to right. Here's what each part does:
    <br>
- Write SQL or Python models in your `models/` directory. Most teams start with SQL, then add Python when they hit logic that's painful in SQL: API calls, machine learning, or complex business rules.
    <br>
- Linter catches syntax errors before your code runs. This stops bad SQL or Python from hitting your warehouse.
    <br>
- Model Definition is where your validated model becomes a defined entity. This is the central hub where the validation loop happens.
    <br>
- Automated Tests run first to check if your model can produce the desired output. This happens before any data processing.
    <br>
- Signals check if all dependencies are met. If a model depends on upstream data or external services, signals verify they're available. Only then does the loop feed back to Model Definition.
    <br>
- Assertions and Checks validate data quality. Bad data gets blocked here, before it reaches your semantic layer or warehouse.
    <br>
- Semantics layer only powers up after models pass validation. This is where you define business metrics and dimensions. It's your single source of truth, but only because everything feeding it has been validated.
    <br>
- Automatic APIs generate from your semantic layer. REST, Python, and Graph APIs appear without manual code. Business users get self-service analytics, developers get programmatic access.
    <br>
- Postgres or Snowflake receives error-free data. The validation loop ensures only clean data reaches production.
    <br>
- Production Data Products are what business users and developers actually access. Everything flows here after passing through the validation gauntlet.


</details>

## What you get

**SQL or Python**: Write transformations in either language, or mix both in the same project. Use SQL for heavy-lifting transformations, switch to Python for complex logic. No new languages to learn.

**Visibility and control**: The linter catches syntax errors before deployment. CLI commands show you exactly what's running, what's queued, and what failed. You intercept errors in your Postgres or Snowflake environment before they trigger notifications. See what breaks before you break it.

**Ship without errors**: Assertions block bad data at the door. Tests run locally without touching your warehouse, so you get fast feedback without warehouse costs. CI/CD plans changes and shows you the full impact before deploying. Review what changed, approve when ready, roll back if something goes wrong.

Why bother with **Semantic Models**? Because it's the difference between a messy data warehouse and a single source of truth for your entire business metrics. Define metrics and dimensions once in your semantic layer. Vulcan generates REST, Python, and Graph APIs automatically. No manual API code needed. Business users get self-service analytics through the semantic layer, developers get programmatic access through the APIs.

## Getting started

The [quickstart guide](guides/get-started/docker.md) walks you through setting up Vulcan and creating your first project. By the end, you'll have implemented your first audit or data quality check. That's the difference between just installing Vulcan and actually getting value from it.





# Vulcan configuration

Source: https://tmdc-io.github.io/vulcan-book/references/configuration/

---

# Vulcan configuration

This page lists all Vulcan configuration options and their parameters. For an overview of Vulcan configuration, see the [configuration overview](../configurations/overview.md).

Model-specific configuration options are listed in the [model configuration reference](./model_configuration.md).

## Root configurations

A Vulcan project configuration contains root-level parameters that define other parameters.

The main root-level parameters are [`gateways`](#gateways) and [gateway/connection defaults](#gatewayconnection-defaults), which have their own sections below.

This section covers the other root-level configuration parameters.

### Projects

Configuration options for Vulcan project directories.

| Option             | Description                                                                                                                 |     Type     | Required |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------- | :----------: | :------: |
| `ignore_patterns`  | Files that match glob patterns specified in this list are ignored when scanning the project folder (Default: `[]`)          | list[string] |    N     |
| `project`          | The project name of this config. Used for multi-repo setups.                                     | string       |    N     |
| `cache_dir`        | The directory to store the Vulcan cache. Can be an absolute path or relative to the project directory. (Default: `.cache`) | string       |    N     |
| `log_limit`        | The default number of historical log files to keep (Default: `20`)                                                          | int          |    N     |

### Database (Physical Layer)

Configuration options for how Vulcan manages database objects in the [physical layer](./glossary.md#physical-layer). The physical layer stores actual data in database tables.

| Option                        | Description                                                                                                                                                                                                                                                                                        | Type                 | Required |
|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------:|:--------:|
| `snapshot_ttl`                | The period of time that a model snapshot not a part of any environment should exist before being deleted. This is defined as a string with the default `in 1 week`. Other [relative dates](https://dateparser.readthedocs.io/en/latest/) can be used, such as `in 30 days`. (Default: `in 1 week`) | string               | N        |
| `physical_schema_override`    | (Deprecated) Use `physical_schema_mapping` instead. A mapping from model schema names to names of schemas in which physical tables for the corresponding models will be placed.                                                                                                                    | dict[string, string] | N        |
| `physical_schema_mapping`     | A mapping from regular expressions to names of schemas in which physical tables for the corresponding models will be placed. (Default physical schema name: `vulcan__[model schema]`)                                                        | dict[string, string] | N        |
| `physical_table_naming_convention`| Sets which parts of the model name are included in the physical table names. Options are `schema_and_table`, `table_only` or `hash_md5`. (Default: `schema_and_table`)                                     | string               | N        |

### Environments (Virtual Layer)

Configuration options for how Vulcan manages environment creation and promotion in the [virtual layer](./glossary.md#virtual-layer). The virtual layer provides views over physical tables.

| Option                        | Description                                                                                                                                                                                                                                                                                        | Type                 | Required |
|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------:|:--------:|
| `environment_ttl`             | The period of time that a development environment should exist before being deleted. This is defined as a string with the default `in 1 week`. Other [relative dates](https://dateparser.readthedocs.io/en/latest/) can be used, such as `in 30 days`. (Default: `in 1 week`)                      | string               | N        |
| `pinned_environments`         | The list of development environments that are exempt from deletion due to expiration                                                                                                                                                                                                               | list[string]         | N        |
| `default_target_environment`  | The name of the environment that will be the default target for the `vulcan plan` and `vulcan run` commands. (Default: `prod`)                                                                                                                                                                   | string               | N        |
| `environment_suffix_target`   | Whether Vulcan views should append their environment name to the `schema`, `table` or `catalog`. (Default: `schema`)                                                                                                      | string               | N        |
| `gateway_managed_virtual_layer`   | Whether Vulcan views of the virtual layer will be created by the default gateway or model specified gateways. (Default: False)                                                                                | boolean              | N        |
| `environment_catalog_mapping` | A mapping from regular expressions to catalog names. The catalog name is used to determine the target catalog for a given environment.                                                                                                                                                             | dict[string, string] | N        |
| `virtual_environment_mode` | Determines the Virtual Data Environment (VDE) mode. If set to `full`, VDE is used in both production and development environments. The `dev_only` option enables VDE only in development environments, while in production, no virtual layer is used and models are materialized directly using their original names (i.e., no versioned physical tables). (Default: `full`) | string | N |

### Models

| Option                        | Description                                                                                                                                                                                                                                                                                        | Type                 | Required |
|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------:|:--------:|
| `time_column_format`          | The default format to use for all model time columns. This time format uses [python format codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes) (Default: `%Y-%m-%d`)                                                                                        | string               | N        |
| `infer_python_dependencies`   | Whether Vulcan will statically analyze Python code to automatically infer Python package requirements. (Default: True)                                                                                                                                                                            | boolean              | N        |
| `model_defaults`              | Default [properties](./model_configuration.md#model-defaults) to set on each model. At a minimum, `dialect` must be set.                                                                                                                                                                           | dict[string, any]    | Y        |

The `model_defaults` key is **required** and must contain a value for the `dialect` key.

See all the keys allowed in `model_defaults` at the [model configuration reference page](./model_configuration.md#model-defaults).

### Variables

The `variables` key provides values for user-defined variables. Access them using:

- [`@VAR` macro function](../../components/advanced-features/macros/built_in.md#global-variables) in SQL model definitions
- [`context.var` method](../../components/model/types/python_models.md#user-defined-variables) in Python model definitions
- [`evaluator.var` method](../../components/advanced-features/macros/built_in.md#accessing-global-variable-values) in Python macro functions

The `variables` key maps variable names to their values. See examples on the [macros page](../../components/advanced-features/macros/built_in.md#global-variables). Variable names are case-insensitive.

Global variable values may be any of the data types in the table below or lists or dictionaries containing those types.

| Option      | Description                         | Type                                                         | Required |
|-------------|-------------------------------------|:------------------------------------------------------------:|:--------:|
| `variables` | Mapping of variable names to values | dict[string, int \| float \| bool \| string \| list \| dict] | N        |

### Before_all / after_all

The `before_all` and `after_all` keys specify lists of SQL statements and Vulcan macros that run at the start and end of `vulcan plan` and `vulcan run` commands. For more information and examples, see [Execution Hooks](../configurations/options/execution_hooks.md).

| Option       | Description                                                                          | Type         | Required |
|--------------|--------------------------------------------------------------------------------------|:------------:|:--------:|
| `before_all` | List of SQL statements to be executed at the start of the `plan` and `run` commands. | list[string] |    N     |
| `after_all`  | List of SQL statements to be executed at the end of the `plan` and `run` commands.   | list[string] |    N     |

## Plan

Configuration for the `vulcan plan` command.

| Option                    | Description                                                                                                                                                                                                                                             | Type                 | Required |
|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------:|:--------:|
| `auto_categorize_changes` | Whether Vulcan automatically [categorizes](../guides/plan.md#change-categories) model changes during plan creation per model source type | dict[string, string] | N        |
| `include_unmodified`      | Whether to create views for all models in the target development environment or only for modified ones (Default: False)                                                                                                                       | boolean              | N        |
| `auto_apply`              | Whether to automatically apply a new plan after creation (Default: False)                                                                                                                                                                     | boolean              | N        |
| `forward_only`            | Whether the plan should be [forward-only](../guides/plan.md#forward-only-plans) (Default: False)                                                                                                                                           | boolean              | N        |
| `enable_preview`          | Whether to enable [data preview](../guides/plan.md#data-preview-for-forward-only-changes) for forward-only models when targeting a development environment (Default: True, except for dbt projects where the target engine does not support cloning)                | Boolean              | N        |
| `no_diff`                 | Don't show diffs for changed models (Default: False)                                                                                                                                                                                                    | boolean              | N        |
| `no_prompts`              | Disables interactive prompts in CLI (Default: True)                                                                                                                                                                                                     | boolean              | N        |
| `always_recreate_environment`              | Always recreates the target environment from the environment specified in `create_from` (by default `prod`) (Default: False)                                                                                                                                                                                                     | boolean              | N        |

## Run

Configuration for the `vulcan run` command. This only applies when using the [builtin](#builtin) scheduler.

| Option                       | Description                                                                                                        | Type | Required |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------ | :--: | :------: |
| `environment_check_interval` | The number of seconds to wait between attempts to check the target environment for readiness (Default: 30 seconds) | int  |    N     |
| `environment_check_max_wait` | The maximum number of seconds to wait for the target environment to be ready (Default: 6 hours)                    | int  |    N     |

## Format

Formatting settings for the `vulcan format` command and UI.

| Option                | Description                                                                                    |  Type   | Required |
| --------------------- | ---------------------------------------------------------------------------------------------- | :-----: | :------: |
| `normalize`           | Whether to normalize SQL (Default: False)                                                      | boolean |    N     |
| `pad`                 | The number of spaces to use for padding (Default: 2)                                           |   int   |    N     |
| `indent`              | The number of spaces to use for indentation (Default: 2)                                       |   int   |    N     |
| `normalize_functions` | Whether to normalize function names. Supported values are: 'upper' and 'lower' (Default: None) | string  |    N     |
| `leading_comma`       | Whether to use leading commas (Default: False)                                                 | boolean |    N     |
| `max_text_width`      | The maximum text width in a segment before creating new lines (Default: 80)                    |   int   |    N     |
| `append_newline`      | Whether to append a newline to the end of the file (Default: False)                            | boolean |    N     |
| `no_rewrite_casts`    | Preserve the existing casts, without rewriting them to use the :: syntax. (Default: False)                  | boolean |    N     |


## Janitor

Configuration for the `vulcan janitor` command.

| Option                          | Description                                                                                                                | Type    | Required |
|---------------------------------|----------------------------------------------------------------------------------------------------------------------------|:-------:|:--------:|
| `warn_on_delete_failure`        | Whether to warn instead of erroring if the janitor fails to delete the expired environment schema / views (Default: False) | boolean | N        |
| `expired_snapshots_batch_size`  | Maximum number of expired snapshots to clean in a single batch (Default: 200)                                              | int     | N        |


## Gateways

The `gateways` dictionary defines how Vulcan connects to the data warehouse, state backend, test backend, and scheduler.

You can define one or more named gateway configurations, each with its own connections. Gateway names are case-insensitive. Vulcan normalizes all gateway names to lowercase during configuration validation. A gateway doesn't need to specify all four components. It uses defaults if any are omitted. See [gateway defaults](#gatewayconnection-defaults) below.

For example, a project might configure the `gate1` and `gate2` gateways:

```yaml linenums="1"
gateways:
  gate1:
    connection:
      ...
    state_connection: # defaults to `connection` if omitted
      ...
    test_connection: # defaults to `connection` if omitted
      ...
    scheduler: # defaults to `builtin` if omitted
      ...
  gate2:
    connection:
      ...
```

Find additional information about gateways in the [configuration overview](../configurations/overview.md#gateways).

### Gateway

Configuration for each named gateway.

#### Connections

A named gateway key may define any or all of a data warehouse connection, state backend connection, state schema name, test backend connection, and scheduler.

Some connections use default values if not specified:

- The `connection` key may be omitted if a [`default_connection`](#default-connectionsscheduler) is specified.
- The state connection defaults to `connection` if omitted.
- The test connection defaults to `connection` if omitted.

NOTE: Spark and Trino engines may not be used for the state connection.

| Option             | Description                                                                                                                                                                     | Type                                                         | Required                                                               |
|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------:|:----------------------------------------------------------------------:|
| `connection`       | The data warehouse connection for core Vulcan functions.                                                                                                                       | [connection configuration](#connection)                      | N (if [`default_connection`](#default-connectionsscheduler) specified) |
| `state_connection` | The data warehouse connection where Vulcan will store internal information about the project. (Default: `connection` if using builtin scheduler, otherwise scheduler database) | [connection configuration](#connection)                      | N                                                                      |
| `state_schema`     | The name of the schema where state information should be stored. (Default: `vulcan`)                                                                                           | string                                                       | N                                                                      |
| `test_connection`  | The data warehouse connection Vulcan will use to execute tests. (Default: `connection`)                                                                                        | [connection configuration](#connection)                      | N                                                                      |
| `scheduler`        | The scheduler Vulcan will use to execute tests. (Default: `builtin`)                                                                                                           | [scheduler configuration](#scheduler)                        | N                                                                      |
| `variables`        | The gateway-specific variables which override the root-level [variables](#variables) by key.                                                                                    | dict[string, int \| float \| bool \| string \| list \| dict] | N                                                                      |

### Connection

Configuration for a data warehouse connection.

Most parameters are specific to the connection engine `type` - see [below](#engine-specific). The default data warehouse connection type is an in-memory DuckDB database.

#### General

| Option              | Description                                                                                                                                                             | Type | Required |
|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----:|:--------:|
| `type`              | The engine type name, listed in engine-specific configuration pages below.                                                                                              | str  |    Y     |
| `concurrent_tasks`  | The maximum number of concurrent tasks that will be run by Vulcan. (Default: 4 for engines that support concurrent tasks.)                                             | int  |    N     |
| `register_comments` | Whether Vulcan should register model comments with the SQL engine (if the engine supports it). (Default: `true`.)                                                      | bool |    N     |
| `pre_ping`          | Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive. This can only be enabled for engines with transaction support. | bool |    N     |
| `pretty_sql`        | If SQL should be formatted before being executed, not recommended in a production setting. (Default: `false`.)                                                          | bool |    N     |

#### Engine-specific

These pages describe the connection configuration options for each execution engine.

* [Postgres](./integrations/engines/postgres.md)
* [Snowflake](./integrations/engines/snowflake.md)

### Scheduler

Specifies which scheduler backend to use. The scheduler stores metadata and executes [plans](../guides/plan.md).

By default, the scheduler type is `builtin` and uses the gateway's connection to store metadata.

Configuration options for each scheduler type are listed below. For more details, see the [configuration overview](../configurations/overview.md#scheduler).

#### Builtin

**Type:** `builtin`

No configuration options are supported by this scheduler type.

## Gateway/connection defaults

Default gateway and connection keys specify what happens when gateways or connections aren't explicitly specified. For more details, see the [configuration overview](../configurations/overview.md#gateways).

### Default gateway

If a configuration contains multiple gateways, Vulcan will use the first one in the `gateways` dictionary by default. The `default_gateway` key is used to specify a different gateway name as the Vulcan default.

| Option            | Description                                                                                                                  |  Type  | Required |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------- | :----: | :------: |
| `default_gateway` | The name of a gateway to use if one is not provided explicitly (Default: the gateway defined first in the `gateways` option). Gateway names are case-insensitive. | string |    N     |

### Default connections/scheduler

The `default_connection`, `default_test_connection`, and `default_scheduler` keys are used to specify shared defaults across multiple gateways.

For example, you might have a specific connection where your tests should run regardless of which gateway is being used. Instead of duplicating the test connection information in each gateway specification, specify it once in the `default_test_connection` key.

| Option                    | Description                                                                                                                                            |    Type     | Required |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------: | :------: |
| `default_connection`      | The default connection to use if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database)                   | connection  |    N     |
| `default_test_connection` | The default connection to use when running tests if one is not specified in a gateway (Default: A DuckDB connection that creates an in-memory database) | connection |    N     |
| `default_scheduler`       | The default scheduler configuration to use if one is not specified in a gateway (Default: built-in scheduler)                                          |  scheduler  |    N     |

## Debug mode

Enable debug mode in one of two ways:

- Pass the `--debug` flag between the CLI command and the subcommand. For example, `vulcan --debug plan`.
- Set the `VULCAN_DEBUG` environment variable to one of the following values: "1", "true", "t", "yes" or "y".

Enabling debug mode prints full backtraces in the CLI. The log level is set to `DEBUG` when debug mode is enabled.

Example enabling debug mode for the CLI command `vulcan plan`:

=== "Bash"

    ```bash
    $ vulcan --debug plan
    ```

    ```bash
    $ VULCAN_DEBUG=1 vulcan plan
    ```

=== "MS Powershell"

    ```powershell
    PS> vulcan --debug plan
    ```

    ```powershell
    PS> $env:VULCAN_DEBUG=1
    PS> vulcan plan
    ```

=== "MS CMD"

    ```cmd
    C:\> vulcan --debug plan
    ```

    ```cmd
    C:\> set VULCAN_DEBUG=1
    C:\> vulcan plan
    ```

<!-- ## Runtime Environment

Vulcan can run in different runtime environments. For example, you might run it in a regular command-line terminal, in a Jupyter notebook, or in Github's CI/CD platform.

When it starts up, Vulcan automatically detects the runtime environment and adjusts its behavior accordingly. For example, it registers `%magic` commands if in a Jupyter notebook and adjusts logging behavior if in a CI/CD environment.

If necessary, you may force Vulcan to use a specific runtime environment by setting the `VULCAN_RUNTIME_ENVIRONMENT` environment variable.

It accepts the following values, which will cause Vulcan to behave as if it were in the runtime environment in parentheses:

- `terminal` (CLI console)
- `databricks` (Databricks notebook)
- `google_colab` (Google Colab notebook)
- `jupyter` (Jupyter notebook)
- `debugger` (Debugging output)
- `ci` (CI/CD or other non-interactive environment) -->

## Parallel loading

Vulcan uses all available cores when loading models and snapshots. It uses `fork`, which isn't available on Windows. By default, it uses the same number of workers as cores if fork is available.

Override this by setting the `MAX_FORK_WORKERS` environment variable. Set it to `1` to disable forking and load sequentially.



# Environments

Source: https://tmdc-io.github.io/vulcan-book/references/environments/

---

# Environments

Environments are isolated namespaces for testing and previewing changes.

Vulcan distinguishes between production and development environments. Only the environment named `prod` is treated as production. All other environments are development environments.

[Models](../../components/model/overview.md) in development environments get a suffix appended to the schema portion of their names. For example, to access data for a model named `db.model_a` in the `my_dev` environment, use `db__my_dev.model_a` in queries. Models in production use their original names.

## Why use environments

Data pipelines grow more complex over time. Assessing the impact of local changes becomes difficult. You may not know all downstream consumers or may underestimate a change's impact.

You need to test model changes using production dependencies and data without affecting existing production datasets or pipelines. Recreating the entire data warehouse would show the full impact, but it's expensive and time-consuming.

Vulcan environments create lightweight clones of the data warehouse. Vulcan identifies which models changed compared to the target environment and only computes data gaps caused by those changes. Changes or backfills in one environment don't affect other environments. Computations done in one environment can be reused in others.

## How to use environments

When running [`vulcan plan`](./plans.md), provide the environment name as the first argument. You can use any string as an environment name. The only special name is `prod`, which refers to production. All other names create development environments.

By default, `vulcan plan` targets the `prod` environment.

### Example

Create or update a development environment by providing a custom name:

```bash
vulcan plan my_dev
```

Vulcan creates the environment automatically the first time you apply a plan to it.

## How environments work

When a model definition changes, Vulcan creates a new model snapshot with a unique fingerprint. The fingerprint identifies whether a model variant exists in other environments or is new. Because models depend on other models, the fingerprint includes fingerprints of upstream dependencies. If a fingerprint already exists, Vulcan reuses the existing physical table for that model variant. The logic that populates the table is identical.

An environment is a collection of references to model snapshots.

For more details, see [plan application](./plans.md#plan-application).

## Date range

A development environment includes a start date and end date. When creating a development environment, you usually test changes on a subset of data. The subset size is determined by the time range defined by the start and end dates. You provide both dates during [plan](./plans.md) creation.



# Glossary

Source: https://tmdc-io.github.io/vulcan-book/references/glossary/

---

# Glossary

## Abstract Syntax Tree
A tree representation of the syntactic structure of source code. Each tree node represents a construct that occurs. The tree is abstract because it does not represent every detail appearing in the actual syntax; it also does not have a standard representation.

## Backfill
Load or refresh model data, triggered by a vulcan plan command.

## Catalog
A catalog is a collection of schemas. A schema is a collection of database objects such as tables and views.

## CI/CD
A process that combines Continuous Integration (automated code creation and testing) and Continuous Delivery (deployment of code and tests). Vulcan implements this with [tests](../../components/tests/tests.md) and [audits](../../components/audits/audits.md).

## CTE
A Common Table Expression is a temporary named result set created from a SELECT statement, which can then be used in a subsequent SELECT statement. For more information, see [tests](../../components/tests/tests.md).

## DAG
Directed Acyclic Graph. In this type of graph, objects are represented as nodes with relationships that show the dependencies between them; as such, the relationships are directed, meaning there is no way for data to travel through the graph in a loop that can circle back to the starting point. Vulcan uses a DAG to keep track of a project's models. This allows Vulcan to easily determine a model's lineage and to identify upstream and downstream dependencies.

## Data modeling
Data modeling allows practitioners to visualize and conceptually represent how data is stored in a data warehouse. This can be done using diagrams that represent how data is interrelated.

## Data pipeline
The set of tools and processes for moving data from one system to another. Datasets are then organized, transformed, and inserted into some type of database, tool, or app, where data scientists, engineers, and analysts can access the data for analysis, insights, and reporting.

## Data transformation
Data transformation is the process of converting data from one format to another; for example, by converting raw data into a form usable for analysis by harmonizing data types, removing duplicate data, and organizing data.

## Data warehouse
The repository that houses the single source of truth where data is stored, which is integrated from various sources. This repository, normally a relational database, is optimized for handling large volumes of data.

## Direct Modification
A change to a model's definition from the user instead of being inherited from an upstream dependency like [Indirect Modification](#indirect-modification).

## ELT
Acronym for Extract, Load, and Transform. The process of retrieving data from various sources, loading it into a data warehouse, and then transforming it into a usable and reliable resource for data practitioners.

## ETL
Acronym for Extract, Transform, and Load. The process of retrieving data from various sources, transforming the data into a usable and reliable resource, and then loading it into a data warehouse for data practitioners.

## Full refresh
In a full data refresh, a complete dataset is deleted and then entirely overwritten with an updated dataset.

## Idempotency
The property that, given a particular operation, the same outputs will be produced when given the same inputs no matter how many times the operation is applied.

## Incremental Loads
Incremental loads update only data that changed since the last refresh. This is faster and more efficient than full refreshes. Vulcan provides variables and macros to help define incremental models. See [Model Kinds](../../components/model/model_kinds.md) for more information.

## Indirect Modification
A change to model's upstream dependency and not to the model itself like a [Direct Modification](#direct-modification).

## Integration
Combining data from various sources (such as from a data warehouse) into one unified view.

## Lineage
The lineage of your data is a visualization of the life cycle of your data as it flows from data sources downstream to consumption.

## Physical Layer
The physical layer is where Vulcan stores and manages data in database tables and materialized views. It is the concrete data storage layer of the SQL engine, in contrast to the [Vulcan virtual layer's](#virtual-layer) views. Vulcan handles the management and maintenance of the physical layer automatically, and users should rarely interact with it directly.

## Plan Summaries
An upcoming feature that allows users to see a summary of changes applied to a given environment.

## Semantic Understanding
Vulcan understands the full meaning of a SQL model. It validates that SQL is valid and can transpile SQL into other engine dialects when needed.

## Slowly Changing Dimension (SCD)
A dimension (in a data warehouse, typically a dataset) containing relatively static data that can change slowly but unpredictably, rather than on a regular schedule. Some examples of typical slowly changing dimensions are places and products.

## Table
A table is the visual representation of data stored in rows and columns.

## User-Defined Function (UDF)
Functions that a user of a database server provides to extend its functionality, in contrast to built-in functions that are already provided. UDFs are typically written to satisfy the particular requirements of the user.

## View
A view is the result of a SQL query on a database.

## Virtual Environments
Vulcan's approach to environments that provides both environment isolation and the ability to share tables across environments. This ensures data consistency and accuracy. See [plan application](./plans.md#plan-application) for more information.

## Virtual Layer
The virtual layer is Vulcan's abstraction layer over the [physical layer and physical data storage](#physical-layer). While the physical layer consists of tables where data is actually stored, the virtual layer consists of views that expose tables in the underlying physical layer. Most users should only interact with the virtual layer when building models or querying data.

## Virtual Update
A plan that can be applied without loading additional data or building additional tables. See [Virtual Update](./plans.md#virtual-update) for more information.

## Virtual Preview
The ability to create an environment without building additional tables. By comparing model versions in the repo against what currently exists, Vulcan creates an environment that matches the repo by updating views.



# Athena

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/athena/

---

# Athena

## Installation

```
pip install "vulcan[athena]"
```

## Connection options

### PyAthena connection options

Vulcan leverages the [PyAthena](https://github.com/laughingman7743/PyAthena) DBAPI driver to connect to Athena. Therefore, the connection options relate to the PyAthena connection options.
Note that PyAthena uses [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) under the hood so you can also use [boto3 environment variables](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables) for configuration.

| Option                  | Description                                                                                                                                                  |  Type  | Required |
|-------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                  | Engine type name - must be `athena`                                                                                                                          | string |    Y     |
| `aws_access_key_id`     | The access key for your AWS user                                                                                                                             | string |    N     |
| `aws_secret_access_key` | The secret key for your AWS user                                                                                                                             | string |    N     |
| `role_arn`              | The ARN of a role to assume once authenticated                                                                                                               | string |    N     |
| `role_session_name`     | The session name to use when assuming `role_arn`                                                                                                             | string |    N     |
| `region_name`           | The AWS region to use                                                                                                                                        | string |    N     |
| `work_group`            | The Athena [workgroup](https://docs.aws.amazon.com/athena/latest/ug/workgroups-manage-queries-control-costs.html) to send queries to                         | string |    N     |
| `s3_staging_dir`        | The S3 location for Athena to write query results. Only required if not using `work_group` OR the configured `work_group` doesnt have a results location set | string |    N     |
| `schema_name`           | The default schema to place objects in if a schema isnt specified. Defaults to `default`                                                                     | string |    N     |
| `catalog_name`          | The default catalog to place schemas in. Defaults to `AwsDataCatalog`                                                                                        | string |    N     |

### Vulcan connection options

These options are specific to Vulcan itself and are not passed to PyAthena

| Option                  | Description                                                                                                                                                                                      | Type   | Required |
|-------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|
| `s3_warehouse_location` | Set the base path in S3 where Vulcan will instruct Athena to place table data. Only required if you arent specifying the location in the model itself. See [S3 Locations](#s3-locations) below. | string | N        |

## Model properties

The Athena adapter utilises the following model top-level [properties](configurations-old/components/model/overview.md#model-properties):

| Name             | Description                                                                                                                                                                                                                                                                                                                          | Type   | Required |
|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|
| `table_format`   | Sets the [table_type](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html#ctas-table-properties) Athena uses when creating the table. Valid values are `hive` or `iceberg`.                                                                                                                                            | string | N        |
| `storage_format` | Configures the file format to be used by the `table_format`. For Hive tables, this sets the [STORED AS](https://docs.aws.amazon.com/athena/latest/ug/create-table.html#parameters) option. For Iceberg tables, this sets [format](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html#ctas-table-properties) property. | string | N        |

The Athena adapter recognises the following model [physical_properties](configurations-old/components/model/overview.md#physical_properties):

| Name              | Description                                                                                                                                                                               | Type   | Default |
|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|---------|
| `s3_base_location`| `s3://` base URI of where the snapshot tables for this model should be written. Overrides `s3_warehouse_location` if one is configured.                                                   | string |         |


## S3 Locations
When creating tables, Athena needs to know where in S3 the table data is located. You cannot issue a `CREATE TABLE` statement without specifying a `LOCATION` for the table data.

In addition, unlike other engines such as Trino, Athena will not infer a table location if you set a _schema_ location via `CREATE SCHEMA <schema> LOCATION 's3://schema/location'`.

Therefore, in order for Vulcan to issue correct `CREATE TABLE` statements to Athena, you need to configure where the tables should be stored. There are two options for this:

- **Project-wide:** set `s3_warehouse_location` in the connection config. Vulcan will set the table `LOCATION` to be `<s3_warehouse_location>/<schema_name>/<snapshot_table_name>` when it creates a snapshot of your model.
- **Per-model:** set `s3_base_location` in the model `physical_properties`. Vulcan will set the table `LOCATION` to be `<s3_base_location>/<snapshot_table_name>` every time it creates a snapshot of your model. This takes precedence over any `s3_warehouse_location` set in the connection config.


## Limitations
Athena was initially designed to read data stored in S3 and to do so without changing that data. This means that it does not have good support for mutating tables. In particular, it will not delete data from Hive tables.

Consequently, [forward only changes](configurations-old/guides/plan.md#forward-only-change) that mutate the schemas of existing tables have a high chance of failure because Athena supports very limited schema modifications on Hive tables.

However, Athena does support [Apache Iceberg](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html) tables which allow a full range of operations. These can be used for more complex model types such as [`INCREMENTAL_BY_UNIQUE_KEY`](configurations-old/components/model/model_kinds.md#incremental_by_unique_key) and [`SCD_TYPE_2`](configurations-old/components/model/model_kinds.md#scd-type-2).

To use an Iceberg table for a model, set `table_format iceberg` in the model [properties](configurations-old/components/model/overview.md#model-properties).

In general, Iceberg tables offer the most flexibility and you'll run into the least Vulcan limitations when using them. However, we create Hive tables by default because Athena creates Hive tables by default, so Iceberg tables are opt-in rather than opt-out.



# Azure SQL

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/azuresql/

---

# Azure SQL

[Azure SQL](https://azure.microsoft.com/en-us/products/azure-sql) is "a family of managed, secure, and intelligent products that use the SQL Server database engine in the Azure cloud."

## Local/Built-in Scheduler
**Engine Adapter Type**: `azuresql`

### Installation
#### User / Password Authentication:
```
pip install "vulcan[azuresql]"
```
#### Microsoft Entra ID / Azure Active Directory Authentication:
```
pip install "vulcan[azuresql-odbc]"
```

### Connection options

| Option            | Description                                                      |     Type     | Required |
| ----------------- | ---------------------------------------------------------------- | :----------: | :------: |
| `type`            | Engine type name - must be `azuresql`                            |    string    |    Y     |
| `host`            | The hostname of the Azure SQL server                             |    string    |    Y     |
| `user`            | The username / client ID to use for authentication with the Azure SQL server |    string    |    N     |
| `password`        | The password / client secret to use for authentication with the Azure SQL server |    string    |    N     |
| `port`            | The port number of the Azure SQL server                          |     int      |    N     |
| `database`        | The target database                                              |    string    |    N     |
| `charset`         | The character set used for the connection                        |    string    |    N     |
| `timeout`         | The query timeout in seconds. Default: no timeout                |     int      |    N     |
| `login_timeout`   | The timeout for connection and login in seconds. Default: 60     |     int      |    N     |
| `appname`         | The application name to use for the connection                   |    string    |    N     |
| `conn_properties` | The list of connection properties                                | list[string] |    N     |
| `autocommit`      | Is autocommit mode enabled. Default: false                       |     bool     |    N     |
| `driver`         | The driver to use for the connection. Default: pymssql            |    string    |    N     |
| `driver_name`     | The driver name to use for the connection. E.g., *ODBC Driver 18 for SQL Server* |    string    |    N     |
| `odbc_properties` | The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more [here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16). | dict |    N     |


# BigQuery

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/bigquery/

---

# BigQuery

## Introduction

This guide provides step-by-step instructions on how to connect Vulcan to the BigQuery SQL engine.

It will walk you through the steps of installing Vulcan and BigQuery connection libraries locally, configuring the connection in Vulcan, and running the [quickstart project](configurations-old/guides/get-started/docker.md).

## Prerequisites

This guide assumes the following about the BigQuery project being used with Vulcan:

- The project already exists
- Project [CLI/API access is enabled](https://cloud.google.com/endpoints/docs/openapi/enable-api)
- Project [billing is configured](https://cloud.google.com/billing/docs/how-to/manage-billing-account) (i.e. it's not a sandbox project)
- Vulcan can authenticate using an account with permissions to execute commands against the project

## Installation

Follow the [quickstart guide](configurations-old/guides/get-started/docker.md) to set up Vulcan, then install the necessary BigQuery libraries.

Instead of installing just Vulcan core, we will also include the BigQuery engine libraries:

```bash
> pip install "vulcan[bigquery]"
```

### Install Google Cloud SDK

Vulcan connects to BigQuery via the Python [`google-cloud-bigquery` library](https://pypi.org/project/google-cloud-bigquery/), which uses the [Google Cloud SDK `gcloud` tool](https://cloud.google.com/sdk/docs) for [authenticating with BigQuery](https://googleapis.dev/python/google-api-core/latest/auth.html).

Follow these steps to install and configure the Google Cloud SDK on your computer:

- Download the appropriate installer for your system from the [Google Cloud installation guide](https://cloud.google.com/sdk/docs/install)
- Unpack the downloaded file with the `tar` command:

    ```bash
    > tar -xzvf google-cloud-cli-{SYSTEM_SPECIFIC_INFO}.tar.gz
    ```

- Run the installation script:

    ```bash
    > ./google-cloud-sdk/install.sh
    ```

- Reload your shell profile (e.g., for zsh):

    ```bash
    > source $HOME/.zshrc
    ```

- Run [`gcloud init` to setup authentication](https://cloud.google.com/sdk/gcloud/reference/init)

## Configuration

### Configure Vulcan for BigQuery

Add the following gateway specification to your Vulcan project's `config.yaml` file:

```yaml
bigquery:
  connection:
    type: bigquery
    project: <your_project_id>

default_gateway: bigquery
```

This creates a gateway named `bigquery` and makes it your project's default gateway.

It uses the [`oauth` authentication method](#authentication-methods), which does not specify a username or other information directly in the connection configuration. Other authentication methods are [described below](#authentication-methods).

In BigQuery, navigate to the dashboard and select the BigQuery project your Vulcan project will use. From the Google Cloud dashboard, use the arrow to open the pop-up menu:

![BigQuery Dashboard](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-1.png)

Now we can identify the project ID needed in the `config.yaml` gateway specification above. Select the project that you want to work with, the project ID that you need to add to your yaml file is the ID label from the pop-up menu.

![BigQuery Dashboard: selecting your project](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-2.png)

For this guide, the Docs-Demo is the one we will use, thus the project ID for this example is `healthy-life-440919-s0`.

## Usage

### Test the connection

Run the following command to verify that Vulcan can connect to BigQuery:

```bash
> vulcan info
```

The output will look something like this:

![Terminal Output](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-3.png)

- **Set quota project (optional)**

    You may see warnings like this when you run `vulcan info`:

    ![Terminal Output with warnings](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-4.png)

    You can avoid these warnings about quota projects by running:

    ```bash
    > gcloud auth application-default set-quota-project <your_project_id>
    > gcloud config set project <your_project_id>
    ```


### Create and run a plan

We've verified our connection, so we're ready to create and execute a plan in BigQuery:

```bash
> vulcan plan
```

### View results in BigQuery Console

Let's confirm that our project models are as expected.

First, navigate to the BigQuery Studio Console:

![Steps to the Studio](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-5.png)

Then use the left sidebar to find your project and the newly created models:

![New Models](../../configurations-old/configurations-old/configurations-old/integrations/engines/bigquery/bigquery-6.png)

We have confirmed that our Vulcan project is running properly in BigQuery!

## Local/Built-in Scheduler

**Engine Adapter Type**: `bigquery`

### Installation
```
pip install "vulcan[bigquery]"
```

### Connection options

| Option                          | Description                                                                                                                                                       |  Type  | Required |
|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                          | Engine type name - must be `bigquery`                                                                                                                             | string |    Y     |
| `method`                        | Connection methods - see [allowed values below](#authentication-methods). Default: `oauth`.                                                                           | string |    N     |
| `project`                       | The ID of the GCP project                                                                                                                                       | string |    N     |
| `location`                      | The location of for the datasets (can be regional or multi-regional)                                                                                              | string |    N     |
| `execution_project`             | The name of the GCP project to bill for the execution of the models. If not set, the project associated with the model will be used.                              | string |    N     |
| `quota_project`                 | The name of the GCP project used for the quota. If not set, the `quota_project_id` set within the credentials of the account is used to authenticate to BigQuery. | string |    N     |
| `keyfile`                       | Path to the keyfile to be used with service-account method                                                                                                        | string |    N     |
| `keyfile_json`                  | Keyfile information provided inline (not recommended)                                                                                                             |  dict  |    N     |
| `token`                         | OAuth 2.0 access token                                                                                                                                            | string |    N     |
| `refresh_token`                 | OAuth 2.0 refresh token                                                                                                                                           | string |    N     |
| `client_id`                     | OAuth 2.0 client ID                                                                                                                                               | string |    N     |
| `client_secret`                 | OAuth 2.0 client secret                                                                                                                                           | string |    N     |
| `token_uri`                     | OAuth 2.0 authorization server's token endpoint URI                                                                                                                | string |    N     |
| `scopes`                        | The scopes used to obtain authorization                                                                                                                           |  list  |    N     |
| `impersonated_service_account`  | If set, Vulcan will attempt to impersonate this service account                                                                                                                                | string |    N     |
| `job_creation_timeout_seconds`  | The maximum amount of time, in seconds, to wait for the underlying job to be created.                                                                             |  int   |    N     |
| `job_execution_timeout_seconds` | The maximum amount of time, in seconds, to wait for the underlying job to complete.                                                                               |  int   |    N     |
| `job_retries`                   | The number of times to retry the underlying job if it fails. (Default: `1`)                                                                                       |  int   |    N     |
| `priority`                      | The priority of the underlying job. (Default: `INTERACTIVE`)                                                                                                      | string |    N     |
| `maximum_bytes_billed`          | The maximum number of bytes to be billed for the underlying job.                                                                                                  |  int   |    N     |

## Authentication Methods
- [oauth](https://google-auth.readthedocs.io/en/master/reference/google.auth.html#google.auth.default) (default)
    - Related Credential Configuration:
        - `scopes` (Optional)
- [oauth-secrets](https://google-auth.readthedocs.io/en/stable/reference/google.oauth2.credentials.html)
    - Related Credential Configuration:
        - `token` (Optional): Can be None if refresh information is provided.
        - `refresh_token` (Optional): If specified, credentials can be refreshed.
        - `client_id` (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.
        - `client_secret` (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.
        - `token_uri` (Optional): Must be specified for refresh, can be left as None if the token can not be refreshed.
        - `scopes` (Optional): OAuth 2.0 credentials can not request additional scopes after authorization. The scopes must be derivable from the refresh token if refresh information is provided (e.g. The refresh token scopes are a superset of this or contain a wild card scope like 'https://www.googleapis.com/auth/any-api')
- [service-account](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html#google.oauth2.service_account.IDTokenCredentials.from_service_account_file)
    - Related Credential Configuration:
        - `keyfile` (Required)
        - `scopes` (Optional)
- [service-account-json](https://google-auth.readthedocs.io/en/master/reference/google.oauth2.service_account.html#google.oauth2.service_account.IDTokenCredentials.from_service_account_info)
    - Related Credential Configuration:
        - `keyfile_json` (Required)
        - `scopes` (Optional)

If the `impersonated_service_account` argument is set, Vulcan will:

1. Authenticate user account credentials with one of the methods above
2. Attempt to impersonate the service account with those credentials

The user account must have [sufficient permissions to impersonate the service account](https://cloud.google.com/docs/authentication/use-service-account-impersonation).

## Permissions Required
With any of the above connection methods, ensure these BigQuery permissions are enabled to allow Vulcan to work correctly.

- [`BigQuery Data Owner`](https://cloud.google.com/bigquery/docs/access-control#bigquery.dataOwner)
- [`BigQuery User`](https://cloud.google.com/bigquery/docs/access-control#bigquery.user)



# ClickHouse

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/clickhouse/

---

# ClickHouse

This page describes Vulcan support for the ClickHouse engine, including configuration options specific to ClickHouse.

!!! note
    ClickHouse may not be used for the Vulcan [state connection](configurations-old/configurations-old/configuration.md#connections).

## Background

[ClickHouse](https://clickhouse.com/) is a distributed, column-oriented SQL engine designed to rapidly execute analytical workloads.

It provides users fine-grained control of its behavior, but that control comes at the cost of complex configuration.

This section provides background information about ClickHouse, providing context for how to use Vulcan with the ClickHouse engine.

### Object naming

Most SQL engines use a three-level hierarchical naming scheme: tables/views are nested within _schemas_, and schemas are nested within _catalogs_. For example, the full name of a table might be `my_catalog.my_schema.my_table`.

ClickHouse instead uses a two-level hierarchical naming scheme that has no counterpart to _catalog_. In addition, it calls the second level in the hierarchy "databases." Vulcan and its documentation refer to this second level as "schemas."

Vulcan fully supports ClickHouse's two-level naming scheme without user action.

### Table engines

Every ClickHouse table is created with a ["table engine" that controls how the table's data is stored and queried](https://clickhouse.com/docs/en/engines/table-engines). ClickHouse's (and Vulcan's) default table engine is `MergeTree`.

The [`MergeTree` engine family](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree) requires that every table be created with an `ORDER BY` clause.

Vulcan will automatically inject an empty `ORDER BY` clause into every `MergeTree` family table's `CREATE` statement, or you can specify the columns/expressions by which the table should be ordered.

### ClickHouse modes of operation

Conceptually, it may be helpful to view ClickHouse as having three modes of operation: single server, cluster, and ClickHouse Cloud. Vulcan supports all three modes.

#### Single server mode

Single server mode is similar to other SQL engines: aside from choosing each table's engine, you do not need to worry about how computations are executed. You issue standard SQL commands/queries, and ClickHouse executes them.

#### Cluster mode

Cluster mode allows you to scale your ClickHouse engine to any number of networked servers. This enables massive workloads, but requires that you specify how computations are executed by the networked servers.

ClickHouse coordinates the computations on the networked servers with [ClickHouse Keeper](https://clickhouse.com/docs/en/architecture/horizontal-scaling) (it also supports [Apache ZooKeeper](https://zookeeper.apache.org/)).

You specify named virtual clusters of servers in the Keeper configuration, and those clusters provide namespaces for data objects and computations. For example, you might include all networked servers in the cluster you name `MyCluster`.

In general, you must be connected to a ClickHouse server to execute commands. By default, each command you execute runs in single-server mode on the server you are connected to.

To associate an object with a cluster, DDL commands that create or modify it must include the text `ON CLUSTER [your cluster name]`.

If you provide a cluster name in your Vulcan connection configuration, Vulcan will automatically inject the `ON CLUSTER` statement into the DDL commands for all objects created while executing the project. We provide more information about clusters in Vulcan [below](#cluster-specification).

#### ClickHouse Cloud mode

[ClickHouse Cloud](https://clickhouse.com/cloud) is a managed ClickHouse platform. It allows you to scale ClickHouse without administering a cluster yourself or modifying your SQL commands to run on the cluster.

ClickHouse Cloud automates ClickHouse's cluster controls, which sometimes constrains ClickHouse's flexibility or how you execute SQL commands. For example, creating a table with a `SELECT` command must [occur in two steps on ClickHouse Cloud](https://clickhouse.com/docs/en/sql-reference/statements/create/table#from-select-query). Vulcan handles this limitation for you.

Aside from those constraints, ClickHouse Cloud mode is similar to single server mode - you run standard SQL commands/queries, and ClickHouse Cloud executes them.

## Permissions

In the default Vulcan configuration, users must have sufficient permissions to create new ClickHouse databases.

Alternatively, you can configure specific databases where Vulcan should create table and view objects.

### Environment views

Use the [`environment_suffix_target` key in your project configuration](../../guides-old/configuration.md#disable-environment-specific-schemas) to specify that environment views should be created within the model's database instead of in a new database:

``` yaml
environment_suffix_target: table
```

### Physical tables

Use the [`physical_schema_mapping` key in your project configuration](../../guides-old/configuration.md#physical-table-schemas) to specify the databases where physical tables should be created.

The key accepts a dictionary of regular expressions that map model database names to the corresponding databases where physical tables should be created.

Vulcan will compare a model's database name to each regular expression and use the first match to determine which database a physical table should be created in.

For example, this configuration places every model's physical table in the `model_physical_tables` database because the regular expression `.*` matches any database name:

``` yaml
physical_schema_mapping:
  '.*': model_physical_tables
```

## Cluster specification

A ClickHouse cluster allows multiple networked ClickHouse servers to operate on the same data object. Every cluster must be named in the ClickHouse configuration files, and that name is passed to a table's DDL statements in the `ON CLUSTER` clause.

For example, we could create a table `my_schema.my_table` on cluster `TheCluster` like this: `CREATE TABLE my_schema.my_table ON CLUSTER TheCluster (col1 Int8)`.

To create Vulcan objects on a cluster, provide the cluster name to the `cluster` key in the Vulcan connection definition (see all connection parameters [below](#localbuilt-in-scheduler)).

Vulcan will automatically inject the `ON CLUSTER` clause and cluster name you provide into all project DDL statements.

## Model definition

This section describes how you control a table's engine and other ClickHouse-specific functionality in Vulcan models.

### Table engine

Vulcan uses the `MergeTree` table engine with an empty `ORDER BY` clause by default.

Specify a different table engine by passing the table engine definition to the model DDL's `storage_format` parameter. For example, you could specify the `Log` table engine like this:

``` sql linenums="1" hl_lines="4"
MODEL (
    name my_schema.my_log_table,
    kind full,
    storage_format Log,
);

select
    *
from other_schema.other_table;
```

You may also specify more complex table engine definitions. For example:

``` sql linenums="1" hl_lines="4"
MODEL (
    name my_schema.my_rep_table,
    kind full,
    storage_format ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name', '{replica}', ver),
);

select
    *
from other_schema.other_table;
```

#### ORDER BY

`MergeTree` family engines require that a table's `CREATE` statement include the `ORDER BY` clause.

Vulcan will automatically inject an empty `ORDER BY ()` when creating a table with an engine in the `MergeTree` family. This creates the table without any ordering.

You may specify columns/expressions to `ORDER BY` by passing them to the model `physical_properties` dictionary's `order_by` key.

For example, you could order by columns `col1` and `col2` like this:

``` sql linenums="1" hl_lines="4-6"
MODEL (
    name my_schema.my_log_table,
    kind full,
    physical_properties (
        order_by = (col1, col2)
    )
);

select
    *
from other_schema.other_table;
```

Note that there is an `=` between the `order_by` key name and value `(col1, col2)`.

Complex `ORDER BY` expressions may need to be passed in single quotes, with interior single quotes escaped by the `\` character.

#### PRIMARY KEY

Table engines may also accept a `PRIMARY KEY` specification. Similar to `ORDER BY`, specify a primary key in the model DDL's `physical_properties` dictionary. For example:

``` sql linenums="1" hl_lines="6"
MODEL (
    name my_schema.my_log_table,
    kind full,
    physical_properties (
        order_by = (col1, col2),
        primary_key = col1
    )
);

select
    *
from other_schema.other_table;
```

Note that there is an `=` between the `primary_key` key name and value `col1`.

### TTL

ClickHouse tables accept a [TTL expression that triggers actions](https://clickhouse.com/docs/en/guides/developer/ttl) like deleting rows after a certain amount of time has passed.

Similar to `ORDER_BY` and `PRIMARY_KEY`, specify a TTL key in the model DDL's `physical_properties` dictionary. For example:

``` sql linenums="1" hl_lines="6"
MODEL (
    name my_schema.my_log_table,
    kind full,
    physical_properties (
        order_by = (col1, col2),
        primary_key = col1,
        ttl = timestamp + INTERVAL 1 WEEK
    )
);

select
    *
from other_schema.other_table;
```

Note that there is an `=` between the `ttl` key name and value `timestamp + INTERVAL 1 WEEK`.

### Partitioning

Some ClickHouse table engines support partitioning. Specify the partitioning columns/expressions in the model DDL's `partitioned_by` key.

For example, you could partition by columns `col1` and `col2` like this:

``` sql linenums="1" hl_lines="4"
MODEL (
    name my_schema.my_log_table,
    kind full,
    partitioned_by (col1, col2),
);

select
    *
from other_schema.other_table;
```

Learn more below about how Vulcan uses [partitioned tables to improve performance](#performance-considerations).

## Settings

ClickHouse supports an [immense number of settings](https://clickhouse.com/docs/en/operations/settings), many of which can be altered in multiple places: ClickHouse configuration files, Python client connection arguments, DDL statements, SQL queries, and others.

This section discusses how to control ClickHouse settings in Vulcan.

### Connection settings

Vulcan connects to Python with the [`clickhouse-connect` library](https://clickhouse.com/docs/en/integrations/python). Its connection method accepts a dictionary of arbitrary settings that are passed to ClickHouse.

Specify these settings in the `connection_settings` key. This example demonstrates how to set the `distributed_ddl_task_timeout` setting to `300`:

``` yaml linenums="1" hl_lines="8-9"
clickhouse_gateway:
  connection:
    type: clickhouse
    host: localhost
    port: 8123
    username: user
    password: pw
    connection_settings:
      distributed_ddl_task_timeout: 300
  state_connection:
    type: duckdb
```

### DDL settings

ClickHouse settings may also be specified in DDL commands like `CREATE`.

Specify these settings in a model DDL's [`physical_properties` key](https://vulcan.readthedocs.io/en/stable/concepts/models/overview/?h=physical#physical_properties) (where the [`order_by`](#order-by) and [`primary_key`](#primary-key) values are specified, if present).

This example demonstrates how to set the `index_granularity` setting to `128`:

``` sql linenums="1" hl_lines="4-6"
MODEL (
    name my_schema.my_log_table,
    kind full,
    physical_properties (
        index_granularity = 128
    )
);

select
    *
from other_schema.other_table;
```

Note that there is an `=` between the `index_granularity` key name and value `128`.

### Query settings

ClickHouse settings may be specified directly in a model's query with the `SETTINGS` keyword.

This example demonstrates setting the `join_use_nulls` setting to `1`:

``` sql linenums="1" hl_lines="9"
MODEL (
    name my_schema.my_log_table,
    kind full,
);

select
    *
from other_schema.other_table
SETTINGS join_use_nulls = 1;
```

Multiple settings may be specified in a query with repeated use of the `SETTINGS` keyword: `SELECT * FROM other_table SETTINGS first_setting = 1 SETTINGS second_setting = 2;`.

#### Usage by Vulcan

The ClickHouse setting `join_use_nulls` affects the behavior of Vulcan SCD models and table diffs. This section describes how Vulcan uses query settings to control that behavior.

^^Background^^

In general, table `JOIN`s can return empty cells for rows not present in both tables.

For example, consider `LEFT JOIN`ing two tables `left` and `right`, where the column `right_column` is only present in the `right` table. Any rows only present in the `left` table will have no value for `right_column` in the joined table.

In other SQL engines, those empty cells are filled with `NULL`s.

In contrast, ClickHouse fills the empty cells with data type-specific default values (e.g., 0 for integer column types). It will instead fill the cells with `NULL`s if you set the `join_use_nulls` setting to `1`.

^^Vulcan^^

Vulcan automatically generates SQL queries for both SCD Type 2 models and table diff comparisons. These queries include table `JOIN`s and calculations based on the presence of `NULL` values.

Because those queries expect `NULL` values in empty cells, Vulcan automatically adds `SETTINGS join_use_nulls = 1` to the generated SCD and table diff SQL code.

The SCD model definition query is embedded as a CTE in the full Vulcan-generated query. If run alone, the model definition query would use the ClickHouse server's current `join_use_nulls` value.

If that value is not `1`, the Vulcan setting on the outer query would override the server value and produce incorrect results.

Therefore, Vulcan uses the following procedure to ensure the model definition query runs with the correct `join_use_nulls` value:

- If the model query sets `join_use_nulls` itself, do nothing
- If the model query does not set `join_use_nulls` and the current server `join_use_nulls` value is `1`, do nothing
- If the model query does not set `join_use_nulls` and the current server `join_use_nulls` value is `0`, add `SETTINGS join_use_nulls = 0` to the CTE model query
    - All other CTEs and the outer query will still execute with a `join_use_nulls` value of `1`

## Performance considerations

ClickHouse is optimized for writing/reading records, so deleting/replacing records can be extremely slow.

This section describes why Vulcan needs to delete/replace records and how the ClickHouse engine adapter works around the limitations.

### Why delete or replace?

Vulcan "materializes" model kinds in a number of ways, such as:

- Replacing an entire table ([`FULL` models](configurations-old/components/model/model_kinds.md#full))
- Replacing records in a specific time range ([`INCREMENTAL_BY_TIME_RANGE` models](configurations-old/components/model/model_kinds.md#incremental_by_time_range))
- Replacing records with specific key values ([`INCREMENTAL_BY_UNIQUE_KEY` models](configurations-old/components/model/model_kinds.md#incremental_by_unique_key))
- Replacing records in specific partitions ([`INCREMENTAL_BY_PARTITION` models](configurations-old/components/model/model_kinds.md#incremental_by_partition))

Different SQL engines provide different methods for performing record replacement.

Some engines natively support updating or inserting ("upserting") records. For example, in some engines you can `merge` a new table into an existing table based on a key. Records in the new table whose keys are already in the existing table will update/replace the existing records. Records in the new table without keys in the existing table will be inserted into the existing table.

Other engines do not natively support upserts, so Vulcan replaces records in two steps: delete the records to update/replace from the existing table, then insert the new records.

ClickHouse does not support upserts, and it performs the two step delete/insert operation so slowly as to be unusable. Therefore, Vulcan uses a different method for replacing records.

### Temp table swap

Vulcan uses what we call the "temp table swap" method of replacing records in ClickHouse.

Because ClickHouse is optimized for writing and reading records, it is often faster to copy most of a table than to delete a small portion of its records. That is the approach used by the temp table swap method (with optional performance improvements [for partitioned tables](#partition-swap)).

The temp table swap has four steps:

1. Make an empty temp copy of the existing table that has the same structure (columns, data types, table engine, etc.)
2. Insert new records into the temp table
3. Insert the existing records that should be **kept** into the temp table
4. Swap the table names, such that the temp table now has the existing table's name

Figure 1 illustrates these four steps:
<br></br>

![ClickHouse table swap steps](../../configurations-old/configurations-old/configurations-old/integrations/engines/clickhouse/clickhouse_table-swap-steps.png){ loading=lazy }
_Figure 1: steps to execute a temp table swap_
<br></br>

The weakness of this method is that it requires copying all existing rows to keep (step three), which can be problematic for large tables.

To address this weakness, Vulcan instead uses *partition* swapping if a table is partitioned.

### Partition swap

ClickHouse supports *partitioned* tables, which store groups of records in separate files, or "partitions."

A table is partitioned based on a table column or SQL expression - the "partitioning key." All records with the same value for the partitioning key are stored together in a partition.

For example, consider a table containing each record's creation date in a datetime column. If we partition the table by month, all the records whose timestamp was in January will be stored in one partition, records from February in another partition, and so on.

Table partitioning provides a major benefit for improving swap performance: records can be inserted, updated, or deleted in individual partitions.

Vulcan leverages this to avoid copying large numbers of existing records into a temp table. Instead, it only copies the records that are in partitions affected by a load's newly ingested records.

Vulcan automatically uses partition swapping for any incremental model that specifies the [`partitioned_by`](configurations-old/components/model/overview.md#partitioned_by) key.

#### Choosing a partitioning key

The first step of partitioning a table is choosing its partitioning key (columns or expression). The primary consideration for a key is the total number of partitions it will generate, which affects table performance.

Too many partitions can drastically decrease performance because the overhead of handling partition files swamps the benefits of copying fewer records. Too few partitions decreases swap performance because many existing records must still be copied in each incremental load.

!!! question "How many partitions is too many?"

    ClickHouse's documentation [specifically warns against tables having too many partitions](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/custom-partitioning-key), suggesting a maximum of 1000.

The total number of partitions in a table is determined by the actual data in the table, not by the partition column/expression alone.

For example, consider a table partitioned by date. If we insert records created on `2024-10-23`, the table will have one partition. If we then insert records from `2024-10-24`, the table will have two partitions. One partition is created for each unique value of the key.

For each partitioned table in your project, carefully consider the number of partitions created by the combination of your partitioning expression and the characteristics of your data.

#### Incremental by time models

`INCREMENTAL_BY_TIME_RANGE` kind models must be partitioned by time. If the model's `time_column` is not present in any `partitioned_by` expression, Vulcan will automatically add it as the first partitioning expression.

By default, `INCREMENTAL_BY_TIME_RANGE` models partition by week, so the maximum recommended 1000 partitions corresponds to about 19 years of data. Vulcan projects have widely varying time ranges and data sizes, so you should choose a model's partitioning key based on the data your system will process.

If a model has many records in each partition, you may see additional performance benefits by including the time column in the model's [`ORDER_BY` expression](#order-by).

!!! info "Partitioning by time"
    `INCREMENTAL_BY_TIME_RANGE` models must be partitioned by time.

    Vulcan will automatically partition them by **week** unless the `partitioned_by` configuration key includes the time column or an expression based on it.

    Choose a model's time partitioning granularity based on the characteristics of the data it will process, making sure the total number of partitions is 1000 or fewer.

## Local/Built-in Scheduler

**Engine Adapter Type**: `clickhouse`

| Option                    | Description                                                                                                                                                                                                                                                                     |  Type  | Required |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----: | :------: |
| `type`                    | Engine type name - must be `clickhouse`                                                                                                                                                                                                                                         | string |    Y     |
| `host`                    | ClickHouse server hostname or IP address                                                                                                                                                                                                                                        | string |    Y     |
| `username`                | ClickHouse user name                                                                                                                                                                                                                                                            | string |    Y     |
| `password`                | ClickHouse user password                                                                                                                                                                                                                                                        | string |    N     |
| `port`                    | The ClickHouse HTTP or HTTPS port (Default: `8123`)                                                                                                                                                                                                                             |  int   |    N     |
| `cluster`                 | ClickHouse cluster name                                                                                                                                                                                                                                                         | string |    N     |
| `connect_timeout`         | Connection timeout in seconds (Default: `10`)                                                                                                                                                                                                                                   |  int   |    N     |
| `send_receive_timeout`    | Send/receive timeout in seconds (Default: `300`)                                                                                                                                                                                                                                |  int   |    N     |
| `query_limit`             | Query result limit (Default: `0` - no limit)                                                                                                                                                                                                                                    |  int   |    N     |
| `use_compression`         | Whether to use compression (Default: `True`)                                                                                                                                                                                                                                    |  bool  |    N     |
| `compression_method`      | Compression method to use                                                                                                                                                                                                                                                       | string |    N     |
| `http_proxy`              | HTTP proxy address (equivalent to setting the HTTP_PROXY environment variable)                                                                                                                                                                                                  | string |    N     |
| `verify`                  | Verify server TLS/SSL certificate (Default: `True`)                                                                                                                                                                                                                             |  bool  |    N     |
| `ca_cert`                 | Ignored if verify is `False`. If verify is `True`, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Not necessary if the ClickHouse server certificate is a globally trusted root as verified by the operating system.    | string |    N     |
| `client_cert`             | File path to a TLS Client certificate in .pem format (for mutual TLS authentication). The file should contain a full certificate chain, including any intermediate certificates.                                                                                                | string |    N     |
| `client_cert_key`         | File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file.                                                                                                                                           | string |    N     |
| `https_proxy`             | HTTPS proxy address (equivalent to setting the HTTPS_PROXY environment variable)                                                                                                                                                                                                | string |    N     |
| `server_host_name`        | The ClickHouse server hostname as identified by the CN or SNI of its TLS certificate. Set this to avoid SSL errors when connecting through a proxy or tunnel with a different hostname.                                                                                         | string |    N     |
| `tls_mode`                | Controls advanced TLS behavior. proxy and strict do not invoke ClickHouse mutual TLS connection, but do send client cert and key. mutual assumes ClickHouse mutual TLS auth with a client certificate.                                                                          | string |    N     |
| `connection_settings`     | Additional [connection settings](https://clickhouse.com/docs/integrations/python#settings-argument)                                                                                                                                                                             |  dict  |    N     |
| `connection_pool_options` | Additional [options](https://clickhouse.com/docs/integrations/python#customizing-the-http-connection-pool)                                                                                                                                         for the HTTP connection pool |  dict  |    N     |


# Engines Coming Soon

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/coming_soon/

---

# Engines Coming Soon

The following execution engines are coming soon:

- Athena
- Azure SQL
- BigQuery
- ClickHouse
- Databricks
- DuckDB
- Fabric
- MotherDuck
- MSSQL
- MySQL
- GCP Postgres
- Redshift
- RisingWave
- Snowflake
- Spark
- Trino




# Databricks

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/databricks/

---

# Databricks

This page provides information about how to use Vulcan with the Databricks SQL engine. It begins with a description of the three methods for connecting Vulcan to Databricks.

After that is a [Connection Quickstart](#connection-quickstart) that demonstrates how to connect to Databricks, or you can skip directly to information about using Databricks with the [built-in](#localbuilt-in-scheduler).

## Databricks connection methods

Databricks provides multiple computing options and connection methods. This section describes the three methods for connecting with Vulcan.

### Databricks SQL Connector

Vulcan connects to Databricks with the [Databricks SQL Connector](https://docs.databricks.com/dev-tools/python-sql-connector.html) library by default.

The SQL Connector is bundled with Vulcan and automatically installed when you include the `databricks` extra in the command `pip install "vulcan[databricks]"`.

The SQL Connector has all the functionality needed for Vulcan to execute SQL models on Databricks and Python models that do not return PySpark DataFrames.

If you have Python models returning PySpark DataFrames, check out the [Databricks Connect](#databricks-connect) section.

### Databricks Connect

If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan must use the [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html) library to connect to Databricks (instead of the Databricks SQL Connector library).

Vulcan **DOES NOT** include/bundle the Databricks Connect library. You must [install the version of Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/python/install.html) that matches the Databricks Runtime used in your Databricks cluster.

Find [more configuration details below](#databricks-connect).

### Databricks notebook interface

If you are always running Vulcan commands directly in a Databricks Cluster interface, the SparkSession provided by Databricks is used to execute all Vulcan commands.

Find [more configuration details below](#databricks-notebook-interface).

## Connection quickstart

Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Databricks.

It demonstrates connecting to a Databricks [All-Purpose Compute](https://docs.databricks.com/en/compute/index.html) instance with the `databricks-sql-connector` Python library bundled with Vulcan.

!!! tip
    This quickstart assumes you are familiar with basic Vulcan commands and functionality.

    If you're not, work through the [Vulcan Quickstart](configurations-old/guides/get-started/docker.md) before continuing!

### Prerequisites

Before working through this connection quickstart, ensure that:

1. You have a Databricks account with access to an appropriate Databricks Workspace
    - The Workspace must support authenticating with [personal access tokens](https://docs.databricks.com/en/dev-tools/auth/pat.html) (Databricks [Community Edition workspaces do not](https://docs.databricks.com/en/admin/access-control/tokens.html))
    - Your account must have Workspace Access and Create Compute permissions (these permissions are enabled by default)
2. Your Databricks compute resources have [Unity Catalog](https://docs.databricks.com/aws/en/data-governance/unity-catalog/) activated
3. Your computer has Vulcan installed with the Databricks extra available
    - Install from the command line with the command `pip install "vulcan[databricks]"`
4. You have initialized a Vulcan example project on your computer
    - Open a command line interface and navigate to the directory where the project files should go
    - Initialize the project with the command `vulcan init duckdb`

!!! important "Unity Catalog required"

    Databricks compute resources used by Vulcan must have [Unity Catalog](https://docs.databricks.com/aws/en/data-governance/unity-catalog/) activated.

### Get connection info

The first step to configuring a Databricks connection is gathering the necessary information from your Databricks compute instance.

#### Create Compute

We must have something to connect to, so we first create and activate a Databricks compute instance. If you already have one running, skip to the [next section](#get-jdbcodbc-info).

We begin in the default view for our Databricks Workspace. Access the Compute view by clicking the `Compute` entry in the left-hand menu:

![Databricks Workspace default view](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_workspace.png){ loading=lazy }

In the Compute view, click the `Create compute` button:

![Databricks Compute default view](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_compute.png){ loading=lazy }

Modify compute cluster options if desired and click the `Create compute` button:

![Databricks Create Compute view](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_compute-create.png){ loading=lazy }

#### Get JDBC/ODBC info

Scroll to the bottom of the view and click the open the `Advanced Options` view:

![Databricks Compute Advanced Options link](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_compute-advanced-options-link.png){ loading=lazy }

Click the `JDBC/ODBC` tab:

![Databricks Compute Advanced Options JDBC/ODBC tab](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_advanced-options.png){ loading=lazy }

Open your project's `config.yaml` configuration file in a text editor and add a new gateway named `databricks` below the existing `local` gateway:

![Project config.yaml databricks gateway](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_config-yaml.png){ loading=lazy }

Copy the `server_hostname` and `http_path` connection values from the Databricks JDBC/ODBC tab to the `config.yaml` file:

![Copy server_hostname and http_path to config.yaml](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_copy-server-http.png){ loading=lazy }

#### Get personal access token

The final piece of information we need for the `config.yaml` file is your personal access token.

!!! warning
    **Do not share your personal access token with anyone.**

    Best practice for storing secrets like access tokens is placing them in [environment variables that the configuration file loads dynamically](../../guides-old/configuration.md#environment-variables). For simplicity, this guide instead places the value directly in the configuration file.

    This code demonstrates how to use the environment variable `DATABRICKS_ACCESS_TOKEN` for the configuration's `access_token` parameter:

    ```yaml linenums="1"
    gateways:
      databricks:
          connection:
            type: databricks
            access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}
    ```

<br></br>
To create a personal access token, click on your profile logo and go to your profile's `Settings` page:

![Navigate to profile Settings page](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_profile-settings-link.png){ loading=lazy }

Go to the `Developer` view in the User menu. Depending on your account's role, your page may not display the Workspace Admin section of the page.

![Navigate to User Developer view](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_profile-settings-developer.png){ loading=lazy }

Click the `Manage` button in the Access Tokens section:

![Navigate to Access Tokens management](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_access-tokens-link.png){ loading=lazy }

Click the `Generate new token` button:

![Open the token generation menu](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_access-tokens-generate-button.png){ loading=lazy }

Name your token in the `Comment` field, and click the `Generate` button:

![Generate a new token](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_access-tokens-generate.png){ loading=lazy }

Click the copy button and paste the token into the `access_token` key:

![Copy token to config.yaml access_token key](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_copy-token.png){ loading=lazy }

!!! warning
    **Do not share your personal access token with anyone.**

    Best practice for storing secrets like access tokens is placing them in [environment variables that the configuration file loads dynamically](../../guides-old/configuration.md#environment-variables). For simplicity, this guide instead places the value directly in the configuration file.

    This code demonstrates how to use the environment variable `DATABRICKS_ACCESS_TOKEN` for the configuration's `access_token` parameter:

    ```yaml linenums="1"
    gateways:
      databricks:
          connection:
            type: databricks
            access_token: {{ env_var('DATABRICKS_ACCESS_TOKEN') }}
    ```

### Check connection

We have now specified the `databricks` gateway connection information, so we can confirm that Vulcan is able to successfully connect to Databricks. We will test the connection with the `vulcan info` command.

First, open a command line terminal. Now enter the command `vulcan --gateway databricks info`.

We manually specify the `databricks` gateway because it is not our project's default gateway:

![Run vulcan info command in CLI](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-info.png){ loading=lazy }

The output shows that our data warehouse connection succeeded:

![Successful data warehouse connection](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-info-succeeded.png){ loading=lazy }

However, the output includes a `WARNING` about using the Databricks SQL engine for storing Vulcan state:

![Databricks state connection warning](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-info-warning.png){ loading=lazy }

!!! warning
    Databricks is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.

    Learn more about storing Vulcan state [here](../../guides-old/configuration.md#state-connection).

### Specify state connection

We can store Vulcan state in a different SQL engine by specifying a `state_connection` in our `databricks` gateway.

This example uses the DuckDB engine to store state in the local `databricks_state.db` file:

![Specify DuckDB state connection](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_state-connection.png){ loading=lazy }

Now we no longer see the warning when running `vulcan --gateway databricks info`, and we see a new entry `State backend connection succeeded`:

![No state connection warning](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-info-no-warning.png){ loading=lazy }

### Run a `vulcan plan`

For convenience, we can omit the `--gateway` option from our CLI commands by specifying `databricks` as our project's `default_gateway`:

![Specify databricks as default gateway](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_default-gateway.png){ loading=lazy }

And run a `vulcan plan` in Databricks:

![Run vulcan plan in databricks](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-plan.png){ loading=lazy }

And confirm that our schemas and objects exist in the Databricks catalog:

![Vulcan plan objects in databricks](../../configurations-old/configurations-old/configurations-old/integrations/engines/databricks/db-guide_sqlmesh-plan-objects.png){ loading=lazy }

Congratulations - your Vulcan project is up and running on Databricks!

!!! tip
    Vulcan connects to your Databricks Cluster's default catalog by default. Connect to a different catalog by specifying its name in the connection configuration's [`catalog` parameter](#connection-options).

## Local/Built-in Scheduler
**Engine Adapter Type**: `databricks`

### Installation
```
pip install "vulcan[databricks]"
```

### Connection method details

Databricks provides multiple computing options and connection methods. The [section above](#databricks-connection-methods) explains how to use them with Vulcan, and this section provides additional configuration details.

#### Databricks SQL Connector

Vulcan uses the [Databricks SQL Connector](https://docs.databricks.com/dev-tools/python-sql-connector.html) to connect to Databricks by default. Learn [more above](#databricks-sql-connector).

#### Databricks Connect

If you want Databricks to process PySpark DataFrames in Vulcan Python models, then Vulcan needs to use the [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html) to connect to Databricks (instead of the Databricks SQL Connector).

Vulcan **DOES NOT** include/bundle the Databricks Connect library. You must [install the version of Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/python/install.html) that matches the Databricks Runtime used in your Databricks cluster.

If Vulcan detects that you have Databricks Connect installed, then it will automatically configure the connection and use it for all Python models that return a Pandas or PySpark DataFrame.

To have databricks-connect installed but ignored by Vulcan, set `disable_databricks_connect` to `true` in the connection configuration.

Databricks Connect can execute SQL and DataFrame operations on different clusters by setting the Vulcan `databricks_connect_*` connection options. For example, these options could configure Vulcan to run SQL on a [Databricks SQL Warehouse](https://docs.databricks.com/sql/admin/create-sql-warehouse.html) while still routing DataFrame operations to a normal Databricks Cluster.

!!! note
    If using Databricks Connect, make sure to learn about the Databricks [requirements](https://docs.databricks.com/dev-tools/databricks-connect.html#requirements) and [limitations](https://docs.databricks.com/dev-tools/databricks-connect.html#limitations).

#### Databricks notebook interface

If you are always running Vulcan commands directly on a Databricks Cluster (like in a Databricks Notebook), the SparkSession provided by Databricks is used to execute all Vulcan commands.

The only relevant Vulcan configuration parameter is the optional `catalog` parameter.

### Connection options

| Option                               | Description                                                                                                                                                                                                                                         |  Type  | Required |
|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                               | Engine type name - must be `databricks`                                                                                                                                                                                                             | string |    Y     |
| `server_hostname`                    | Databricks instance host name                                                                                                                                                                                                                       | string |    N     |
| `http_path`                          | HTTP path, either to a DBSQL endpoint (such as `/sql/1.0/endpoints/1234567890abcdef`) or to an All-Purpose cluster (such as `/sql/protocolv1/o/1234567890123456/1234-123456-slid123`)                                                               | string |    N     |
| `access_token`                       | HTTP Bearer access token, such as Databricks Personal Access Token                                                                                                                                                                                  | string |    N     |
| `catalog`                            | The name of the catalog to use for the connection. [Defaults to use Databricks cluster default](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#the-default-catalog-configuration-when-unity-catalog-is-enabled). | string |    N     |
| `auth_type`                          | SQL Connector Only: Set to 'databricks-oauth' or 'azure-oauth' to trigger OAuth (or dont set at all to use `access_token`)                                                                                                                          | string |    N     |
| `oauth_client_id`                    | SQL Connector Only: Optional [M2M](https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication) OAuth Client ID to use when `auth_type` is set                                                   | string |    N     |
| `oauth_client_secret`                | SQL Connector Only: Optional [M2M](https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication) OAuth Client Secret to use when `auth_type` is set                                               | string |    N     |
| `http_headers`                       | SQL Connector Only: An optional dictionary of HTTP headers that will be set on every request                                                                                                                                                        |  dict  |    N     |
| `session_configuration`              | SQL Connector Only: An optional dictionary of Spark session parameters. Execute the SQL command `SET -v` to get a full list of available commands.                                                                                                  |  dict  |    N     |
| `databricks_connect_server_hostname` | Databricks Connect Only: Databricks Connect server hostname. Uses `server_hostname` if not set.                                                                                                                                                     | string |    N     |
| `databricks_connect_access_token`    | Databricks Connect Only: Databricks Connect access token. Uses `access_token` if not set.                                                                                                                                                           | string |    N     |
| `databricks_connect_cluster_id`      | Databricks Connect Only: Databricks Connect cluster ID. Uses `http_path` if not set. Cannot be a Databricks SQL Warehouse.                                                                                                                          | string |    N     |
| `databricks_connect_use_serverless`  | Databricks Connect Only: Use a serverless cluster for Databricks Connect instead of `databricks_connect_cluster_id`.                                                                                                                                |  bool  |    N     |
| `force_databricks_connect`           | When running locally, force the use of Databricks Connect for all model operations (so don't use SQL Connector for SQL models)                                                                                                                      |  bool  |    N     |
| `disable_databricks_connect`         | When running locally, disable the use of Databricks Connect for all model operations (so use SQL Connector for all models)                                                                                                                          |  bool  |    N     |
| `disable_spark_session`              | Do not use SparkSession if it is available (like when running in a notebook).                                                                                                                                                                       |  bool  |    N     |

## Model table properties to support altering tables

If you are making a change to the structure of a table that is [forward only](configurations-old/guides/incremental_by_time.md#forward-only-models), then you may need to add the following to your model's `physical_properties`:


```sql
MODEL (
    name vulcan_example.new_model,
    ...
    physical_properties (
        'delta.columnMapping.mode' = 'name'
    ),
)
```

If you attempt to alter without having this property set, you will get an error similar to `databricks.sql.exc.ServerOperationError: [DELTA_UNSUPPORTED_DROP_COLUMN] DROP COLUMN is not supported for your Delta table.`.
[Databricks Documentation for more details](https://docs.databricks.com/en/delta/column-mapping.html#requirements).



# DuckDB

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/duckdb/

---

# DuckDB

!!! warning "DuckDB state connection limitations"
    DuckDB is a [single user](https://duckdb.org/docs/connect/concurrency.html#writing-to-duckdb-from-multiple-processes) database. Using it for a state connection in your Vulcan project limits you to a single workstation. This means your project cannot be shared amongst your team members or your CI/CD infrastructure. This is usually fine for proof of concept or test projects but it will not scale to production usage.

    For production projects, use [Tobiko Cloud](https://tobikodata.com/product.html) or a more robust state database such as [Postgres](../../configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md).

## Local/Built-in Scheduler
**Engine Adapter Type**: `duckdb`

### Connection options

| Option             | Description                                                                                                                                                                                                                                     |   Type    | Required |
|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------:|:--------:|
| `type`             | Engine type name - must be `duckdb`                                                                                                                                                                                                             |  string   |    Y     |
| `database`         | The optional database name. If not specified, the in-memory database is used. Cannot be defined if using `catalogs`.                                                                                                                            |  string   |    N     |
| `catalogs`         | Mapping to define multiple catalogs. Can [attach DuckDB catalogs](#duckdb-catalogs-example) or [catalogs for other connections](#other-connection-catalogs-example). First entry is the default catalog. Cannot be defined if using `database`. |   dict    |    N     |
| `extensions`       | Extension to load into duckdb. Only autoloadable extensions are supported.                                                                                                                                                                      |   list    |    N     |
| `connector_config` | Configuration to pass into the duckdb connector.                                                                                                                                                                                                |   dict    |    N     |
| `secrets`          | Configuration for authenticating external sources (e.g., S3) using DuckDB secrets. Can be a list of secret configurations or a dictionary with custom secret names.                                                                             | list/dict |    N     |
| `filesystems`      | Configuration for registering `fsspec` filesystems to the DuckDB connection.                                                                                                                                                                    |   dict    |    N     |

#### DuckDB Catalogs Example

This example specifies two catalogs. The first catalog is named "persistent" and maps to the DuckDB file database `local.duckdb`. The second catalog is named "ephemeral" and maps to the DuckDB in-memory database.

`persistent` is the default catalog since it is the first entry in the dictionary. Vulcan will place models without an explicit catalog, such as `my_schema.my_model`, into the `persistent` catalog `local.duckdb` DuckDB file database.

Vulcan will place models with the explicit catalog "ephemeral", such as `ephemeral.other_schema.other_model`, into the `ephemeral` catalog DuckDB in-memory database.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: duckdb
          catalogs:
            persistent: 'local.duckdb'
            ephemeral: ':memory:'
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=DuckDBConnectionConfig(
                    catalogs={
                        "persistent": "local.duckdb"
                        "ephemeral": ":memory:"
                    }
                )
            ),
        }
    )
    ```

#### DuckLake Catalog Example

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: duckdb
          catalogs:
            ducklake:
              type: ducklake
              path: 'catalog.ducklake'
              data_path: data/ducklake
              encrypted: True
              data_inlining_row_limit: 10
    ```
    
=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )
    from vulcan.core.config.connection import DuckDBAttachOptions

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=DuckDBConnectionConfig(
                    catalogs={
                        "ducklake": DuckDBAttachOptions(
                            type="ducklake",
                            path="catalog.ducklake",
                            data_path="data/ducklake",
                            encrypted=True,
                            data_inlining_row_limit=10,
                        ),
                    }
                )
            ),
        }
    )
    ```

#### Other Connection Catalogs Example

Catalogs can also be defined to connect to anything that [DuckDB can be attached to](https://duckdb.org/docs/sql/statements/attach.html).

Below are examples of connecting to a SQLite database and a PostgreSQL database.
The SQLite database is read-write, while the PostgreSQL database is read-only.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      my_gateway:
        connection:
          type: duckdb
          catalogs:
            memory: ':memory:'
            sqlite:
              type: sqlite
              path: 'test.db'
            postgres:
              type: postgres
              path: 'dbname=postgres user=postgres host=127.0.0.1'
              read_only: true
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )
    from vulcan.core.config.connection import DuckDBAttachOptions

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect=<dialect>),
        gateways={
            "my_gateway": GatewayConfig(
                connection=DuckDBConnectionConfig(
                    catalogs={
                        "memory": ":memory:",
                        "sqlite": DuckDBAttachOptions(
                            type="sqlite",
                            path="test.db"
                        ),
                        "postgres": DuckDBAttachOptions(
                            type="postgres",
                            path="dbname=postgres user=postgres host=127.0.0.1",
                            read_only=True
                        ),
                    }
                )
            ),
        }
    )
    ```

##### Catalogs for PostgreSQL

In PostgreSQL, the catalog name must match the actual catalog name it is associated with, as shown in the example above, where the database name (`dbname` in the path) is the same as the catalog name.

##### Connectors without schemas

Some connections, like SQLite, do not support schema names and therefore objects will be attached under the default schema name of `main`.

Example: mounting a SQLite database with the name `sqlite` that has a table `example_table` will be accessible as `sqlite.main.example_table`.

##### Sensitive fields in paths

If a connector, like Postgres, requires sensitive information in the path, it might support defining environment variables instead.
[See DuckDB Documentation for more information](https://duckdb.org/docs/extensions/postgres#configuring-via-environment-variables).

#### Cloud service authentication

DuckDB can read data directly from cloud services via extensions (e.g., [httpfs](https://duckdb.org/docs/extensions/httpfs/s3api), [azure](https://duckdb.org/docs/extensions/azure)).

The `secrets` option allows you to configure DuckDB's [Secrets Manager](https://duckdb.org/docs/configuration/secrets_manager.html) to authenticate with external services like S3. This is the recommended approach for cloud storage authentication in DuckDB v0.10.0 and newer, replacing the [legacy authentication method](https://duckdb.org/docs/stable/extensions/httpfs/s3api_legacy_authentication.html) via variables.

##### Secrets Configuration

The `secrets` option supports two formats:

1. **List format** (default secrets): A list of secret configurations where each secret uses DuckDB's default naming
2. **Dictionary format** (named secrets): A dictionary where keys are custom secret names and values are the secret configurations

This flexibility allows you to organize multiple secrets of the same type or reference specific secrets by name in your SQL queries.

##### List Format Example (Default Secrets)

Using a list creates secrets with DuckDB's default naming:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      duckdb:
        connection:
          type: duckdb
          catalogs:
            local: local.db
            remote: "s3://bucket/data/remote.duckdb"
          extensions:
            - name: httpfs
          secrets:
            - type: s3
              region: "YOUR_AWS_REGION"
              key_id: "YOUR_AWS_ACCESS_KEY"
              secret: "YOUR_AWS_SECRET_KEY"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="duckdb"),
        gateways={
            "duckdb": GatewayConfig(
                connection=DuckDBConnectionConfig(
                    catalogs={
                        "local": "local.db",
                        "remote": "s3://bucket/data/remote.duckdb"
                    },
                    extensions=[
                        {"name": "httpfs"},
                    ],
                    secrets=[
                        {
                            "type": "s3",
                            "region": "YOUR_AWS_REGION",
                            "key_id": "YOUR_AWS_ACCESS_KEY",
                            "secret": "YOUR_AWS_SECRET_KEY"
                        }
                    ]
                )
            ),
        }
    )
    ```

##### Dictionary Format Example (Named Secrets)

Using a dictionary allows you to assign custom names to your secrets for better organization and reference:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      duckdb:
        connection:
          type: duckdb
          catalogs:
            local: local.db
            remote: "s3://bucket/data/remote.duckdb"
          extensions:
            - name: httpfs
          secrets:
            my_s3_secret:
              type: s3
              region: "YOUR_AWS_REGION"
              key_id: "YOUR_AWS_ACCESS_KEY"
              secret: "YOUR_AWS_SECRET_KEY"
            my_azure_secret:
              type: azure
              account_name: "YOUR_AZURE_ACCOUNT"
              account_key: "YOUR_AZURE_KEY"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import (
        Config,
        ModelDefaultsConfig,
        GatewayConfig,
        DuckDBConnectionConfig
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="duckdb"),
        gateways={
            "duckdb": GatewayConfig(
                connection=DuckDBConnectionConfig(
                    catalogs={
                        "local": "local.db",
                        "remote": "s3://bucket/data/remote.duckdb"
                    },
                    extensions=[
                        {"name": "httpfs"},
                    ],
                    secrets={
                        "my_s3_secret": {
                            "type": "s3",
                            "region": "YOUR_AWS_REGION",
                            "key_id": "YOUR_AWS_ACCESS_KEY",
                            "secret": "YOUR_AWS_SECRET_KEY"
                        },
                        "my_azure_secret": {
                            "type": "azure",
                            "account_name": "YOUR_AZURE_ACCOUNT",
                            "account_key": "YOUR_AZURE_KEY"
                        }
                    }
                )
            ),
        }
    )
    ```

After configuring the secrets, you can directly reference S3 paths in your catalogs or in SQL queries without additional authentication steps.

Refer to the official DuckDB documentation for the full list of [supported S3 secret parameters](https://duckdb.org/docs/stable/extensions/httpfs/s3api.html#overview-of-s3-secret-parameters) and for more information on the [Secrets Manager configuration](https://duckdb.org/docs/configuration/secrets_manager.html).

> Note: Loading credentials at runtime using `load_aws_credentials()` or similar deprecated functions may fail when using Vulcan.

##### File system configuration example for Microsoft Onelake

The `filesystems` accepts a list of file systems to register in the DuckDB connection. This is especially useful for Azure Storage Accounts, as it adds write support for DuckDB which is not natively supported by DuckDB (yet).


=== "YAML"

    ```yaml linenums="1"
    gateways:
      ducklake:
        connection:
          type: duckdb
          catalogs:
            ducklake:
              type: ducklake
              path: myducklakecatalog.duckdb
              data_path: abfs://MyFabricWorkspace/MyFabricLakehouse.Lakehouse/Files/DuckLake.Files
        extensions:
          - ducklake
        filesystems:
          - fs: abfs
            account_name: onelake
            account_host: onelake.blob.fabric.microsoft.com
            client_id: {{ env_var('AZURE_CLIENT_ID') }}
            client_secret: {{ env_var('AZURE_CLIENT_SECRET') }}
            tenant_id: {{ env_var('AZURE_TENANT_ID') }}
            # anon: False # To use azure.identity.DefaultAzureCredential authentication 
    ```


Refer to the documentation for `fsspec` [fsspec.filesystem](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.filesystem) and `adlfs` [adlfs.AzureBlobFileSystem](https://fsspec.github.io/adlfs/api/#api-reference) for a full list of storage options. 



# Fabric

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/fabric/

---

# Fabric

!!! info
    The Fabric engine adapter is a community contribution. Due to this, only limited community support is available.

## Local/Built-in Scheduler
**Engine Adapter Type**: `fabric`

NOTE: Fabric Warehouse is not recommended to be used for the Vulcan [state connection](configurations-old/configurations-old/configuration.md#connections).

### Installation
#### Microsoft Entra ID / Azure Active Directory Authentication:
```
pip install "vulcan[fabric]"
```

### Connection options

| Option            | Description                                                  |     Type     | Required |
| ----------------- | ------------------------------------------------------------ | :----------: | :------: |
| `type`            | Engine type name - must be `fabric`                           |    string    |    Y     |
| `host`            | The hostname of the Fabric Warehouse server                             |    string    |    Y     |
| `user`            | The client id to use for authentication with the Fabric Warehouse server |    string    |    N     |
| `password`        | The client secret to use for authentication with the Fabric Warehouse server |    string    |    N     |
| `port`            | The port number of the Fabric Warehouse server                          |     int      |    N     |
| `database`        | The target database                                          |    string    |    N     |
| `charset`         | The character set used for the connection                    |    string    |    N     |
| `timeout`         | The query timeout in seconds. Default: no timeout            |     int      |    N     |
| `login_timeout`   | The timeout for connection and login in seconds. Default: 60 |     int      |    N     |
| `appname`         | The application name to use for the connection               |    string    |    N     |
| `conn_properties` | The list of connection properties                            | list[string] |    N     |
| `autocommit`      | Is autocommit mode enabled. Default: false                   |     bool     |    N     |
| `driver`          | The driver to use for the connection. Default: pyodbc            |    string    |    N     |
| `driver_name`     | The driver name to use for the connection. E.g., *ODBC Driver 18 for SQL Server* |    string    |    N     |
| `tenant_id`          | The Azure / Entra tenant UUID                             |    string    |    Y     |
| `workspace_id`       | The Fabric workspace UUID. The preferred way to retrieve it is by running `notebookutils.runtime.context.get("currentWorkspaceId")` in a python notebook.                             |    string    |    Y     |
| `odbc_properties` | The dict of ODBC connection properties. E.g., authentication: ActiveDirectoryServicePrincipal. See more [here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16). | dict |    N     |



# GCP Postgres

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/gcp-postgres/

---

# GCP Postgres

## Local/Built-in Scheduler
**Engine Adapter Type**: `gcp_postgres`

### Installation
```
pip install "vulcan[gcppostgres]"
```

### Connection options

| Option                       | Description                                                                                            |    Type    | Required |
|------------------------------|--------------------------------------------------------------------------------------------------------|:----------:|:--------:|
| `type`                       | Engine type name - must be `gcp_postgres`                                                              |   string   |    Y     |
| `instance_connection_string` | Connection name for the postgres instance                                                              |   string   |    Y     |
| `user`                       | The username (postgres or IAM) to use for authentication                                               |   string   |    Y     |
| `password`                   | The password to use for authentication. Required when connecting as a Postgres user                    |   string   |    N     |
| `enable_iam_auth`            | Enables IAM authentication. Required when connecting as an IAM user                                    |  boolean   |    N     |
| `keyfile`                    | Path to the keyfile to be used with enable_iam_auth instead of ADC                                     |   string   |    N     |
| `keyfile_json`               | Keyfile information provided inline (not recommended)                                                  |    dict    |    N     |
| `db`                         | The name of the database instance to connect to                                                        |   string   |    Y     |
| `ip_type`                    | The IP type to use for the connection. Must be one of `public`, `private`, or `psc`. Default: `public` |   string   |    N     |
| `timeout`                    | The connection timeout in seconds. Default: `30`                                                       |  integer   |    N     |
| `scopes`                     | The scopes to use for the connection. Default: `(https://www.googleapis.com/auth/sqlservice.admin,)`   | tuple[str] |    N     |
| `driver`                     | The driver to use for the connection. Default: `pg8000`. Note: only `pg8000` is tested                 |   string   |    N     |



# MotherDuck

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/motherduck/

---

# MotherDuck

This page provides information about how to use Vulcan with MotherDuck.

It begins with a [Connection Quickstart](#connection-quickstart) that demonstrates how to connect to MotherDuck, or you can skip directly to information about using MotherDuck with the built-in scheduler.

## Connection quickstart

Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with MotherDuck.

It demonstrates connecting to MotherDuck with the `duckdb` library bundled with Vulcan.

MotherDuck provides a single way to authorize a connection. This quickstart demonstrates authenticating with a token.

!!! tip
    This quick start assumes you are familiar with basic Vulcan commands and functionality.

    If you're not familiar, work through the [Vulcan Quickstart](configurations-old/guides/get-started/docker.md) before continuing.

### Prerequisites

Before working through this quickstart guide, ensure that:

1. You have a motherduck account and an access token.
2. Your computer has Vulcan installed with the DuckDB extra available.
   1. Install from command line with the command `pip install “vulcan[duckdb]”`
3. You have initialized a Vulcan example project on your computer
   1. Open a command line interface and navigate to the directory where the project files should go.
   2. Initialize the project with the command `vulcan init duckdb`, since `duckdb` is the dialect.

#### Access control permissions

Vulcan must have sufficient permissions to create and access your MotherDuck databases. Since permission is granted to specific databases for a specific user, you should create a service account for Vulcan that will contain the credentials for writing to MotherDuck.

### Configure the connection

We now have what is required to configure Vulcan’s connection to MotherDuck.

We start the configuration by adding a gateway named `motherduck` to our example project’s config.yaml file and making it our `default gateway`, as well as adding our token, persistent, and ephemeral catalogs.

```yaml
gateways:
  motherduck:
    connection:
      type: motherduck
        catalogs:
          persistent: "md:"
          ephemeral: ":memory:"
      token: <your_token>

default_gateway: motherduck
```

Catalogs can be defined to connect to anything that [DuckDB can be attached to](../../configurations-old/configurations-old/configurations-old/integrations/engines/duckdb.md#other-connection-catalogs-example).

!!! warning
    Best practice for storing secrets like tokens is placing them in [environment variables that the configuration file loads dynamically](../../guides-old/configuration.md#environment-variables). For simplicity, this guide instead places the value directly in the configuration file.

    This code demonstrates how to use the environment variable `MOTHERDUCK_TOKEN` for the configuration's `token` parameter:

    ```yaml linenums="1" hl_lines="5"
    gateways:
      motherduck:
        connection:
          type: motherduck
          token: {{ env_var('MOTHERDUCK_TOKEN') }}
    ```

### Check connection

We have now specified the `motherduck` gateway connection information, so we can confirm that Vulcan is able to successfully connect to MotherDuck. We will test the connection with the `vulcan info` command.

First, open a command line terminal. Now enter the command `vulcan info`:

![](./motherduck/sqlmesh_info.png)

The output shows that our data warehouse connection succeeded:

![](./motherduck/info_output.png)

### Run a `vulcan plan`

Now we're ready to run a `vulcan plan` in MotherDuck:

![](./motherduck/sqlmesh_plan.png)

And confirm that our schemas and objects exist in the MotherDuck catalog:

![](./motherduck/motherduck_ui.png)

Congratulations \- your Vulcan project is up and running on MotherDuck\!


## Local/Built-in Scheduler

**Engine Adapter Type**: `motherduck`

### Connection options

| Option             | Description                                                                                                 | Type   | Required |
|--------------------|-------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`             | Engine type name - must be `motherduck`                                                                     | string | Y        |
| `database`         | The database name.                                                                                          | string | Y        |
| `token`            | The optional MotherDuck token. If not specified, the user will be prompted to login with their web browser. | string | N        |
| `extensions`       | Extension to load into duckdb. Only autoloadable extensions are supported.                                  | list   | N        |
| `connector_config` | Configuration to pass into the duckdb connector.                                                            | dict   | N        |
| `secrets`   | Configuration for authenticating external sources (e.g. S3) using DuckDB secrets.                           | dict   | N        |


# MSSQL

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/mssql/

---

# MSSQL

## Installation

### User / Password Authentication:
```
pip install "vulcan[mssql]"
```
### Microsoft Entra ID / Azure Active Directory Authentication:
```
pip install "vulcan[mssql-odbc]"
```

## Incremental by unique key `MERGE`

Vulcan executes a `MERGE` statement to insert rows for [incremental by unique key](configurations-old/components/model/model_kinds.md#incremental_by_unique_key) model kinds.

By default, the `MERGE` statement updates all non-key columns of an existing row when a new row with the same key values is inserted. If all column values match between the two rows, those updates are unnecessary.

Vulcan provides an optional performance optimization that skips unnecessary updates by comparing column values with the `EXISTS` and `EXCEPT` operators.

Enable the optimization by setting the `mssql_merge_exists` key to `true` in the [`physical_properties`](configurations-old/components/model/overview.md#physical_properties) section of the `MODEL` statement.

For example:

```sql linenums="1" hl_lines="7-9"
MODEL (
    name vulcan_example.unique_key,
    kind INCREMENTAL_BY_UNIQUE_KEY (
        unique_key id
    ),
    cron '@daily',
    physical_properties (
        mssql_merge_exists = true
    )
);
```

!!! warning "Not all column types supported"
    The `mssql_merge_exists` optimization is not supported for all column types, including `GEOMETRY`, `XML`, `TEXT`, `NTEXT`, `IMAGE`, and most user-defined types.

    Learn more in the [MSSQL `EXCEPT` statement documentation](https://learn.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql?view=sql-server-ver17#arguments).

## Local/Built-in Scheduler
**Engine Adapter Type**: `mssql`

### Connection options

| Option            | Description                                                                                                                                                                                                               |     Type     | Required |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------: | :------: |
| `type`            | Engine type name - must be `mssql`                                                                                                                                                                                        |    string    |    Y     |
| `host`            | The hostname of the MSSQL server                                                                                                                                                                                          |    string    |    Y     |
| `user`            | The username / client id to use for authentication with the MSSQL server                                                                                                                                                  |    string    |    N     |
| `password`        | The password / client secret to use for authentication with the MSSQL server                                                                                                                                              |    string    |    N     |
| `port`            | The port number of the MSSQL server                                                                                                                                                                                       |     int      |    N     |
| `database`        | The target database                                                                                                                                                                                                       |    string    |    N     |
| `charset`         | The character set used for the connection                                                                                                                                                                                 |    string    |    N     |
| `timeout`         | The query timeout in seconds. Default: no timeout                                                                                                                                                                         |     int      |    N     |
| `login_timeout`   | The timeout for connection and login in seconds. Default: 60                                                                                                                                                              |     int      |    N     |
| `appname`         | The application name to use for the connection                                                                                                                                                                            |    string    |    N     |
| `conn_properties` | The list of connection properties                                                                                                                                                                                         | list[string] |    N     |
| `autocommit`      | Is autocommit mode enabled. Default: false                                                                                                                                                                                |     bool     |    N     |
| `driver`          | The driver to use for the connection. Default: pymssql                                                                                                                                                                    |    string    |    N     |
| `driver_name`     | The driver name to use for the connection (e.g., *ODBC Driver 18 for SQL Server*).                                                                                                                                          |    string    |    N     |
| `odbc_properties` | ODBC connection properties (e.g., *authentication: ActiveDirectoryServicePrincipal*). See more [here](https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver16). |     dict     |    N     |


# MySQL

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/mysql/

---

# MySQL

## Local/Built-in Scheduler
**Engine Adapter Type**: `mysql`

### Installation
```
pip install "vulcan[mysql]"
```

### Connection options

| Option         | Description                                                  | Type   | Required |
|----------------|--------------------------------------------------------------|:------:|:--------:|
| `type`         | Engine type name - must be `mysql`                           | string | Y        |
| `host`         | The hostname of the MysQL server                             | string | Y        |
| `user`         | The username to use for authentication with the MySQL server | string | Y        |
| `password`     | The password to use for authentication with the MySQL server | string | Y        |
| `port`         | The port number of the MySQL server                          | int    | N        |
| `charset`      | The character set used for the connection                    | string | N        |
| `ssl_disabled` | Is SSL disabled                                              | bool   | N        |



# Postgres

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/postgres/

---

# Postgres

## Local/Built-in Scheduler
**Engine Adapter Type**: `postgres`

### Connection options

| Option             | Description                                                                     | Type   | Required |
|--------------------|---------------------------------------------------------------------------------|:------:|:--------:|
| `type`             | Engine type name - must be `postgres`                                           | string | Y        |
| `host`             | The hostname of the Postgres server                                             | string | Y        |
| `user`             | The username to use for authentication with the Postgres server                 | string | Y        |
| `password`         | The password to use for authentication with the Postgres server                 | string | Y        |
| `port`             | The port number of the Postgres server                                          | int    | Y        |
| `database`         | The name of the database instance to connect to                                 | string | Y        |
| `keepalives_idle`  | The number of seconds between each keepalive packet sent to the server.         | int    | N        |
| `connect_timeout`  | The number of seconds to wait for the connection to the server. (Default: `10`) | int    | N        |
| `role`             | The role to use for authentication with the Postgres server                     | string | N        |
| `sslmode`          | The security of the connection to the Postgres server                           | string | N        |
| `application_name` | The name of the application to use for the connection                           | string | N        |



# Redshift

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/redshift/

---

# Redshift

## Local/Built-in Scheduler
**Engine Adapter Type**: `redshift`

### Installation
```
pip install "vulcan[redshift]"
```

### Connection options

| Option                  | Description                                                                                                 |  Type  | Required |
|-------------------------|-------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                  | Engine type name - must be `redshift`                                                                       | string |    Y     |
| `user`                  | The username to use for authentication with the Amazon Redshift cluster                                     | string |    N     |
| `password`              | The password to use for authentication with the Amazon Redshift cluster                                     | string |    N     |
| `database`              | The name of the database instance to connect to                                                             | string |    N     |
| `host`                  | The hostname of the Amazon Redshift cluster                                                                 | string |    N     |
| `port`                  | The port number of the Amazon Redshift cluster                                                              |  int   |    N     |
| `ssl`                   | Is SSL enabled. SSL must be enabled when authenticating using IAM (Default: `True`)                         |  bool  |    N     |
| `sslmode`               | The security of the connection to the Amazon Redshift cluster. `verify-ca` and `verify-full` are supported. | string |    N     |
| `timeout`               | The number of seconds before the connection to the server will timeout.                                     |  int   |    N     |
| `tcp_keepalive`         | Is [TCP keepalive](https://en.wikipedia.org/wiki/Keepalive#TCP_keepalive) used. (Default: `True`)           |  bool  |    N     |
| `application_name`      | The name of the application                                                                                 | string |    N     |
| `preferred_role`        | The IAM role preferred for the current connection                                                           | string |    N     |
| `principal_arn`         | The ARN of the IAM entity (user or role) for which you are generating a policy                              | string |    N     |
| `credentials_provider`  | The class name of the IdP that will be used for authenticating with the Amazon Redshift cluster             | string |    N     |
| `region`                | The AWS region of the Amazon Redshift cluster                                                               | string |    N     |
| `cluster_identifier`    | The cluster identifier of the Amazon Redshift cluster                                                       | string |    N     |
| `iam`                   | If IAM authentication is enabled. IAM must be True when authenticating using an IdP                         |  dict  |    N     |
| `is_serverless`         | If the Amazon Redshift cluster is serverless (Default: `False`)                                             |  bool  |    N     |
| `serverless_acct_id`    | The account ID of the serverless cluster                                                                    | string |    N     |
| `serverless_work_group` | The name of work group for serverless end point                                                             | string |    N     |
| `enable_merge`         | Whether the incremental_by_unique_key model kind will use the native Redshift MERGE operation or Vulcan's logical merge. (Default: `False`)           |  bool  |    N     |

## Performance Considerations

### Timestamp Macro Variables and Sort Keys

When working with Redshift tables that have a `TIMESTAMP` sort key, using the standard `@start_dt` and `@end_dt` macro variables may lead to performance issues. These macros render as `TIMESTAMP WITH TIME ZONE` values in SQL queries, which prevents Redshift from performing efficient pruning when filtering against `TIMESTAMP` (without timezone) sort keys.

This can result in full table scans instead, causing significant performance degradation.

**Solution**: Use the `_dtntz` (datetime no timezone) variants of macro variables:

- `@start_dtntz` instead of `@start_dt`
- `@end_dtntz` instead of `@end_dt`

These variants render as `TIMESTAMP WITHOUT TIME ZONE`, allowing Redshift to properly utilize sort key optimizations.

**Example**:

```sql linenums="1"
-- Inefficient: May cause full table scan
SELECT * FROM my_table
WHERE timestamp_column >= @start_dt
  AND timestamp_column < @end_dt

-- Efficient: Uses sort key optimization
SELECT * FROM my_table
WHERE timestamp_column >= @start_dtntz
  AND timestamp_column < @end_dtntz

-- Alternative: Cast to timestamp
SELECT * FROM my_table
WHERE timestamp_column >= @start_ts::timestamp
  AND timestamp_column < @end_ts::timestamp
```



# RisingWave

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/risingwave/

---

# RisingWave

This page provides information about how to use Vulcan with the [RisingWave](https://risingwave.com/) streaming database engine.

!!! info
    The RisingWave engine adapter is a community contribution. Due to this, only limited community support is available.

## Local/Built-in Scheduler

**Engine Adapter Type**: `risingwave`

### Installation

```
pip install "vulcan[risingwave]"
```

## Connection options

RisingWave is based on Postgres and uses the same `psycopg2` connection library. Therefore, the connection parameters are very similar to [Postgres](../../configurations-old/configurations-old/configurations-old/integrations/engines/postgres.md).

| Option         | Description                                                       | Type   | Required |
|----------------|-------------------------------------------------------------------|:------:|:--------:|
| `type`         | Engine type name - must be `risingwave`                           | string | Y        |
| `host`         | The hostname of the RisingWave server                             | string | Y        |
| `user`         | The username to use for authentication with the RisingWave server | string | Y        |
| `password`     | The password to use for authentication with the RisingWave server | string | N        |
| `port`         | The port number of the RisingWave engine server                   | int    | Y        |
| `database`     | The name of the database instance to connect to                   | string | Y        |
| `role`         | The role to use for authentication with the RisingWave server     | string | N        |
| `sslmode`      | The security of the connection to the RisingWave server           | string | N        |

## Extra Features

As a streaming database engine, RisingWave contains some extra features tailored specifically to streaming usecases.

Primarily, these are:
 - [Sources](https://docs.risingwave.com/sql/commands/sql-create-source) which are used to stream records into RisingWave from streaming sources like Kafka
 - [Sinks](https://docs.risingwave.com/sql/commands/sql-create-sink) which are used to write the results of data processed by RisingWave to an external target, such as an Apache Iceberg table in object storage.

RisingWave exposes these features via normal SQL statements, namely `CREATE SOURCE` and `CREATE SINK`. To utilize these in Vulcan, you can use them in [pre / post statements](configurations-old/components/model/types/sql_models.md#optional-prepost-statements).

Here is an example of creating a Sink from a Vulcan model using a post statement:

```sql
MODEL (
    name vulcan_example.view_model,
    kind VIEW (
      materialized true
    )
);

SELECT
  item_id,
  COUNT(DISTINCT id) AS num_orders,
FROM
  vulcan_example.incremental_model
GROUP BY item_id;

CREATE
  SINK IF NOT EXISTS kafka_sink
FROM
  @this_model
WITH (
  connector='kafka',
  "properties.bootstrap.server"='localhost:9092',
  topic='test1',
)
FORMAT PLAIN
ENCODE JSON (force_append_only=true);
```

!!! info "@this_model"
    The `@this_model` macro resolves to the physical table for the current version of the model. See [here](configurations-old/components/advanced-features/macros/variables.md#runtime-variables) for more information.



# Snowflake

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/snowflake/

---

# Snowflake

This page provides information about how to use Vulcan with the Snowflake SQL engine.

It begins with a [Connection Quickstart](#connection-quickstart) that demonstrates how to connect to Snowflake, or you can skip directly to information about using Snowflake with the [built-in](#localbuilt-in-scheduler).

## Connection quickstart

Connecting to cloud warehouses involves a few steps, so this connection quickstart provides the info you need to get up and running with Snowflake.

It demonstrates connecting to Snowflake with the `snowflake-connector-python` library bundled with Vulcan.

Snowflake provides multiple methods of authorizing a connection (e.g., password, SSO, etc.). This quickstart demonstrates authorizing with a password, but configurations for other methods are [described below](#snowflake-authorization-methods).

!!! tip
    This quickstart assumes you are familiar with basic Vulcan commands and functionality.

    If you're not, work through the [Vulcan Quickstart](configurations-old/guides/get-started/docker.md) before continuing!

### Prerequisites

Before working through this connection quickstart, ensure that:

1. You have a Snowflake account and know your username and password
2. Your Snowflake account has at least one [warehouse](https://docs.snowflake.com/en/user-guide/warehouses-overview) available for running computations
3. Your computer has Vulcan installed with the Snowflake extra available
    - Install from the command line with the command `pip install "vulcan[snowflake]"`
4. You have initialized a Vulcan example project on your computer
    - Open a command line interface and navigate to the directory where the project files should go
    - Initialize the project with the command `vulcan init snowflake`

### Access control permissions

Vulcan must have sufficient permissions to create and access different types of database objects.

Vulcan's core functionality requires relatively broad permissions, including:

1. Ability to create and delete schemas in a database
2. Ability to create, modify, delete, and query tables and views in the schemas it creates

If your project uses materialized views or dynamic tables, Vulcan will also need permissions to create, modify, delete, and query those object types.

We now describe how to grant Vulcan appropriate permissions.

#### Snowflake roles

Snowflake allows you to grant permissions directly to a user, or you can create and assign permissions to a "role" that you then grant to the user.

Roles provide a convenient way to bundle sets of permissions and provide them to multiple users. We create and use a role to grant our user permissions in this quickstart.

The role must be granted `USAGE` on a warehouse so it can execute computations. We describe other permissions below.

#### Database permissions
The top-level object container in Snowflake is a "database" (often called a "catalog" in other engines). Vulcan does not need permission to create databases; it may use an existing one.

The simplest way to grant Vulcan sufficient permissions for a database is to give it `OWNERSHIP` of the database, which includes all the necessary permissions.

Alternatively, you may grant Vulcan granular permissions for all the actions and objects it will work with in the database.

#### Granting the permissions

This section provides example code for creating a `vulcan` role, granting it sufficient permissions, and granting it to a user.

The code must be executed by a user with `USERADMIN` level permissions or higher. We provide two versions of the code, one that grants database `OWNERSHIP` to the role and another that does not.

Both examples create a role named `vulcan`, grant it usage of the warehouse `compute_wh`, create a database named `demo_db`, and assign the role to the user `demo_user`. The step that creates the database can be omitted if the database already exists.

=== "With database ownership"

    ```sql linenums="1"
    USE ROLE useradmin; -- This code requires USERADMIN privileges or higher

    CREATE ROLE vulcan; -- Create role for permissions
    GRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse

    CREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)
    GRANT OWNERSHIP ON DATABASE demo_db TO ROLE vulcan; -- Role owns database

    GRANT ROLE vulcan TO USER demo_user; -- Grant role to user
    ALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role
    ```

=== "Without database ownership"

    ```sql linenums="1"
    USE ROLE useradmin; -- This code requires USERADMIN privileges or higher

    CREATE ROLE vulcan; -- Create role for permissions
    CREATE DATABASE demo_db; -- Create database for Vulcan to use (omit if database already exists)

    GRANT USAGE ON WAREHOUSE compute_wh TO ROLE vulcan; -- Can use warehouse
    GRANT USAGE ON DATABASE demo_db TO ROLE vulcan; -- Can use database

    GRANT CREATE SCHEMA ON DATABASE demo_db TO ROLE vulcan; -- Can create SCHEMAs in database
    GRANT USAGE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can use schemas it creates
    GRANT CREATE TABLE ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create TABLEs in schemas
    GRANT CREATE VIEW ON FUTURE SCHEMAS IN DATABASE demo_db TO ROLE vulcan; -- Can create VIEWs in schemas
    GRANT SELECT, INSERT, TRUNCATE, UPDATE, DELETE ON FUTURE TABLES IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify TABLEs in schemas
    GRANT REFERENCES, SELECT ON FUTURE VIEWS IN DATABASE demo_db TO ROLE vulcan; -- Can SELECT and modify VIEWs in schemas

    GRANT ROLE vulcan TO USER demo_user; -- Grant role to user
    ALTER USER demo_user SET DEFAULT ROLE = vulcan; -- Make role user's default role
    ```

### Get connection info

Now that our user has sufficient access permissions, we're ready to gather the information needed to configure the Vulcan connection.

#### Account name

Snowflake connection configurations require the `account` parameter that identifies the Snowflake account Vulcan should connect to.

Snowflake account identifiers have two components: your organization name and your account name. Both are embedded in your Snowflake web interface URL, separated by a `/`.

This shows the default view when you log in to your Snowflake account, where we can see the two components of the account identifier:

![Snowflake account info in web URL](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_account-url.png){ loading=lazy }

In this example, our organization name is `idapznw`, and our account name is `wq29399`.

We concatenate the two components, separated by a `-`, for the Vulcan `account` parameter: `idapznw-wq29399`.

#### Warehouse name

Your Snowflake account may have more than one warehouse available - any will work for this quickstart, which runs very few computations.

Some Snowflake user accounts may have a default warehouse they automatically use when connecting.

The connection configuration's `warehouse` parameter is not required, but we recommend specifying the warehouse explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default warehouse changes.

#### Database name

Snowflake user accounts may have a "Default Namespace" that includes a default database they automatically use when connecting.

The connection configuration's `database` parameter is not required, but we recommend specifying the database explicitly in the configuration to ensure Vulcan's behavior doesn't change if the user's default namespace changes.

### Configure the connection

We now have the information we need to configure Vulcan's connection to Snowflake.

We start the configuration by adding a gateway named `snowflake` to our example project's config.yaml file and making it our `default_gateway`:

```yaml linenums="1" hl_lines="2-6"
gateways:
  snowflake:
    connection:
      type: snowflake

default_gateway: snowflake

model_defaults:
  dialect: snowflake
  start: 2024-07-24
```

And we specify the `account`, `user`, `password`, `database`, and `warehouse` connection parameters using the information from above:

```yaml linenums="1" hl_lines="5-9"
gateways:
  snowflake:
    connection:
      type: snowflake
      account: idapznw-wq29399
      user: DEMO_USER
      password: << password here >>
      database: DEMO_DB
      warehouse: COMPUTE_WH

default_gateway: snowflake

model_defaults:
  dialect: snowflake
  start: 2024-07-24
```

!!! warning
    Best practice for storing secrets like passwords is placing them in [environment variables that the configuration file loads dynamically](../../guides-old/configuration.md#environment-variables). For simplicity, this guide instead places the value directly in the configuration file.

    This code demonstrates how to use the environment variable `SNOWFLAKE_PASSWORD` for the configuration's `password` parameter:

    ```yaml linenums="1" hl_lines="5"
    gateways:
      snowflake:
        connection:
          type: snowflake
          password: {{ env_var('SNOWFLAKE_PASSWORD') }}
    ```

### Check connection

We have now specified the `snowflake` gateway connection information, so we can confirm that Vulcan is able to successfully connect to Snowflake. We will test the connection with the `vulcan info` command.

First, open a command line terminal. Now enter the command `vulcan info`:

![Run vulcan info command in CLI](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-info.png){ loading=lazy }

The output shows that our data warehouse connection succeeded:

![Successful data warehouse connection](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-info-succeeded.png){ loading=lazy }

However, the output includes a `WARNING` about using the Snowflake SQL engine for storing Vulcan state:

![Snowflake state connection warning](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-info-warning.png){ loading=lazy }

!!! warning
    Snowflake is not designed for transactional workloads and should not be used to store Vulcan state even in testing deployments.

    Learn more about storing Vulcan state [here](../../guides-old/configuration.md#state-connection).

### Specify state connection

We can store Vulcan state in a different SQL engine by specifying a `state_connection` in our `snowflake` gateway.

This example uses the DuckDB engine to store state in the local `snowflake_state.db` file:

```yaml linenums="1" hl_lines="10-12"
gateways:
  snowflake:
    connection:
      type: snowflake
      account: idapznw-wq29399
      user: DEMO_USER
      password: << your password here >>
      database: DEMO_DB
      warehouse: COMPUTE_WH
    state_connection:
      type: duckdb
      database: snowflake_state.db

default_gateway: snowflake

model_defaults:
  dialect: snowflake
  start: 2024-07-24
```

Now we no longer see the warning when running `vulcan info`, and we see a new entry `State backend connection succeeded`:

![No state connection warning](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-info-no-warning.png){ loading=lazy }

### Run a `vulcan plan`

Now we're ready to run a `vulcan plan` in Snowflake:

![Run vulcan plan in snowflake](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-plan.png){ loading=lazy }

And confirm that our schemas and objects exist in the Snowflake catalog:

![Vulcan plan objects in snowflake](../../configurations-old/configurations-old/configurations-old/integrations/engines/snowflake/snowflake_db-guide_sqlmesh-plan-objects.png){ loading=lazy }

Congratulations - your Vulcan project is up and running on Snowflake!

### Where are the row counts?

Vulcan reports the number of rows processed by each model in its `plan` and `run` terminal output.

However, due to limitations in the Snowflake Python connector, row counts cannot be determined for `CREATE TABLE AS` statements. Therefore, Vulcan does not report row counts for certain model kinds, such as `FULL` models.

Learn more about the connector limitation [on Github](https://github.com/snowflakedb/snowflake-connector-python/issues/645).

## Local/Built-in Scheduler
**Engine Adapter Type**: `snowflake`

### Installation
```
pip install "vulcan[snowflake]"
```

### Connection options

| Option                   | Description                                                                                                                                                                    |  Type  | Required |
|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                   | Engine type name - must be `snowflake`                                                                                                                                         | string |    Y     |
| `account`                | The Snowflake account name                                                                                                                                                     | string |    Y     |
| `user`                   | The Snowflake username                                                                                                                                                         | string |    N     |
| `password`               | The Snowflake password                                                                                                                                                         | string |    N     |
| `authenticator`          | The Snowflake authenticator method                                                                                                                                             | string |    N     |
| `warehouse`              | The Snowflake warehouse name                                                                                                                                                   | string |    N     |
| `database`               | The Snowflake database name                                                                                                                                                    | string |    N     |
| `role`                   | The Snowflake role name                                                                                                                                                        | string |    N     |
| `token`                  | The Snowflake OAuth 2.0 access token                                                                                                                                           | string |    N     |
| `private_key`            | The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). | string |    N     |
| `private_key_path`       | The optional path to the private key to use for authentication. This would be used instead of `private_key`.                                                                   | string |    N     |
| `private_key_passphrase` | The optional passphrase to use to decrypt `private_key` (if in PEM format) or `private_key_path`. Keys can be created without encryption so only provide this if needed.       | string |    N     |
| `session_parameters`     | The optional session parameters to set for the connection.                                                                                                                     | dict   |    N     |


### Lowercase object names

Snowflake object names are case-insensitive by default, and Snowflake automatically normalizes them to uppercase. For example, the command `CREATE SCHEMA vulcan` will generate a schema named `VULCAN` in Snowflake.

If you need to create an object with a case-sensitive lowercase name, the name must be double-quoted in SQL code. In the Vulcan configuration file, it also requires outer single quotes.

For example, a connection to the database `"my_db"` would include:

``` yaml
connection:
  type: snowflake
  <other connection options>
  database: '"my_db"' # outer single and inner double quotes
```

### Snowflake authorization methods

The simplest (but arguably least secure) method of authorizing a connection with Snowflake is with a username and password.

This section describes how to configure other authorization methods.

#### Snowflake SSO Authorization

Vulcan supports Snowflake SSO authorization connections using the `externalbrowser` authenticator method. For example:

```yaml
gateways:
  snowflake:
    connection:
      type: snowflake
      account: ************
      user: ************
      authenticator: externalbrowser
      warehouse: ************
      database: ************
      role: ************
```

#### Snowflake OAuth Authorization

Vulcan supports Snowflake OAuth authorization connections using the `oauth` authenticator method. For example:

=== "YAML"

    ```yaml linenums="1"
    gateways:
      snowflake:
        connection:
          type: snowflake
          account: account
          user: user
          authenticator: oauth
          token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="snowflake"),
        gateways={
           "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="user",
                    account="account",
                    authenticator="oauth",
                    token="eyJhbGciOiJSUzI1NiIsImtpZCI6ImFmZmM...",
                ),
            ),
        }
    )
    ```

#### Snowflake Private Key Authorization

Vulcan supports Snowflake private key authorization connections by providing the private key as a path, Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or as bytes (Python Only).

The `account` and `user` parameters are required for each of these methods.

__Private Key Path__

Note: `private_key_passphrase` is only needed if the key was encrypted with a passphrase.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      snowflake:
        connection:
          type: snowflake
          account: account
          user: user
          private_key_path: '/path/to/key.key'
          private_key_passphrase: supersecret
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="snowflake"),
        gateways={
           "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="user",
                    account="account",
                    private_key_path="/path/to/key.key",
                    private_key_passphrase="supersecret",
                ),
            ),
        }
    )
    ```


__Private Key PEM__

Note: `private_key_passphrase` is only needed if the key was encrypted with a passphrase.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      snowflake:
        connection:
          type: snowflake
          account: account
          user: user
          private_key: |
            -----BEGIN PRIVATE KEY-----
            ...
            -----END PRIVATE KEY-----
          private_key_passphrase: supersecret
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="snowflake"),
        gateways={
           "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="user",
                    account="account",
                    private_key="""
                    -----BEGIN PRIVATE KEY-----
                    ...
                    -----END PRIVATE KEY-----""",
                    private_key_passphrase="supersecret",
                ),
            ),
        }
    )
    ```


#### Private Key Base64

Note: This is base64 encoding of the bytes of the key itself and not the PEM file contents.

=== "YAML"

    ```yaml linenums="1"
    gateways:
      snowflake:
        connection:
          type: snowflake
          account: account
          user: user
          private_key: 'MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU='
    ```

=== "Python"

    ```python linenums="1"
    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="snowflake"),
        gateways={
           "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="user",
                    account="account",
                    private_key="MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvMKgsYzoDMnl7QW9nWTzAMMQToyUTslgKlH9MezcEYUvvCv+hYEsY9YGQ5dhI5MSY1vkQ+Wtqc6KsvJQzMaHDA1W+Z5R/yA/IY+Mp2KqJijQxnp8XjZs1t6Unr0ssL2yBjlk2pNOZX3w4A6B6iwpkqUi/HtqI5t2M15FrUMF3rNcH68XMcDa1gAasGuBpzJtBM0bp4/cHa18xWZZfu3d2d+4CCfYUvE3OYXQXMjJunidnU56NZtYlJcKT8Fmlw16fSFsPAG01JOIWBLJmSMi5qhhB2w90AAq5URuupCbwBKB6KvwzPRWn+fZKGAvvlR7P3CGebwBJEJxnq85MljzRAgMBAAECggEAKXaTpwXJGi6dD+35xvUY6sff8GHhiZrhOYfR5TEYYWIBzc7Fl9UpkPuyMbAkk4QJf78JbdoKcURzEP0E+mTZy0UDyy/Ktr+L9LqnbiUIn8rk9YV8U9/BB2KypQTY/tkuji85sDQsnJU72ioJlldIG3DxdcKAqHwznXz7vvF7CK6rcsz37hC5w7MTtguvtzNyHGkvJ1ZBTHI1vvGR/VQJoSSFkv6nLFs2xl197kuM2x+Ss539Xbg7GGXX90/sgJP+QLyNk6kYezekRt5iCK6n3UxNfEqd0GX03AJ1oVtFM9SLx0RMHiLuXVCKlQLJ1LYf8zOT31yOun6hhowNmHvpLQKBgQDzXGQqBLvVNi9gQzQhG6oWXxdtoBILnGnd8DFsb0YZIe4PbiyoFb8b4tJuGz4GVfugeZYL07I8TsQbPKFH3tqFbx69hENMUOo06PZ4H7phucKk8Er/JHW8dhkVQVg1ttTK8J5kOm+uKjirqN5OkLlUNSSJMblaEr9AHGPmTu21MwKBgQC4SeYzJDvq/RTQk5d7AwVEokgFk95aeyv77edFAhnrD3cPIAQnPlfVyG7RgPA94HrSAQ5Hr0PL2hiQ7OxX1HfP+66FMcTVbZwktYULZuj4NMxJqwxKbCmmzzACiPF0sibg8efGMY9sAmcQRw5JRS2s6FQns1MqeksnjzyMf3196wKBgFf8zJ5AjeT9rU1hnuRliy6BfQf+uueFyuUaZdQtuyt1EAx2KiEvk6QycyCqKtfBmLOhojVued/CHrc2SZ2hnmJmFbgxrN9X1gYBQLOXzRxuPEjENGlhNkxIarM7p/frva4OJ0ZXtm9DBrBR4uaG/urKOAZ+euRtKMa2PQxU9y7vAoGAeZWX4MnZFjIe13VojWnywdNnPPbPzlZRMIdG+8plGyY64Km408NX492271XoKoq9vWug5j6FtiqP5p3JWDD/UyKzg4DQYhdM2xM/UcR1k7wRw9Cr7TXrTPiIrkN3OgyHhgVTavkrrJDxOlYG4ORZPCiTzRWMmwvQJatkwTUjsD0CgYEA8nAWBSis9H8n9aCEW30pGHT8LwqlH0XfXwOTPmkxHXOIIkhNFiZRAzc4NKaefyhzdNlc7diSMFVXpyLZ4K0l5dY1Ou2xRh0W+xkRjjKsMib/s9g/crtam+tXddADJDokLELn5PAMhaHBpti+PpOMGqdI3Wub+5yT1XCXT9aj6yU=",
                ),
            ),
        }
    )
    ```

__Private Key Bytes__

=== "YAML"

    Base64 encode the bytes and follow [Private Key Base64](#private-key-base64) instructions.

=== "Python"

    ```python
    from vulcan.core.config import (
        Config,
        GatewayConfig,
        ModelDefaultsConfig,
        SnowflakeConnectionConfig,
    )

    from cryptography.hazmat.primitives import serialization

    key = """-----BEGIN PRIVATE KEY-----
    ...
    -----END PRIVATE KEY-----""".encode()

    p_key= serialization.load_pem_private_key(key, password=None)

    pkb = p_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )

    config = Config(
        model_defaults=ModelDefaultsConfig(dialect="snowflake"),
        gateways={
           "my_gateway": GatewayConfig(
                connection=SnowflakeConnectionConfig(
                    user="user",
                    account="account",
                    private_key=pkb,
                ),
            ),
        }
    )
    ```

The authenticator method is assumed to be `snowflake_jwt` when `private_key` is provided, but it can also be explicitly provided in the connection configuration.

## Configuring Virtual Warehouses

The Snowflake Virtual Warehouse a model should use can be specified in the `session_properties` attribute of the model definition:

```sql linenums="1"
MODEL (
  name schema_name.model_name,
  session_properties (
    'warehouse' = TEST_WAREHOUSE,
  ),
);
```

## Custom View and Table types

Vulcan supports custom view and table types for Snowflake models. You can apply these modifiers to either the physical layer or virtual layer of a model using the `physical_properties` and `virtual_properties` attributes respectively. For example:

### Secure Views

A table can be exposed through a `SECURE` view in the virtual layer by specifying the `creatable_type` property and setting it to `SECURE`:

```sql linenums="1"
MODEL (
  name schema_name.model_name,
  virtual_properties (
      creatable_type = SECURE
  )
);

SELECT a FROM schema_name.model_b;
```

### Transient Tables

A model can use a `TRANSIENT` table in the physical layer by specifying the `creatable_type` property and setting it to `TRANSIENT`:

```sql linenums="1"
MODEL (
  name schema_name.model_name,
  physical_properties (
      creatable_type = TRANSIENT
  )
);

SELECT a FROM schema_name.model_b;
```

### Iceberg Tables

In order for Snowflake to be able to create an Iceberg table, there must be an [External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume) configured to store the Iceberg table data on.

Once that is configured, you can create a model backed by an Iceberg table by using `table_format iceberg` like so:

```sql linenums="1" hl_lines="4 6-7"
MODEL (
  name schema_name.model_name,
  kind FULL,
  table_format iceberg,
  physical_properties (
    catalog = 'snowflake',
    external_volume = '<external volume name>'
  )
);
```

To prevent having to specify `catalog = 'snowflake'` and `external_volume = '<external volume name>'` on every model, see the Snowflake documentation for:

  - [Configuring a default Catalog](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-catalog-integration#set-a-default-catalog-at-the-account-database-or-schema-level)
  - [Configuring a default External Volume](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume#set-a-default-external-volume-at-the-account-database-or-schema-level)

Alternatively you can also use [model defaults](../../guides-old/configuration.md#model-defaults) to set defaults at the Vulcan level instead.

To utilize the wide variety of [optional properties](https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table-snowflake#optional-parameters) that Snowflake makes available for Iceberg tables, simply specify them as `physical_properties`:

```sql linenums="1" hl_lines="8"
MODEL (
  name schema_name.model_name,
  kind FULL,
  table_format iceberg,
  physical_properties (
    catalog = 'snowflake',
    external_volume = 'my_external_volume',
    base_location = 'my/product_reviews/'
  )
);
```

!!! warning "External catalogs"

    Setting `catalog = 'snowflake'` to use Snowflake's internal catalog is a good default because Vulcan needs to be able to write to the tables it's managing and Snowflake [does not support](https://docs.snowflake.com/en/user-guide/tables-iceberg#catalog-options) writing to Iceberg tables configured under external catalogs.

    You can however still reference a table from an external catalog in your model as a normal [external table](configurations-old/components/model/types/external_models.md).

## Troubleshooting

### Frequent Authentication Prompts

When using Snowflake with security features like Multi-Factor Authentication (MFA), you may experience repeated prompts for authentication while running Vulcan commands. This typically occurs when your Snowflake account isn't configured to issue short-lived tokens.

To reduce authentication prompts, you can enable token caching in your Snowflake connection configuration:

- For general authentication, see [Connection Caching Documentation](https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-use#using-connection-caching-to-minimize-the-number-of-prompts-for-authentication-optional)
- For MFA specifically, see [MFA Token Caching Documentation](https://docs.snowflake.com/en/user-guide/security-mfa#using-mfa-token-caching-to-minimize-the-number-of-prompts-during-authentication-optional).



# Spark

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/spark/

---

# Spark

## Local/Built-in Scheduler
**Engine Adapter Type**: `spark`

NOTE: Spark may not be used for the Vulcan [state connection](configurations-old/configurations-old/configuration.md#connections).

### Connection options

| Option       | Description                                                                                   |  Type  | Required |
|--------------|-----------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`       | Engine type name - must be `spark`                                                            | string |    Y     |
| `config_dir` | Value to set for `SPARK_CONFIG_DIR`                                                           | string |    N     |
| `catalog`    | The catalog to use when issuing commands. See [Catalog Support](#catalog-support) for details | string |    N     |
| `config`     | Key/value pairs to set for the Spark Configuration.                                           |  dict  |    N     |

## Catalog Support

Vulcan's Spark integration is only designed/tested with a single catalog usage in mind. 
Therefore all Vulcan models must be defined with a single catalog.

If `catalog` is not set, then the behavior changes based on spark release:

* If >=3.4, then the default catalog is determined at runtime
* If <3.4, then the default catalog is `spark_catalog` 



# Trino

Source: https://tmdc-io.github.io/vulcan-book/references/integrations/engines/trino/

---

# Trino

## Local/Built-in Scheduler
**Engine Adapter Type**: `trino`

NOTE: Trino may not be used for the Vulcan [state connection](configurations-old/configurations-old/configuration.md#connections).

## Installation
```
pip install "vulcan[trino]"
```

If you are using Oauth for Authentication, it is recommended to install keyring cache:
```
pip install "trino[external-authentication-token-cache]"
```

### Trino Connector Support

The trino engine adapter has been tested against the [Hive Connector](https://trino.io/docs/current/connector/hive.html), [Iceberg Connector](https://trino.io/docs/current/connector/iceberg.html), and [Delta Lake Connector](https://trino.io/docs/current/connector/delta-lake.html).

Please let us know on [Slack](https://tobikodata.com/slack) if you are wanting to use another connector or have tried another connector.

#### Hive Connector Configuration

Recommended hive catalog properties configuration (`<catalog_name>.properties`):

```properties linenums="1"
hive.metastore-cache-ttl=0s
hive.metastore-refresh-interval=5s
hive.metastore-timeout=10s
hive.allow-drop-table=true
hive.allow-add-column=true
hive.allow-drop-column=true
hive.allow-rename-column=true
hive.allow-rename-table=true
```

#### Iceberg Connector Configuration

If you're using a hive metastore for the Iceberg catalog, the [properties](https://trino.io/docs/current/connector/metastores.html#general-metastore-configuration-properties) are mostly the same as the Hive connector.

```properties linenums="1"
iceberg.catalog.type=hive_metastore
# metastore properties as per the Hive Connector Configuration above
```

**Note**: The Trino Iceberg Connector must be configured with an `iceberg.catalog.type` that supports views. At the time of this writing, this is `hive_metastore`, `glue`, and `rest`.

The `jdbc` and `nessie` iceberg catalog types do not support views and are thus incompatible with Vulcan.

!!! info "Nessie"
    Nessie is supported when used as an Iceberg REST Catalog (`iceberg.catalog.type=rest`).
    For more information on how to configure the Trino Iceberg connector for this, see the [Nessie documentation](https://projectnessie.org/nessie-latest/trino/).

#### Delta Lake Connector Configuration

The Trino adapter Delta Lake connector has only been tested with the Hive metastore catalog type.

The [properties file](https://trino.io/docs/current/connector/delta-lake.html#general-configuration) must include the Hive metastore URI and catalog name in addition to any other [general properties](https://trino.io/docs/current/object-storage/metastores.html#general-metastore-properties).

``` properties linenums="1"
hive.metastore.uri=thrift://example.net:9083
delta.hive-catalog-name=datalake_delta # example catalog name, can be any valid string
```

#### AWS Glue

[AWS Glue](https://aws.amazon.com/glue/) provides an implementation of the Hive metastore catalog.

Your Trino project's physical data objects are stored in a specific location, such as an [AWS S3](https://aws.amazon.com/s3/) bucket. Hive provides a default location, which you can override in its configuration file.

Set the default location for your project's tables in the Hive catalog configuration's [`hive.metastore.glue.default-warehouse-dir` parameter](https://trino.io/docs/current/object-storage/metastores.html#aws-glue-catalog-configuration-properties).

For example:

```linenums="1"
hive.metastore=glue
hive.metastore.glue.default-warehouse-dir=s3://my-bucket/
```

### Connection options

| Option                    | Description                                                                                                                                                                             |  Type  | Required |
|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|:--------:|
| `type`                    | Engine type name - must be `trino`                                                                                                                                                      | string |    Y     |
| `user`                    | The username (of the account) to log in to your cluster. When connecting to Starburst Galaxy clusters, you must include the role of the user as a suffix to the username.               | string |    Y     |
| `host`                    | The hostname of your cluster. Don't include the `http://` or `https://` prefix.                                                                                                         | string |    Y     |
| `catalog`                 | The name of a catalog in your cluster.                                                                                                                                                  | string |    Y     |
| `http_scheme`             | The HTTP scheme to use when connecting to your cluster. By default, it's `https` and can only be `http` for no-auth or basic auth.                                                      | string |    N     |
| `port`                    | The port to connect to your cluster. By default, it's `443` for `https` scheme and `80` for `http`                                                                                      |  int   |    N     |
| `roles`                   | Mapping of catalog name to a role                                                                                                                                                       |  dict  |    N     |
| `http_headers`            | Additional HTTP headers to send with each request.                                                                                                                                      |  dict  |    N     |
| `session_properties`      | Trino session properties. Run `SHOW SESSION` to see all options.                                                                                                                        |  dict  |    N     |
| `retries`                 | Number of retries to attempt when a request fails. Default: `3`                                                                                                                         |  int   |    N     |
| `timezone`                | Timezone to use for the connection. Default: client-side local timezone                                                                                                                 | string |    N     |
| `schema_location_mapping` | A mapping of regex patterns to S3 locations to use for the `LOCATION` property when creating schemas. See [Table and Schema locations](#table-and-schema-locations) for more details.   |  dict  |    N     |
| `catalog_type_overrides`  | A mapping of catalog names to their connector type. This is used to enable/disable connector specific behavior. See [Catalog Type Overrides](#catalog-type-overrides) for more details. |  dict  |    N     |

## Table and Schema locations

When using connectors that are decoupled from their storage (such as the Iceberg, Hive or Delta connectors), when creating new tables Trino needs to know the location in the physical storage it should write the table data to.

This location gets stored against the table in the metastore so that any engine trying to read the data knows where to look.

### Default behaviour

Trino allows you to optionally configure a `default-warehouse-dir` property at the [Metastore](https://trino.io/docs/current/object-storage/metastores.html) level. When creating objects, Trino will infer schema locations to be `<default warehouse dir>/<schema name>` and table locations to be `<default warehouse dir>/<schema name>/<table name>`.

However, if you dont set this property, Trino can still infer table locations if a *schema* location is explicitly set.

For example, if you specify the `LOCATION` property when creating a schema like so:

```sql
CREATE SCHEMA staging_data
WITH (LOCATION = 's3://warehouse/production/staging_data')
```

Then any tables created under that schema will have their location inferred as `<schema location>/<table name>`.

If you specify neither a `default-warehouse-dir` in the metastore config nor a schema location when creating the schema, you must specify an explicit table location when creating the table or Trino will produce an error.

Creating a table in a specific location is very similar to creating a schema in a specific location:

```sql
CREATE TABLE staging_data.customers (customer_id INT)
WITH (LOCATION = 's3://warehouse/production/staging_data/customers')
```

### Configuring in Vulcan

Within Vulcan, you can configure the value to use for the `LOCATION` property when Vulcan creates tables and schemas. This overrides what Trino would have inferred based on the cluster configuration.

#### Schemas

To configure the `LOCATION` property that Vulcan will specify when issuing `CREATE SCHEMA` statements, you can use the `schema_location_mapping` connection property. This applies to all schemas that Vulcan creates, including its internal ones.

The simplest example is to emulate a `default-warehouse-dir`:

```yaml title="config.yaml"
gateways:
  trino:
    connection:
      type: trino
      ...
      schema_location_mapping:
        '.*': 's3://warehouse/production/@{schema_name}'
```

This will cause all schemas to get created with their location set to `s3://warehouse/production/<schema name>`. The table locations will be inferred by Trino as `s3://warehouse/production/<schema name>/<table name>` so all objects will effectively be created under `s3://warehouse/production/`.

It's worth mentioning that if your models are using fully qualified three part names, eg `<catalog>.<schema>.<name>` then string being matched against the `schema_location_mapping` regex will be `<catalog>.<schema>` and not just the `<schema>` itself. This allows you to set different locations for the same schema name if that schema name is used across multiple catalogs.

If your models are using two part names, eg `<schema>.<table>` then only the `<schema>` part will be matched against the regex.

Here's an example:

```yaml title="config.yaml"
gateways:
  trino:
    connection:
      type: trino
      ...
      schema_location_mapping:
        '^utils$': 's3://utils-bucket/@{schema_name}'
        '^landing\..*$': 's3://raw-data/@{catalog_name}/@{schema_name}'
        '^staging.*$': 's3://bucket/@{schema_name}_dev'
        '^vulcan.*$': 's3://vulcan-internal/dev/@{schema_name}'
```

This would perform the following mappings:

- a schema called `sales` would not be mapped to a location at all because it doesnt match any of the patterns. It would be created without a `LOCATION` property
- a schema called `utils` would be mapped to the location `s3://utils-bucket/utils` because it directly matches the `^utils$` pattern
- a schema called `transactions` in a catalog called `landing` would be mapped to the location `s3://raw-data/landing/transactions` because the string `landing.transactions` matches the `^landing\..*$` pattern
- schemas called `staging_customers` and `staging_accounts` would be mapped to the locations `s3://bucket/staging_customers_dev` and `s3://bucket/staging_accounts_dev` respectively because they match the `^staging.*$` pattern
- a schema called `accounts` in a catalog called `staging` would be mapped to the location `s3://bucket/accounts_dev` because the string `staging.accounts` matches the `^staging.*$` pattern
- schemas called `vulcan__staging_customers` and `vulcan__staging_utils` would be mapped to the locations `s3://vulcan-internal/dev/vulcan__staging_customers` and `s3://vulcan-internal/dev/vulcan__staging_utils` respectively because they match the `^vulcan.*$` pattern

!!! info "Placeholders"
    You may use the `@{catalog_name}` and `@{schema_name}` placeholders in the mapping value.

    If there is a match on one of the patterns then the catalog / schema that Vulcan is about to use in the `CREATE SCHEMA` statement will be substituted into these placeholders.

    Note the use of curly brace syntax `@{}` when referencing these placeholders - learn more [here](configurations-old/components/advanced-features/macros/built_in.md#embedding-variables-in-strings).

#### Tables

Often, you don't need to configure an explicit table location because if you have configured explicit schema locations, table locations are automatically inferred by Trino to be a subdirectory under the schema location.

However, if you need to, you can configure an explicit table location by adding a `location` property to the model `physical_properties`.

Note that you need to use the [@resolve_template](configurations-old/components/advanced-features/macros/built_in.md#resolve_template) macro to generate a unique table location for each model version. Otherwise, all model versions will be written to the same location and clobber each other.

```sql hl_lines="5"
MODEL (
  name staging.customers,
  kind FULL,
  physical_properties (
    location = @resolve_template('s3://warehouse/@{catalog_name}/@{schema_name}/@{table_name}')
  )
);

SELECT ...
```

This will cause Vulcan to set the specified `LOCATION` when issuing a `CREATE TABLE` statement.

## Catalog Type Overrides

Vulcan attempts to determine the connector type of a catalog by querying the `system.metadata.catalogs` table and checking the `connector_name` column.
It checks if the connector name is `hive` for Hive connector behavior or contains `iceberg` or `delta_lake` for Iceberg or Delta Lake connector behavior respectively.
However, the connector name may not always be a reliable way to determine the connector type, for example when using a custom connector or a fork of an existing connector.
To handle such cases, you can use the `catalog_type_overrides` connection property to explicitly specify the connector type for specific catalogs.
For example, to specify that the `datalake` catalog is using the Iceberg connector and the `analytics` catalog is using the Hive connector, you can configure the connection as follows:

```yaml title="config.yaml"
gateways:
  trino:
    connection:
      type: trino
      ...
      catalog_type_overrides:
        datalake: iceberg
        analytics: hive
```

## Authentication

=== "No Auth"
    | Option     | Description                              |  Type  | Required |
    |------------|------------------------------------------|:------:|:--------:|
    | `method`   | `no-auth` (Default)                      | string |    N     |

    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        user: [user]
        host: [host]
        catalog: [catalog]
        # Most likely you will want http for scheme when not using auth
        http_scheme: http
    ```


=== "Basic Auth"

    | Option     | Description                                                                                                                                                                  |  Type  | Required |
    | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----: | :------: |
    | `method`   | `basic`                                                                                                                                                                      | string |    Y     |
    | `password` | The password to use when authenticating.                                                                                                                                     | string |    Y     |
    | `verify`   | Boolean flag for whether SSL verification should occur. Default: [trinodb Python client](https://github.com/trinodb/trino-python-client) default (`true` as of this writing) |  bool  |    N     |


    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: basic
        user: [user]
        password: [password]
        host: [host]
        catalog: [catalog]
    ```

    * [Trino Documentation on Basic Authentication](https://trino.io/docs/current/security/password-file.html)
    * [Python Client Basic Authentication](https://github.com/trinodb/trino-python-client#basic-authentication)

=== "LDAP"

    | Option               | Description                                                             |  Type  | Required |
    |----------------------|-------------------------------------------------------------------------|:------:|:--------:|
    | `method`             | `ldap`                                                                  | string |    Y     |
    | `password`           | The password to use when authenticating.                                | string |    Y     |
    | `impersonation_user` | Override the provided username. This lets you impersonate another user. | string |    N     |

    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: ldap
        user: [user]
        password: [password]
        host: [host]
        catalog: [catalog]
    ```

    * [Trino Documentation on LDAP Authentication](https://trino.io/docs/current/security/ldap.html)
    * [Python Client LDAP Authentication](https://github.com/trinodb/trino-python-client#basic-authentication)

=== "Kerberos"

    | Option                           | Description                                                                       |  Type  | Required |
    |----------------------------------|-----------------------------------------------------------------------------------|:------:|:--------:|
    | `method`                         | `kerberos`                                                                        | string |    Y     |
    | `keytab`                         | Path to keytab. Ex: `/tmp/trino.keytab`                                           | string |    Y     |
    | `krb5_config`                    | Path to config. Ex: `/tmp/krb5.conf`                                              | string |    Y     |
    | `principal`                      | Principal.  Ex: `user@company.com`                                                | string |    Y     |
    | `service_name`                   | Service name (default is `trino`)                                                 | string |    N     |
    | `hostname_override`              | Kerberos hostname for a host whose DNS name doesn't match                         | string |    N     |
    | `mutual_authentication`          | Boolean flag for mutual authentication. Default: `false`                          |  bool  |    N     |
    | `force_preemptive`               | Boolean flag to preemptively initiate the Kerberos GSS exchange. Default: `false` |  bool  |    N     |
    | `sanitize_mutual_error_response` | Boolean flag to strip content and headers from error responses. Default: `true`   |  bool  |    N     |
    | `delegate`                       | Boolean flag for credential delegation (`GSS_C_DELEG_FLAG`). Default: `false`     |  bool  |    N     |

    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: kerberos
        user: user
        keytab: /tmp/trino.keytab
        krb5_config: /tmp/krb5.conf
        principal: trino@company.com
        host: trino.company.com
        catalog: datalake
    ```

    * [Trino Documentation on Kerberos Authentication](https://trino.io/docs/current/security/kerberos.html)
    * [Python Client Kerberos Authentication](https://github.com/trinodb/trino-python-client#kerberos-authentication)

=== "JWT"

    | Option      | Description     |  Type  | Required |
    |-------------|-----------------|:------:|:--------:|
    | `method`    | `jwt`           | string |    Y     |
    | `jwt_token` | The JWT string. | string |    Y     |

    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: jwt
        user: [user]
        password: [password]
        host: [host]
        catalog: [catalog]
    ```

    * [Trino Documentation on JWT Authentication](https://trino.io/docs/current/security/jwt.html)
    * [Python Client JWT Authentication](https://github.com/trinodb/trino-python-client#jwt-authentication)

=== "Certificate"

    | Option               | Description                                       |  Type  | Required |
    |----------------------|---------------------------------------------------|:------:|:--------:|
    | `method`             | `certificate`                                     | string |    Y     |
    | `cert`               | The full path to a certificate file               | string |    Y     |
    | `client_certificate` | Path to client certificate. Ex: `/tmp/client.crt` | string |    Y     |
    | `client_private_key` | Path to client private key. Ex: `/tmp/client.key` | string |    Y     |


    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: certificate
        user: [user]
        password: [password]
        host: [host]
        catalog: [catalog]
        cert: [path/to/cert_file]
        client_certificate: [path/to/client/cert]
        client_private_key: [path/to/client/key]
    ```

=== "Oauth"

    | Option               | Description                                       |  Type  | Required |
    |----------------------|---------------------------------------------------|:------:|:--------:|
    | `method`             | `oauth`                                           | string |    Y     |

    ```yaml linenums="1"
    gateway_name:
      connection:
        type: trino
        method: oauth
        host: trino.company.com
        catalog: datalake
    ```

    * [Trino Documentation on Oauth Authentication](https://trino.io/docs/current/security/oauth2.html)
    * [Python Client Oauth Authentication](https://github.com/trinodb/trino-python-client#oauth2-authentication)



# Model configuration

Source: https://tmdc-io.github.io/vulcan-book/references/model_configuration/

---

# Model configuration

This page lists all Vulcan model configuration options and their parameters.

Learn more about specifying Vulcan model properties in the [model overview](../../components/model/overview.md#model-properties).


## General model properties

Configuration options for Vulcan model properties. Supported by all model kinds other than [`SEED` models](#seed-models).

| Option                | Description                                                                                                                                                                                                                                                                                                                                                 |       Type        | Required |
|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------:|:--------:|
| `name`                | The model name. Must include at least a qualifying schema (`<schema>.<model>`) and may include a catalog (`<catalog>.<schema>.<model>`). Can be omitted if [infer_names](#model-naming) is set to true.                                                                                                                                                     |        `str`        |    N     |
| `project`             | The name of the project the model belongs to - used in multi-repo deployments                                                                                                                                                                                                                                                                               |        `str`        |    N     |
| `kind`                | The model kind ([Additional Details](#model-kind-properties)). (Default: `VIEW`)                                                                                                                                                                                                                                                                            |    `str` \| `dict`    |    N     |
| `audits`              | Vulcan [audits](../../components/audits/audits.md) that should run against the model's output                                                                                                                                                                                                                                                                          |    `array[str]`     |    N     |
| `dialect`             | The SQL dialect in which the model's query is written. All SQL dialects [supported by the SQLGlot library](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/dialect.py) are allowed.                                                                                                                                                           |        `str`        |    N     |
| `owner`               | The owner of a model; may be used for notification purposes                                                                                                                                                                                                                                                                                                 |        `str`        |    N     |
| `stamp`               | Arbitrary string used to indicate a model's version without changing the model name                                                                                                                                                                                                                                                                         |        `str`        |    N     |
| `tags`                | Arbitrary strings used to organize or classify a model                                                                                                                                                                                                                                                                                                      |    `array[str]`     |    N     |
| `cron`                | The cron expression specifying how often the model should be refreshed. (Default: `@daily`)                                                                                                                                                                                                                                                                 |        `str`        |    N     |
| `interval_unit`       | The temporal granularity of the model's data intervals. Supported values: `year`, `month`, `day`, `hour`, `half_hour`, `quarter_hour`, `five_minute`. (Default: inferred from `cron`)                                                                                                                                                                       |        `str`        |    N     |
| `start`               | The date/time that determines the earliest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as `1 year ago`. (Default: `yesterday`)                                                                                                                                     |    `str` \| `int`     |    N     |
| `end`                 | The date/time that determines the latest date interval that should be processed by a model. Can be a datetime string, epoch time in milliseconds, or a relative datetime such as `1 year ago`.                                                                                                                                                              |    `str` \| `int`     |    N     |
| `description`         | Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine).                                                                                                                                                                                                                      |        `str`        |    N     |
| `column_descriptions` | A key-value mapping of column names to column comments that will be registered in the SQL engine's table COMMENT field (if supported by the engine). Specified as key-value pairs (`column_name = 'column comment'`). If present, [inline column comments](../../components/model/overview.md#inline-column-comments) will not be registered in the SQL engine. |       `dict`        |    N     |
| `grains`              | The column(s) whose combination uniquely identifies each row in the model                                                                                                                                                                                                                                                                                   | `str` \| `array[str]` |    N     |
| `profiles`            | The column(s) to profile for data quality checks using Soda profiling                                                                                                                                                                                                                                                                                      | `str` \| `array[str]` |    N     |
| `references`          | The model column(s) used to join to other models' grains                                                                                                                                                                                                                                                                                                    | `str` \| `array[str]` |    N     |
| `depends_on`          | Models on which this model depends, in addition to the ones inferred from the model's query. (Default: dependencies inferred from model code)                                                                                                                                                                                                               |    `array[str]`     |    N     |
| `table_format`        | The table format that should be used to manage the physical files (eg `iceberg`, `hive`, `delta`); only applicable to engines such as Spark and Athena                                                                                                                                                                                                      |        `str`        |    N     |
| `storage_format`      | The storage format that should be used to store physical files (eg `parquet`, `orc`); only applicable to engines such as Spark and Athena                                                                                                                                                                                                                   |        `str`        |    N     |
| `partitioned_by`      | The column(s) and/or column expressions used define a model's partitioning key. Required for the `INCREMENTAL_BY_PARTITION` model kind. Optional for all other model kinds; used to partition the model's physical table in engines that support partitioning.                                                                                              | `str` \| `array[str]` |    N     |
| `clustered_by`        | The column(s) and/or column expressions used to cluster the model's physical table; only applicable to engines that support clustering                                                                                                                                                                                                                                                |        `str`        |    N     |
| `columns`             | The column names and data types returned by the model. Disables [automatic inference of column names and types](../../components/model/overview.md#conventions) from the SQL query.                                                                                                                                                                             |    `array[str]`     |    N     |
| `physical_properties` | A key-value mapping of arbitrary properties specific to the target engine that are applied to the model table / view in the physical layer. Specified as key-value pairs (`key = value`). The view/table type (e.g. `TEMPORARY`, `TRANSIENT`) can be added with the `creatable_type` key.                                                                   |       `dict`        |    N     |
| `virtual_properties`  | A key-value mapping of arbitrary properties specific to the target engine that are applied to the model view in the virtual layer. Specified as key-value pairs (`key = value`). The view type (e.g. `SECURE`) can be added with the `creatable_type` key.                                                                                                  |       `dict`        |    N     |
| `session_properties`  | A key-value mapping of arbitrary properties specific to the target engine that are applied to the engine session. Specified as key-value pairs (`key = value`).                                                                                                                                                                                             |       `dict`        |    N     |
| `allow_partials`      | Whether this model can process partial (incomplete) data intervals                                                                                                                                                                                                                                                                                          |       `bool`        |    N     |
| `enabled`             | Whether the model is enabled. This attribute is `true` by default. Setting it to `false` causes Vulcan to ignore this model when loading the project.                                                                                                                                                                                                      |       `bool`        |    N     |
| `optimize_query`             | Whether the model's query should be optimized. This attribute is `true` by default. Setting it to `false` causes Vulcan to disable query canonicalization & simplification. This should be turned off only if the optimized query leads to errors such as surpassing text limit.                                                                                                                                                                                                      |       `bool`        |    N     |
| `ignored_rules`             |  A list of linter rule names (or "ALL") to be ignored/excluded for this model                                                                                                                                                                                             |       `str` \| `array[str]`        |    N     |
| `formatting`             | Whether the model will be formatted. All models are formatted by default. Setting this to `false` causes Vulcan to ignore this model during `vulcan format`.                                                                                                                                                                                                      |       `bool`        |    N     |

### Model defaults

The Vulcan project-level configuration must contain the `model_defaults` key and must specify a value for its `dialect` key. Other values are set automatically unless explicitly overridden in the model definition. Learn more about project-level configuration in the [configuration overview](../../configurations/overview.md).

In `physical_properties`, `virtual_properties`, and `session_properties`, when both project-level and model-specific properties are defined, they are merged, with model-level properties taking precedence. To unset a project-wide property for a specific model, set it to `None` in the `MODEL`'s DDL properties or within the `@model` decorator for Python models.

For example, with the following `model_defaults` configuration:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      start: 2022-01-01
      physical_properties:
        partition_expiration_days: 7
        require_partition_filter: True
        project_level_property: "value"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
      model_defaults=ModelDefaultsConfig(
        dialect="snowflake",
        start="2022-01-01",
        physical_properties={
          "partition_expiration_days": 7,
          "require_partition_filter": True,
          "project_level_property": "value"
        },
      ),
    )
    ```

To override `partition_expiration_days`, add a new `creatable_type` property and unset `project_level_property`, you can define the model as follows:

=== "SQL"

    ```sql linenums="1"
    MODEL (
      ...,
      physical_properties (
        partition_expiration_days = 14,
        creatable_type = TRANSIENT,
        project_level_property = None,
      )
    );
    ```

=== "Python"

    ```python linenums="1"
    @model(
      ...,
      physical_properties={
        "partition_expiration_days": 14,
        "creatable_type": "TRANSIENT",
        "project_level_property": None
      },
    )
    ```

You can also use the `@model_kind_name` variable to fine-tune control over `physical_properties` in `model_defaults`. This holds the current model's kind name and is useful for conditionally assigning a property. For example, to disable `creatable_type` for your project's `VIEW` kind models:

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: snowflake
      start: 2022-01-01
      physical_properties:
        creatable_type: "@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
      model_defaults=ModelDefaultsConfig(
        dialect="snowflake",
        start="2022-01-01",
        physical_properties={
          "creatable_type": "@IF(@model_kind_name != 'VIEW', 'TRANSIENT', NULL)",
        },
      ),
    )
    ```

You can aso define `pre_statements`, `post_statements` and `on_virtual_update` statements at the project level that will be applied to all models. These default statements are merged with any model-specific statements, with default statements executing first, followed by model-specific statements.

=== "YAML"

    ```yaml linenums="1"
    model_defaults:
      dialect: duckdb
      pre_statements:
        - "SET timeout = 300000"
      post_statements:
        - "@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)"
      on_virtual_update:
        - "GRANT SELECT ON @this_model TO ROLE analyst_role"
    ```

=== "Python"

    ```python linenums="1"
    from vulcan.core.config import Config, ModelDefaultsConfig

    config = Config(
      model_defaults=ModelDefaultsConfig(
        dialect="duckdb",
        pre_statements=[
          "SET query_timeout = 300000",
        ],
        post_statements=[
          "@IF(@runtime_stage = 'evaluating', ANALYZE @this_model)",
        ],
        on_virtual_update=[
          "GRANT SELECT ON @this_model TO ROLE analyst_role",
        ],
      ),
    )
    ```


The Vulcan project-level `model_defaults` key supports the following options, described in the [general model properties](#general-model-properties) table above:

- kind
- dialect
- cron
- owner
- start
- table_format
- storage_format
- physical_properties
- virtual_properties
- session_properties (on per key basis)
- on_destructive_change (described [below](#incremental-models))
- on_additive_change (described [below](#incremental-models))
- audits (described [here](../../components/audits/audits.md#generic-audits))
- optimize_query
- allow_partials
- enabled
- interval_unit
- pre_statements (described [here](../../components/model/types/sql_models.md#optional-prepost-statements))
- post_statements (described [here](../../components/model/types/sql_models.md#optional-prepost-statements))
- on_virtual_update (described [here](../../components/model/types/sql_models.md#optional-on-virtual-update-statements))


### Model Naming

Configuration option for name inference. Learn more in the [configuration overview](../../configurations/overview.md).

| Option        | Description                                                                                    | Type | Required |
|---------------|------------------------------------------------------------------------------------------------|:----:|:--------:|
| `infer_names` | Whether to automatically infer model names based on the directory structure (Default: `False`) | `bool` |    N     |


## Model kind properties

Configuration options for kind-specific Vulcan model properties, in addition to the [general model properties](#general-model-properties) listed above.

Learn more about model kinds at the [model kinds page](../../components/model/model_kinds.md). Learn more about specifying model kind in Python models at the [Python models page](../../components/model/types/python_models.md#model-specification).

### `VIEW` models

Configuration options for models of the [`VIEW` kind](../../components/model/model_kinds.md#view) (in addition to [general model properties](#general-model-properties)).

| Option         | Description                                                                                          | Type | Required |
|----------------|------------------------------------------------------------------------------------------------------|:----:|:--------:|
| `materialized` | Whether views should be materialized (for engines supporting materialized views). (Default: `False`) | `bool` |    N     |

Python model kind `name` enum value: [ModelKindName.VIEW](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

### `FULL` models

The [`FULL` model kind](../../components/model/model_kinds.md#full) does not support any configuration options other than the [general model properties listed above](#general-model-properties).

Python model kind `name` enum value: [ModelKindName.FULL](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

### Incremental models

Configuration options for all incremental models (in addition to [general model properties](#general-model-properties)).

| Option                  | Description                                                                                                                                                                                                                                                                                                                                              | Type | Required |
|-------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----:|:--------:|
| `forward_only`          | Whether the model's changes should always be classified as [forward-only](./plans.md#forward-only-change). (Default: `False`)                                                                                                                                                                                                                  | `bool` |    N     |
| `on_destructive_change` | What should happen when a change to a [forward-only model](../../guides/incremental_by_time.md#forward-only-models) or incremental model in a [forward-only plan](./plans.md#forward-only-plans) causes a destructive modification to the model schema. Valid values: `allow`, `warn`, `error`, `ignore`. (Default: `error`)                         | `str`  |    N     |
| `on_additive_change`    | What should happen when a change to a [forward-only model](../../guides/incremental_by_time.md#forward-only-models) or incremental model in a [forward-only plan](./plans.md#forward-only-plans) causes an additive modification to the model schema (like adding new columns). Valid values: `allow`, `warn`, `error`, `ignore`. (Default: `allow`) | `str`  |    N     |
| `disable_restatement`   | Whether [restatements](./plans.md#restatement-plans) should be disabled for the model. (Default: `False`)                                                                                                                                                                                                                                      | `bool` |    N     |

#### Incremental by time range

Configuration options for [`INCREMENTAL_BY_TIME_RANGE` models](../../components/model/model_kinds.md#incremental_by_time_range) (in addition to [general model properties](#general-model-properties) and [incremental model properties](#incremental-models)).

| Option              | Description                                                                                                                                                                                                                                                                                                                      | Type | Required |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--: | :------: |
| **`time_column`**       | **The model column containing each row's timestamp. Should be UTC time zone.**                                                                                                                                                                                                                                                       | **`str`**  |    **Y**     |
| `format`            | Argument to `time_column`. Format of the time column's data. (Default: `%Y-%m-%d`)                                                                                                                                                                                                                                               | `str`  |    N     |
| `batch_size`        | The maximum number of intervals that can be evaluated in a single backfill task. If this is `None`, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of `batch_size` intervals. (Default: `None`) | `int`  |    N     |
| `batch_concurrency` | The maximum number of batches that can run concurrently for this model. (Default: the number of concurrent tasks set in the connection settings)                                                                                                                                                                                 | `int`  |    N     |
| `lookback`          | The number of `interval_unit`s prior to the current interval that should be processed. [Learn more](../../components/model/overview.md#lookback). (Default: `0`)                                                                                                                                                                                                        | `int`  |    N     |

Python model kind `name` enum value: [ModelKindName.INCREMENTAL_BY_TIME_RANGE](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

#### Incremental by unique key

Configuration options for [`INCREMENTAL_BY_UNIQUE_KEY` models](../../components/model/model_kinds.md#incremental_by_unique_key) (in addition to [general model properties](#general-model-properties) and [incremental model properties](#incremental-models)). Batch concurrency cannot be set for incremental by unique key models because they cannot safely be run in parallel.

| Option         | Description                                                                                                                                                                                                                                                                                                                      | Type              | Required |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|----------|
| **`unique_key`**   | **The model column(s) containing each row's unique key**                                                                                                                                                                                                                                                                             | **`str` \| `array[str]`** | **Y**        |
| `when_matched` | SQL logic used to update columns when a match occurs - only available on engines that support `MERGE`. (Default: update all columns)                                                                                                                                                                                             | `str`               | N        |
| `merge_filter` | A single or a conjunction of predicates used to filter data in the ON clause of a MERGE operation - only available on engines that support `MERGE`                                                                                                                                                                               | `str`               | N        |
| `batch_size`   | The maximum number of intervals that can be evaluated in a single backfill task. If this is `None`, all intervals will be processed as part of a single task. If this is set, a model's backfill will be chunked such that each individual task only contains jobs with the maximum of `batch_size` intervals. (Default: `None`) | `int`               | N        |
| `lookback`     | The number of time unit intervals prior to the current interval that should be processed. (Default: `0`)                                                                                                                                                                                                                         | `int`               | N        |

Python model kind `name` enum value: [ModelKindName.INCREMENTAL_BY_UNIQUE_KEY](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

#### Incremental by partition

The [`INCREMENTAL_BY_PARTITION` models](../../components/model/model_kinds.md#incremental_by_partition) kind does not support any configuration options other than the [general model properties](#general-model-properties) and [incremental model properties](#incremental-models).

Python model kind `name` enum value: [ModelKindName.INCREMENTAL_BY_PARTITION](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

#### SCD Type 2 models

Configuration options for [`SCD_TYPE_2` models](../../components/model/model_kinds.md#scd-type-2) (in addition to [general model properties](#general-model-properties) and [incremental model properties](#incremental-models)).

| Option                    | Description                                                                                                                                                                                 |    Type    | Required |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------: | :------: |
| **`unique_key`**              | **The model column(s) containing each row's unique key**                                                                                                                                        | **`array[str]`** |    **Y**     |
| `valid_from_name`         | The model column containing each row's valid from date. (Default: `valid_from`)                                                                                                             |    `str`     |    N     |
| `valid_to_name`           | The model column containing each row's valid to date. (Default: `valid_to`)                                                                                                                 |    `str`     |    N     |
| `invalidate_hard_deletes` | If set to true, when a record is missing from the source table it will be marked as invalid. See [here](../../components/model/model_kinds.md#deletes) for more information. (Default: `True`) |    `bool`    |    N     |

##### SCD Type 2 By Time

Configuration options for [`SCD_TYPE_2_BY_TIME` models](../../components/model/model_kinds.md#scd-type-2) (in addition to [general model properties](#general-model-properties), [incremental model properties](#incremental-models), and [SCD Type 2 properties](#scd-type-2-models)).

| Option                     | Description                                                                                                                                                                      | Type | Required |
| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--: | :------: |
| `updated_at_name`          | The model column containing each row's updated at date. (Default: `updated_at`)                                                                                                  | `str`  |    N     |
| `updated_at_as_valid_from` | By default, for new rows the `valid_from` column is set to 1970-01-01 00:00:00. This sets `valid_from` to the value of `updated_at` when the row is inserted. (Default: `False`) | `bool` |    N     |

Python model kind `name` enum value: [ModelKindName.SCD_TYPE_2_BY_TIME](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

##### SCD Type 2 By Column

Configuration options for [`SCD_TYPE_2_BY_COLUMN` models](../../components/model/model_kinds.md#scd-type-2) (in addition to [general model properties](#general-model-properties), [incremental model properties](#incremental-models), and [SCD Type 2 properties](#scd-type-2-models)).

| Option                         | Description                                                                                                                                                                 |       Type        | Required |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------: | :------: |
| **`columns`**                      | **Columns whose changed data values indicate a data update (instead of an `updated_at` column). `*` to represent that all columns should be checked.**                          | **`str` \| `array[str]`** |    **Y**     |
| `execution_time_as_valid_from` | By default, for new rows `valid_from` is set to 1970-01-01 00:00:00. This changes the behavior to set it to the execution_time of when the pipeline ran. (Default: `False`) |       `bool`        |    N     |

Python model kind `name` enum value: [ModelKindName.SCD_TYPE_2_BY_COLUMN](https://vulcan.readthedocs.io/en/stable/_readthedocs/html/vulcan/core/model/kind.html#ModelKindName)

### `SEED` models

Configuration options for [`SEED` models](../../components/model/model_kinds.md#seed). `SEED` models do not support all the general properties supported by other models; they only support the properties listed in this table.

Top-level options inside the MODEL DDL:

| Option        | Description                                                                                                                                                                                                                |    Type    | Required |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------: | :------: |
| `name`        | The model name. Must include at least a qualifying schema (`<schema>.<model>`) and may include a catalog (`<catalog>.<schema>.<model>`). Can be omitted if [infer_names](#model-naming) is set to true.                                                                                |    `str`     |    N     |
| **`kind`**        | **The model kind. Must be `SEED`.**                                                                                                                                                                                            |    **`str`**     |    **Y**     |
| `columns`     | The column names and data types in the CSV file. Disables automatic inference of column names and types by the pandas CSV reader. NOTE: order of columns overrides the order specified in the CSV header row (if present). | `array[str]` |    N     |
| `audits`      | Vulcan [audits](../../components/audits/audits.md) that should run against the model's output                                                                                                                                         | `array[str]` |    N     |
| `owner`       | The owner of a model; may be used for notification purposes                                                                                                                                                                |    `str`     |    N     |
| `stamp`       | Arbitrary string used to indicate a model's version without changing the model name                                                                                                                                        |    `str`     |    N     |
| `tags`        | Arbitrary strings used to organize or classify a model                                                                                                                                                                     | `array[str]` |    N     |
| `description` | Description of the model. Automatically registered in the SQL engine's table COMMENT field or equivalent (if supported by the engine).                                                                                     |    `str`     |    N     |

Options specified within the top-level `kind` property:

| Option         | Description                                                                                                                                                                             | Type | Required |
| -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | -------- |
| **`path`**         | **Path to seed CSV file.**                                                                                                                                                                  | **`str`**  | **Y**        |
| `batch_size`   | The maximum number of CSV rows ingested in each batch. All rows ingested in one batch if not specified.                                                                                 | `int`  | N        |
| `csv_settings` | Pandas CSV reader settings (overrides [default values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). Specified as key-value pairs (`key = value`). | `dict` | N        |

<a id="csv_settings"></a>
Options specified within the `kind` property's `csv_settings` property (overrides [default Pandas CSV reader settings](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)):

| Option             | Description                                                                                                                                                                                                                                                                         | Type | Required |
| ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | -------- |
| `delimiter`        | Character or regex pattern to treat as the delimiter. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                              | `str`  | N        |
| `quotechar`        | Character used to denote the start and end of a quoted item. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                       | `str`  | N        |
| `doublequote`      | When quotechar is specified, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). | `bool` | N        |
| `escapechar`       | Character used to escape other characters. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                                         | `str`  | N        |
| `skipinitialspace` | Skip spaces after delimiter. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                                                       | `bool` | N        |
| `lineterminator`   | Character used to denote a line break. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                                             | `str`  | N        |
| `encoding`         | Encoding to use for UTF when reading/writing (ex. 'utf-8'). More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                        | `str`  | N        |
| `na_values` | An array of values that should be recognized as NA/NaN. In order to specify such an array per column, a mapping in the form of `(col1 = (v1, v2, ...), col2 = ...)` can be passed instead. These values can be integers, strings, booleans or NULL, and they are converted to their corresponding Python values. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                        | `array[value]` \| `array[array[key = value]]`  | N        |
| `keep_default_na` | Whether or not to include the default NaN values when parsing the data. More information at the [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).                                                                                                                       | `bool` | N        |


# Overview

Source: https://tmdc-io.github.io/vulcan-book/references/overview/

---

# Overview

This page explains what Vulcan does and how its components work together.

## What Vulcan is

Vulcan is a Python framework that automates data transformation workflows. It works with multiple execution engines and scales with your data and organization size.

You control Vulcan through the [CLI](../../getting_started/cli.md).

## How Vulcan works

### Create models

You write business logic in SQL or Python. Each model contains code that populates a single table or view, plus metadata like the model name.

### Make a plan

Changing models in large data systems affects downstream dependencies. Interdependencies make it hard to see the full impact of a single model change.

You need to understand both the logical impact and the computational cost before running the changes.

Vulcan creates a plan that identifies all affected models and required computations. When you run [`vulcan plan`](../../getting_started/cli.md#plan), Vulcan generates the plan for the environment you specify (dev, test, prod).

The plan shows the full scope of changes by identifying directly and indirectly affected models. You see all impacts before applying changes.

Learn more about [plans](./plans.md).

#### Apply the plan

After reviewing a plan, you apply it to execute the computations. You specify the scope of computations to run.

The computations depend on both the code changes in the plan and the backfill parameters you set.

Backfilling updates existing data to match your changed models. If a model change alters a calculation, existing data becomes inaccurate. Backfilling recalculates those fields.

Most business data is temporal. Each data point has a timestamp. Backfill cost depends on how much historical data needs recalculation.

The plan identifies which models and dates need backfill. You specify the date ranges for backfill before applying the plan.

#### Build a Virtual Environment

Test changes in non-production environments before deploying to production.

Using multiple environments usually means running backfills twice: once in non-production, again in production. This doubles time and compute costs.

Vulcan tracks all model versions and changes. It detects when non-production computations produce identical outputs to production.

Vulcan creates a Virtual Environment by replacing references to outdated production tables with references to newly computed non-production tables. It promotes views and tables from non-production to production without recomputation or data movement.

Promoting changes to production is fast and has no downtime.

## Test your code and data

Bad data causes more problems than missing data. Test your transformation code and results to prevent bad data.

### Tests

Vulcan tests work like unit tests, where each model is a unit. Tests validate model code. You specify input data and expected output. Vulcan runs the test and compares expected and actual output.

Vulcan runs tests automatically when you apply a plan. You can also run them with [`vulcan test`](../../getting_started/cli.md#test).

### Audits

Audits validate the results of model code applied to your actual data.

You write SQL queries that should return 0 rows. For example, to ensure `your_field` has no NULL values, include `WHERE your_field IS NULL`. If NULLs exist, the query returns rows and the audit fails.

Audits can target specific models or use [macros](../../components/advanced-features/macros/overview.md) for reusable audits. Vulcan includes built-in audits for common cases like detecting NULL or duplicate values.

Specify audits in model metadata properties. To apply them globally, add them to model defaults configuration.

Vulcan runs audits automatically when you apply a plan to an environment. You can also run them with [`vulcan audit`](../../getting_started/cli.md#audit).

## Infrastructure and orchestration

Vulcan works with any SQL or analytics engine. It only needs access to the target engine.

Vulcan tracks model versions and processed data intervals using your existing infrastructure. It creates a `vulcan` schema in your data warehouse for internal metadata.



# Plans

Source: https://tmdc-io.github.io/vulcan-book/references/plans/

---

# Plans

A plan summarizes the difference between your local project state and a target [environment](./environments.md). To apply model changes to a target environment, create and apply a plan.

## Plan Architecture Overview

The following diagram illustrates the complete plan lifecycle, from local changes to environment updates:

```mermaid
flowchart TD
    subgraph "1. Local Development"
        A[Developer modifies model files<br/>Edit SQL/Python models]
        B[Local project state<br/>Your changes ready]
    end

    subgraph "2. Plan Creation"
        C[vulcan plan<br/>Command execution]
        D[Compare local vs environment<br/>State comparison]
        E{Changes detected?}
        F[Generate plan summary<br/>Plan ready for review]
        G[Change categorization<br/>Breaking / Non-breaking / Forward-only]
    end

    subgraph "3. Plan Review"
        H[Review plan output<br/>Check changes & impacts]
        I{Apply plan?}
        J[Cancel<br/>No changes applied]
    end

    subgraph "4. Plan Application"
        K[Create model variants<br/>With unique fingerprints]
        L[Create physical tables<br/>In data warehouse]
        M[Backfill data<br/>Process historical data]
        N[Update virtual layer<br/>Create/update views]
        O[Update environment references<br/>Point to new variants]
    end

    subgraph "5. Result"
        P[Environment updated<br/>Changes deployed]
        Q[Models accessible via views<br/>Ready for queries]
    end

    A -->|"to"| B
    B -->|"to"| C
    C -->|"to"| D
    D -->|"to"| E
    E -->|"Yes"| F
    E -->|"No"| P
    F -->|"to"| G
    G -->|"to"| H
    H -->|"to"| I
    I -->|"Yes"| K
    I -->|"No"| J
    K -->|"to"| L
    L -->|"to"| M
    M -->|"to"| N
    N -->|"to"| O
    O -->|"to"| P
    P -->|"to"| Q

    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style C fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000
    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style P fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
    style E fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style I fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style J fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
```

### Plan Components

```mermaid
graph LR
    subgraph "Plan Contents"
        PC1[Added Models<br/>New models to create]
        PC2[Removed Models<br/>Models to delete]
        PC3[Modified Models<br/>With diffs]
        PC4[Indirectly Affected<br/>Downstream models]
        PC5[Backfill Requirements<br/>Date ranges]
    end

    subgraph "Change Types"
        CT1[Breaking Change<br/>Requires downstream backfill<br/>Cascading impact]
        CT2[Non-Breaking Change<br/>Only direct model backfill<br/>Isolated impact]
        CT3[Forward-Only<br/>Reuses existing tables<br/>Cost-effective]
    end

    PC3 -->|"Breaking"| CT1
    PC3 -->|"Non-breaking"| CT2
    PC3 -->|"Forward-only"| CT3
    PC4 -->|"Breaking"| CT1

    style PC1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style PC2 fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    style PC3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style PC4 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style PC5 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style CT1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style CT2 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style CT3 fill:#fff9c4,stroke:#fbc02d,stroke-width:3px,color:#000
```

During plan creation:

* The local state of the Vulcan project is compared to the state of a target environment. The difference between the two and the actions needed to synchronize the environment with the local state are what constitutes a plan.
* You may be prompted to [categorize changes](#change-categories) for existing models so Vulcan can determine actions for indirectly affected models (downstream models that depend on updated models). By default, Vulcan categorizes changes automatically. Change this behavior through [configuration](./configuration.md#plan).
* Each plan requires a date range to which it will be applied. If not specified, the date range is derived automatically based on model definitions and the target environment.

The benefit of plans is that all changes can be reviewed and verified before they are applied to the data warehouse and any computations are performed. A typical plan contains a combination of the following:

* A list of added models
* A list of removed models
* A list of directly modified models and a text diff of changes that have been made
* A list of indirectly modified models
* Missing data intervals for affected models
* A date range that will be affected by the plan application

To create a new plan, run the following command:
```bash
vulcan plan [environment name]
```

If no environment name is specified, the plan is generated for the `prod` environment.

## Change categories
Categories only need to be provided for models that have been modified directly. The categorization of indirectly modified downstream models is inferred based on the types of changes to the directly modified models.

If more than one upstream dependency of an indirectly modified model has been modified and they have conflicting categories, the most conservative category (breaking) is assigned to this model.

### Change Propagation Flow

The following diagram illustrates how changes propagate through the dependency graph:

```mermaid
graph TD
    subgraph "Model Dependencies"
        A[raw.raw_orders<br/>Upstream]
        B[sales.daily_sales<br/>Midstream]
        C[sales.weekly_sales<br/>Downstream]
        D[analytics.revenue_report<br/>Downstream]
    end

    subgraph "Scenario 1: Non-Breaking Change"
        NB1[Add column to daily_sales<br/>New column added]
        NB2[Only daily_sales backfilled<br/>Single model update]
        NB3[weekly_sales NOT affected<br/>No cascade]
        NB4[revenue_report NOT affected<br/>No cascade]
    end

    subgraph "Scenario 2: Breaking Change"
        BC1[Add WHERE clause to daily_sales<br/>Filter logic changed]
        BC2[daily_sales backfilled<br/>Data reprocessed]
        BC3[weekly_sales backfilled<br/>Indirect Breaking<br/>Cascading impact]
        BC4[revenue_report backfilled<br/>Indirect Breaking<br/>Cascading impact]
    end

    A -->|"to"| B
    B -->|"to"| C
    B -->|"to"| D

    NB1 -->|"to"| B
    B -->|"to"| NB2
    NB2 -.->|"No cascade"| C
    NB2 -.->|"No cascade"| D

    BC1 -->|"to"| B
    B -->|"to"| BC2
    BC2 -->|"Cascade"| BC3
    BC2 -->|"Cascade"| BC4
    BC3 -->|"to"| C
    BC4 -->|"to"| D

    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    style NB1 fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    style NB2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#000
    style NB3 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style NB4 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style BC1 fill:#ffebee,stroke:#d32f2f,stroke-width:3px,color:#000
    style BC2 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
    style BC3 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
    style BC4 fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000
```

### Breaking change
If a directly modified model change is categorized as breaking, it and its downstream dependencies will be backfilled.

In general, this is the safest option because it guarantees all downstream dependencies will reflect the change. However, it is a more expensive option because it involves additional data reprocessing, which has a runtime cost associated with it (refer to [backfilling](#backfilling)).

Choose this option when a change has been made to a model's logic that has a functional impact on its downstream dependencies. For example, adding or modifying a model's `WHERE` clause is a breaking change because downstream models contain rows that would now be filtered out.

### Non-breaking change
A directly-modified model that is classified as non-breaking will be backfilled, but its downstream dependencies will not.

This is a common choice in scenarios such as an addition of a new column, an action which doesn't affect downstream models, as new columns can't be used by downstream models without modifying them directly to select the column.

If any downstream models contain a `select *` from the model, Vulcan attempts to infer breaking status on a best-effort basis. We recommend explicitly specifying a query's columns to avoid unnecessary recomputation.

### Summary

| Change Category                      | Change Type                                                                                | Behaviour                                          |
|--------------------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------|
| [Breaking](#breaking-change)         | [Direct](./glossary.md#direct-modification) or [Indirect](./glossary.md#indirect-modification) | [Backfill](./glossary.md#backfill)                   |
| [Non-breaking](#non-breaking-change) | [Direct](./glossary.md#direct-modification)                                                  | [Backfill](./glossary.md#backfill)                   |
| [Non-breaking](#non-breaking-change) | [Indirect](./glossary.md#indirect-modification)                                              | [No Backfill](./glossary.md#backfill)                |

## Forward-only change
In addition to categorizing a change as breaking or non-breaking, it can also be classified as forward-only.

A model change classified as forward-only will continue to use the existing physical table once the change is deployed to production (the `prod` environment). This means that no backfill will take place.

While iterating on forward-only changes in the development environment, the model's output will be stored in either a temporary table or a shallow clone of the production table if supported by the engine.

In either case the data produced this way in the development environment can only be used for preview and will **not** be reused once the change is deployed to production. See [Forward-only Plans](#forward-only-plans) for more details.

This category is assigned by Vulcan automatically either when a user opts into using a [forward-only plan](#forward-only-plans) or when a model is explicitly configured to be forward-only.

## Plan application
Once a plan has been created and reviewed, it is then applied to the target [environment](environments.md) in order for its changes to take effect.

When a model changes as part of a plan, Vulcan creates a new model variant behind the scenes (a snapshot with a unique fingerprint). Each model variant's data is stored in a separate physical table. Data between different variants of the same model is never shared, except for [forward-only](#forward-only-plans) plans.

When a plan is applied to an environment, the environment gets associated with the set of model variants that are part of that plan. In other words, each environment is a collection of references to model variants and the physical tables associated with them.

### Model Versioning Architecture

The following diagram shows how model variants, physical tables, and environments relate:

```mermaid
graph TB
    subgraph "Model Definitions"
        M1[Model: sales.daily_sales<br/>Version 1<br/>Original]
        M2[Model: sales.daily_sales<br/>Version 2 - Modified<br/>Updated]
    end

    subgraph "Model Variants & Snapshots"
        V1[Variant 1<br/>Fingerprint: abc123<br/>Unique snapshot]
        V2[Variant 2<br/>Fingerprint: def456<br/>Unique snapshot]
        S1[Snapshot 1<br/>Immutable state]
        S2[Snapshot 2<br/>Immutable state]
    end

    subgraph "Physical Tables"
        T1[Physical Table 1<br/>db.vulcan__sales.daily_sales__abc123<br/>Actual data storage]
        T2[Physical Table 2<br/>db.vulcan__sales.daily_sales__def456<br/>Actual data storage]
    end

    subgraph "Virtual Layer Views"
        VL1[View: sales.daily_sales<br/>Points to Variant 1<br/>Reference mapping]
        VL2[View: sales.daily_sales<br/>Points to Variant 2<br/>Reference mapping]
    end

    subgraph "Environments"
        PROD[Production Environment<br/>References Variant 1<br/>Live production data]
        DEV[Dev Environment<br/>References Variant 2<br/>Testing environment]
    end

    M1 -->|"to"| V1
    M2 -->|"to"| V2
    V1 -->|"to"| S1
    V2 -->|"to"| S2
    S1 -->|"to"| T1
    S2 -->|"to"| T2
    T1 -->|"to"| VL1
    T2 -->|"to"| VL2
    PROD -->|"to"| V1
    DEV -->|"to"| V2

    style M1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    style M2 fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    style V1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#000
    style V2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000
    style S1 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style S2 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000
    style T1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000
    style T2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000
    style VL1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style VL2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    style PROD fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#000
    style DEV fill:#ffe082,stroke:#f9a825,stroke-width:3px,color:#000
```

![Each model variant gets its own physical table, while environments only contain references to these tables](plans/model_versioning.png)

*Each model variant gets its own physical table while environments only contain references to these tables.*

This unique approach to understanding and applying changes is what enables Vulcan's Virtual Environments. It allows Vulcan to ensure complete isolation between environments while allowing it to share physical data assets between environments when appropriate and safe to do so.

Additionally, since each model change is captured in a separate physical table, reverting to a previous version becomes a simple and quick operation (refer to [Virtual Update](#virtual-update)) as long as its physical table hasn't been garbage collected by the janitor process.

Vulcan makes it easy to be correct and really hard to accidentally and irreversibly break things.

### Backfilling
Despite all the benefits, the approach described above is not without trade-offs.

When a new model version is just created, a physical table assigned to it is empty. Therefore, Vulcan needs to re-apply the logic of the new model version to the entire date range of this model in order to populate the new version's physical table. This process is called backfilling.

We use the term backfilling broadly to describe any situation in which a model is updated. That includes these operations:

* When a VIEW model is created
* When a FULL model is built
* When an INCREMENTAL model is built for the first time
* When an INCREMENTAL model has recent data appended to it
* When an INCREMENTAL model has older data inserted (i.e., resolving a data gap or prepending historical data)

Note for incremental models: despite the fact that backfilling can happen incrementally (see `batch_size` parameter on models), there is an extra cost associated with this operation due to additional runtime involved. If the runtime cost is a concern, use a [forward-only plan](#forward-only-plans) instead.

### Virtual Update
A benefit of Vulcan's approach is that data for a new model version can be fully pre-built while still in a development environment. That way all changes and their downstream dependencies can be fully previewed before they are promoted to the production environment.

With this approach, the process of promoting a change to production is reduced to reference swapping.

If during plan creation no data gaps have been detected and only references to new model versions need to be updated, then the update is referred to as a Virtual Update. Virtual Updates impose no additional runtime overhead or cost.

### Start and end dates

The `plan` command provides two temporal options: `--start` and `--end`. These options are only applicable to plans for non-prod environments.

Every model has a start date. Specify it in [the model definition](../../components/model/overview.md#start), in the [project configuration's `model_defaults`](../../configurations/options/model_defaults.md), or use Vulcan's default value of yesterday.

Because the prod environment supports business operations, prod plans ensure every model is backfilled from its start date until the most recent completed time interval. Due to that restriction, the `plan` command's `--start` and `--end` options are not supported for regular plans against prod. The options are supported for [restatement plans](#restatement-plans) against prod to allow re-processing a subset of existing data.

Non-prod plans are typically used for development, so their models can optionally be backfilled for any date range with the `--start` and `--end` options. Limiting the date range makes backfills faster and development more efficient, especially for incremental models using large tables.

#### Model kind limitations

Some model kinds do not support backfilling a limited date range.

For context, Vulcan strives to make models _idempotent_, meaning that if we ran them multiple times we would get the same correct result every time.

However, some model kinds are inherently non-idempotent:

- [INCREMENTAL_BY_UNIQUE_KEY](../../components/model/model_kinds.md#incremental_by_unique_key)
- [INCREMENTAL_BY_PARTITION](../../components/model/model_kinds.md#incremental_by_partition)
- [SCD_TYPE_2_BY_TIME](../../components/model/model_kinds.md#scd-type-2-by-time-recommended)
- [SCD_TYPE_2_BY_COLUMN](../../components/model/model_kinds.md#scd-type-2-by-column)
- Any model whose query is self-referential (i.e., the contents of new data rows are affected by the data rows already present in the table)

Those model kinds will behave as follows in a non-prod plan that specifies a limited date range:

- If the `--start` option date is the same as or before the model's start date, the model is fully refreshed for all of time
- If the `--start` option date is after the model's start date, only a preview is computed for this model which can't be reused when deploying to production

#### Example

Consider a Vulcan project with a default start date of 2024-09-20.

It contains the following `INCREMENTAL_BY_UNIQUE_KEY` model that specifies an explicit start date of 2024-09-23:

```sql linenums="1" hl_lines="6"
MODEL (
  name vulcan_example.start_end_model,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key item_id
  ),
  start '2024-09-23'
);

SELECT
  item_id,
  num_orders
FROM
  vulcan_example.full_model
```

When we run the project's first plan, we see that Vulcan correctly detected a different start date for our `start_end_model` than the other models (which have the project default start of 2024-09-20):

```bash linenums="1" hl_lines="17"
❯ vulcan plan
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
`prod` environment will be initialized

Models:
└── Added:
    ├── vulcan_example.full_model
    ├── vulcan_example.incremental_model
    ├── vulcan_example.seed_model
    └── vulcan_example.start_end_model
Models needing backfill (missing dates):
├── vulcan_example.full_model: 2024-09-20 - 2024-09-26
├── vulcan_example.incremental_model: 2024-09-20 - 2024-09-26
├── vulcan_example.seed_model: 2024-09-20 - 2024-09-26
└── vulcan_example.start_end_model: 2024-09-23 - 2024-09-26
Apply - Backfill Tables [y/n]:
```

After executing that plan, we add columns to both the `incremental_model` and `start_end_model` queries.

We then execute `vulcan plan dev` to create the new `dev` environment:

```bash linenums="1" hl_lines="23-26"

❯ vulcan plan dev
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
New environment `dev` will be created from `prod`

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── vulcan_example__dev.start_end_model
│   └── vulcan_example__dev.incremental_model
└── Indirectly Modified:
    └── vulcan_example__dev.full_model

[...model diff omitted...]

Directly Modified: vulcan_example__dev.incremental_model (Non-breaking)
└── Indirectly Modified Children:
    └── vulcan_example__dev.full_model (Indirect Non-breaking)

[...model diff omitted...]

Directly Modified: vulcan_example__dev.start_end_model (Non-breaking)
Models needing backfill (missing dates):
├── vulcan_example__dev.incremental_model: 2024-09-20 - 2024-09-26
└── vulcan_example__dev.start_end_model: 2024-09-23 - 2024-09-26
Enter the backfill start date (eg. '1 year', '2020-01-01') or blank to backfill from the beginning of history:
```

Note two things about the output:

1. As before, Vulcan displays the complete backfill time range for each model, using the project default start of 2024-09-20 for `incremental_model` and 2024-09-23 for `start_end_model`
2. Vulcan prompted us for a backfill start date because we didn't pass the `--start` option to the `vulcan plan dev` command

Let's cancel that plan and start a new one, passing a start date of 2024-09-24.

The `start_end_model` is of kind `INCREMENTAL_BY_UNIQUE_KEY`, which is non-idempotent and cannot be backfilled for a limited time range.

Because the command's `--start` of 2024-09-24 is after `start_end_model`'s start date 2024-09-23, `start_end_model` is marked as preview:

``` bash linenums="1" hl_lines="12-13 20-21"
❯ vulcan plan dev --start 2024-09-24
======================================================================
Successfully Ran 1 tests against duckdb
----------------------------------------------------------------------
New environment `dev` will be created from `prod`

Differences from the `prod` environment:

Models:
├── Directly Modified:
│   ├── vulcan_example__dev.start_end_model
│   └── vulcan_example__dev.incremental_model
└── Indirectly Modified:
    └── vulcan_example__dev.full_model

[...model diff omitted...]

Directly Modified: vulcan_example__dev.start_end_model (Non-breaking)
Models needing backfill (missing dates):
├── vulcan_example__dev.incremental_model: 2024-09-24 - 2024-09-26
└── vulcan_example__dev.start_end_model: 2024-09-24 - 2024-09-26 (preview)
Enter the backfill end date (eg. '1 month ago', '2020-01-01') or blank to backfill up until '2024-09-27 00:00:00':
```

#### Minimum intervals

When you run a plan with a fixed `--start` or `--end` date, you create a virtual data environment with a limited subset of data. However, if the time range specified is less than the size of an interval on one of your models, that model will be skipped by default.

For example, if you have a model like so:

```sql
MODEL(
    name vulcan_example.monthly_model,
    kind INCREMENTAL_BY_TIME_RANGE (
        time_column month
    ),
    cron '@monthly'
);

SELECT SUM(a) AS sum_a, MONTH(day) AS month
FROM vulcan_example.upstream_model
WHERE day BETWEEN @start_ds AND @end_ds
```

make a change to it and run the following:

```bash linenums="1" hl_lines="8"
$ vulcan plan dev --start '1 day ago' 

Models:
└── Added:
    └── vulcan_example__dev.monthly_model
Apply - Virtual Update [y/n]: y

SKIP: No model batches to execute
```

No data will be backfilled because `1 day ago` does not contain a complete month. However, you can use the `--min-intervals` option to override this behaviour like so:

```bash linenums="1" hl_lines="11"
$ vulcan plan dev --start '1 day ago' --min-intervals 1

Models:
└── Added:
    └── vulcan_example__dev.monthly_model
Apply - Virtual Update [y/n]: y

[1/1] vulcan_example__dev.monthly_model   [insert 2025-06-01 - 2025-06-30]   0.08s   
Executing model batches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1 • 0:00:00                                                             
                                                                                                                                                    
✔ Model batches executed
```

This will ensure that regardless of the plan `--start` date, all added or modified models will have at least `--min-intervals` intervals considered for backfill.

!!! info

    If you are running plans manually you can just adjust the `--start` date to be wide enough to cover the models in question.

    The `--min-intervals` option is primarily intended for automation scenarios where the plan is always run with a default relative start date and you always want (for example) "2 weeks worth of data" in the target environment.

### Data preview for forward-only changes
As mentioned earlier, the data output produced by [forward-only changes](#forward-only-change) in a development environment can only be used for preview and will not be reused in production.

The same holds true for any subsequent changes that depend on undeployed forward-only changes - data can be previewed but can't be reused in production.

Backfills that are exclusively for preview purposes and will not be reused upon deployment to production are explicitly labeled with `(preview)` in the plan summary:
```bash
Models needing backfill (missing dates):
├── sushi__dev.customers: 2023-12-22 - 2023-12-28 (preview)
├── sushi__dev.waiter_revenue_by_day: 2023-12-22 - 2023-12-28
├── sushi__dev.top_waiters: 2023-12-22 - 2023-12-28
└── sushi__dev.waiter_as_customer_by_day: 2023-12-22 - 2023-12-28 (preview)
```

## Forward-only plans
Sometimes the runtime cost associated with rebuilding an entire physical table is too high and outweighs the benefits a separate table provides. This is when a forward-only plan comes in handy.

When a forward-only plan is applied to the `prod` environment, none of the plan's changed models will have new physical tables created for them. Instead, physical tables from previous model versions are reused.

The benefit of this is that no backfilling is required, so there is no runtime overhead or cost. The drawback is that reverting to a previous version is no longer simple and requires a combination of additional forward-only changes and [restatements](#restatement-plans).

Note that once a forward-only change is applied to `prod`, all development environments that referred to the previous versions of the updated models will be impacted.

A core component of the development process is to execute code and verify its behavior. To enable this while preserving isolation between environments, `vulcan plan [environment name]` evaluates code in non-`prod` environments while targeting shallow (a.k.a. "zero-copy") clones of production tables for engines that support them or newly created temporary physical tables for engines that don't.

This means that only a limited preview of changes is available in the development environment before the change is promoted to `prod`. The date range of the preview is provided as part of plan creation command.

Engines for which table cloning is supported include:

* `BigQuery`
* `Databricks`
* `Snowflake`

Note that all changes made as part of a forward-only plan automatically get a **forward-only** category assigned to them. These types of changes can't be mixed together with [breaking and non-breaking changes](#change-categories) within the same plan.

To create a forward-only plan, add the `--forward-only` option to the `plan` command:
```bash
vulcan plan [environment name] --forward-only
```

!!! note
    The `--forward-only` flag isn't required when applying changes to models explicitly configured as [forward-only](../../components/model/overview.md#forward_only).

    Use it only if you need to provide a time range for the preview window or the [effective date](#effective-date).

### Destructive changes

Some model changes destroy existing data in a table. Vulcan automatically detects and optionally prevents destructive changes to [forward-only models](../../guides/incremental_by_time.md#forward-only-models). Learn more [here](../../guides/incremental_by_time.md#destructive-changes).

Forward-only plans treats all of the plan's model changes as forward-only. In these plans, Vulcan will check all modified incremental models for destructive schema changes, not just forward-only models.

Vulcan determines what to do for each model based on this setting hierarchy: 

- **For destructive changes**: the [model's `on_destructive_change` value](../../guides/incremental_by_time.md#schema-changes) (if present), the `on_destructive_change` [model defaults](./model_configuration.md#model-defaults) value (if present), and the Vulcan global default of `error`
- **For additive changes**: the [model's `on_additive_change` value](../../guides/incremental_by_time.md#schema-changes) (if present), the `on_additive_change` [model defaults](./model_configuration.md#model-defaults) value (if present), and the Vulcan global default of `allow`

If you want to temporarily allow destructive changes to models that don't allow them, use the `plan` command's `--allow-destructive-model` selector to specify which models. 
Similarly, if you want to temporarily allow additive changes to models configured with `on_additive_change=error`, use the `--allow-additive-model` selector. 

For example, to allow destructive changes to all models in the `analytics` schema:
```bash
vulcan plan --forward-only --allow-destructive-model "analytics.*"
```

Or to allow destructive changes to multiple specific models:
```bash
vulcan plan --forward-only --allow-destructive-model "sales.revenue_model" --allow-destructive-model "marketing.campaign_model"
```

Learn more about model selectors [here](../../guides/model_selection.md).

### Effective date
Changes that are part of the forward-only plan can also be applied retroactively to the production environment by specifying the effective date:

```bash
vulcan plan --forward-only --effective-from 2023-01-01
```

This way Vulcan will know to recompute data intervals starting from the specified date once forward-only changes are deployed to production.

## Restatement plans

Models sometimes need to be re-evaluated for a given time range, even though the model definition has not changed.

For example, these scenarios all require re-evaluating model data that already exists:

- Correcting an upstream data issue by reprocessing some of a model's existing data
- Retroactively applying a [forward-only plan](#forward-only-plans) change to some historical data
- Fully refreshing a model

In Vulcan, reprocessing existing data is called a "restatement."

Restate one or more models' data with the `plan` command's `--restate-model` selector. The [selector](../../guides/model_selection.md) lets you specify which models to restate by name, wildcard, or tag (syntax [below](#restatement-examples)).

!!! warning "No changes allowed"

    Unlike regular plans, restatement plans ignore changes to local files. They can only restate the model versions already in the target environment.

    You cannot restate a new model - it must already be present in the target environment. If it's not, add it first by running `vulcan plan` without the `--restate-model` option.

Applying a restatement plan will trigger a cascading backfill for all selected models, as well as all models downstream from them. Models with restatement disabled will be skipped and not backfilled.

You can restate external models. An [external model](../../components/model/types/external_models.md) is metadata about an external table, so the model doesn't reprocess anything. Instead, it triggers a cascading backfill of all downstream models.

The plan's `--start` and `--end` date options determine which data intervals will be reprocessed. Some model kinds cannot be backfilled for limited date ranges, though - learn more [below](#model-kind-limitations).

!!! info "Just catching up"

    Restatement plans "catch models up" to the latest time interval already processed in the environment. They cannot process additional intervals because the required data has not yet been processed upstream.

    If you pass an `--end` date later than the environment's most recent time interval, Vulcan will just catch up to the environment and will ignore any additional intervals.

To prevent models from being restated, set the [disable_restatement](../../components/model/overview.md#disable_restatement) attribute to `true`.

<a name="restatement-examples"></a>
These examples demonstrate how to select which models to restate based on model names or model tags.

=== "Names Only"

    ```bash
    vulcan plan --restate-model "db.model_a" --restate-model "tag:expensive"
    ```

=== "Upstream"

    ```bash
    # All selected models (including upstream models) will also include their downstream models
    vulcan plan --restate-model "+db.model_a" --restate-model "+tag:expensive"
    ```

=== "Wildcards"

    ```bash
    vulcan plan --restate-model "db*" --restate-model "tag:exp*"
    ```

=== "Upstream + Wildcards"

    ```bash
    vulcan plan --restate-model "+db*" --restate-model "+tag:exp*"
    ```

=== "Specific Date Range"

    ```bash
    vulcan plan --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
    ```

### Restating production vs development

Restatement plans behave differently depending on whether you're targeting the `prod` environment or a [development environment](./environments.md#how-to-use-environments).

If you target a development environment by including an environment name like `dev`:

```bash
vulcan plan dev --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
```

the restatement plan will restate the requested intervals for the specified model in the `dev` environment. In other environments, the model will be unaffected.

However, if you target the `prod` environment by omitting an environment name:

```bash
vulcan plan --restate-model "db.model_a" --start "2024-01-01" --end "2024-01-10"
```

the restatement plan will restate the intervals in the `prod` table *and clear the model's time intervals from state in every other environment*.

The next time you do a run in `dev`, the intervals already reprocessed in `prod` are reprocessed in `dev` as well. This is to prevent old data from getting promoted to `prod` in the future.

This behavior also clears the affected intervals for downstream tables that only exist in development environments. Consider the following example:

 - Table `A` exists in `prod`
 - A virtual environment `dev` is created with new tables `B` and `C` downstream of `A`
    - the DAG in `prod` looks like `A`
    - the DAG in `dev` looks like `A <- B <- C`
 - A restatement plan is executed against table `A` in `prod`
 - Vulcan will clear the affected intervals for `B` and `C` in `dev` even though those tables do not exist in `prod`

!!! info "Bringing development environments up to date"

    A restatement plan against `prod` clears time intervals from state for models in development environments, but it does not trigger a run to reprocess those intervals.

    Execute `vulcan run <environment name>` to trigger reprocessing in the development environment.

    This is necessary because a `prod` restatement plan only does work in the `prod` environment for speed and efficiency.


# State

Source: https://tmdc-io.github.io/vulcan-book/references/state/

---

# State

Vulcan stores project information in a state database, usually separate from your main warehouse.

The state database contains:

- Information about every [model version](../../components/model/overview.md) in your project (query, loaded intervals, dependencies)
- A list of every [Virtual Data Environment](./environments.md) in the project
- Which model versions are [promoted](./plans.md#plan-application) into each [Virtual Data Environment](./environments.md)
- Information about any [auto restatements](../../components/model/overview.md#auto_restatement_cron) in your project
- Other metadata such as current Vulcan and SQLGlot versions

The state database lets Vulcan remember what it's done before, so it computes the minimum set of operations to apply changes instead of rebuilding everything each time. It also tracks which historical data has already been backfilled for [incremental models](../../components/model/model_kinds.md#incremental_by_time_range), so you don't need branching logic in model queries.

!!! info "State database performance"

    The state database workload is OLTP and requires transaction support.

    Use databases designed for OLTP workloads such as [PostgreSQL](./integrations/engines/postgres.md).

    Using your warehouse OLAP database for state works for proof-of-concept projects but isn't suitable for production. It leads to poor performance and consistency.

    For more information on engines suitable for the Vulcan state database, see the [configuration guide](../../configurations/overview.md#gateways).

## Exporting / Importing State

Vulcan exports the state database to a `.json` file. You can inspect the file with any text editor or tool. You can also transfer the file and import it into another Vulcan project.

### Exporting state

Vulcan can export the state database to a file like so:

```bash
$ vulcan state export -o state.json
Exporting state to 'state.json' from the following connection:

Gateway: dev
State Connection:
├── Type: postgres
├── Catalog: sushi_dev
└── Dialect: postgres

Continue? [y/n]: y

    Exporting versions ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3   • 0:00:00
   Exporting snapshots ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 17/17 • 0:00:00
Exporting environments ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1   • 0:00:00

State exported successfully to 'state.json'
```

This will produce a file `state.json` in the current directory containing the Vulcan state.

The state file is a simple `json` file that looks like:

```json
{
    /* State export metadata */
    "metadata": {
        "timestamp": "2025-03-16 23:09:00+00:00", /* UTC timestamp of when the file was produced */
        "file_version": 1, /* state export file format version */
        "importable": true /* whether or not this file can be imported with `vulcan state import` */
    },
    /* Library versions used to produce this state export file */
    "versions": {
        "schema_version": 76 /* vulcan state database schema version */,
        "sqlglot_version": "26.10.1" /* version of SQLGlot used to produce the state file */,
        "vulcan_version": "0.165.1" /* version of Vulcan used to produce the state file */,
    },
    /* array of objects containing every Snapshot (physical table) tracked by the Vulcan project */
    "snapshots": [
        { "name": "..." }
    ],
    /* object for every Virtual Data Environment in the project. key = environment name, value = environment details */
    "environments": {
        "prod": {
            /* information about the environment itself */
            "environment": {
                "..."
            },
            /* information about any before_all / after_all statements for this environment */
            "statements": [
                "..."
            ]
        }
    }
}
```

#### Specific environments

You can export a specific environment like so:

```sh
$ vulcan state export --environment my_dev -o my_dev_state.json
```

Every snapshot that is part of the environment is exported, not just differences from `prod`. This lets you import the environment elsewhere without assuming which snapshots already exist in state.

#### Local state

You can export local state like so:

```bash
$ vulcan state export --local -o local_state.json
```

This exports the state of the local context, including local changes that haven't been applied to any virtual data environments.

A local state export only has `snapshots` populated. `environments` is empty because virtual data environments exist only in the warehouse or remote state. The file is marked as **not importable**, so you can't use it with `vulcan state import`.

### Importing state

!!! warning "Back up your state database first!"

    Create an independent backup of your state database before importing state.

    Vulcan tries to wrap the state import in a transaction, but some database engines don't support transactions against DDL. An import error can leave the state database in an inconsistent state.

Vulcan can import a state file into the state database like so:

```bash
$ vulcan state import -i state.json --replace
Loading state from 'state.json' into the following connection:

Gateway: dev
State Connection:
├── Type: postgres
├── Catalog: sushi_dev
└── Dialect: postgres

[WARNING] This destructive operation will delete all existing state against the 'dev' gateway
and replace it with what\'s in the 'state.json' file.

Are you sure? [y/n]: y

State File Information:
├── Creation Timestamp: 2025-03-31 02:15:00+00:00
├── File Version: 1
├── Vulcan version: 0.170.1.dev0
├── Vulcan migration version: 76
└── SQLGlot version: 26.12.0

    Importing versions ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 3/3   • 0:00:00
   Importing snapshots ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 17/17 • 0:00:00
Importing environments ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.0% • 1/1   • 0:00:00

State imported successfully from 'state.json'
```

The state database structure must be present and up to date. Run `vulcan migrate` before `vulcan state import` if you get a version mismatch error.

To merge a partial state export (for example, a single environment), omit the `--replace` parameter:

```bash
$ vulcan state import -i state.json
...

[WARNING] This operation will merge the contents of the state file to the state located at the 'dev' gateway.
Matching snapshots or environments will be replaced.
Non-matching snapshots or environments will be ignored.

Are you sure? [y/n]: y

...
State imported successfully from 'state.json'
```


### Specific gateways

If your project has [multiple gateways](../../configurations/overview.md#gateways) with different state connections per gateway, target a specific gateway's state connection like this:

```bash
# state export
$ vulcan --gateway <gateway> state export -o state.json

# state import
$ vulcan --gateway <gateway> state import -i state.json
```

## Version Compatibility

When importing state, the state file must have been produced with the same major and minor Vulcan version you're using to import it.

If you attempt to import state with an incompatible version, you will get the following error:

```bash
$ vulcan state import -i state.json
...SNIP...

State import failed!
Error: Vulcan version mismatch. You are running '0.165.1' but the state file was created with '0.164.1'.
Please upgrade/downgrade your Vulcan version to match the state file before performing the import.
```

### Upgrading a state file

Upgrade a state file from an old Vulcan version to be compatible with a newer version:

1. Load it into a local database using the older Vulcan version
2. Install the newer Vulcan version
3. Run `vulcan migrate` to upgrade the state in the local database
4. Run `vulcan state export` to export it again. The new export is compatible with the newer Vulcan version.

Below is an example of how to upgrade a state file created with Vulcan `0.164.1` to be compatible with Vulcan `0.165.1`.

First, create and activate a virtual environment to isolate the Vulcan versions from your main environment:

```bash
$ python -m venv migration-env

$ . ./migration-env/bin/activate

(migration-env)$
```

Install the Vulcan version compatible with your state file. The correct version to use is printed in the error message, eg `the state file was created with '0.164.1'` means you need to install Vulcan `0.164.1`:

```bash
(migration-env)$ pip install "vulcan==0.164.1"
```

Add a gateway to your `config.yaml` like so:

```yaml
gateways:
  migration:
    connection:
      type: duckdb
      database: ./state-migration.duckdb
```

Define just enough config for Vulcan to use a local database for state export/import commands. Vulcan still needs to inherit `model_defaults` from your project to migrate state correctly, which is why we haven't used an isolated directory.

!!! warning

    From here on, specify `--gateway migration` to all Vulcan commands or you risk accidentally overwriting state on your main gateway.

You can now import your state export using the same version of Vulcan it was created with:

```bash
(migration-env)$ vulcan --gateway migration migrate

(migration-env)$ vulcan --gateway migration state import -i state.json
...
State imported successfully from 'state.json'
```

With the state imported, upgrade Vulcan and export the state from the new version. The new version was printed in the original error message, for example `You are running '0.165.1'`.

To upgrade Vulcan, simply install the new version:

```bash
(migration-env)$ pip install --upgrade "vulcan==0.165.1"
```

Migrate the state to the new version:

```bash
(migration-env)$ vulcan --gateway migration migrate
```

And finally, create a new state file which is now compatible with the new Vulcan version:

```bash
 (migration-env)$ vulcan --gateway migration state export -o state-migrated.json
```

The `state-migrated.json` file is now compatible with the newer version of Vulcan.
You can then transfer it to the place you originally needed it and import it in:

```bash
$ vulcan state import -i state-migrated.json
...
State imported successfully from 'state-migrated.json'
```
